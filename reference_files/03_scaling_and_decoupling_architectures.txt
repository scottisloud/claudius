Elastic Load Balancing
Reviewing AWS Global Infrastructure
All right, hello, and welcome to this course within the AWS Solutions Architect Associate learning path. This course is going to look at scaling and decoupling architectures using several different important services within the AWS cloud. The very first module, which we're about to begin right now, is about Elastic Load Balancing. Now before we jump into load balancing itself, we need to review some global infrastructure within AWS because it is fundamental knowledge that you must have going into the exam. At the highest level, AWS has what are called regions. Now I assume you understand what regions are, but again, I like to review these fundamentals before we dive deeper into some of these services. For those that need a refresher, a region is the, again, highest level of abstraction that we have within the cloud. Currently within AWS they have at least 31 active geographic regions, and they're working on even more as we speak. So by the time you actually see this lesson and view this clip, there might even be more out there at that time. Now, what we see here are current active regions and regions that were listed as coming soon. We also have what is known as a GovCloud Region. GovCloud Regions are far more strict in what they offer in terms of services and features, and they're used for very sensitive workloads for government agencies. An AWS region is going to provide full redundancy and connectivity to the AWS network and the internet. The regions consist of multiple availability zones, and typically there's going to be at least three AZs within a region. Each of these AZs is a fully‑isolated partition, consisting of different data centers, and they all have their own power, networking, internet connectivity, etc. Now, in addition to regions, we also have edge locations. Now, how this works is Amazon actually works with thousands of different telecom carriers around the world, so it makes it really well‑connected for major access networks, which helps optimize performance in terms of latency and bandwidth. Now, the edge locations are connected to regions through the network backbone within AWS, so these are fully‑redundant, multiple parallel fibers that essentially circle the world and link with tens of thousands of networks. These are used to deliver content to your end users with much lower latency. An example of this would be the Amazon CloudFront service, which leverages edge locations. Now don't worry right now, we're going to talk about CloudFront much more in depth later on, but understand that some services directly use these edge locations. Now, let's go ahead and zoom into one particular area of the world and look at the regions and talk about availability zones. So for this, we're going to look at the North American area, and we're going to look at some of their regions they have. Now remember, a region has multiple AZs, and AZs, or availability zones, are their own discrete geographically‑separated data centers. So in theory, there are four AZs right now in us‑west‑2, and each of these AZs is completely separate from one another. What that means is if there was some type of catastrophic event in us‑west‑2a, well then us‑west‑2b, c, and d would in theory be untouched and operate as usual. Now when you're in the console, I'm pretty sure you have seen this when you're working in the sandboxes, etc., but we have the active regions up here, up top, and then on the bottom we have regions that are otherwise disabled, and you can either activate them or you can leave them inactive, it's up to you and your security compliance needs. Now moving on to one thing that's extremely important for a solutions architect, availability zone IDs. Now I can tell you this probably won't come up on the exam, but I like to teach this because these are the most important aspect of multi‑account, multi‑AZ deployments. Availability zones that are listed in your AWS account are going to map differently to different AZ IDs depending on the account that they're in. So what we see on the left here when we're looking at availability zones in a subnet, these are abstracted naming conventions for essentially just ease of reference when we're deploying our resources. So this is what a majority of people refer to when they're talking about multi‑AZ. Did you deploy across 2a, 2b, and 2c? However, to the right of this, we have Availability Zone ID. These are the most critical thing you need to know as a solutions architect in AWS. These matter the most when you're planning multi‑account, multi‑AZ infrastructures. For instance, if you want to make sure you have compute instances that exist in two different accounts in the same availability zone in the same data centers, well, in that case, you wouldn't want to reference the AZ, you'd want to reference the AZ ID. These AZ IDs always map to the exact same data centers on the back end. That's why they're so important. Now, last thing to review here, highly‑available versus fault‑tolerant designs. These are very easily confusable, so we want to really make sure we understand this. When you're working within AWS, and of course other cloud providers, but we're specifically talking about AWS, you basically have two design approaches to mitigate the risk of your infrastructure having downtime and affecting your customers and your applications. These are known as highly‑available or high‑availability (HA) and fault‑tolerant or fault‑tolerance (FT). Now they're very easily confused, like we just mentioned. Highly‑available is when you have your infrastructure set up to have the ability to essentially self‑recover after a component fails. What this does is it allows you to stay up and operational most of the time, and typically you're going to see five nines of uptime, so 99.999% is the uptime goal. Now, fault‑tolerant, on the other hand, is where you have infrastructure that has the ability to easily and quickly switch services or applications from failed components to another healthy component without data loss and without service interruption. So it tolerates faults. Now, really do your best to understand these two and their differences, we're going to cover them a lot coming up within this module and really this entire course, but for now, let's go ahead and end here, and I will see you in the next clip.

Elastic Load Balancing (ELB) Introduction
All right, let's dive into Elastic Load Balancing. Elastic Load Balancing, or otherwise shortened to ELB, is a way to automatically distribute incoming application traffic across multiple targets on the back end, which include things like Amazon EC2 instances. Now typically when you use ELBs in AWS, you're going to spread your traffic across multiple targets across multiple availability zones for high‑availability. Now let's actually look at this a little bit more in depth here. What this is is a managed service. So, managed ELBs are there to let you handle load and distribute that load across your targets automatically. So you're offloading that burden to the AWS service. One of the benefits of using an ELB in AWS is you can set up your own custom DNS to point to that for your own application hosting. So if you have a web app, you can point your DNS to point to your load balancer, which sends traffic to your compute on the back end. Now, in Route 53, if you're using that for DNS management, you're going to use an alias record to reference that value target. Now, we'll look at that later on when we set some demos up, but do remember you use an alias record in Route 53. Now, third thing here, ELBs offer health checks, so this is one of the biggest aspects besides the load balancing aspect. They monitor the health of your back‑end targets, and what's cool is they will only route to healthy ones. So if you have an EC2 instance that's unhealthy, it's going to automatically stop sending traffic to that until it comes back online. Fourthly, you can actually implement SSL or TLS termination (for example, HTTPS) for your websites and your applications. So you can have it handle the cryptographic operations for encryption and decryption at the load balancer level. Now, a pro tip here, when you're using ELBs in AWS, you should be creating different tiers to separate public and private network traffic. So this is a best practice. What this means is you would have the ELB more than likely in a public subnet and then your application compute in the back end within a private subnet so there's no direct access. In AWS there are four types of Elastic Load Balancers. There's an Application Load Balancer, or an ALB. There's a Network Load Balancer, or an NLB. We have a Gateway Load Balancer, which we're going to shorten to GWLB. And you have a Classic Load Balancer, or CLB. Now, I'm going to tell you right now, we're only going to focus on these first three. This last one here, the Classic Load Balancer, is version 1 of ELB. What that means is you're going to likely want to avoid using them. Now, they might come up on the exam; however, I'll go ahead and let you know it's going to be very unlikely that you'll ever use these, they're a lot more complex, and they offer far less features compared to the other ELBs. Moving on, let's talk about the schemes. So what do we mean by a scheme? Well there are two schemes for your ELBs, there's internet‑facing, which is public, and this is where you deploy them to a public subnet, and then they can route requests over the Internet using public IP addresses. So these will typically be used for public‑facing or customer‑facing applications that you're hosting on the web. We then have internal, or in other words, private. This is where you deploy into private subnets, and the requests are only routed using private IP space. So this might be good if you have a private internal application that you want to load balance for an organization. For instance, maybe you have a security tool, etc., that you want to send traffic to, but you don't want it to be publicly available on the internet, this is a perfect use case. Moving on, you also need to know routing algorithms. This is important when you're designing your architecture, and it is possible they come up on the exam. First up, we have round robin. So this is going to be the default behavior for your ELBs. This is where your requests that come in are going to be routed evenly across the healthy targets on the back end in a sequential order. So essentially it's going to cycle through each healthy target over and over again whenever it can. Now we see the word target group here, and we're going to discuss target groups much more in depth when we start looking at each load balancer, but understand for now that a target group hosts your back‑end instances and your back‑end targets that are actually performing or hosting your application. The second one is a least‑outstanding requests algorithm. This is where your requests are going to be routed to the different targets on the back end that currently have the lowest number of in‑progress requests. For instance, maybe you have two back‑end instances for your application. The first instance might be handling a majority of the requests, so this is going to look at the other instance and say, hey, you don't have a lot compared to instance one, I'm going to send my request to you so we can balance this appropriately. And then the third important one, weighted random. This is where your requests that are coming in get routed evenly across healthy targets in a random order, so it's not sequential. That's the big difference between this and round robin. Now you can use something called automatic target weights, which are a little out of scope, but essentially what that does is it allows you to manipulate the load‑balancing algorithm just a little bit. Moving on, we also need to talk about health checks. These are extremely important as well. You can actually set up your ELBs to periodically send requests to your back‑end registered targets and test their current status. So for instance, maybe you want to make sure that your application or your web page is up and running and available, well you can health check that. What happens is the back‑end resources that are healthy or up and they pass your health check, at the time of the health check are considered in service, so they will receive requests. Now with ELB, when you're leveraging their health checks, traffic is only ever going to be sent to healthy instances that are classified as InService. When you're setting them up, you specify a port and a path, so a port number and some type of path to check on your target. Now this could be your default page on a web page or maybe you have an actual health check endpoint set up on your application that you want to check for. And lastly, status codes. When a healthy instance replies that it is healthy, it will give a 200 status code by default. If anything else is received, it is marked as unhealthy and no longer in service. Moving on, let's look at the architecture for an Elastic Load Balancer. Remember, multi‑tiered VPC architecture designs are a best practice, and you should always be using them. You've probably seen this if you've taken some other courses in this path, but let's review it. In this architecture, we have three tiers. We have the web tier, we have the app tier, which is a private tier, and Tier 3, which is the database tier, which is another private tier. So Tier 1 is public, Tier 2 and 3 are both private, and these are all separate. Now, if we simplify this just a little bit, and we go to a two‑tier design with an internet‑facing load balancer, what we've done here is we've deployed our ELBv2 into a public subnet, and this can make it internet‑facing, so now we can reach it from the internet. Now from a high‑availability standpoint, you want to deploy your nodes for your ELBs into multiple‑availability zones in your region. So in this case, we deploy to 1a and 1c. Now, once you have your infrastructure in place, like your load balancers and your infrastructure in the back end, like EC2 instances, you can leverage security groups on those resources to have and implement network access controls. So you can restrict who can talk to what using your security groups, which is, again, a best practice. Now when you configure health checks, they can be configured to check for status, for instance, like the following. In this, we have a protocol of HTTP, it's checking on port 8080 of our back‑end instance, and it's checking the path of healthz. So this is a typical common application path for health checks. Now, moving on, let's look at an example security group configuration. We talked about restricting network access using them, so let's see how they play together. So, on the top here we have our ELB Security Group, and we've set it up to be public‑facing or internet‑facing. We've allowed port 443 and port 80 to be reached from anywhere based on the IP CIDR that we've implemented. On the bottom we have our EC2 instance security groups that are leveraging the ELB to front their application. In this, what we've done is we've allowed only port 80 from the source security group, which is the security group for our public load balancer at the top. So this is the best practice way that you can reference changing IPs and changing network resources within a different tier for a web application. So we've locked this security group down to only allow port 80 from our ELB. Now before we move on, let's have a quick exam scenario. Let's say you have an application running on EC2 instances spread across many different availability zones. You've deployed an internet‑facing ELB for customers to use, but you only want to allow access to the instances from the ALB itself, nothing else. Well, the best way to do this is to deploy your instances in a separate private subnet, a different tier, and reference the security group for the ELB within the application's Security Group inbound rules, like we just looked at. Please remember that for the exam. With that being said, let's wrap up here. That was a lot of information. Coming up next, we're going to start diving into the different load balancers.

Application Load Balancers (ALB)
Okay, let's start talking about Application Load Balancers, otherwise known as an ALB. Application Load Balancers work at Layer 7 of the OSI model. Because of this and where they operate, it supports many different aspects of application routing, so content types, you can look for cookies, you can have custom headers, and you can even use other Layer 7 attributes, which otherwise classify this as a smart load balancer. Now when you're listening and using an Application Load Balancer over the internet or even internally, it can only listen for HTTP or HTTPS traffic. So that is one of the restrictions for an ALB. If you have a web application that needs to leverage one of these protocols, this is probably going to be the best bet. One important aspect of an ALB, it does not support what is known as unbroken TLS or SSL. What we mean by that is that when you leverage TLS certs with a load balancer, the encryption is terminated at the load balancer itself and stuff being sent to the back end or your connection being sent to the back end is now unencrypted. Even though it is within the AWS network and secure, it is not encrypted. That is so important to know. Now, in addition to HTTP and HTTPS, it does support HTTP/2, or HTTP2, and even WebSockets. So if you have a requirement for WebSockets in your exam scenario, this is a great choice to implement load balancing. And then the last thing here, these are perfect for containerized applications. So if you're running Docker containers on one of the hosted services or on your own compute, well, ALBs are perfect for that, they integrate very well with placing containers, and we'll look at some of those examples later on within this course when we talk about container orchestration. Let's talk about some important components of an ALB. First up, we have a listener. So you create a listener and attach it to your ALB. This is going to check for connection requests from your clients on the internet or within your private network, and it's going to be listening on the port that you specify for the protocol that you specify. So, for instance, port 80 for HTTP protocols or port 443 for HTTPS protocols. In addition to the listener, you also define a rule, which is priority ordered, to determine how the requests get routed to your back‑end targets. Now speaking of rules, these are what you attach to each listener, like we just talked about. And when they're attached and conditions are met for the rule that you specify, they perform an action. Now, each listener has a default rule, and it is required. However, you can optionally define additional rules to set up complex routing rules for an ALB. And, we're going to look at some examples of these coming up very shortly in an architecture overview. The last major component here is a target group. A target group is the very back‑end portion of your architecture. This is where all of your requests are actually routed, and they're routed to what are called targets. So a target, which can be things like EC2 instances and Lambda functions, belong to a target group, which is what receives your traffic based on your listeners and your rules. You configure the target groups using a protocol and port number that you get to specify, so it doesn't have to match exactly the same as the listener on the ALB, it could be different. The big thing to remember here is that the listener listens for port and protocol, it uses rules to match traffic, and rules send traffic via an action to a target group hosting your back‑end resources. Now let's look at some rule conditions and some rule action examples. You need to be familiar with these for the exam, and honestly, if you're going to use them in any architecture in the cloud. First up, rule conditions. So these are all supported conditions for your listener rules. You can look for host headers, HTTP headers, you can match path patterns or query strings, you can even look for specific HTTP methods, and source IPs. Now, once one of your conditions are met, you can go ahead and use an action to determine what you want to do with that traffic. So you can forward traffic to a target group. You can redirect traffic to a different port, which is perfect for HTTP redirecting to HTTPS. You can send back a fixed response, so maybe you just want generic text sent back saying, hey, this is a work in progress, it's not available yet. You can authenticate via an OpenID external identity provider. Or you can authenticate via something called Cognito, which we're going to cover later on within this course as well. Big thing to remember is you can use them to authenticate. Now let's look at rule examples for an ALB. In this instance, we're going to deploy a listener on port 443 listening for HTTPS traffic. After we define our listener, we can create separate listener rules for that one listener. Remember, there's a default, but you can define additional ones as needed. So here, we're going to define three different rules: a query string rule, a host‑header rule, and a path‑based rule. Now with these rules we can have different actions, remember. We can forward traffic directly to a target group, we can redirect to different ports, or we can return a fixed response. Now, in our private subnet here, we're going to go ahead and create three separate target groups for us to use. Now we're going to use these depending on the rule conditions. So we can send traffic to different target groups using the same load balancer based on different listener rules and different target group configurations. So these target groups are what actually host our application and our business processing logic. Now it's important to know, for each target group within your architecture here, they have their very own health check that you configure on the load balancer for it to use. So you configure your health check, you say, hey, I want to look on port 80 for our /healthpath, please check this ELB and make sure our instances are in service and healthy; otherwise, don't send traffic to it. So now with this in place, let's demonstrate what this would look like. For the first one, let's assume we're going to go query string. So our user looks at pluralsight.com/csaa, and then they put in a query string where they're looking for lesson=001. Well, this matches our listener. It says, okay, well, this is a query string rule, so this matches query strings, let's go ahead and send it to Target Group 1 only. Next up, let's look at a host header rule. So Pluralsight again, but this time it's www.pluralsight.com. Well, our listener realizes, okay, this request matches our host header rule, I want to send it to this target group, which is Target Group 2. And then thirdly, path‑based. So again, pluralsight.com, but this time we're doing /csaa only, and it's going to say, okay, this matches path‑based, let's go ahead and look for our path‑based rule set, which action is set up to forward to Target Group 3. So these are all real‑world use cases for different rules and actions with different back‑end target groups. So please, do me a favor, take some time to review this, this is very important for you to know. Now, an exam scenario, let's say you have an Application Load Balancer and it's configured to listen for both HTTP and HTTPS via different listeners, but you want to make sure that requests only get forwarded via HTTPS, so you don't want them to actually connect via HTTP, you want to redirect it. Well, remember, you can create a listener rule on that HTTP listener on the ALB to redirect that traffic to the HTTPS listener itself. So it would connect, say, hey, this is over unencrypted traffic, I'm going to redirect you to port 443 instead, and let's go ahead and encrypt this connection. Moving on, let's look at some concepts that are very important regarding ALBs that you must know for the exam. First up, target types. So these are the currently supported target types on the back end for an ALB within a target group. You can look at EC2 instances, which means you're actually identifying and referencing by the instance ID. You can specify ECS tasks, which are containers running on Amazon ECS, and we'll look at those later on within this course. It supports Lambda functions, so you can directly invoke Lambda functions. And you can even reference private IPs. So if you want to reference the private IP space instead of the instance ID, for example, you can do that. In addition to that, private IPs are good for if you have some type of VPN connection or Direct Connect connection in place and you want to balance or send traffic to a private IP in that external network. Now to use an HTTPS listener, so a secured listener, you have to deploy at least one TLS cert. Now we'll look at this later on as well, but typically you're going to do this via a service called AWS Certificate Manager, or ACM. Thirdly here, to use custom DNS host names in Route 53 specifically, remember, you can use an alias record. So you create your value, then you create an alias record for it, and you specify the target as the load balancer resource. Next up, you need to know these headers. These are very important. There are three big ones that could come up. X‑Forwarded‑For, which is going to be the header that identifies the original IP address of the client. So what this means is that if your client or customer has an IP address of 1234 and it connects to your load balancer, typical behavior is that you're going to see the load balancer IP on the back end. So you need to reference this header if you want to see the actual originating IP address from the client itself. X‑Forwarded‑Port is to forward the destination port or identify the destination port that the client was using to connect to your load balancer, so 443 or port 80. And then lastly, X‑Forwarded‑Proto. This is going to forward or identify the protocol that your client used, so HTTP or HTTPS. Last thing here, port mapping. This is a very important feature in a service called Amazon ECS, and it allows you to dynamically assign target container ports on your host server like an EC2 instance, and then connect that to the ELB. Now we're going to dive into this also much more in depth when we talk about Amazon ECS in its own module later on. However, please understand port mapping at a high level for now. Now, last thing here, exam pro tip before we wrap things up, do not ever attempt to reference public IPs for your Application Load Balancer. The reason is, these are always revolving, so they're not really going to be a great source of truth for allowlist or a whitelist on a firewall, for example. Instead, there are other methods to do this, and we're going to explore those when we talk about a Network Load Balancer later on. But, for now, let's go ahead and wrap up. Again, that was a lot of information. We'll end here, and I will see you in an upcoming clip.

Demo: Setting up an Application Load Balancer (ALB)
All right, let's get started with our demonstration clip here where we're going to work on setting up an Application Load Balancer. Before we jump into the sandbox environment, let's have a quick high‑level overview of the architecture we're going to put into place. What we're going to do in this demo is we're going to create several resources. We're going to create several EC2 instances, we're going to create some different target groups hosting those different instances, and we're going to create a load balancer for us to use to set up a path‑based rule to test sending to different back‑end resources. Now on this ALB, we're going to have a listener on port 80 for HTTP protocols, and we're going to test two different paths, an A path and a B path, and we're going to test that our rules actually work as expected. So without further ado, let's jump over into the console now. All right, welcome to our sandbox, I'm logged in as cloud_user, let's get started. First thing I want to do here is I'm going to create security groups for our back‑end resources and our load balancer. So under Security groups, I'm going to select this and I'm going to create new security groups. So this first one we're going to create is for the load balancer, so let me go ahead and name it appropriately. I will give it a description, and then, of course, I select my custom VPC, which has two tiers, a public and a private tier. So we have the details filled in, I need to create an inbound rule. Now, principle of least privilege is we're only going to allow exactly what we need. So I'm going to look for HTTP, I'm going to select it, and I'm going to say anywhere from IPv4 address space. So we're allowing HTTP on port 80 from anywhere on the internet. I'll scroll down here, we'll leave everything else the same, and I'll create. Perfect. So this is created, now I need to create my target group or my target security groups. So under Security Groups I'm going to create two more. I'm going to call this A, I'll give it a description, I'll select the same, a VPC, and for inbound rules, I'm going to follow the same principle here, I'm only going to allow HTTP on port 80, but this time, I'm going to look for my security group that I just created, PublicLoadBalancer. This is the best way you can secure your network resources. We're only allowing network interfaces that belong to this security group to hit this one that we're making now via this protocol and port. Another benefit is that you don't have to track the IP space and add it and remove it from the inbound rules either. So, following principle of least privilege, we allow just the ELB security group. I'm going to go down and create. It's created, and now I'm going to do the same thing for our B security group. So I'm going to go ahead and fly through this really quickly. Perfect. So now we've created our B Security group, and it's got the same rule in place where it only allows our public load balancer. Awesome. Next up, let's go ahead and create our resources. So the first thing I need to do is I'm going to go to my dashboard here, I'm going to launch a new instance. We'll go ahead and call this APP A, I'm going to create two of them, we'll leave most of this stuff the default here. I'm going to go down and change the instance type. I'll say no key pair because I don't need access to this host, and I need to change my network settings. So I'm going to select my custom VPC, I'm going to spin these up in Private Subnet A, so let me go ahead and select private A. No public IP, and I'm going to select my existing security group. So I'm going to find my A security group, select it, and then under Advanced details I'm going to do one thing here, I'm going to execute some user data. So I'll paste this in, and I'll kind of go over this really quickly. Now, this will be available for you if you want to go ahead and follow along in this demo later on, but at a very high level, we're running an HTTPD server, and I'm creating some basic web pages, so I'm creating a default one for a health check later on. And then I'm making a directory specific to our path. Now this is very important to call out. For path‑based routing, especially with an Apache Web Server like we're spinning up, the path has to exist on the actual underlying host. So I'm making that path, and then I'm creating a custom index file for that path alone. Perfect. So this is in place, I'll launch my instances. We see them here. I'm going to go back to my dashboard and create one more for our B application. We're going to follow pretty much all of the same type of configuration settings, but for network here, I'm going to select my custom, we'll select Private Subnet C this time instead. I'll leave everything else the same here for IP. I'll select my B security group, and then under User data, I have custom user data for that as well. So let me go down here, I'll paste this in, and the only difference here is essentially the path. So I have a /b path for my web server. I'm going to click on Launch Instance, we're only launching one of these, there we go. So now I'll go to EC2, look at my running instances here, and eventually we will see all three, which is perfect. Okay, next up, we're going to work from the bottom up, so we've created our security groups, our instances, I want to create our target groups now. So I'm going to go down under Load Balancing, select Target Groups, and let's create our two different target groups for our two different applications. First up is our target type. Now remember, the target type is dependent on the load balancer that you're actually attaching these to. So for ALBs, instances, IPs, and Lambdas are supported, but ALBs are a target type only for an NLB, which makes sense. So for this we're going to choose instances. However, for the exam it's important you know Lambda functions are supported and private IPs. Private IPs are perfect for, just like you see here, if you have some type of networking connection like a Direct Connect or a VPN and you have a connection to your on‑prem resources and you want to load balance those. That is a possibility using this configuration. For this, though, I'm going to choose Instances, I'm going to give my target group a name, and I'll just call it A, and then you specify the protocol and the port for your target group. So, we're using HTTP, but notice all of the configurations and the respective load balancers. So I'll choose HTTP on the default port, we'll leave it at IPv4. I'll select my custom VPC. We'll leave protocol version default as well, but it does support HTTP2 and gRPC. And then we get down to health check settings. So we're going to use the same protocol, HTTP, and remember, I created a default index file here to use the root path. So if you did want to, you could make this a /health, a healthz, etc. For this, I'm going to leave it as default because this will come back. Now, next up, advanced health check settings. So here is a very important thing to know. You can use the traffic port settings, or you can override them and send your traffic to a custom port for health checks only. Now, I'm going to use the traffic port because it's all on port 80, and let me set some of these numbers a little bit lower. First up, healthy threshold, so the number of consecutive health checks that have to pass before it's considered healthy. I'm going to set this to 2 to speed it up, and then we have unhealthy. So this is the opposite. How many failures in a row are required until it's unhealthy? I'll leave this at 2. We get down to timeout. Timeout is going to be how long can a response wait before it gets nothing back and it's considered a failed health check? So I'm going to set this down to 2, and then for interval, which is the amount of time between health checks, I'm going to set this to 5. So all I've done is set these as low as possible to speed up the successful codes. And speaking of successful codes, you can set your custom success code if you want, or like we covered in a previous clip, the default is 200. So I'll leave this at default, I'm going to go down, I click on Next, and now we can register our instances. So this is the view we get because we selected instances as the target type. So I'm going to select both of my A instances, I'm going to include them as pending here. I can scroll down, and let's create our target group. Awesome. Now, one thing to notice here, it's unused, they're not unhealthy, they're not healthy. Well, that's because the target group is not set up for traffic yet, so these are just listed as unused. So while this is in here, what I'm going to do now is I'm going to fast‑forward and create our target group B with the same settings, except for it's going to be for our B instance. So let me go ahead and fast‑forward now. Perfect. So now I've created both of my target groups; A has our two A instances, B has our solo B instance. All right, we're almost there. Now I want to create my load balancer itself. So under Load Balancers, Create load balancer, I'm going to find and select Application Load Balancer, and I have to give my load balancer a name. So let's just call this Pluralsight. It's going to be internet‑facing because we want it to be public, and we want it to use IPv4 addresses. Now you can change it to internal for private, and you can also select Dualstack or Dualstack without IPv4 if you want to, but this only works for public‑facing load balancers. Now we'll just use the default here. I'm going to go down to Network mapping, select my custom VPC, and I've only deployed to A and C, so I'm just going to select A and C for my availability zones, but understand you want to do at least two, but ideally three AZs for a load balancer. What this does is deploy a node in these specific subnets to balance traffic. So what we're going to do is I'm going to select Public A and Public C because we want it to be publicly accessible, and then I go down to Security groups. So under here I'm going to select my PublicLoadBalancer security group that allows HTTP from anywhere. In addition to that, this is trusted by our internal security groups for our applications. So I select this, and I go down to listeners. So remember, on an ALB or any ELB at all, you set up a protocol and a port on your listener. So since this is an ALB application layer, we have HTTP or HTTPS. Now I'm going to select HTTP, we'll do HTTPS later on in this course, and I'll leave the default at 80. Now when you're initially creating your load balancer, you have to set a default action and you have to select a target group. So for this I'm selecting A, but we're going to change this later on. But for now, I'll scroll down, we'll skip through some of these complex settings, this is just for security, we'll cover WAF in a different course, Global Accelerator will be covered later on, and CloudFront will be covered later on, but these are just things you can add on and integrate with your AWS ELB. So I'll scroll down here, we see the review, I click on Create, and there we go, it's now provisioning. So while this is provisioning, we can see here on the bottom our listener. So right now, it's default's set to this, I'm going to select my listener, and let's go ahead and change this default rule. So I'm going to select this, Actions, Edit rule, and now we can change this. So, what I want to do for the default is I actually want to return a fixed response. Remember, this is one of the options that you have for your actions. So what I'm going to do is I'm going to paste in some plain text here. Whoops, looks like you hit a dead end, this is not a valid page, and we will return a 503 error. I'll save changes, and now our default will be a fixed response, which is perfect. Next up, let's add our path‑based rules. So I'm going to add a new rule, I'll call this A. We'll go Next, we add a condition. So remember, for ALBs, we have all of these valid conditions that you can look for because it's application aware, it balances at Layer 7. So for this, we're going to choose Path. I'm going to give my path a, and Confirm. So anytime our DNS comes in with a path of /a, it's going to match this rule. I'll click on Next, and we can choose our actions. So do you want to redirect, return a fixed response, or forward to a target group? We want to forward to a target group, so I'm going to select A, and then I'm going to go ahead and click on Next. Now, remember, you have to get a priority number for your rules, this is just similar to an ACL where the lowest number takes the highest priority. So I'm going to set this to 1, click on Next, click on Create, and there we go, we see our new rule here, Path Pattern rule conditions, priority 1, name tag, so this is perfect. Now let's create it for B. So I'm going to do the same thing. Add rule, call it B, go to Next. For condition, I'm going to choose path‑based. We'll say /b. I'll click on Confirm, go to Next, select our B target group, and then click on Next, give it a priority of 2, which is really irrelevant for this because we're specifically looking for a specific path, but again, you have to do that, so I'll set it to 2, Next, and then Create. Perfect. So now we have our two different path‑based rules here and our default. So what I'll do here is go back to my load balancer, and what I'm going to do is make sure this is active, which it is, and let's test this out. I'm going to go ahead and navigate to our ELB DNS that's provided to us by AWS. It says it's not secure, that's fine. Perfect, look at that. So, we now have our default fixed response that we put in. So in theory, this should work for anything. So if I just do a /abc, we get the same thing, that's because this is the default rule, nothing matched. Now, how about I do a /a instead? Perfect. So it matched our path‑based rule. Hello Gurus! I am hosting Application A! And now if I refresh, we get different instances because it's round robin by default. But let me go ahead and try B. So I'm going to go ahead and go to /b instead. Perfect. And now if I refresh, remember we only have one instance, so I don't get any new instances here. Awesome, so it worked. We've now created two path‑based rules for us to work on with this load balancer. We added them to our listener, which was listening for a protocol HTTP on port 80, and we tested it all out. So let's go ahead, we're going to end this demonstration here. Please feel free to play around, do your own thing. I will include these files that I used in this on the module assets, so feel free to use those as well. But for now, let's go ahead and wrap this up, and I'll see you in the next clip.

Network Load Balancers (NLB)
All right, up next, Network Load Balancers, or NLBs. NLBs operate at Layer 4 of the OSI model, so this is the transport layer. What this means is that there's no smart routing like you would have with an Application Load Balancer. So you can't look for headers, you can't look for cookies, sessions, etc. These load balancers are strictly meant to be high‑performant. Now with them, the supported protocols change compared to an ALB as well. So they only support TCP, TLS, UDP, and TCP_UDP. So these are drastically different compared to an Application Load Balancer, and that's because these operate at a different OSI layer. Now the beauty of a Network Load Balancer is this is the highest‑performing version 2 load balancer that they offer. It allows for millions of requests per second and ultra low latency. So if there's ever a scenario where you need something like that, a Network Load Balancer should immediately pop into your head. Now, one of the nice things about Network Load Balancers is the ability to have static IPs. You can have a static IP per AZ that you deploy one of the nodes in. So this allows for easy referencing. In addition to that, it allows for you to use public static Elastic IP addresses to assign to your load balancer's AZs so that you can easily whitelist or allowlist a Network Load Balancer for a client, a firewall, etc. So if there's ever a scenario where you need static IPs for a load balancer, immediately think NLB. Now, I do like to call this out, please don't confuse Layer 4 (Transport) and Layer 3 (Network). It is unfortunate how they name these load balancers, but they are completely different layers in how they function. Network Load Balancers are Layer 4 load balancers. Please remember that. Moving on, let's review components. These are going to look extremely similar to ALB because they're pretty much the same, minus a few configuration options. Again, with an NLB, you also specify a listener, so it's doing the same thing as an ALB, but the protocols and the ports are different. So remember, you can only listen for TCP, UDP, TCP_UDP, and TLS traffic. Now, the port numbers can be whatever you want, but the protocols have to be one of those four. And, again, there are rules for your listeners. So these operate the exact same way, except you have far less options for your rules and your actions. Since this is not a smart or application‑aware load balancer, you really can only forward traffic to a particular target group. So that's very important to remember. You don't have the same amount of options that you would with an ALB. And then lastly, again, you have target groups. Target groups receive your requests and your traffic to their registered targets, and you can actually use different health checks here. So while the listener only supports those four protocols, health checks, depending on the back‑end resource, can support TCP, HTTP, and HTTPS health checks. The asterisks next to the last two there are because you can specify an Application Load Balancer as a target, which can support that health check. We'll look at that here coming up shortly. But for now, exam pro tip number one: remember that PrivateLink, which is covered in a completely different course, allows for secure access to services within AWS over a highly‑available, highly‑performant network connection. Well, PrivateLink leverages Network Load Balancers in order to operate and offer this feature. This is the back‑end portion of the networking for PrivateLink. Now, let's discuss some additional important concepts here. Target types. So, these are the supported target types for your target groups, for your rules and your actions. You can forward traffic to EC2 instances and reference by EC2 ID. You can forward to Application Load Balancers, which is a very important concept to understand. And, you can actually send to private IPs just like an ALB. So if you have some type of merged network, peered network, or a Direct Connect and VPN connection, you can use private IPs to set that traffic up. Now, to leverage TLS on your listener, again, just like ALBs, you have to deploy at least one certificate, and typically this is going to be within AWS Certificate Manager. And to leverage custom DNS names, again, just like an ALB, you use an alias record. Now, I'm sure you're starting to see a lot of similarities, and that's what makes it slightly difficult on the exam. You need to know some of the smaller nuances and important details like target types and supported traffic protocols, etc., to really determine what load balancer you would go with. So really make sure you take the time to understand the differences between all of these ELBs. And then lastly here, an exam pro tip: PrivateLink, static EIP requirements, or ultra high‑performance and ultra low‑latency are all indicators of a Network Load Balancer within an exam scenario. If you see those, I would immediately at least consider a Network Load Balancer. Exam pro tip two: Remember, you can place an NLB with an EIP assigned to it, and then use an ALB target to have static IP addressing referencing, if needed. So maybe you have a customer or a third party that needs to whitelist an IP address to interact with your load balancers, well you can force an NLB in front of an ALB and set up multi‑tiered architectures that way. Now, speaking of architectures, let's look at a couple before we wrap things up. First here, we have a TLS listener. So, a similar design to what we saw with the ALB, but this time you'll notice we have an NLB in place with a different listener. Because the NLB operates at Layer 4, you do not have the same smart routing features like you do with an ALB. I know I keep repeating this, but you need to remember this. So an example is there's no path‑based routing rules, it's just straight up. What we would do is we deploy a listener, listening on port 443 for TLS traffic. So on this internet‑facing NLB, we've set up this listener. So now, when our traffic comes in over https://pluralsight.com, well, that matches TLS, so it's going to match this listener. Now, for TLS, remember, you can assign an ACM TLS certificate for TLS offloading at the ELB level. What this does is it allows you to offload the computational burden of performing cryptographic operations like encryption and decryption at the ELB level, so that frees up your back‑end compute. Now, once our request comes in, it matches our rule, it can forward to our target group hosting something like an EC2 instance. So remember, target groups are limited to TCP, UDP, TCP_UDP, or TLS protocols. They're drastically different than an ALB. Now, let's shift gears a little bit and look at something known as unbroken, SSL or otherwise called end‑to‑end encryption with this flow. So this time, same architecture for the most part, but now what we've done is deployed a listener on our internet‑facing NLB on port 443, however, it's not TLS, it's now TCP traffic instead. So what this does is it allows us to go ahead and pass through this encrypted connection to our back‑end instances. So this implements end‑to‑end encryption. Now, one of the important things to know here is that this approach puts a lot more computational burden for the cryptographic operations on your compute. So you have to take that into account when you're designing this. You're going to need more CPU power to perform those operations. To get it to work, what you would do is you would install your public TLS certs on your back‑end compute themselves. So, maybe like a Let's Encrypt cert, a Comodo cert, etc., well, you install that on your back‑end instances and perform encryption and decryption there instead of the Network Load Balancer. So this is a valid architecture to keep aware of for the exam. Now, before we wrap things up, let's look at an exam scenario. Let's say you're hosting an application behind an NLB and you want to pass encrypted traffic all the way through to the back end, so you don't want to offload TLS. Well, perfectly just like our example, you create a TCP listener listening on port 443, and you pass that traffic through without decrypting it there, and you handle it at the back‑end resource level. Now, with that in mind, let's go ahead and we're going to end this clip here. I'll see you in an upcoming one where we're going to start looking at some demos.

Demo: Setting up a Network Load Balancer (NLB)
Okay, let's get started with this demonstration clip. In this demo, very simple, we're going to set up a brand new Network Load Balancer, and then just simply send some traffic to a back‑end instance in one of our target groups. Before we get started, let's look at the architecture overview. What we're going to do is deploy an NLB in a public subnet in a two‑tiered VPC, and we're going to set up an internet‑facing listener on port 80 for TCP traffic. So that's a key difference. It will forward to our application web server, but it's listening to a different protocol, TCP. Now what we're going to do is simply leverage a default rule for a single target group on the back end, and we'll test the load balancing out. So without further ado, let's go ahead and jump in the console now. All righty, I'm in my sandbox environment logged in as cloud_user, let's go ahead and get started. The first thing I want to do is I'm going to do the same thing like I did in the ALB demonstration, where we're going to create the back‑end resources first, and then the load balancer. So what I'm going to do is I'm going to go to Security Groups, and once this loads, I'm going to create two different security groups, one for our back‑end instance, and then one for our load balancer. So first things first, let's make the load balancer. I'm going to call it PublicNLB, I'll give it a description. I'm going to select my custom VPC, which is a two‑tiered VPC, one public, one private subnet, and let's click on Inbound rules. Now remember, this is a Network Load Balancer, and it operates at the Transport layer, so it doesn't look for application protocols like HTTP or HTTPS, it's looking for those transport protocols like TCP, UDP, etc. So I'll select Custom TCP, I'm going to say port 80, and the source will be anywhere on the internet because this is public‑facing, so we'll just open it up. I'm going to scroll down here, click on Create security group. All right, so, now that this is created, let's create our application security group. So I'm going to go back, create a new one, we'll just call this Application. We'll give it a description, we put it in the same VPC, of course, and then for inbound rules, Custom TCP, we're going to say port 80, and for its source we're going to say Custom. What I'm going to do here, similar to the ALB demonstration, is follow the principle of least privilege and only allow our public NLB security group to actually hit these resources. So I select it, I go down and click on Create. Perfect. So now we have our two security groups that we're going to use, Public, NLB, and Application. Let's go here and let's create our instance now. So I'm going to go to my dashboard, launch an instance, we'll call this Application. We'll leave most of the defaults. I'm going to change the instance type here. Go down to Key pair, I'm going to say none because I don't need it. And under Network settings, select my custom VPC. I'm going to put this into a private subnet, so I'm going to go ahead and say Private C. We have no public IP, I'm going to select that security group I created called Application here. And then under Advanced details, the only other thing I want to do is go to the bottom and paste in some user data. So I'm going to paste in my default user data here, which just launches a very simplified index HTML file, I'm going to click on Launch instance, and there we go. So now we have our instance running, I can open it up in a new tab. Perfect. So the next thing we need to do is I need to now create our target group. We have our security groups in place, we have the target up, but now I need a target group and a load balancer. So I'm going to go here to my first tab, I'm going to find Target Groups under Load Balancing and create that. So Create target group, I'm going to choose Instances on this, but this does support IPs and application load balancers. Lambda functions are not supported, so again, please make sure you understand the target types that are supported for each ELB. For simplicity, I'm choosing Instances. I'm going to give my target group a name. We see the protocol and port. So, TCP, which is perfect, Network Load Balancer supported, and port 80. We'll leave IP address the same. I'm going to choose our custom VPC. And, under health checks here, we can now do three different health checks compared to our ALB demo. So this is actually really neat. You could just go ahead and look for TCP health checks, but we actually want to check that our web page is up. So I'm going to use HTTP as the health check on our default root path. So this should come back just fine. Under Advanced here, I'm going to leave the traffic port and I'm going to set these to as low as possible to speed things up. So let me go ahead and run through that now. We'll leave success codes defined, and notice, the big difference here as well, there's a range compared to the default for ALBs. So this is saying anything status code‑wise between 200 and 399 is successful, and it will mark it as healthy. Now with that out of the way, I'm going to go down, click on Next, select my application instance and include it as pending and then create my target group. Perfect. So again, like I mentioned before in the ALB one, this is unused because we haven't selected this to be a target group for a rule on our NLB. But, let's go ahead and change that. What I'm going to do here is go to Load Balancers, create a new load balancer, select Network Load Balancer, and let's begin giving some information in here. So I'm going to give it a name, I'll call this Pluralsight, it's internet‑facing, we'll leave IPs the default. And under Network mapping, I want to choose my custom two‑tiered VPC, I'm going to map to A and C, just for the sake of high availability, and this needs to be in our public subnets, so Public A and Public C. Again, what this does is it injects ENI or nodes into the subnets so that we can load balance in those AZs. That's exactly how this works. So, I'll go ahead, select this, and then under Security groups, I'm going to select our public NLB security group, deselect default, and there we go. Now the next thing we have to do, which is identical to an ALB, minus the protocols, essentially, is add a listener and a rule and an action. So, default rule and listener here, TCP protocol on port 80. So we're going to leave that as is. I'm going to select my target group because we want this to be the default. I'm going to scroll down, and I'm going to click on Create load balancer. Perfect. So while this is provisioning, let's just check this out. Under our listener here on TCP for port 80, we have it set to forward to our application target group, which I will open up in a different tab here, and we'll see this is in the initial phase. So eventually this will be healthy, and once this is healthy, we'll go ahead and start testing traffic. So what I'll do here is I'm going to keep refreshing, I'll fast‑forward, and once this is ready, we'll begin. Okay, so that took several minutes, only because our NLB was provisioning for a little bit, but, once it was active, we now see after two health checks, we are healthy. So let's test this out. I'm going to go back to my Pluralsight load balancer, we see it's active here, when I refresh, you'll see the status. We see our listener and our default action. Let's test it out. So I'm going to copy this and I'm going to navigate to it, and, there we go, we get our website. Now you'll notice it is HTTP, which is fine, but this is really listening for TCP, so HTTP is part of a TCP protocol, obviously, so the Network Load Balancer is just sending this traffic through as is because it's on port 80 in a TCP connection. So we get our one instance that we deployed in our one AZ, and this is working perfectly. So there we go, we've set up a brand new Network Load Balancer with a target group and a default action, and that's how easy it is. Now, I'll include the module files for you to use if you want to do this as well, so feel free. But before we wrap up, I want to do a little bonus here. I want to show you how you can assign static EIPs to your Network Load Balancers as well. So what I'm going to do here is go to Elastic IPs, click on Allocate, I'm going to accept the default here and allocate this, and I'm going to do this one more time. So we'll allocate one more, and there we go. So now we have two allocated IPs for us to use, which are right here, 23 and 52. So, what I want to do is use these. So, I'll go to Load Balancers. Let's create a brand new load balancer, we'll make it a network one. I'll give my load balancer a name. This is not going to matter because we're not going to finish this. I'll leave it internet‑facing, choose my VPC here, and if I select two AZs, you'll notice now we can assign Elastic IPs. So this allows us to have static IP addresses for our Network Load Balancer. This is a popular exam scenario. If you need static IP references for a managed ELB in AWS, this is the method to do it. Now, if you need an Application Load Balancer in the mix as well, what you would do is create this, and you would create a target group that sends to an Application Load Balancer, and then the Application Load Balancer would have its very own target group for your instances. So you're essentially making a stack of load balancers. Now, I'm not going to do this because I have to create another target group, but I just wanted to show you how easy it is to assign static IP addresses to a Network Load Balancer for easy reference. With that being said, let's wrap this demonstration up. Hopefully, you learned a lot, and I will see you in the next one.

Gateway Load Balancers (GWLB)
All righty, next up in our load balancer discussion, let's look at Gateway Load Balancers. Gateway Load Balancers work at Layer 3 of the OSI model, so the Network layer. What this means is that it looks for actual IP packets in traffic to learn what it needs to actually load balance. These Gateway Load Balancers are meant for specific use cases. Generally speaking, you're going to use them to scale and manage virtual network appliances. Now, we say specific because typically you're going to use it with security appliances or appliances that use the GENEVE protocol. This is a specific encapsulation protocol with the network layer, and anything using it is supposed to use port 6‑0‑8‑1, or 6081. So in other words, if you see this protocol mentioned on the exam or you have requirements for this protocol and you need to do some type of a load balancing, this is a perfect solution for you. Here are some examples of what you would use a Gateway Load Balancer for. You could use it for intrusion detection systems. You can front intrusion prevention systems. And you can even set up Deep Packet Inspection software so you can use this to inspect traffic before it hits the actual destination. Now you do need to know the target types for a Gateway Load Balancer. You can send traffic to an EC2 instance, or you can send it to private IP addresses, so private internal IP CIDRs. Now with it, let's look at an architecture for it, because this has such a specific use case, we're going to simplify this as much as possible. On the first portion here, your traffic is directed to your ALB that we've deployed into a public subnet, and it's fronting your application in an autoscaling group. However, any traffic that actually is going toward that load balancer is configured to actually set up to be sent through the Gateway Load Balancer first. So what happens is the Gateway Load Balancer is going to intercept this traffic, it encapsulates it, and then it maintains the important packet information within it. So the source, destination, etc. Once it does this via that protocol, that GENEVE protocol, it goes ahead and sends it to your packet inspection IDS or IPS solution on the back end, which could be living in an autoscaling group of EC2 instances. Now these instances could be in a separate security VPC via a transit gateway or some type of peering connection, or they can live in the same VPC in a different local private subnet. Once the traffic is inspected, it's analyzed, it's okay to send through, it'll go back to the Gateway Load Balancer, and then it finally gets sent to the actual destination, which is the Application Load Balancer that was the original traffic destination. So now, once it hits the ALB, it can follow whatever rules that you set up for your listener, so that traffic could go ahead and get sent to your web application autoscaling group via your listener and your different conditions and rules. Now, it's important to note that the outbound traffic could follow a similar flow, but obviously in the opposite direction. So the responses could go through the ALB, that would go through the Gateway Load Balancer, which would run through your inspection software, and then back out your internet gateway. The big thing to take away here is that it fronts applications to do some type of traffic inspection or intrusion detection and prevention. And because of the GENEVE protocol that it uses, it encapsulates everything, so the traffic itself is not really affected or altered. Now moving on, let's look at some exam scenarios where this might come up. Let's assume you have a three‑tiered VPC and you're hosting a web application. Now you have web servers in a public subnet, so they're publicly accessible. You have app servers in a private subnet. And you have databases in a separate set of private subnets. So again, three tier. We're going to say your networking teams deployed an additional separate inspection VPC that hosts a third‑party virtual firewall appliance for IDS and IPS purposes. Now because of this, you need to integrate the web application with that virtual firewall appliance because they want to be able to inspect all traffic sent to and from the application before it actually reaches the destination. Well, this is a perfect use case for a Gateway Load Balancer. You can set that up to send your traffic to a local security VPC, or you can send it to a peered VPC, or over a transit gateway, etc., and inspect that traffic without really affecting the overall flow. Now, exam pro tip here as well, in addition to the scenario. Remember that these are perfect if you need to scale virtual security appliances or something specifically using the GENEVE protocol. If you see GENEVE, I would immediately consider a Gateway Load Balancer. With that being said, let's end this clip here, and we're going to move on to optimizing load balancing.

Elastic Load Balancing Optimization
Let's go ahead and start looking at optimizing Elastic Load Balancers within AWS. Now that we know when to use a specific ELB type, well, we need to know how to optimize them as well. It's not going to be good enough just to know which one to use; for this exam, you need to know how to make them as optimized as possible. The first feature we're going to talk about here is session affinity. Now this is sometimes referred to as sticky sessions as well. What this is is it's a feature that's going to allow you to enable clients to always reach the same target. In other words, they stick to the same target. Now with session affinity, it's supported by CLBs, so Classic Load Balancers, ALBs, and even NLBs. So, even though NLBs are not smart in terms of handling cookies and sessions to the same level as an ALB, you can use sticky session cookies to go ahead and stick to a target. So that is one exception to remember for an NLB. How it works is it creates a cookie that's going to determine which target is used for that particular session. So when they initiate the connection, the NLB will generate a cookie that will stick to that session and say, hey, I want you to use target A all the time until you can't. Now, with these cookies, you can actually specify and control when you want them to expire. So you can set it to 1 second, all the way up to 7 days. However, if you use this and you specify several days, be aware that this essentially overrides load balancing algorithms to a certain extent. It's going to ignore a round robin algorithm if you set something where you're setting a sticky session up, because, of course, it's going to notice and respect the cookie value that you set and send it to the target on the back end every time as long as that cookie is present and not expired. Now, first exam pro tip here, sticky sessions are perfect for maintaining user sessions. For example, maybe you have users on an online e‑commerce website and you want them to store items in their online carts between different sessions and different tabs. Well, this is exactly where a sticky session would come in handy. In addition to that, let's talk about an exam scenario. If you're having a situation where incoming traffic to a load balancer is heavily favoring one particular EC2 instance, and because of this, user latency is increasing, well, you can solve this or attempt to solve this by disabling session affinity. So remember that. If you're getting uneven load balancing, you might have session affinity enabled, so try disabling it and see if the load balancing round robin or algorithms that are in place actually go back and work correctly. Next up, deregistration delay, or in other words, connection draining. This is a feature that allows your load balancers to basically keep their existing connections open if the EC2 instance is considered deregistered or it might become unhealthy. In other words, it enables this load balancer to actually complete any and all in‑flight requests that are made to those instances. However, with this, it completes the in‑flight requests, even if they're not technically a part of the load balancer, but you don't send any new requests to those same back‑end resources. It's only for in‑flight requests. With this feature, you can set a delay timing from 1 second, all the way up to 3600 seconds. Or if you want to, you can disable it completely by setting the value to 0. So do your best to remember those values, but for the most part, remember what this feature actually does. And the last thing here, exam pro tip regarding this. You can disable this if you want your load balancer to immediately close connections to instances that are deregistering or considered unhealthy. If there's a scenario where connections are maintaining with unhealthy instances, this might be something to troubleshoot. Next up on the list, SSL certs and HTTPS listeners. ELBs use X.509 certificates for TLS secured connections. Please remember that. They use standardized X.509 certs. Now, you can easily, and should easily, manage and integrate certs using AWS Certificate Manager. Now, we talk about this in a different course within this path, but at a high level, this allows you to easily issue and deploy publicly trusted certificates for certain supported resources, and load balancers or ELBs are one of those resources. In addition to this, you can also even import and upload your own TLS certs that you got from another provider, so DigiCert, Comodo, etc., those are supported, but you have to upload and import. Fourth thing here, you have to specify a default certificate when you're setting this up. However, Server Name Indication, or SNI, is a supported feature. Now if you don't know what SNI is, just hold tight, we're going to look at that here in a moment, but for the fifth point here, you need to choose specific security policies for your load balancer when you use TLS and HTTPS. What these are are policies that essentially allow you to choose which ciphers and protocols are allowed or not allowed when connecting to your actual listener on your load balancer. Now, I told you we would talk about SNI. Let's look at an example on how this would work. In this example, we're navigating to two different sites, pluralsight.com and acloud.guru. Now, what we can do is we can actually set up our Route 53 domain to go ahead and direct these different host names or domain names to a single Application Load Balancer. Now, when it's doing that, we can leverage different target groups to host separate back‑end resources or even completely different applications for each of those sites. In this case, both of our website DNS names resolve to the same alias record pointing to our Application Load Balancer. Now, what SNI does is it allows you to store multiple TLS certs for different host names or domain names on the same load balancer. How it works is that the client itself is essentially indicating what target it wants as the SSL connection is established. By using that information, the server, or in other words, the load balancer itself in this case, can then select the correct certificate for the connection. So it sees, hey, I want to go to pluralsight.com, the ALB notices in the request, the domain name of pluralsight.com, so it automatically handles and selects the correct TLS cert for us. Now, it's important to call out here, this only works with ALBs, NLBs, and Amazon CloudFront, which is another service we'll cover in depth later on. But, at a high level, just understand SNI allows you to have multiple domain names and multiple TLS certs on the same load balancer. Moving on to SSL offloading or TLS offloading, we talked about this briefly before, but let's dive into it. When you offload SSL, you are allowing the ELB listener and the ELB itself to use HTTP or TLS, and the connection itself gets terminated at the ELB. So the connection from the client to the load balancer is secured via encryption. However, the back‑end connections are then established using TCP, UDP, or HTTP, and this offers the least performance impact for your actual compute. The next type is pass‑through. So, this is where you can configure an NLB listener for TCP 443, like we looked at earlier, and what happens is encryption and decryption are essentially bypassing the NLB itself. In other words, the connections are passed all the way through to the back‑end resources for unbroken SSL requirements. To have this work, you have to install the certs on the back‑end compute for this to actually function. Just remember that. However, if you do do this, it does require more computational effort on the instances themselves. And then lastly, the third type of SSL offloading, bridging. So this is where an ELB listener is configured for HTTPS or TLS, and the connections do get terminated at the load balancer. However, where this differs compared to offloading is that the back‑end connections are then established using a new SSL connection, which also, of course, requires certs on the instances. I'll be honest, bridging is pretty rare, I don't think I've ever seen it on the exam, but you need to know how it works. Two of the more popular ones are offloading and pass‑through. Now let's look at an exam scenario here. If you need to make sure you're not burdening your back‑end resources with TLS computation requirements, TLS or SSL offloading at the ELB is a perfect way to do it. Moving on to the next optimization feature, Cross‑zone Load Balancing. Cross‑zone Load Balancing is a feature you can turn on for specific load balancers that allow you to essentially optimize the balancing even further of your traffic. So in this case, let's assume it's off and our client is trying to hit pluralsight.com. We've deployed two nodes between two AZs and we have our compute on the back end. With Cross‑zone Load Balancing disabled, the ELB nodes can only split traffic evenly across the AZs that they live in. So in this case, 50/50. Because of this, that traffic is then going to be passed directly to the back‑end instances. So, each target receives their split of that 50% of the total traffic. So for four instances, they're getting 12.5%, for these six instances, it's roughly 8.33%. So you can notice that it's very uneven in terms of handling the workloads. Some of your instances are getting used more, and others a lot less. Now, if we enable Cross‑zone Load Balancing, which is highly recommended, and we try and go to pluralsight.com with the same set up of two different AZs, well now, when our AZs or our nodes in our AZs receive that traffic, they can spread the traffic evenly across all targets across all of the availability zones. So in this case, since us‑east‑1a has 40% of the total compute, it gets 40% of the traffic, us‑east‑1b has 60% of compute, so it gets 60% of the traffic. And now, you'll see it's more evenly spread on the back end, so each target receives 10% of total traffic. So this is a much more optimized way to handle load balancing for back‑end compute. Now that's going to do it for optimizing Elastic Load Balancing in AWS. We talked about a lot of features here, but let's go ahead, we'll wrap this up, and I'll see you in the next clip where we're going to have a demonstration.

Demo: Setting up an HTTPS Listener
In this demonstration, we're going to work on setting up an HTTPS listener on an Application Load Balancer. Let's go ahead and jump into the console now. All righty, I'm in my sandbox environment here, logged in as cloud_user. Real quick, let's go over some existing architecture to speed this up, and then we'll get started. I've already deployed an application server here, it's only in a private subnet within a custom two‑tiered VPC. You can see Private Subnet C, VPCA, and no public IP. Now, from a security standpoint, I have a security group here called Application that is referencing an Application Load Balancer's security group. So if I select this here, and I open this in the new tab, we have a public Application Load Balancer security group that allows port 80 and 443, so HTTP and HTTPS from anywhere. So what we've done is in this instances security group is we've allowed port 80 from only that source security group for the load balancer. Now with that out of the way, let's go ahead and begin creating our infrastructure. First thing I want to do here is I'm going to navigate to ACM. So, Certificate Manager is a service that we cover in a different course within this learning path. But at a high level, this is where you can manage public TLS certs for supported resources. So, to request this cert, I have to know my DNS. So I'm going to load Route 53 as well, and in the sandbox, you should have a public‑hosted zone that you can use. So if I go to hosted zones, we see we have our public hosted zone here, the account number, realhandsonlabs.net. So I'm going to use this to essentially tell Certificate Manager that, hey, I own this domain, this should be a valid certificate. So under ACM, I'm going to request a new certificate here. We'll request a public, click on Next, and now we have to give it a fully‑qualified domain name. So what I'm going to do is I'm going to call this pluralsight, and we're going to do it for the domain name here. So, this account number, I'll go ahead and copy this, we'll say pluralsight. this domain name, and there we go. Now, you could also do an asterisk with your domain name, etc. Some security teams are a little wary around wildcards, which is normal. But for this, we're going to specifically lock it down to this fully‑qualified domain name. Now for us, we're going to choose validation method of DNS, I'm not going to get into a lot of this information because it's out of scope for this course, but I'm going to click on Request. Perfect. So now we have a pending validation status, which is expected. I'll refresh here. Perfect. So now what I need to do is I need to create records in Route 53 to validate that I own this certificate. So I'm going to click this easy button here, because this creates the records for us in Route 53. I'll say yes. I'll go to my hosted zone here, I'll refresh, and there we go; we have the name, the value, the type, etc. And within a minute or so this should come back as valid. So what I'll do is I'll pause here, I'll wait until this is valid and issued, and we'll resume. All right, so that took maybe another 30 seconds, we see it's issued, now we can use this on our load balancers. So what I'll do here is go to my instance details. I'm going to go ahead and go to Target Groups, and I want to create a new target group. So I'm going to select Instances, we'll give it a name, I'm going to leave the protocol and the port the same, change the VPC, leave everything else the default except for health check settings. I'm going to minimize to speed this up. Scroll down here, go to Next, select my application instance and include it for registering in my target group, and then create. Perfect, so the target group's there, now we need our load balancer. I'm going to create a new load balancer, Application Load Balancer. We'll give this one a name, I'll just call it pluralsight. It's Internet‑facing. We're going to leave a lot of the defaults here except for VPC. I'm going to choose A and C for high availability. They're both the public ones in their respective AZ. Scroll down here, set my security group to my PublicApplicationLoadBalancer. Now remember, this allows HTTP and HTTPS from anywhere, and in addition to that, this is trusted by our internal security group on our application server. Now we have our listeners and our routing, so this is perfect. What I'm going to do is I'm going to set up HTTPS 443, and we're going to send it to our application target group that we set up. Now, if you notice, since we selected HTTPS, we have more settings now. We had to secure this listener. So the first thing we set is the security policy. So we're trying to decide, hey, what TLS versions, what kind of ciphers, etc. do we want to support? Well, there's FIPs‑specific ones, so if you have some extreme compliance you have to meet, or under all, there are many other ones. You can restrict it to TLS version 1.3, etc. There's a lot of stuff on here. Now some of this is out of scope for this exam, so I'm not going to cover it, but understand you can select different security policies to restrict what can actually be used to connect to your load balancer. So I'm going to choose the recommended one. And then we see our certificate we have to set. Now remember, when you set any secure listener, whether it be TLS or HTTPS, you have to have a cert as a default. So you can import it to IAM, which is a little antiquated now. You can import it directly via the actual files themselves, this is not recommended, but it is possible. And you would use this if you have potentially imported or need to import from a third‑party vendor like DigiCert, something like that. Now, what I'm going to do is choose ACM because we have a cert here that we created, validated, and issued. So I select our cert, I'm going to leave mutual authentication off for now, but you can turn it on. I'm going to skip down here, and I'm going to click on Create load balancer. So now we have our load balancer, it's provisioning. We have our listener on 443 that forwards traffic to our application target group. So if I load this in another tab here, within the next 20 or 30 seconds, we should see this come up as healthy. Now one thing I want to talk about here, under the load balancer, on this rule, remember that what's happening is we are performing SSL offloading, so the connection or the encrypted connection is actually ending once it is established on this load balancer via this listener. The back‑end target group is getting HTTP traffic, that's very important to remember. You can see here, Protocol‑Port that we specified is different than HTTPS on the load balancer. So really remember this for the exam. This is SSL offloading, where the SSL connection is decrypted and established on the load balancer and everything on the back end is now plain text, essentially. If you needed end‑to‑end encryption, you would create a Network Load Balancer and create a TCP protocol listener on 443, and then encrypt instead on your back‑end instances. Now, with that out of the way, what we're going to do here is I'm going to let this come up. Our load balancer is still provisioning, so our instances are not actually registered yet, which is fine. I'm going to pause here, and what we'll do is once this is active, I will resume. Perfect. So, now it's active. Our target group should be healthy here, so if I go down, we see it as healthy. So what I'm going to do here is I'm going to close out these two tabs, and what I want to do now is navigate to our load balancer. Now, the difference is it's not going to be HTTPS here on this because that's not the domain that we set up our TLS cert for. This is the domain. So what I want to do is I need to create a record in Route 53 for that. So I'm going to create a record, we're going to call it pluralsight, so now it matches our TLS cert. We'll leave the record type to A. Now, with a load balancer in AWS, you'll use an alias record. So, what I'll do here is choose my endpoint, I'm going to choose an Application Load Balancer, choose my region, which is us‑east1, and we can choose the load balancer. Now I'm going to tell you this really quickly. For some reason, this UI is buggy, this will drive you nuts if you didn't know this, you need to get rid of this dual‑stack portion here. Now I'm going to use this, and this is actually our load balancer itself. I don't know why they do this, it's super irritating, but just hopefully it will save you some headache. So I'm creating an alias record with the pluralsight. name, we'll use a simple routing policy, and I'm going to create my record. Perfect. So now we should be able to navigate to this DNS. So I'll go ahead and actually go to that, and there we go, HTTPS, we have our Pluralsight DNS name, it's hitting our instance. So everything is working perfectly. We have a secure connection here, which is awesome. All right. But what happens if we didn't let people know that we only have HTTPS? Well, what we can do is we can add a redirect. So let's do that really quickly before we wrap up. What I'm going to do is add a listener, I'm going to put it on HTTP port 80, and we're going to go ahead and redirect. So what we're going to do is we could redirect to a completely different URL, I don't want to do that. I'm going to select the URI parts, it's going to redirect to HTTPS on 443, and what this is saying is, hey, if we hit port 80, redirect to the local 443 instead. I'll click on Add, and there we go. So now we have a listener on port 80 to say, hey, if you're hitting this, well, I actually want to redirect you to my 443 listener instead. All right, so now what we can do is test this out. What I'll do here is I'll open up a brand new tab, I'm going to paste in HTTP for our URL here. I'm going to open up my console window here before we do that, and let me go ahead and click on Enter. Perfect, so you see, we get a redirect back to our secured website. So now we're on HTTPS, everything is working as expected. That's how easy it is to create a secured listener on an ALB with an ACM cert. Let's end here, and I'll see you in the next clip.

Module Summary and Exam Tips
All right, welcome, and congratulations, you've made it to the module summary and exam tips clip. Way to hang in there, let's go ahead and review some important details you need to know for the exam. First up, AZs versus AZ IDs. First up, availability zones. Remember, the actual data center groupings that these reference can actually change for each account. So for instance, my us‑east‑1a might not map to the same data centers that your east‑1a map to. To ensure that you're using the same exact data centers, you can use AZ IDs. These will always reference the same data center, and honestly, are the most important thing to know when planning multi‑account strategies. Next up, HA versus FT. Highly‑available. This is going to generally mean you have the ability to self‑recover after a component failure, and you're going to stay up most of the time. Fault‑tolerant is where your designs can actually handle failed components without any data loss and any service interruption. In other words, it handles faults. Moving on, please remember the ELB types and some of their nuances. ALBs. These are at the application layer, so Layer 7. What this means is they load balance HTTP and HTTPS traffic. This is going to be the smartest Elastic Load Balancing offered, and it handles rules for hostnames, paths, query strings, methods, and even source IP. If you have any requests or scenarios dealing with some of those conditions, think ALB. Next up was a Network Load Balancer. This operates at Layer 4 of the OSI model, in other words, the Transport layer. This load balances TCP, UDP, TLS, and TCP_UDP traffic. This is going to be the highest‑performing ELB that they offer. It can handle millions of requests per second with ultra‑low latency. In addition to that, it also allows for static EIP mapping. So if you have to whitelist a load balancer, this is going to be the best route. Thirdly, we have the Gateway Load Balancer. This load balances Layer 3 traffic, and it's specific for appliances that support the GENEVE protocol on port 6081. This is going to be very useful deploying inline virtual appliances to inspect and capture network traffic. Fourth, Classic Load Balancers. These are legacy load balancers that you really should be avoiding if possible. They do support both Layer 4 and 7 traffic, but they're not as scalable as the other options. Also, if you have multiple domain names, like what you would want to use SNI for, well, this doesn't support that. You have to have multiple CLBs in order to set that up. Moving on to some important concepts. Remember, you can leverage ACM for easy X.509 certificates on your ELBs for TLS or HTTPS traffic. Deregistration delay. This keeps existing connections alive to back‑end instances, even if they're not technically a part of the target group anymore, or if they're unhealthy. Session affinity, or sticky sessions, is when you use cookies to enable users to stick to the same instance on the back end. This is perfect for shopping carts on e‑commerce platforms. You do need to know when to use SSL offloading, SSL pass‑through, and sometimes, even though it's very rare, SSL bridging. So go back and review those if you need to, but I will be honest, the first two are probably the most popular. And then lastly, Server Name Indication, or SNI. This is where your clients specify the hostname at the start of the TLS handshake. You can use ALBs and NLBs to handle this for you so you can have multiple domain names and multiple TLS certs on the same load balancer. Next up, cross‑zone load balancing. Remember that this ensures each ELB node distributes traffic across registered targets in all enabled availability zones. So to remind you of what that looks like here, we'll run through this architecture really quickly. Let's assume we're hitting Pluralsight.com and we have two nodes split between two AZs. Well, once these nodes are deployed and we have our instances on the back end, with Cross‑zone Load Balancing enabled, it can spread that traffic evenly across the back end based on this configuration. So 40% goes to 4 of the 10, 60% goes to 6 of the 10 instances. Now, that's going to do it for this module. Thank you so much for hanging in there. Let's wrap things up, and when you're ready, I will see you in the next module.

Auto Scaling and High-Availability
Horizontal Versus Vertical Scaling
Okay, welcome to the second module in this course, Auto Scaling and High‑Availability. In this first clip, we're going to review horizontal and vertical scaling. These are two very important concepts you must understand when planning architectures in the cloud. First thing I want to define here, based on the Gartner.com website, scalability. This is simply put, the measure of your system's ability to increase and decrease performance and cost, all based on response to changes in application processing demands. In other words, if you have a ton of people trying to use your application, how well can you scale? Can you increase or decrease on demand? Moving off of that, we can talk about the different two types of scaling ‑ vertical and horizontal. You must understand the differences and use cases for each of these. First up, let's look at vertical. In the simplest terms, you scale your resources to be larger. An example of this would be increasing the size of your instances. So, maybe you're running an application on an m5.large and you realize, hey, I can't keep up with the processing, my memory's dawdling, etc. Well you can just go ahead and change the instance size to an extra large instead and take advantage of more resources on the same instance. Now, a typical use case for this would be scaling databases. So if you're hosting a database on an EC2, well, it's not as easy as just duplicating instances, so instead, you might want to just increase the size of your instance to handle the increase in demand. Now, with these, what goes along with it are some limits, and the limits are usually going to be based on the hardware itself, since you're just changing the size of the hardware. So things like CPU, RAM, networking bandwidth, etc. Those are all dependent on the size of the instance you deploy. On the other side of that, we have horizontal. This is where you increase the number of resources that you're using. So east to west, as opposed to north and south. So an example of this would be having two instances and you realize, hey, I am going to load balance these, I need to increase it to three or four instances instead to really handle the amount of traffic that's coming in. So to build off of that, maybe you have that same one application on an m5.large, you're putting it behind a load balancer and it's starting to get some timeouts. Well, maybe you want to scale out to three m5.larges instead, so then you can distribute the traffic evenly and you're scaling out to meet the demand. So this is a horizontal scaling approach. Now some common use cases for this would be web applications. So just like we just talked about, a web application fronted by an Application Load Balancer, well, you can scale it horizontally to meet demand and lessen the amount of resources when demand is less intense. Now, these are easily done in AWS via services like Amazon EC2, and there's typically going to be less limitations because it's just scaling in and out as opposed to growing and shrinking in physical size. Before we wrap things up, I wanted to show you a graphical representation of what vertical and horizontal would look like at a high level. On the left, we have vertical. You can see we have our m5.large, and we're just making it larger by scaling up (or vertically) to a 12.xlarge. And then on the right here, we have horizontal. So it's all still the same instance size, but instead we're increasing the number of instances so that we can spread all of the demand equally between four of them instead of one. So it's really just going to come down to your use case on which method you choose. But with that being said, let's wrap up here, and we're going to talk about horizontal auto scaling using Auto Scaling groups coming up next.

Auto Scaling Groups (ASGs)
All right, we just got done reviewing horizontal and vertical scaling, and building off of that knowledge, we're going to start looking at Auto Scaling groups to horizontally scale within AWS. First things first, let's define an Auto Scaling group. An Auto Scaling group is an actual logical resource within AWS that's going to contain collections of EC2 instances. These instances are collected and treated as a single logical group for the purpose of easier scaling capabilities and management needs. Now, why would you want to use Auto Scaling groups? Well, first things first, they are extremely important in any well‑designed architecture, and one of those reasons is because they offer self‑healing and cost optimization by leveraging an Auto Scaling group for your EC2 instances. If one of your instances fails, well then the Auto Scaling group will notice this, and it's going to attempt to automatically restart a brand new instance to replace that one. So this is what we mean when we say self‑healing. It's a highly‑available solution. It also offers cost optimization. You can set certain boundaries for your Auto Scaling group, which we're going to look at later on, and these boundaries can help you lower costs by spinning down instances that are not needed and then spinning up only when demand is high and you need more compute. Now some terms you're going to see throughout these different clips. First up, scale out. Scale out is a horizontal scaling term. What it means in this aspect is we're going to add additional EC2 instances or compute to essentially handle the increased workloads that are coming in for our applications. On the opposite side of this, we have scaling in. Scaling in is where we remove unnecessary or unneeded instances and compute based on decrease in workloads. So if our workloads are getting higher in demand, we scale out. When they're decreasing in demand, we scale back in. Now when you're defining an Auto Scaling group, you can easily set three primary configuration options. They're a minimum, a maximum, and a desired amount of instances. Now, we're going to look at these far more in depth later on, but understand there are these boundaries that you set when you're creating the group. And then lastly here, integration options. Auto Scaling groups are meant to easily integrate with your Elastic Load Balancers, so you can register your different compute as targets for your listeners. Now, we looked at Elastic Load Balancing earlier on in a different module, and you would want to use Auto Scaling groups with your target groups to make it a well‑oiled machine or a highly‑efficient architecture. One thing to call out here, it's important to note, Auto Scaling groups themselves are free to use. You can leverage them for absolutely no cost. You do, however, pay for the resources that are being scaled. For instance, if you scale to four EC2 instances, well, then you're paying for that EC2 compute while they're running. If you scale back down to two instances, well then you're only paying for the two instances instead. Moving on to the all‑important launch templates, or LT for short. You use launch templates to go ahead and specify all of the different required settings that usually go into building out and deploying an EC2 instance. The nice thing about using a template is that you configure those settings one time so that you don't have to walk through it over and over and over again to spin up more EC2 instances that are identical later on to handle increased workloads. When you're creating a launch template, there are several important settings you have to be aware of that you can actually specify. You specify the machine image, as well as the instance type. So maybe Ubuntu, Amazon Linux, and maybe an m5.large or a t3 small. You specify those with the launch template so you can easily redeploy. Now in addition to this, you can specify repeatable user data, specific EBS volume configurations. You can set the SSH key pairs that you want on every single machine. You can attach IAM roles repeatedly, so if you have specific permissions that you need your application to use, like accessing an S3 bucket, you can specify that here as well. Now you also get to specify, if you want, the VPC networking information. So a good example of this would be the subnets or the availability zones, which go hand‑in‑hand, so that you can successfully deploy a multi‑AZ highly‑available solution. Now the last thing here is you can also specify ELB information. So while you're defining this, you can actually say, hey, I want this launch template to launch instances within an Auto Scaling group that attached to an Elastic Load Balancer that I have running in a public subnet. Now, one big benefit besides the idea that you can just configure these once and redeploy is that you can also create versions of these templates. What this means is if you have a small update like maybe a tag value or maybe you have to update your user data within your launch templates, you can create a new version and then just reference the new version and keep the old one on hand if you ever need it. Moving on to comparing launch templates, which are the new thing you should be using in launch configurations. First up, launch template. We just looked at these, but these are the newest option. They support many more features for Auto Scaling and ELBs, and they just have more capabilities and options. Now you might see something called a launch configuration on your exam, and you will definitely see it in the console itself when you're poking around and deploying Auto Scaling groups. These are deprecated. They're still there, but you should avoid using them because they have limited features and there's no versioning capabilities. What that means is if you have to make a small change, like one particular character in your user data, well, you have to create an entirely new launch configuration and reference it. There's no versioning. The long story short, in the summary of all of this, always use a launch template and avoid launch configurations. Trust me, save yourself the headache, use a launch template. Now moving on, let's look at some Auto Scaling diagrams. What we have here is a very simplified diagram of a multi‑AZ deployment using an Auto Scaling group. What we would do is we would define the launch template, we create it, and it contains all of the necessary information that we need to deploy an Auto Scaling group, including things like VPC information, so, your subnets, your security groups, your key pairs, as well as other important factors like the instance type itself and the AMI that we launch with. After we get our launch template solidified, it's finalized, we can then deploy an Auto Scaling group using it. When we deploy the Auto Scaling group, we set the following important information: minimum, maximum, and desired. Minimum will be the least number of instances that will be kept running in your group. So if you set it to two, your Auto Scaling group will do its best to always have at least two instances running. Maximum is the maximum number of instances that should be running, so if you set this to 10, well then your Auto Scaling group will never spin up more than 10 instances regardless of the workload and the demand. And then lastly, desired. This is the initial number of instances you want to deploy. So if you set it to three, like we have in the diagram, then immediately three instances are deployed, and then from there the minimum and maximum values start taking over based on scaling metrics. Now we're going to look at scaling metrics much more in depth in a separate clip, so just hold on for that, but understand these three values for now. Now one of the most critical benefits of using an Auto Scaling group is that it gives you the ability to easily run multi‑AZ workloads for high‑availability requirements. Now, it's a general best practice to spread instance placement across at least three availability zones in your region, but sometimes two is enough, depending on the scenario and the requirements. Moving on, let's look at how this would handle scaling. Let's assume we have high user demand and a lot of incoming traffic into our application on our EC2 instances. Well, since we're leveraging an Auto Scaling group, it's going to handle scaling in (terminating) and scaling out (launching) of instances for us based on workload demands. And, since we defined that launch template earlier, it knows exactly what type of instance to configure and deploy with the exact same settings each time. So that's a huge benefit. This Auto Scaling group automatically handles creation and deletion whenever it's actually needed based on the values that we set earlier. Now, it should not breach minimum and maximum settings, but if there's an AZ outage, then it could be bypassed. However, it's extremely rare that you would ever have any issue maintaining minimum and maximum settings. Now moving on to another additional diagram here. Exam pro tip here. If any of the instances in an Auto Scaling group get marked as unhealthy, or they just fail to launch or they crash, then the Auto Scaling group will attempt to automatically replace that instance for us. Again, this is one of the biggest benefits of an Auto Scaling group, it offers a self‑healing capability. Now last diagram here before we wrap this clip up, Auto scaling and ELBs. Auto Scaling groups allow you to very easily integrate with ELBs. You can configure your instances in the groups to be registered to and removed from your target groups on the back end of your listeners for your Elastic Load Balancers. Also, in addition to that, you can offload health checks to the ELB itself. Now we're going to talk about that later on within this module, but just keep that in mind for now. And with that out of the way, let's go ahead and wrap this up. We'll end here, that's a lot of information, and I'll see you in our next clip where we're going to have a demonstration.

Demo: Create a Launch Template and Auto Scaling Group
Let's go ahead and get started with this demonstration clip. In this clip, we're going to jump into our sandbox environment, we're going to create a brand new launch template, and then we're going to create an Auto Scaling group referencing that exact launch template we created. Let's go ahead and jump in here now. All right, I'm in my sandbox environment, I'm in the us‑east‑1 region here as cloud_user, and I've loaded up my EC2 dashboard. Now, in here, first thing I want to do, I want to create my launch template. So I'm going to click on Launch Templates over here on the left. Create launch template, and let's begin filling in this information. First thing we have to do is give our launch template a name. So I'm going to give it a cool launch template name, and then we can give our version a name, or a description, I should say. Remember, with launch templates, you can have multiple versions of the same one with different settings. So, what we'll do is we'll call this v1. Now, what I'm going to do is I'm going to select Provide guidance because I want to show you what's required for us to configure with a launch template in order to use it with an Auto Scaling group. So I will select this, and let's go ahead and begin filling in the required info. First thing I have to do, I have to select my AMI. Now what I'll do is under Quick Start, select Amazon Linux 2023, but in most scenarios, you're going to want to probably use this to create an image template of a custom image that you've already created for your own application, your own requirements, etc. So on the exam you're going to see a lot more specific things regarding application AMIs, but for this demo, we're going to use the base AMI. The next thing we do here is we can specify an instance type. Now we don't have to do this, but we can. So what I'm going to do is I'm going to go ahead and specify. I want this to run on a t3.small. So I select t3.small, and this will be the default instance now in our Auto Scaling group, but you'll see we can override this if we need to. Next up is the key pair. Now, we don't have to, and I'm not going to, but you can include this if you want. So every instance that gets created with this template would have this key pair injected into it, but I don't have one and I don't want to include it, so I'm going to say don't include. Next up is the network settings. Now, subnets don't really do much in terms of when we use this with an Auto Scaling group because this will get overridden, but you can specify a subnet here if you're just launching an individual instance with it. But since we're not doing that, I'm going to say don't include, and then we specify our security group. So, we do need to specify what security group we want to place on these instances. Now, I already have one created called allow‑http, and this does exactly what you think it would, it actually allows HTTP traffic from anywhere. Now if we wanted to, we can mess with some of the more advanced network settings, I'm not going to do that, that's way out of scope for this exam, but we're going to move on. Next thing is our storage. So, what kind of EBS volumes do we want to attach to our instances? So what we can do is change the size here from the default of 8 GiB, and let's go ahead and make it double, I'm going to make it 16 GiB. Now every instance is going to be based on a t3 that we put up here, t3.small, no key pair, it'll have this security group, with this size EBS volume. So, I'll leave this, we'll scroll down here, and now we can even specify other stuff. So, first up, IAM instance profile. So I have an EC2 test role here, which allows Systems Manager's Session Manager permissions, so I can say, hey, I want you to go ahead and attach this to every instance as well, so I don't have to do it every time. Now this other stuff is out of scope for this exam, or at least this portion of the exam, so we're going to skip over most of this. But on the bottom here, we can also require user data. So what I'm actually going to do is copy and paste some user data in really quickly. I'll paste it in, and all it is, I'm sure you've seen this before, is a simple web page that I use in pretty much all my demos, so it's going to start an HTTPD server, create this index.html file, and we can view it. And that's going to do it. So, what I'll do now is create my launch template, I'm going to view it, and there we go, OurCool launch template, default version set to one, latest set to one. All right, so, let's use this. I've selected my launch template, I'm going to go to Actions, Create Auto Scaling group. I give my Auto Scaling group a name, and then I select my launch template. Under Launch Templates, I'm going to go ahead and refresh, we see our only available version is OurCoolLT, we have one version, which is the default, and it's got our information here for us. You also see the storage information. So I'm going to click on Next, and this is what I was talking about earlier, you can override some of the settings. So if I wanted to, I could override this launch template. So I could say actually, I don't want a t3.small, I want an a1.2xlarge, etc. Now I'm going to reset. We're going to use the default here, but you can override if you end up needing to. Next up is our network settings. So what VPC do we want to deploy these in? Now I'm going to use the default VPC, which I don't recommend for anything real‑world scenario‑wise, but for the simplicity and the speed, I'm going to choose this. After the VPC, we choose our AZs and our subnets. So for this, I'm going to follow best practice, I want to deploy across three availability zones, 1a, 1b, and 1c. For the distribution, we can choose balanced best effort or balanced only, the only difference here is balanced best effort would say, okay, if us‑east‑1a is not working, I'll try and spin it up in us‑1b or 1c. Balanced only is going to say, hey, if us‑east‑1a is not working, well, I'm going to keep retrying until it does. I'll leave balanced best effort on. I'll click on Next, and now we get to some integrations. So with Auto Scaling groups, we're going to do this later on, but you can actually integrate with an ELB. So we can attach to existing, attach to new, or select none. I'm going to select none. We can also integrate with VPC Lattice. Now, we cover this later on within this module actually, but we're going to leave this to no for right now as that's out of scope for this demo. We'll skip over ARC, that's way out of scope, and let's get down to health checks. Now, we're going to dive into health checks much deeper later on, but the big thing to know right now is it's an EC2 health check \with how we have it configured. What this means is it's using the default instance status checks, etc. Now I'm going to set the health check grace period to much shorter, I'm going to set it to 120 to speed this up. I'll click on Next, and now we get to our sizing. So, first up, desired capacity or desired size. How many instances do you want to start with? I'm going to say three. After that, we set our scaling limits. So, minimum and maximum. Minimum, I'll set to three, so this means I want you to always maintain at least three instances, and I'll say maximum can be six, so you can spin up to six instances, depending on the workload. We're going to skip over scaling policies, we'll do that much later on in a different demonstration. I'll skip over maintenance policy, skip over the rest here, and then I'm going to click on Next. Now, if we needed to, you could actually add notifications here. So, whenever an Auto Scaling launch or termination occurs, you can get an SNS notification sent via an SNS topic. I'm going to skip that and click on Next, skip over tags, we can review, and then I can create my Auto Scaling group. Perfect. So this is now updating capacity. Eventually, what we're going to see here is three instances spread across our three AZs. In fact, they're showing up now, which is perfect, I didn't even have to pause. You see three different AZs, they're pending right now from a lifecycle standpoint. We have our instance type, and we see our launch template and the version. Now, really important here, notice we've spread across each AZ, so this is a highly‑available architecture. The Auto Scaling group handles this all for us, we just specify it. So, what I'm going to go ahead and do here is once these are all inService, which they are now, which is great, I want to show you what happens when we terminate one. So what I'll do is I'll select us‑east‑1a, I'm going to open it in a new tab here, and let's just go ahead and demonstrate what happens when I terminate this. Now before I do this, I'm going to go back to my Auto Scaling group, I'm going to go to Details, and at the bottom, you're going to see under Advanced configurations, Default cooldown. So to properly do this, I need to change this. So I'm going to click on Edit, I'm going to say Default cooldown, I'm going to set it to 30. So, 30 seconds of a cooldown. Remember, with a cooldown, what you're doing is telling the Auto Scaling group to wait this many seconds before it performs any other scaling activity, so it won't scale in, it won't scale out, etc., for this 30 seconds. I'm going to click on Update, and there we go. So now our Auto Scaling group details should be set to 30 for a cooldown. I'll go back to Instance management, we see our three instances here. I'll open up my new tab again. So, what I'll do is I'll go to Instance state, I'm going to Terminate, Confirm, there we go, it's now starting to shut down and terminate. I'll go to my Auto Scaling group details here, and what we're going to do is in a few minutes we're going to notice this top instance here is going to be gone, and it should be automatically replaced with a brand new instance for us by our group. So what I'll do here is I'm going to go ahead, I'll pause, I'll fast‑forward, and we'll see when this is complete. Okay, so I fast‑forwarded, that took roughly about a minute, I would say, but we can see now our us‑east‑1a instance is unhealthy. So eventually this is going to get replaced, and sure enough, one refresh later, it's gone, and we have a new instance in that same AZ, all handled automatically for us by our Auto Scaling group. So this is a perfect demonstration of what we mean when we say this is a self‑healing capability that this offers, if you use it correctly. Now one last thing I want to demonstrate here is we're all running version 1, but let's say we want to update our launch template. So I'm going to open this in a new tab here, I'm going to create a new version, so I'll go to Actions, Modify template, we'll call this version 2, and the only thing I'm going to change here is I'm going to scroll all the way down, and I'm going to remove my user data. So that's going to be the only thing I change. I'll remove this, click on Create, and there we go, we now have version 2 and 1 of the same template, and the default is set to 1. So what I could do is set the default to 2, not a big deal for this, understand you can set the default version to 2 though. So if I wanted to, I could select this, Actions, Set default version, and then confirm it. And now the latest is also the default. But what I want to do is under Auto Scaling groups, I want to replace these instances with our updated launch template. So what I'll do is under Details, under Launch template, I'll edit. I'm going to select our launch template, which it does it automatically, and luckily for us, I set this to default, so it selects the newest. However, you can specify a specific version, you can use latest or default. So I'm going to say default here, which is version 2, I'm going to scroll down here, click on Update, and there we go. Now any new instances will use that version 2 of our same launch template. But, what if you want to do it now? You don't want to wait for future instances to come up? Easy enough. Under Instance refresh, we can start an instance refresh, which allows us to replace existing instances. Now, I'm going to leave the default here. We talk about these different instance replacement methods in a different clip, so I'll leave all the defaults, I'm going to make sure instance warmup is set to 120, scroll down all the way and click Start instance refresh. Perfect. So now what's going to happen is each one of those instances is going to be replaced by an updated version, and we'll be able to view it in instance management. In fact, you can see it here. Our old version, version 1 in us‑east‑1c, is getting replaced with this pending instance in us‑east‑1c running version 2. So that's how easy it is to force an update using instance refresh for your Auto Scaling groups. Now with that being said, I think that's more than enough for this demo. Let's wrap up here, and I'll see you in the next clip.

Auto Scaling Group Scaling Policies
All right, let's get started with talking about scaling policies for our Auto Scaling groups. First up, a scaling policy. Scaling policies are what allow you to essentially dynamically respond to the different changes in demands and workloads via horizontal scaling. Remember, scaling in is where you remove, scaling out is where you add instances. Now there are several ways to configure scaling via a policy, so let's look at those now. First up, the original way, manual scaling. So this is going to be the simplest way to scale your groups, and it's really meant to be used for urgent tests or urgent requirements. Generally speaking, you don't want to manually scale, that's the whole point of using an Auto Scaling group is you want it to automatically do so. Next is simple. This is one of the original methods. This is going to rely on specific different metrics for scaling, for instance, CPU utilization. Third is target tracking. Target tracking is one of my favorite ways to handle Auto Scaling metrics; you essentially set a scaling metric or value that needs to be maintained at all times. So instead of scaling down and up based on demands, what is going to happen is it measures and watches for a specific metric, and it essentially tries to maintain that metric by spinning up and removing as needed. This is probably the most common way to do it nowadays within AWS. Fourthly, we have step scaling. This is one of the more complicated methods. Now, usually this is not included on the exam in too much depth, but I want to cover it because it is important you understand how it works. This is where you apply stepped adjustments to vary the scaling. Now we'll look at it here later on with a more detailed example; however, just realize you basically take small steps based on measurements of your metrics. And then the last one here, predictive. So you can actually set forecasted future capacity needs by using historical load data. So you can look at old loads, old metrics, and it's going to say, hey, every time after Thanksgiving, it seems like your e‑commerce application gets a lot of load, let's go ahead and forecast this and say, okay, we're going to scale up by 45% to make sure we handle this load based on predictions. Now another one that's similar to this is scheduled scaling. So just like predictive, if you understand you're going to have requirements coming up soon, you can schedule scaling actions as well to meet those needs. So with that same example, if you know there's a particular day where you have a ton of user demand, well, you can schedule scaling actions, which acts similar to predictive. Now exam pro tip here. You can actually scale distributed workloads running on EC2 based on SQS message queue lengths. We'll cover SQS in a different portion of this course, but understand this is a scenario that can come up. What you would look for is this particular metric here, approximate number of messages. So as that number of messages grows, you can scale your EC2 out to start consuming those messages faster. Moving on, let's start looking at some examples. This first example, we're going to use the request count per target metric. So by using this metric, we can essentially make sure that the average request count per target on the back end in a target group behind a load balancer is balanced appropriately. You set the target value for your target group, and it's going to do its best to maintain that. In this example, let's go ahead and assume that we set a target value of five requests per healthy target on the back end. So, if we get 15 requests, that means we're going to get 5 requests per target on the back end because we have 3 instances. So 5, 5, and 5 is 15. Well, let's assume that our application got very popular on a TikTok video, so now everyone's starting to want to visit it. Well, let's assume very suddenly we start to get 25 requests, so our ELB is getting a burst of traffic. Well, using Auto Scaling groups with these metrics, we can have it automatically scale because our metric threshold was breached. So the Auto Scaling group scales out to meet demand, and now we can have five requests per instance, which makes the demand a lot more balanced. Now, let's look at some other scaling metric examples you should be familiar with for the exam. First, ASGAverageCPUUtilization. This is going to measure the average CPU utilization of the entire Auto Scaling group. So an example could be you wanting to check and maintain less than 50%. So, if the metric breaches above 50%, you scale out. If it's below, you scale back in. The ASGroupAverageNetworkIn. So this is going to be the average number of bytes received on all network interfaces within your Auto Scaling group. So this is not specific to the ELB, it's specific to the instances on the back end. There's also network out, which is just the opposite, it's the average number of bytes sent out on all network interfaces. And then lastly, custom. You can create your very own custom metrics to send to CloudWatch to configure scaling. Now these are a lot more complex and we cover CloudWatch in depth in a different course, but understand you can create custom metrics and use those to scale your groups. An example could be memory usage. Memory usage is not a built‑in metric, but you can capture it and send it as a custom metric. Now, moving on to another metric example, Auto Scaling group average CPU. In this example, we're going to use that metric and make sure that the average CPU utilization of the group is maintained at a specified threshold. So in this example, we're going to assume the following. We want to scale out when it's greater or equal to 60%, and scale back in when it's less than 60. So based on what we have below, we have our Auto Scaling Group behind an ELB with users hitting it. We set a minimum of 1, a maximum of 10, and a desired of 3. So immediately we have 3 instances, and you can see we're running at about 30% CPU utilization. Well, when we start getting more traffic, and let's say we spike up to 65%, well, eventually this is going to trigger our metric alarm threshold that we set up. So now, we can go ahead and add instances automatically via our Auto Scaling group until that metric threshold is lower. So our group automatically adds instances to handle that load. Now eventually, of course, when demand goes down, the metric will fall back below this threshold, so now it's at 55%. Again, with using Auto Scaling groups and these metrics, automatically our instances scale back in, so now we only have two, so it's above the minimum, but it's below the desired because we're now easily meeting that metric that we set. So in addition to the Auto Scaling properties, this is cost optimization as well. If we're below the threshold, we don't need to have six or five instances running, we can get away with one or two. I told you we would talk about step scaling, so let's do that really quickly. We're going to use the same metric, the average CPU, but let's look at how you would use this in step scaling. First, you would scale out. So, with a step scaling setup, you could say, hey, I want to add 5 instances if the metric is greater than or equal to 60%. The next step in our step scaling setup would be, okay, if it gets above or equal to 80%, add 10 more instances. So we could add potentially 15 total more instances in the end. The big thing to keep in mind here is if it's between 60 and 80, you're only going to add 5 instances, but as soon as it breaches 80, it's adding 10 additional instances, not just 5 more. And then you can scale in. So on the opposite end, maybe we want to just remove 5 instances if it's less than or equal to 40%, and if it falls to 20% or less, we want to remove 10 more instances. Now, I know this can be a little confusing, but just understand you're basically setting steps for your scaling. Now last thing here before we wrap this lengthy clip up, Auto Scaling group cooldown periods. A scaling cooldown is when after an Auto Scaling group launches or terminates an instance. What's going to happen is they're going to wait for a specified cooldown period to end before completing any more activities. The default is going to be set at 300 seconds. These are meant to allow your groups to essentially stabilize to prevent any premature terminations or launches before the effects of the previous scaling activity are actually visible. So basically, is your instance up and running appropriately before we do something else? Now an exam pro tip here. By using a pre‑baked AMI, you can actually set a lower cooldown period so that you can get up and running a lot quicker, as well as react to scaling requirements faster. So you can set that period to a lot lower, allowing you to scale better. Now that's going to do it for this lengthy clip. Again, like I said, this was a very dense clip, there was a lot of information, please go back and review as needed. But for now we'll wrap up here, and we're going to talk about health checks coming up shortly.

Auto Scaling Group Health Checks
Okay, let's talk about health checks within your Auto Scaling groups. There are several health checks and components and concepts you need to know for this exam. The first type of health check specific to an Auto Scaling group is an EC2. This is where it's going to check the EC2 instance status and the underlying hardware. This is the default option for Auto Scaling groups. You also have an ELB health check type. So like we mentioned, you can go ahead and place your Auto Scaling group instances to be a part of a target group for an ELB, and this is going to offload the health checks to that load balancer. So what it does is it checks if the ELB reports the instances as healthy or unhealthy. So it has nothing to do with the status and underlying hardware checks of an EC2 check. You have to be careful with these because if your application is failing a simple health check and you set a different health end point compared to the application, well then it will be marked as unhealthy and the Auto Scaling group will do what it needs to do to replace it. There's also a VPC Lattice check. Now, this is not common, and that's why I marked it with an asterisk, and a VPC Lattice essentially just allows you to connect services within your VPC. It kind of acts like a service mesh for VPC services. We'll cover that in a completely different lesson later on, but understand you can use VPC Lattice checks to go ahead and see if your instances are healthy. Again, it's not as common right now on the exam, but you need to be aware of it. Fourthly, EBS. You can actually check if the volumes that are attached to your Auto Scaling group instances are reachable, and they can actually handle input/output operations. This is a cool one. Again, it's kind of rare to see this, usually it's EC2 or ELB, but you need to know this exists. And then lastly, just like our metrics from earlier, you can also set a custom health check type. So you can set your own custom health checks that you need to set up based on your own requirements. And again, I would say this is also pretty rare right now, but, just like the other last two, you need to know it exists. If I had to say anything, I would say definitely be familiar with the first two on this list. Now let's talk about health check grace periods. We talked about cooldown periods in a previous clip, well, these are kind of similar, except for that they're specific to Auto Scaling groups on health checking. What grace periods are are a configurable amount of time that you can allow a delay to happen for your health checks on your instances. Just like the cooldown period, the default is 300 seconds, so you're giving your instances time to get up and the health check endpoints to actually be responsive. These are meant to allow your apps to get up and running, you can execute Bootstrap scripts, etc. If you need time to get stuff running on your instances so they don't fail a health check prematurely, this is what you would configure. So remember that for the exam. With that being said, let's end this here. I think that was enough for health checks on your Auto Scaling groups, we're going to wrap up, and I'll see you in the next clip.

ASG Instance Maintenance Policies
Okay, let's get into some more fun stuff, Auto Scaling group instance maintenance policies. When you're creating Auto Scaling groups, you can create or alter what is called an instance maintenance policy, and what this does is it basically affects how your EC2 Auto Scaling events are actually going to handle instances to be replaced. It essentially is going to allow you to optimize availability or cost, depending on the requirements in the scenario. So let's actually explore the three different methods now. First, we have launch before terminating. So when you set this policy, what happens is new instances get provisioned before existing instances are terminated. So for instance, if you're replacing instances in an Auto Scaling group with an updated launch template, if you have this in place, your Auto Scaling group is going to spin up a new instance, wait for that to check in and pass all of its health checks, and then begin to terminate the older, longest‑running instance that's being replaced. There's terminate and launch. So, this is where things are happening at the same time. Again, let's say you're deploying a version 2 of a launch template and you currently have version 1 running, well then those version 2 new instances get launched at the same time that your existing version 1 instances get terminated. And then finally, you can set a custom policy. So if you want to, this is a little bit more advanced, but you can set a custom minimum and maximum range for instance capacity while you're replacing instances. For the exam, it's very important you understand those first two that we looked at. You need to know when you would use them. For launch before terminating, this is best when you have availability as the determining factor. That's because it's leaving instances up and running before it terminates, so you have maximum availability. There's also terminate and launch. When you want to use terminate and launch is when you have cost as the determining factor. You don't want instances running at the same time, and you're okay to handle a little bit of downtime, if necessary, to save on that cost. Remember these both for the exam. That's going to do it for this short lesson, let's wrap things up, and I'll see you in the next clip.

Demo: Fronting an Auto Scaling Group with an ELB
Let's go ahead and get started with our next demonstration lesson where we're going to front our very own Auto Scaling group with an Elastic Load Balancer. Before we jump into the console, let's have a very quick high‑level overview of the architecture itself. When we jump into the console, we're going to have several components already created for us to speed this up. I've already created an Application Load Balancer, and on that load balancer, we're going to have an ACM‑issued public TLS certificate and a listener with port 443 in HTTPS set up on the public internet. Now, in addition to that, we're going to have a default rule that points to a target group on the back end. Now, the issue with this is starting off, we'll have no instances or no compute in that target group. So what we're going to do is the bottom portion here. We're going to create a brand new Auto Scaling group using a launch template, we're going to create new instances with that Auto Scaling group, and we're going to configure it to attach to the target group that we've already created so we can show that process. All right, I'm in my sandbox environment here under us‑east‑1, let's really quickly review some of the already created infrastructure pieces before we continue. Like I mentioned in the overview, I created several important concepts or components that we've already done before, so I skipped those steps, but I do want to review them, so let's look real quick. I've already created our launch template here, as you can see, and I've filled in the required information, t3.smalls, I attached a new security group, etc. Now for this security group, I'll open this up in a new tab. We're attaching this AllowPublicLoadBalancer security group, which has one individual inbound rule here, allowing HTTP on port 80 from our other security group called PublicLoadBalancer. So you can see here the source, which is the PublicLoadBalancer security group, has two separate inbound rules. We're allowing HTTP and HTTPS from anywhere. So this PublicLoadBalancer security group will be attached to our load balancer, as you can see here. Here is the load balancer Pluralsight named, it's an application, internet‑facing scheme, and here is the rest of the information. Now what I've done is I've spread this across three public AZs and subnets, as you can see on the right, so this is what we're going to separate via multi‑tiered architecture. Our load balancer will be in a public subnet, our Auto Scaling group instances will be in a private subnet. So on this, you'll notice we attached the security group, so this allows our inbound connections. Now the next thing I've done is I created a listener. So I created an HTTPS 443 listener that sends traffic to our back‑end target's target group, which I just loaded up, and you'll notice we have zero targets here. So this is what we're going to add our Auto Scaling group instances to once we get through that process. Now, very last thing to review. In Route 53, I've created two records. I validated our ACM cert, which is what's used for our TLS listener, and I created a new record to reference our load balancer via an alias. So this is what we'll use to test once things are up and running. So, with that review out of the way, let's go back here, I'm under my launch template, I'm going to create a new Auto Scaling group. Let me go ahead and give it a name. Our launch template is already selected, I'll click on Next, and we'll scroll down to Network. Now I'm going to choose my custom VPC. And for this, remember, our ALB is in our public subnet, and we want to secure this by only adding these to private subnets and restricting who can actually talk to them via our security group network access controls. So for AZs and subnets, I'm going to select each private subnet in this VPC. So now our instances are not public, they're only reachable via the ELB. Now that I've selected this, I'll go to Next, and now we get to load balancing. So we skipped this in a previous demo, but we're going to change it this time. Now if you want to, you can actually attach it and create a brand new one here, but we've already done that, so I'm going to choose attach to existing. Now with this, we choose the target group that we want these Auto Scaling group instances to become a part of automatically, so I'm going to choose my BackendTargets group that's a part of my listener rule. So now anytime an instance comes up, it's going to automatically register it as a target in this target group. Now with this being said, I'm also going to turn on Elastic Load Balancing health checks here at the bottom under health checks. Now, we do some scaling based on this in a completely different demonstration, but you want to turn this on for best practice because then you can use those health checks to allow the ELB to handle when or when it should not be sending traffic to an instance in a target group. So I'm going to turn this on, and I'm going to knock this grace period down to 120 seconds, and click on Next. Next up is our group sizing. So for this, I'm going to set desired to three because I want to scale across each availability zone that I have an ALB node in. So I set it to three, it automatically adjusted our minimum and maximum. I'm going to skip down, ignore the rest of the settings for this demo. I'm going to click on Next, and now I can click on Skip to review, and I'm going to go ahead and create this group. Perfect. So now we have our application Auto Scaling group here, it's perfect, we have three instances, so under Instance management, we'll start seeing three pending, awesome. Now what I can do here is go to my target group details, and I'm going to refresh this. You're going to notice it's starting to register our instances that are in our Auto Scaling group to our target group, which is perfect. So eventually we're going to see three, and there we go. Now the health status is initial for now because it's just now trying to register and start up. So eventually what's going to happen is these will get reported as healthy, and then we can continue on. So I'll tell you what, I'm going to pause really briefly, wait until these are up and healthy, and then we will resume. All right, so that took, I would say roughly about one more minute for all three of them to come up and be healthy. Keep in mind it might show unhealthy for a minute until it passes the required amount of health checks. However, now we have three instances spread across our three AZs thanks to our Auto Scaling group, and these are all a part of a back‑end target group for our Pluralsight load balancer. So now, in theory, if I hit our Route 53 DNS here, it should send us right to our back‑end target groups, and there we go. So now if I refresh, we should start getting different instances at least every other time. And, there we go, it's now round robining through all of our back‑end instances. Now that's going to do it, that's how easy it is to create an Auto Scaling group and then front it with an Elastic Load Balancer by configuring the Auto Scaling group to become a part of a target group, which is the back end part of a listener rule. Now, with that being said, let's go ahead and wrap this demonstration up. And coming up next in another demonstration, we're actually going to build off of this and use some health checks via the ELB to scale our instances. So let's go ahead and end here, and I'll see you in an upcoming clip.

Demo: Creating and Using a Dynamic Scaling Policy
All right, let's get started with another demonstration. In this demo, we're going to use an existing architecture, and we're going to create and then use a dynamic scaling policy for an Auto Scaling group. Before we jump in the console, let's look at our existing architecture that we can expect. What we'll have already created for us are several components. We're going to have a public internet‑facing Application Load Balancer set up to listen on HTTPS port 443. On that listener, I'm using an existing already issued public TLS certificate using AWS Certificate Manager. Now regarding rules, we're only going to use a default rule, and we're going to forward to an existing target group that's hosting a basic web application that we've used plenty of times before. Now what we're going to do is we're going to use our Auto Scaling group and I'm going to show you how we've implemented it to use these instances as a back‑end target within a target group. What we're going to do is we're going to build off of it, and we're going to implement a target tracking dynamic policy for scaling the instances so that we can maintain a metric value that we put up. What's going to happen is our Auto Scaling group will automatically scale for us, thus the name, all based on that policy metric that we put in place. So let's go ahead and jump in the console now. All righty, I'm in my sandbox environment. Before we jump in, let me review pre‑existing architectural components so you can understand what I've already deployed before we begin scaling our instances. Now, at a high level, I've deployed an Application Load Balancer in a VPC with two tiers, a public and a private. I've deployed it across three different public tiers for the ALB nodes, and these are accessible over the internet using port 80 and 443, which is typical HTTP and HTTPS. Now what I've done is I've only set up a listener for 443 and that forwards traffic to our application target group, which is up here. Now right now, this is accepting port 80 on the back end, and we have one healthy instance that is a part of this. Now this instance belongs to this Auto Scaling group here, ApplicationAutoScalingGroup. So I've launched this with a desired capacity of one, and you can see that same instance here in us‑east‑1a. Now to reference this public load balancer, I've also created a Route 53 record here, as you can see here, app.hostedzone. So if I navigate to this, we get our website. Now, we only have one instance, so it's not really load balancing, but let's go ahead and test this out. What I'm going to do is close this, go back to my Auto Scaling group, and one thing I want to call out is if you're trying to follow along a little bit, under Details, to make this a little bit quicker, I went ahead and knocked down default cooldown to 15 seconds. I don't want to wait more than 15 seconds for scaling to occur. With that understood, I'm going to go up here, we see our desired of 1, min of 1, and max of 6. I'm going to select Automatic scaling here, and let's create a dynamic scaling policy. So in here I'm going to click on Create dynamic scaling policy, and under policy type we have three options: target tracking, which is what we're going to do. There's step scaling, again, which is where you add steps to take for particular metric alarms. So for instance, if our alarm breaches at 50% CPU, maybe we want to add one instance, and then we would add another step saying, okay, well, if CPU is above 70%, maybe we add 5 instances. You take steps based on your alarms. The other option is simple scaling. This is literally the most simple way to scale. You set a policy, you set the alarm you want to watch, if that alarm is breached, you take one action; you either add, remove, or set to a certain amount of number of instances. Now we're going to do target tracking because that offloads all of the work to our Auto Scaling group. We're going to let it manage all of our scaling requirements. So I select target tracking, I'll leave the name as is, Target Tracking Policy, and then we see our metric type. So under metric type we have several different options that we can choose from: Average CPU, Average network in or out, Custom, and we're going to actually use Application Load Balancer request count per target. So again, remember what this does is it's going to make sure that the back‑end instances have enough compute capacity, or in other words, the correct number of instances, to distribute request count to those back ends. So with this selected, I'm going to select our target group because obviously our target group is what's actually accepting the requests, so we're going to select our application target group, and then we can set our target value. Now to make sure this scales very quickly, I'm going to set this to 25. We'll leave instance warmup as is, and what this is saying as a review is, okay, for our target group here, we want to make sure we maintain 25 request counts per target in the target group for whatever load balancer is managing these. I'm going to click on Create, and there we go. Now we have our target tracking policy in place. Let's go ahead and test it out. So what I'm going to do here is go into my terminal, and I'm going to get ready to execute a very simple load testing solution here called Taurus, I've included instructions at a high level for you if you want to use this as well. Please make sure you understand what you're doing when you do use it, if you use it. But again, these will be included in the files for the module assets if you want to follow along or do this on your own. So what I'm going to do here is execute my command, and all this is doing is reading a YAML file that I've configured to go ahead and reach our load balancer. So you'll see we're starting to get hit counts up, you can see it down here as well. And this is cut off, but this is that URL for the load balancer based on the Route 53 record that I created. So you'll notice we're very quickly load testing this load balancer. We already have 200 hits within a couple of seconds. Now this is going to scale up even more over a period of 1 minute, so this is going to increase exponentially very soon. With that being said, let me go back, and eventually here, pretty quickly, we're going to start seeing that our instances are going to scale out. And that's all based on if we go to our load balancer, under Monitoring, we're going to start seeing the number of requests climb very quickly. So what I can do here is set my custom minutes to five, so this looks at the last five. And very shortly, once this catches up, we're going to start seeing a lot more requests in, which is going to mean a lot more target connections on the back end, which means that we're going to scale out to go ahead and manage those requests. So, what I'll do here is I'll go back to my Auto Scaling group, I'll refresh, and I'm going to go ahead and pause for about a minute, and then once this is scaled out, I will resume. Okay, so that took a little over a minute total, but you'll notice now we have a ton of pending instances, and you'll also notice it's six total, which is our max limit. So these are pending, and eventually they're going to get attached to our load balancer via our target group, and now they're already in service, so I should see them over here at least trying to be initialized, and I do. So what I'm going to do is kill my load balancing, go ahead and shut that down, go back into my console here, and eventually these will scale in based on that same target tracking automatic scaling that we set here earlier on. Now you'll notice if I go to my load balancer details, a ton of requests came in. So this is scaling as anticipated. I basically overwhelmed it, it scaled out to meet the demand as best it could based on my maximum configuration, and then that's it. Okay, let's go ahead and wrap this demo up. We created a target tracking policy for an Auto Scaling group that leverages ELB metrics to go ahead and scale out horizontally on demand. Let's go ahead, we'll wrap things up here, and I'll see you in an upcoming clip.

Auto Scaling Group Lifecycle Hooks
Okay, up next in our journey, let's go ahead and look at Auto Scaling group lifecycle hooks. A lifecycle hook is a feature for EC2 instances within an Auto Scaling group that allow you to create solutions that are essentially aware of events that the instance lifecycle is going through. In other words, it allows you to perform custom actions on your instances when the corresponding lifecycle event occurs. Now, one of the benefits of using these is that it actually allows you to have up to 2 hours of waiting. So when an instance is going down or coming up, you can have it wait for up to 2 hours, perform some type of custom action if it takes that long, and then resume. Let's go ahead and look at a use case very briefly on how you might use a lifecycle hook. In this particular example, we're going to assume we're launching a new instance via an Auto Scaling group. So the first thing that happens is the instance is getting launched, and once it's starting to get launched, it's going to enter a WAIT state. Once the instance is in that WAIT state, we can trigger our hook. What that hook does is it can go ahead and kick off a script on the instance to go ahead and run, download, and maybe install custom software packages for a particular application. Once the application is installed, it's up and running and it's configured, we can then have the instance send a complete lifecycle action command to continue the process of launching. Once that complete lifecycle action is received, well then the scaling event finishes, and that instance goes into service and it's ready to go. Now, let's go ahead and look at the hooks process flow. This is a generalized diagram and a workflow diagram of how an instance launches and terminates in the different states that it can reach during those two portions. The first type of lifecycle hook is a scale‑out lifecycle hook. We can see it here on the bottom left, Pending: Wait, Pending: Proceed. During this portion of the lifecycle, you can pause the scaling‑out event, execute your custom scripts, maybe you have to do some type of data ingestion, or maybe you need to perform a schema migration for an application's database, etc. Now once this is complete, the instance again sends a complete lifecycle action API call to the Auto Scaling group, and it says, hey, I'm good to go, go ahead and continue. Now, if the Auto Scaling group never receives this lifecycle action API call, well then the hook can time out. Now, by default, it's set for 3600 seconds, but that is customizable. If the hook times out, it just continues on. The other side of this is a scaling‑in lifecycle hook. So you scale in means you're terminating instances. So we have Terminating: Wait and Terminating: Proceed. Now at this point, you can pause the scaling‑in process and then execute some other custom script for maybe something like application log backups to S3, or maybe you want to back up your data to an EFS file system. Once this lifecycle action is complete, you still send the same API call, a complete lifecycle action API call to the Auto Scaling group itself. Just like a scaling‑out event, that means this Auto Scaling group can continue on with terminating the instance, or, exactly the same as before, it can time out after a specified period. If it times out, then it continues no matter what. Now let's move on to one last thing here, an exam scenario. Let's assume you need to make sure that any EC2 instances that are created via an Auto Scaling group have to check in with a third‑party auditing tool whenever they launch or whenever they terminate. So, maybe when they're launching, they have to register to this third‑party tool, and then when they terminate, they have to let the third‑party tool know, hey, I'm no longer in service, go ahead and stop tracking whatever I'm doing. Well, this is a perfect scenario for lifecycle hooks. Using the lifecycle hooks for scaling in and scaling out, you can run your custom scripts, send the required data to the audit tool, and then go ahead and deregister it once they're terminated. And with that being said, let's go ahead and wrap this up here, and I will see you in an upcoming demonstration.

Demo: Trigger Log Storage via a Lifecycle Hook
Alrighty, let's jump into another demo here. In this demonstration, we're going to trigger log storage to an S3 bucket via a lifecycle hook process. Let's go ahead and jump into the console now and let's get going. All righty, I'm in my sandbox environment here, let's get started. First thing I want to do is I'm going to create a new Auto Scaling group. I'm going to give my Auto Scaling group a name. Now, I'm going to call this Application‑group, and the next thing we do, of course, is select our launch template. Now, I don't have one, so I'm going to create one, and let's walk through some of the important aspects for this demonstration. I'm going to skip through some of the other non‑critical portions because we've already done that in other demos, so I'm only going to talk about the critical portion. So just keep that in mind. First thing I'll do is give my template a name. I'll give it a description. I'm going to make sure I choose provide guidance here, and let's fill out the required stuff. So I'm going to choose Amazon Linux, I'm going to spin up a t3.micro. Under Network settings, I'm going to go ahead and create a new security group called AllowHTTP. I give it a description, and then I'll leave the VPC. I'm going to add a new inbound rule to allow HTTP from anywhere. And, there we go. Now the next important thing here is at the bottom under Advanced details. First things first, I have to choose an instance profile. Now I didn't make one here, so I'm going to create a new one, and the reason we have to do this is because the complete lifecycle action command is an API call to the Auto Scaling service. So because of this, since I'm performing that action from the EC2, I need to create an instance profile and role to grant it access to do so. In addition to that, we're going to be interacting with this S3 bucket to offload our logs, so I'm going to have that in there as well. Now what I'll do here is create a new role, I'm going to say it's from EC2, I'm going to scroll down to Next, I'm going to search for a policy that I made called lifecycle hooks. LIFECYCLE_HOOK_POLICY, and under here I'll show you what this does. I've already prefilled this, and the default blank template will be out there for you to use in a module asset if you feel like you want to. So what we're doing in this first statement here or first section is we're allowing a PutObject into our bucket, which is what is required for uploading our logs. The bottom section is for the autoscaling action. So autoscaling:CompleteLifecycleAction, and we gave it the arn of our group. So this is why I knew to call it application‑group because that's what we're naming it over here. So that's exactly what this role allows, and that's it, I'll click on Next, we'll give it a name, and then I'll go ahead and click on Create role. Perfect. So now I'll close this, go back to launch template, I'll refresh, and I'm going to select LIFECYCLE_HOOK_ROLE. Awesome. So now all of our instances will have these permissions. The next thing I want to do here is paste in user data. Now I'm going to go to my console here, or my IDE, and this is going to be the user data I'm going to use. This is also a template here, you can tell because I have REPLACE_ME in certain aspects or certain parts, but you can use this as well if you want, I'm going to walk through it at a very high level, and I'm going to fill in the values where I need to. First things first, let me fill in these REPLACE_ME values, so for BUCKET_NAME, in the console, I have to copy this bucket name, I'll replace it, and then the next REPLACE_ME is down here under group_name. So I need to fill in the application Auto Scaling group name, which will be in our Auto Scaling group here, I'll copy this, I'm going to paste it back. And then the next one down here is the lifecycle‑hook‑name. Now I'm just going to call this Application‑hook. This isn't filled out yet for us, we're going to do it later, but I'll show you exactly how it works. Now at the bottom here, we should have nothing else to fill in, that's perfect. So now that I've filled in all the REPLACE_MEs, I'll walk through what this does. First thing we're doing is storing a token because we're using version 2 of the IMDS, or the metadata service. From there we define some variables that we're going to use later, including the two log files that we're going to zip up and store in our bucket. Now, I have these functions here, and I'll actually walk through those based on the bottom down here. So the main thing that happens here is function main. So eventually it's going to kick off immediately based on the while true, and we're setting a target state. Now the target state is looking for InService. Now this is running via this up top, the get_target_state function, which actually queries the metadata service for the target lifecycle state, which is a part of an Auto Scaling group action. After it does that, once it sees it's in service, it's going to start zipping up our file, so it gets the instance ID from an instance_id function up here via the metadata service, and then it names our file, it zips it, and then it uploads it to S3. And then finally, it's going to go ahead and complete our lifecycle action. Now this is our function right above main. So what this function is doing is it's getting the instance_id, we set the application Auto Scaling group name in the region, and it's going to go ahead and run this AWS CLI command to complete lifecycle action, as you can see right here. So this is exactly why we had to grant that API permission to the instance profile. Now again, this will be available for you to fill out on your own, but this is what we're going to use for this demo. So I'm going to copy this, go back to my console here, and under launch template, I'm going to paste this in. So now every instance we'll try and kick off this script, and now that it's in there, I'll create my launch template. Perfect, so our launch template is there, I'm going to go back to create Auto Scaling group, I'll refresh, and there we go, ApplicationTemplate. So the next thing I'll do is click on Next, and I'm going to skip through a majority of this stuff or not really talk about it because, again, we've covered this in a previous demo. So for networking, I'll leave the default, I'll select three AZs, just to speed this up, I'll go down, click on Next. I'm going to skip through load balancing, skip through health checks. For group size, I'll leave it desired of 1, min of 1, and I'll set the max to 3. I'll skip through the policies, I'm going to click on Next, and then I can click on Skip to review, and I'm going to go ahead and I'm going to create my Auto Scaling group. Perfect. So now we should have one instance in here eventually once it spins up, which is fine, but what I want to do is I actually want to create a lifecycle hook. So under Instance management, remember, we want to create this hook so we can go ahead and perform our custom actions. So what I'll do is create a new one, and then I give it a name. Now I'm going to copy and paste this in from my IDE, and the next thing is the lifecycle transition. So what part of the lifecycle transition flow, which we looked at in a previous clip, do you want to actually perform your custom action at? There are two different transition points. There's instance launch, instance terminate. Now, I will say typically offloading log storage will occur on an instance terminate, but for the sake of simplicity on this, I'm going to do it at the launch. It functions the same, the only difference is the point in the lifecycle where it kicks off. So for this, I'm going to choose instance launch, and whenever it goes into a PendingWait state and it switches to InService, it's going to go ahead and complete our lifecycle action. Next is the heartbeat timeout. Remember, this is the amount of time that it will wait until it receives a complete lifecycle action command. For this, I'm going to go ahead and set it to 300, but it should be a lot faster than that, and once this heartbeat is in place, we can set our default result. So, if there's a failure or the heartbeat timeout is reached, we can either abandon or we can actually continue. Now I'm going to choose abandon since that's the default here, and I'm going to click on Create. Perfect. So now we have our instance up and running, we created a lifecycle hook, let's test all this out. What I'm going to do here is I'm going to go up and I'm going to scale this out by one instance, I'll click on Update, and what's going to happen here is eventually we're going to get an instance here under instance management, and what I'll do is I'm going to probably fast‑forward, but I'm going to keep refreshing until we see this new instance, we see it's pending, and I'm going to keep refreshing, and at some point we're going to see a PendingWait state. In fact, there it is, I didn't even have to fast‑forward. So now this has reached our lifecycle hook, it reached the PendingWait, and eventually our custom action will occur, which will complete this lifecycle hook, and we're going to be InService. So again, I'm just going to keep refreshing here for a couple seconds, and eventually this should move to InService because this hook that we set up had its custom action performed, it sent that API call, and there we go, it's now in service. So now eventually if I go to my bucket, we should eventually see an object here, and we do. Awesome. So we can actually see here, if I go to my EC2 Auto Scaling group, the most recent one was 0fc, and we'll see here, 0fc in the zip file. So this is a zip file containing our logs. In fact, I'll go ahead and download this, and what I've done is I've actually went ahead and opened this file up. So what I'll do is I'll go into my IDE, I'm going to open a new window, and I'm going to open up that downloaded file. So let me go to my downloads, I'm going to open up this opened zip file, we see it's var log. We have our cloud‑init‑output, and we have our audit.log. So this zipped up our files, saved them to S3, and it essentially offloaded them somewhere else based on our lifecycle hook. So that's it, that's how easy it is to use a lifecycle hook to perform a custom action on your Auto Scaling group instances. With that being said, let's go ahead and wrap up here. Hope you learned a lot. Again, these files will be available for you to use if you want to test on your own. But for now, I'm going to end this here, and I'll see you in an upcoming clip.

Improving Service Connectivity Using VPC Lattice
Let's talk about using VPC Lattice to improve service connectivity within your VPCs. VPC Lattice is a very specialized feature within AWS that allows you to go ahead and use a managed application networking service to connect, secure, and even monitor services for your applications in a VPC. The whole point of this feature is it's meant to help essentially reduce the different networking complexities and any challenges that might arise when you're connecting microservices within a VPC. Now, I like to tell people the easiest way to think of this is that it's very similar to a service mesh for Kubernetes workloads. It essentially allows you to easily connect different microservices without all of the overhead and complexity by leveraging this very simple managed service. Now, it's not the exact same thing as a service mesh, but it operates in a similar manner, so that's the way I like to think of it. Now with that being said, super short clip here, let's go ahead and wrap things up, and we're going to move on to our module summary and exam tips.

Module Summary and Exam Tips
All righty, congratulations. Thank you for hanging in there again for another module. Let's wrap things up really quickly with a summary and some exam tips for you to take with you into your exam. First things first, four questions to ask yourself on the exam when you're sitting there reading the scenarios. First up, is it highly‑available, and is it self‑healing? If not, does it need to be? Remember, Auto Scaling groups allow you to easily meet this requirement because you can spin instances across multiple AZs, as well as setting a minimum and maximum number of instances that should be maintained. If one of those instances goes down, your Auto Scaling group is going to do its best to go ahead and replace it automatically via the launch template that you configured. The second question, what is appropriate for this situation? Do you need horizontal or do you need vertical scaling? Remember, horizontal scaling is where you spin up the same type of instances or compute, and you spin up a lot more of them or less of them depending on the workload. Vertical scaling, on the other hand, is where you make one particular instance or one particular compute much larger or much smaller in its resources. So, more CPU or more RAM, or less CPU and less RAM. A big thing to keep in mind here is that Auto Scaling groups are for horizontal scaling. Please remember that. Thirdly, what is the most cost‑effective way to implement this? Auto Scaling groups offer you a way to meet cost‑effectiveness for scenarios because it can automatically scale down to a minimum number of required instances based on less demand. So, if there's any scenario where you have to go ahead and have some type of disaster recovery architecture in place, and you want it to be readily available, but you want to minimize cost, well, using an Auto Scaling group in that secondary region is a perfect way to solve this. Fourthly here, do I need to focus on availability and resilience, or the cost? This is especially important when you're trying to determine what type of instance maintenance policy you want. Remember, there's launch before terminate and there's terminate and launch. The first one launches new instances before it tears down old ones, which is perfect for availability. The second one does both at the same time, because in that use case, typically cost is going to be the determining factor. Moving on to more specifics about Auto Scaling groups and launch templates. Here are some benefits for using Auto Scaling groups. Now, some of these might be repeated, but the only reason that is is because it is very important you remember them for the exam. First up, Auto Scaling groups give you self‑healing capabilities for groups of EC2 instances. Remember, if one goes down, your Auto Scaling group will do its best to attempt to recover it with a new replacement. You use these to scale in and scale out automatically to meet workload demands. So, the more workload that comes in, you could horizontally scale out, the less workload that comes in, you can horizontally scale in. Thirdly here, they're free to use. This is a completely free feature. You do, however, pay for the resources that are actually being scaled, so you pay for the EC2, etc. One of the big integrations that they offer is tight integration with Elastic Load Balancers. Remember, you can spin up an Auto Scaling group to have the instances become a part of a target group for the different Elastic Load Balancers within AWS. In addition to that, you can offload health checks for your Auto Scaling group to the ELB if you desire to. With Auto Scaling groups, remember you customize them and you specify a minimum required number of instances, a maximum number of instances, and initially, a desired count. This is going to be the initial amount that gets deployed. And then lastly, Auto Scaling groups allow you to very easily achieve high‑availability, because you can easily deploy across multiple AZs within a region using the same Auto Scaling group. Moving on to how we even leverage Auto Scaling groups in the first place, a launch template. Remember, these are the newer version of configuration options for Auto Scaling groups compared to launch configurations. You want to use a launch template whenever possible because they offer many more capabilities and many more features. The beauty of using a launch template is that it allows you to essentially configure one time all of the important information that you want to repeat for future horizontal scaling requirements, so VPC info, AMI, instance type, etc. And then lastly here, launch templates offer versioning. So if you have to make even the smallest update, you can create simply a new version and then reference that, which makes it a lot cleaner in the console and when you're managing the different templates that you have. Now, let's move on to Auto Scaling group scaling policies and health checks. Remember these important concepts. Simple scaling. This is the most straightforward way of scaling, and you base the scaling on different metrics. For instance, maybe you add one instance if the average CPU is greater than 75%. There's step scaling. This is probably one of the more complicated versions of scaling. This is where you perform stepped adjustments to vary the different scaling needs. For instance, maybe at 60% you add 2 instances, and then if it continues to rise and hits 75% CPU, well, you add an additional 4 instances. Those are different steps in scaling out. And then lastly, let's say eventually your workload dies down drastically. In fact, CPU utilization goes all the way down to 25%. Well, maybe at that step, you want to remove a total of five instances. Again, this is one of the more complex scaling methods, but you do need to be aware of it. Thirdly is target tracking. This is probably the most popular one nowadays using different Auto Scaling capabilities with different services, and really it's the easiest to implement in general. This is where you set scaling metrics and values that need to be maintained by your Auto Scaling group at all times. So for instance, maybe you say, hey, I want to always keep CPU utilization at 25%. Well, what's going to happen is the Auto Scaling group is going to do whatever it takes to go ahead and maintain that value. So it could scale all the way down to your minimum, and it could scale all the way up to your maximum, you're essentially letting it handle all of the hard work for you to just maintain your target. Fourthly, predictive. Now this is where you forecast future capacity needs using historical load data. For instance, maybe you want to add 25 instances just a few hours before Black Friday, which is a big holiday and shopping day within the United States of America. Now this does also get confused with scheduled scaling. That is similar. The only difference is you don't forecast your need based on historical load, you just know for a fact that something needs to be scheduled in the future, so you can do something similar, just understand the differences. And then the last thing here is a health check. You need to understand when you should leverage EC2 health checks, or Elastic Load Balancer health checks. They serve similar purposes, but they function completely differently. Now, last thing here. Remember, you can use lifecycle hooks to perform custom actions during scaling‑out and scaling‑in processes. Some examples of what you might do there is you can offload application logs to S3, or you can register instances to third‑party vendors as they go up, and then deregister them as they go down. Now, that's going to do it for this module. Thanks for hanging in there. Let's wrap it up, take a quick break or a long break, whatever you need, and then I will see you in the next module.

Containers and Images
Containers and Images
All right, let's get started with this next module where we're going to start looking at containers and images and how we would use these within Amazon Web Services. First up on the agenda is we're going to look at a review of what containers are. If we want to properly architect for containers, well, we have to understand how they work and some of the fundamental processes and concepts that go along with them. First thing we want to do here is define what a container means. Now, based on the Docker website, this is how you would define a container. In a nutshell, or in a very short summary, it's just a standard unit of software where you're going to package up all of the code, as well as all of its required dependencies so that the application that's running can do so quickly and reliably, and the goal is you can move it from one environment to another, and in theory have no issues running the same application. Now, let's actually look at a container architecture overview. We're going to compare it with a typical VM deployment compared to a containerized deployment. On the left‑hand side here, we have an abstracted view of what a VM deployment would look like. Now virtual machines, which could be things like Amazon EC2 or VMware VMs, basically allow you to abstract the physical hardware, and it allows you to run multiple virtual machines or servers on one underlying host sharing the same hardware. Now, typically, each individual VM that you create is going to contain a full copy of an operating system, a full copy of the application you're trying to run, and all of the necessary binaries and libraries, and to be honest, even some unnecessary ones. Since it's hosting typically a full operating system, there's going to be a lot of fluff on there or wasted space. Now, that's not to say it's all bad. While they do have a lot of this bloat, if you will, they do allow for a lot of customization and a lot of control. So you're going to have a lot more control and customization, however, you also have a bit more overhead. So because of that, they can be slower to manage, slower to deploy, and even sometimes slower to run. Now, if we compare that to a typical container deployment, we can see kind of at a high level how the architecture changes on the left side here. By deploying containers, it's going to allow you to abstract the application layer, and you can package that code and only the necessary dependencies together. What this allows you to do is make a smaller bit of image or package, and deploy several containers on the same underlying machine or even the same underlying VM. The beauty of containers is that they can run on the same host and share the same OS kernel with all of the other containers on the same server. However, even though they're sharing the same underlying hardware like that, they're having their own isolated processes and their own specific user space, so you can allow them to communicate or not communicate at all. And the last big thing here, since it's a very refined package of just what you need, like your application code and only the necessary dependencies and libraries, containers should be taking up far less space than typical VM deployments. In addition to that, because you can package so many containers and stuff so many containers on one server, it's probably going to require fewer underlying virtual machines and operating systems to run them in the first place. So they're typically far more efficient. With that review out of the way, let's look at some important container terms you need to be familiar with for the exam and anytime you're using container orchestration in Amazon Web Services. First up, a Dockerfile. If you've used containers, I'm sure you've seen this. For those of you who are not familiar, this is just a textual document that's going to contain all of the commands, instructions, etc., that are used to build an image. Now, if you're wondering what an image is, let's talk about it. An image is an immutable file that's going to contain the code, libraries, dependencies, and any specific configuration files that are required to run your application. So you run containers based on an image. Now to store your images, you can store them in a registry. So a registry stores Docker images for distribution. They can be both private and they can be public, so you can allow only private internal access or you can allow the world to download and run your images. And then lastly, a container, at the highest level of definition is a running copy of your image that you have stored on a registry somewhere that you've defined with a Dockerfile. All of these are very important concepts and terms you have to be familiar with for this exam. Now before we wrap things up, let's have two use case exam tips for when you would run containers in AWS. First up, microservices. Containers are perfect to run in AWS if you need to meet microservices architecture requirements. So maybe you have a large application, you're shifting them to a microservice approach from a monolithic approach, and you want to containerize all of the separate services so you can better decouple and implement high‑availability. Well, then this is a perfect solution for containers in the cloud. The second use case that's popular to come up is a lift‑and‑shift solution. Since containers essentially contain everything packaged in one that you need for your application, they're generally seen as more flexible and easier to do lift‑and‑shift with. For example, you might have containers running on‑prem that are using Docker images and Docker containers; well, those can easily be moved into AWS and leverage some of the underlying services in the cloud, or even just straight EC2 instances, and they should run the exact same. That's one of the beautiful, nice things about containers is you can move them pretty much anywhere where a container is supported. Now, that's going to do it for this first clip. Let's go ahead and wrap up here, and I'll see you in the next one.

Amazon Elastic Container Registry (ECR)
All right, let's talk about storing images in Amazon Elastic Container Registry. Amazon ECR is a very important service you must understand for this exam, whenever we're talking about containers. The highest‑level definition is that it's a service in AWS that's going to allow you to store and even manage your Docker images. So you can build them locally, test it all out, and then if it passes, you can push them to your ECR repos and registries. Now, an important thing to know here is they support both public and private repos, so you could open up a public repo so anyone can use it, or you can create a private repo where you have internal proprietary images set up. With a private, a good use case is maybe within an organization, you have an internal application that you need multiple accounts to reach, well that's a perfect use case for private repos, as you can share them with a single account or multiple accounts as needed. Within ECR, you're going to be able to push several different types of images and artifacts. So it supports OCI, or Open Container Initiative images, it supports Docker images, and you can even push OCI artifacts. I can tell you the two big ones to know here are going to be OCI images and Docker images, regarding the exam. Now if we touch back on a private repository, you actually control access to that repo via AWS IAM permissions, which makes sense, it's in AWS, and IAM is used for pretty much everything in regards to making API calls to any of their services. So if it's a private repo, you control access with IAM. One of the benefits and features within the ECR repo is you can have a lifecycle policy in place. What these are meant for is to help you manage, automatically, the lifecycle of your images in your repos. If you're working with a large development team, you can get a ton of images back to back, especially if they're making small changes and doing iterative approaches, and at some point, your repos might be too messy or too big with way too many images. Well, you can use a lifecycle policy to automate the exploration of old images based on your requirements. And then lastly here, it does support cross‑region cross‑account replication. So if you want to have some type of disaster recovery or just a backup for more efficient processes in a different region, you can implement cross‑region cross‑account replication. Moving on to some exam concepts you should be familiar with. First up, image scanning. This is where with the click of a button, you can easily enable a feature for your repos where it's going to easily identify software vulnerabilities in your images as they are pushed to the repo. Sometimes this is referred to as Scan on Push. What happens as you upload and push, ECR uses its security vulnerability scanning tools, and then it generates a very easy readable result for you to look at. Second, encryption. You can protect proprietary container images using server‑side encryption using AWS KMS keys. So if you must encrypt images, this is a supported feature using KMS. And then third, tagging and versioning, which kind of go hand‑in‑hand. Your container image contents are going to change over time, as expected. So what you can do is you can leverage different tags to identify versions like latest, prod, version 1, or you can leverage tag immutability to prevent tags from being overwritten. So a lot of people like to overwrite the latest tag, and then in addition have version tags, but if you want it where you don't want to be able to overwrite any tags, you can use tag immutability. So those are two important features. You can tag and version or you can prevent overwriting of tags and versions. Now, let's talk about some commonly‑integrated services within AWS that ECR works with. We know that ECR stores images, but it really works with two of the most popular container orchestration services in the AWS cloud. Now each of these services we're about to look at leverage IAM permissions so that they can pull the images from the repos and deploy them to the containers that they're orchestrating. The first is Amazon ECS. This is Elastic Container Service. Now we're going to talk about ECS really in depth coming up in this module, so hang tight on that, but the second service is Amazon EKS, Elastic Kubernetes Service. Again, just like ECS, we're going to dive in much deeper to EKS as well later on within this module, but for now, just understand ECR very easily integrates with both of these services. Now, before we wrap things up for this short clip, a quick exam scenario. Let's assume you have some workloads running on some ECS containers. You want to go ahead and scan images for Common Vulnerabilities and Exposures, in other words, CVEs. Well, the easiest way to do that with the least operational overhead is you can use your ECR repos and enable Scan on Push so it can filter for the entire repo and automatically scan for any issues in your images. Now, that's going to do it for this clip. Let's wrap things up. We just talked about Amazon ECR, and coming up next, we're going to talk about the services that leverage ECR.

Amazon Elastic Container Service (ECS) Overview
Let's go ahead and dive into Amazon Elastic Container Service, otherwise known as Amazon ECS. We talked about containers and some of the benefits they offer, but once you start using them at scale, let's actually start diving into some of the headaches that could go along with managing those containers. In this image, we have several containers on a single EC2 instance, but what happens when we start scaling out? Now we have 6 different instances hosting 3 containers each, which is 18 containers or 18 applications. Now this is still not necessarily a major deal, but what happens if you keep scaling over and over and over again, and eventually you have hundreds or even thousands of containers running different applications? Well, unfortunately, that's where the headaches start to come in with managing them. However, we do have something called Amazon ECS within the AWS Cloud, which is there to help us handle and navigate this headache. Amazon ECS is their service that is meant to allow you to easily launch, and then even manage several Docker containers that are running on AWS compute. Some of the benefits that go along with this orchestration service include the following. You can easily manage anywhere from a single to thousands of containers at one time using a single console view. It is extremely beneficial for multi‑container orchestration. Another thing is that it will appropriately place your containers on the different instances that can handle them, and it will do its best to keep them online and replace them whenever the existing ones go down due to some type of failure. In other words, they can help you achieve a self‑healing architecture within your containers. Another thing is that they can very easily be registered with your Application Load Balancers and your Network Load Balancers to host your applications depending on your different needs, so if you need Layer 7 or Layer 4 load balancing. What's cool about it is you set it up so that the ECS containers are registered as a target within the target group, and the service itself will go ahead and register and deregister them as required. So as they come online, it registers them, and then if you de‑register to delete them, it handles that as well. Now, a fourth big, big benefit here that you have to know for the exam. Containers can have individual roles, which are called task roles, and you attach these to the containers. Now, we're going to talk about task roles much more in depth in their own clip, but understand at a high level you attach roles to your containers to make security a breeze. Now, at the heart of the ECS service, we have an ECS cluster. This is going to be a logical grouping of what they call tasks, which are essentially your containers and your applications or different services that are going to run on the capacity infrastructure that you set up for your cluster. These clusters themselves are actually controlled or configured based on something called a launch type that you get to choose. Now let's actually look at launch types now. In this exam, you're going to need to be familiar with the three types of launch types that ECS offers. The first is EC2, we have Fargate, and then there's something called ECS Anywhere. We're going to go ahead and talk about the first two in this clip, but ECS Anywhere has its own designated clip later on. So first up, EC2 launch types. EC2 launch types are an option where you get to actually choose the instance type, the number of instances, etc., for how you want to manage your capacity hosting your containers. In other words, you are literally selecting actual EC2 instance types that you want to run multiple containers on. If you use this method, each instance that you're using has to have the ECS Agent configured and in place and running to be able to register with your ECS cluster. This is required because this is how the service even orchestrates your containers in the first place. If the agent is not running, well then it's not going to be usable by the ECS service, which really defeats the purpose. Now, with this launch type, AWS handles the starting and stopping and the different placements of your containers, but you have to maintain the actual instances. That includes things like patching. Now that's a really big tradeoff. You get more customization and more options; however, you are responsible for a lot more. Typically, this will be one of the best options for price optimization if you're running long‑term container workloads. The next one we need to talk about is the Fargate launch type. This is AWS's serverless, pay‑as‑you‑go offering, and it helps because you don't have to provision or manage any underlying infrastructure. What that means is you don't have to worry about running and configuring the instances that host the actual containers. With a Fargate launch type, you create what is called a task definition. That task definition contains pretty much all of the configuration options you need to launch the containers and your apps. Based on the task definition, ECS is going to take it, and then it's going to run tasks, or containers, in other words, and they're all going to pull information from that definition before they actually start running. With Fargate, you only have to specify a required set of fields. These fields include things like CPU and RAM requirements or limits. So those are really the two main things you have to focus on, which is great. You don't have to worry about the underlying host at all, it's handled for you. Once you specify the CPU boundings, the RAM limits or soft limits, etc., you go ahead and say, hey, here's the task definition, and ECS says, I got it, let me go ahead and deploy this, you don't worry about anything else, I got it. Now, with Fargate, this is probably going to be the easiest way to scale within ECS. You literally just have to create additional tasks and set your task limits, and ECS does it all for you. Now, with that being said, Fargate is much more expensive when you compare it to EC2 launch types, depending on the amount of containers you are running. Let's move on to a high‑level architecture comparison here. On the left side, as you can see, we have EC2 launch type, and on the right side, we have the Fargate launch type. Assuming we have two different clusters here, the EC2 launch type is where you specify, again, an EC2 to actually launch and host your containers and the agent. So when you choose your instance, they offer ECS optimize AMIs, which are owned by AWS; however, you can also create a custom AMI. The only thing to keep in mind is it has to support and run that ECS agent. On the Fargate side, it's a lot different, you just have the tasks or the containers that are running within your cluster, and everything else is abstracted from you. Now, with EC2, the benefit is you can host multiple containers on a single instance, so that's where you get a lot of the cost optimization. In Fargate, each separate task is built individually, so that's why the cost can kind of scale up if you have thousands of tasks running. Let's actually look at some scenarios where you might choose one launch type over the other. EC2 is very good for large, long‑running workloads where you want to be sure that the price can be optimized when needed. Fargate is good for large workloads where you want to be sure that you can better optimize operational overhead and resource management. So, for instance, maybe you don't want to have to manage any servers, but you need to run containers. Fargate is also good if you have small, burstable workloads that are temporarily running, and they don't have resource‑intensive requirements. So something really quick that needs to spin up, complete something longer than some other services can handle, and then spin back down. And then lastly, Fargate is also good for periodic, batched workloads where you have to perform a containerized action or process for a short time, and then you can shut it down. The exam tries to trick you a lot of the time on when to choose certain services and launch types. This is a perfect example of when to use Fargate. Now, for the exam, you also have to be familiar with cost‑savings approaches. With ECS, you can use ECS spot instances to host your containers. Now you save a lot on the cost, but please remember the cons that go along with spot instances. The biggest one by far is the fact that they can be terminated at any given moment, so it's not good for production workloads or critical applications. And then, you can also use compute savings plans. This is equally important. Remember, you can use these savings plans and leverage them to save on Fargate costs, which can run within your ECS clusters. Now, moving on to an exam scenario before we wrap up. Let's assume you have a scenario where you have to run applications in containers on ECS. However, maybe you have a small team and you do not want to be responsible for provisioning and managing any infrastructure hosting and running the containerized workload. Well, this is a perfect fit for an ECS cluster running on AWS Fargate launch types. That's going to go ahead and do it for this clip. Let's end here, and we'll move on to the next one.

Amazon ECS Tasks, Task Definitions, and IAM Roles
Up next, let's dive into some more specifics within Amazon ECS including tasks, task definitions, and IAM rules. First up, let's review some important concepts that you have to be familiar with when you're in AWS using this service. The first is a task definition. This is going to be a JSON‑formatted document that is basically serving as a blueprint for your containerized applications. Within this document you specify many critical configuration options including the roles, the launch type, the amount of CPU and memory, you configure the networking settings like the VPC and subnets, and you even include things like environment variables, and much, much more. This is a very complex document that allows you to customize at a very deep level. You also specify a task role. Now a task role gets assigned to each task, and this is completely optional. The containers actually running within the task assume this role, and that allows them to perform API calls. So some examples could be getting objects from S3, maybe you need to use KMS keys and decrypt, or maybe you need to pull messages off an SQS queue, so you give them the ReceiveMessage permission. The big thing to remember is the task role gets assigned to the task, which is essentially the container running your application. So this is what you would use for application requirements. The other side of this is the task execution role. Now this gets confused with a task role quite a bit, and that's because of the naming structure, I think. This is where you actually create a role, and it allows the containers on the outside and the agents themselves to make API calls. In other words, the service itself is using this role. Now you would use this for things like pulling images from an ECR repository, or getting sensitive data from a service like Secrets Manager or maybe Parameter Store. Moving on, let's look at the tasks and the roles architecture. On the left side here, we have our ECS cluster on an EC2 instance. We have a task execution role here at the top, which is configured to be used by the ECS Agent, which is essentially the EC2 host. On the inside of there, we have our container, and we've assigned a task role to that container with our task definition. So within this, the task itself is going to use the actual task role, which is running within the container on ECS, and this allows ECS to perform different actions. Now with ECS on EC2, it's important to understand you can specify multiple tasks per instance, and you can have multiple task roles on the same instance, since it's assigned to the container application itself. The task execution role is what's allowing the ECS agent and the instance to pull the image from our ECR repo, and then it can launch that image as it's needed. Now if you're using Fargate, then the Fargate orchestrator uses this role instead of the ECS agent, but it functions the same way. So once we pull the image, we create our task, it's got a its task role. Well, that task role is then allowing us to perform actions within our application. For instance, we need to maybe get an object from S3. So this is just a way to visualize those explanations we just covered. The big thing that people get confused for this exam is the task execution role and task role. Please understand where you use both of them. Moving on to another very important topic here, passing in sensitive data to your containers. Typically when you're running containerized workloads, you might have some important or sensitive information you need your application to use, like maybe an RDS connection set of credentials, for example. Well, in this instance, let's assume we have a cluster using the Fargate launch type, and we've assigned a task role to our tasks, and the task execution role is being used by the Fargate orchestrator. Well, the task execution role is where you grant the permissions to interact with Secrets Manager or maybe Systems Manager Parameter Store. So on the right side here, we have two different things. We have in Secrets Manager, a database password that's encrypted, and in Parameter Store, we have a parameter with path‑based hierarchy set up. You can see we have db_password at the top, and then we have db_username on the bottom. So to perform that GetSecretValue and GetParameter, we have to give the execution role the permissions. Now, to actually configure these options. In the container definition, it contains a reference using valueFrom within the document. This valueFrom allows you to specify the ARN of each of the values that you want to pull from. So you can see here on the bottom of that, valueFrom for the db_password, valueFrom for the Parameter Store parameter. The big thing to remember here is you need permissions to get the secret and get the parameter, and you specify the ARN for the value of the valueFrom key. What this does is it then injects those sensitive values as data and environment variables into your container, and you can use them within your applications to connect to secure or private resources, for instance, maybe an Amazon RDS database. So this is a very simplified way and secured way to pass in sensitive data to containerized applications. Let's actually talk about non‑sensitive data. There's a couple different ways you can do this that you should be familiar with. First is a task definition for just using the environment key. Within here, you specify the name of the environment variable and the value in plain text. This method itself is good for passing individual environment variables in where you don't have a lot to manage. Now you also have the environmentFiles option. So in this one, within the field, for the value, you specify the ARN of the S3 object, as well as the type, so you can see it says s3. What this does is it pulls this .env file into the containerized application and loads those environment variables for you to reference. This method is best for passing in bulk environment variables, in other words, where you have a lot to pass in and configure, or maybe you have different values based on environment. Now real quick, let's talk about an exam scenario. Let's assume you have an application running on ECS. Within it, the application creates files, and then it needs to store files in S3. Well, for that, create an IAM role with the required S3 permissions and trust the service, and then you can set that role as the task role in the task definition. Now that's going to do it for this clip. Let's end here, and I'll see you in an upcoming one.

Load Balancing Amazon ECS
All right, we talked about some of the important components within ECS, let's go ahead and move on now and talk about load balancing our workloads in the ECS service. One of the nicest things with ECS is that you can specify services and then easily load balance those services using a managed load balancer. Now, a service, by definition is a feature in ECS that allows you to maintain a specific or specified number of instances of a specific task definition within your cluster. Services are perfect for maintaining high‑availability for your containerized apps. If a task fails or it stops for any reason within your service, the service scheduler itself is going to try and launch another instance of your task definition to replace it. In addition to that, ECS services allow you to easily integrate with an Elastic Load Balancer so that you can distribute incoming traffic across different tasks belonging to the service. With this, you can have different services made up as parts of different target groups for the same load balancer. Now, regarding supported load balancers, ALBs, NLBs, and Classic Load Balancers are all supported. However, you really should avoid CLBs because they offer a limited functionality, and they don't even support Fargate, so if you want to use Fargate, you can't use a CLB. Moving on to Application Auto Scaling. This is an AWS service for automatically scaling your scalable resources for other services besides EC2. Application Auto Scaling is commonly used with your ECS clusters and services to handle scaling for you. Building off of that, an exam pro tip: you can leverage application auto‑skilling with your ECS service, and honestly, you should always use it when you can for cost optimization and performance efficiency. And, speaking of scaling ECS services. Amazon ECS publishes CloudWatch metrics with a service's average CPU and memory usage. You can configure your Service Auto Scaling and Application Auto Scaling to leverage these metrics for their scaling policies. So you can set to an average CPU requirement, average memory usage, etc., and scale based on those targets. By using the CloudWatch metrics, it allows you to quickly scale out your service or add tasks during high demand, and scale it in or remove tasks to reduce costs during periods of low demand. In other words, in addition to being performant‑efficient, it's also cost‑optimized. Some of the popular types of scaling that are supported here are target tracking, step scaling, you can scale based on schedules, and you can use forecasted scaling. These are popular examples that come up on the exam. And speaking of exam, let's have a very quick exam scenario. Let's assume you're running an app on an ECS cluster running on Fargate. You're actually monitoring CPU and memory usage in case of traffic spikes, and you want to reduce costs whenever utilization decreases. Well, with Application Auto Scaling, you can leverage target tracking policies to scale ECS clusters and services whenever metrics trigger an Amazon CloudWatch alarm or whenever they get down back below a threshold that you set. The big thing here is use metrics to scale appropriately for cost optimization. Now moving on to one last important feature here, dynamic host port mapping for running ECS on EC2. In this abbreviated task definition, you can see several important components, but the big thing we want to talk about is right here, the portMappings field. In this, you specify the container port, in other words, the application port that is working, and then for hostPort, you would specify a 0. What this allows us to do is leverage dynamic host port mapping automatically, which is a major feature enhancement. It's especially useful for when you're leveraging an ALB in front of your services. What it does is once you specify this, it allows the host port to be dynamically chosen from an ephemeral port range on the container instance, so the EC2 instance. On an ECS‑optimized AMI, it's going to range from 32,768 to 61,000. So any of those port numbers in between could randomly be chosen, and what's awesome is ECS handles it for you, so you don't have to register targets. So, if we look at this in a high‑level architecture, we spin up a couple containers with an ECS using our task definition and dynamic host port mapping. Automatically, ECS creates the service tasks on the EC2 instance, and it automatically dynamically assigns the port on the host from a valid range. Once that port is registered, it actually registers the tasks within a target group for the Application Load Balancer. What this does is it allows you flexibility and way easier automation when you're deploying numerous containers on a single host. So now, instead of having to worry about assigning your different ports appropriately, you can leverage ECS to do it for you by setting up this one specific configuration option in your task definition document. So now the ALB, let's say, is listening on port 443 for HTTPS, and it can automatically send traffic to your instance in the target group, which has the ports registered via the ECS service and cluster that you've created, and it's all handled for you. Now that's going to do it for this clip, let's wrap things up, and I'll see you in an upcoming one whenever you're ready.

Demo: Creating an ECR Repository
All right, I think that's enough talking for now. Let's go ahead and start doing some stuff. In this demonstration, we're going to work on creating a private repository where we're going to push our own Docker image and enable Scan on Push, and we're going to build off of this with a later demonstration, but for now, let's jump into the console and begin. All righty, I've loaded up my hands‑on playground here. A few things I want to review with you as far as infrastructure goes before we continue. First off, I've already created a custom VPC, as you can see here, and what it has is two subnets across three AZs, as you can see here, with the respective routing and network interfaces, like a NAT gateway and an Internet gateway. This is the VPC we're going to use to deploy our different services. Now, in addition to a VPC, I've also created already an Aapplication Load Balancer called EcsTesting. This load balancer is internet‑facing in our public subnets, as you can see here, just take my word for it, these are public, and we've spread it across all three AZs. With this load balancer, I've already created a listener and a default target group, so it's listening on port 80 for HTTP, and one important thing I want to call out here, this target group here, the default one, is set to IP target type. This is very important if you're going to use this or follow along. IP target types are required for Fargate containers. If it was set to instance or some other type, this won't work, if you're following along at least. If you're doing this on your own, feel free to play around, no biggie. Now with that being said, we have our target group, we have our load balancer, and I've created this AllowHttp security group, which is going to allow all HTTP traffic from the internet, and we're going to reference this on our containers when we get that far. For now, though, what I'm going to do is go to ECR. The first thing we have to do is create a repository to host our custom Docker image. So under registry, repositories, I'm going to create a new private repo, and I'm going to call this pluralsight‑app. Awesome. Next, we can select our tag mutability. So do you want to allow tags to be overwritten, in other words, mutable? Or immutable, where you want to prevent them from being overwritten? This comes down to requirements in the scenario. I'm going to choose mutable because I don't really have a preference. After that, we have encryption. So do you want to use the standard default AES‑256? Or, does your scenario require KMS? Remember, KMS allows you to create your own key, customer‑managed, or use an AWS‑managed one. We will dive deeper into KMS in a separate course, but understand this allows you to create a cryptographic key for encryption at rest. For this, I will choose the default because I'm not really worried about it. And the next thing we have is image scanning settings. You'll notice it says it's being deprecated, but what's really happening is it's moving it to the registry level instead of each repository. So what we're going to do, though, is select Scan on Push to enable this for now. What this does, remember, is you can have your images get scanned as they get pushed to your repository to look for any vulnerabilities. So I will enable this, I'll click on Create, and there we go, we now have our repository. So with this created, what I can do now is I'll select it, and then on the right, you'll notice we have this nice little shortcut, View push commands. So what I'm going to do is run through all four of these steps, and we're going to create our image, push our image, tag our image, etc. First thing we do here is copy this command, and you'll notice we're running the AWS CLI ECR namespace. Now, really quickly, I just popped it up on the screen, I've already configured my CLI locally with my cloud user credentials. You have to do that if you're trying to do this on your own. Remember, you have to configure your CLI creds. After we do that and we run this command, we run the ECR, get‑login‑password, it goes ahead and pipes that to the next Docker login command to log us into our repo. So let me jump to my terminal, I'll paste this in here, I'll go ahead and make it a little bit bigger, go ahead and paste, and we're going to get a login succeeded here in a moment. Perfect. So this logged us in, we're now authenticated to that repository. Let me clear this, go back. Next thing we have to do is build our image. So I'll copy this, I'm going to paste it into my terminal here, and this is building our Docker image based on this Dockerfile up here on the top left, which is extremely simple. I'm pulling the default Alpine image for the Apache Web Server and copying in my own custom HTML file. This HTML file is over here on the right, and again, it is very simple, these both will be available for you to use if you want to in the module assets. For now, though, we see that it built, I'll clear this, let's go back and do the third step, and this is where we're going to tag our image so we can prep it to push to our repo. So now if I run a docker image ls, we see our images. I'll go back, I'll copy the push command, and we push it to our repo. So this will take a few seconds just because this connection is a little slow, but it's starting to push the layers in the image to our repository. So now if I go back, close this, eventually what will happen is we will get our image in here. So what I'm going to do is cut forward once this is ready. All right, that took maybe 30 more seconds, I would say, and we have our image, we have our latest tag, the type, date, size, etc. So now if I go in here and select this, I'll scroll down, we can see our Scan on Push settings. So we have nothing in here, and when I say nothing, it just means nothing categorized. You'll see here, we have undefined severity levels for these two CVEs. So these two vulnerability potentials were found on that Scan on Push. So this is a very handy feature, and that's how it works. All right, and with that being said, let's go ahead and wrap up this demonstration, and we're going to build off of this in an upcoming demo where we're going to start deploying ECS resources.

Demo: Creating a Load Balanced ECS Cluster
All right, so we just got done with another demonstration where we created an ECR repository and uploaded our own custom Docker image, let's build off of that now and create a load balanced ECS cluster using that container. All right, so we just got done creating a repo, and with that being said, we have our image here, let's start creating our ECS resources. So what I'm going to do here is go to my Clusters tab, and the first thing we have to do is we need to create a task definition. Remember, your containers will reference a task definition, which is how you define all of the settings, etc. So I'll create a new one here. Let's go ahead and give it a name. I'll call mine pluralsight‑app, and then we get down to infrastructure requirements. So, what kind of launch type are we going to use? You could specify EC2 instances, you could specify Fargate, both, neither, etc. I'm going to choose Fargate because we want serverless, and then we get down to some of the settings here. Now for me, I'm going to go ahead and minimize this as much as possible. So I'm going to go here, select .25, and then .5. After that, we get to a very important thing you have to understand; that is the task role and the task execution role. Remember, the task role gets assigned to your containers to allow them to make AWS API calls. I've already created one here called EcsTaskRole, and I'll go ahead and view it really quickly. What this is doing is simply allowing me to push logs. So if I look for task role, you'll see permissions is a managed policy, which again, allows me to perform, create logs, put logs, etc. Now from a service trust relationship standpoint, it's trusting the ECS tasks service. So, we're allowing our tasks to assume this role to put logs. With that understood, let me close that. We then have our task execution role. Remember, this is for allowing the container agent, in other words, the service basically, to make API calls. This is very important because this is how we allow the service to pull our image that we've specified. So, I used one that was already created for me, and if you need to, you can create a new one, but that's all this is, this was created for me earlier, or it already existed, so I'm using this. After I configure my permissions, I'm going to go down here to our container. So this is where we start specifying container information. I can give my container a name, I'll just call it app, and then we specify the URI. So this could be a public repo URI like the Docker Hub, for instance, or, what we're going to use here is the URI, so I'm going to copy this, I'm going to go back, paste it in, and now we're pulling from our private repo using our permissions that we assigned earlier for the task execution role. After we configure that, I'm going to say yes, it's an essential container, and I'm going to skip down to some of the important settings for the exam. First up, port mapping. So this is where you can add or remove custom port mappings. Now, it's important to know, with Fargate, you're kind of limited, the host port is always going to be port 80, but the container port can be whatever you need. So, if you have maybe something listening on port 5000, 8080,etc., you can specify that. You have to remember the container port is the actual app container, and the host port is on the underlying host. And with Fargate, you're limited in your selection, so I'm going to choose 80 because we're using a basic web server, protocol is good, I'll let it fill out the port name, and the app protocol is good. After that, we can go down here, environment variables. This is important as well. Using this, you can specify static env variables, so maybe I have env, dev, it pulls this value, and it passes it in for the container to use. Or you can use valueFrom. ValueFrom works hand‑in‑hand with Secrets Manager and Parameter Store. Now we cover Parameter Store in a different module in this course later on, and Secrets Manager is covered in a different course. The big thing to know is you can pull from those two services using valueFrom, or you can specify static values. We then have environment variables, so this is probably more common. This is going to allow you to specify an S3 object for your environment file that you want to pass in and load environment variables in bulk. So maybe you have one for development, production, testing,etc., you could point to it, it'll pull it in, and then it loads those variables into your container to use. This, again, is probably more common than statically doing it, but statically doing it is good for testing. Moving on to logging, this is why I added that task role. I am going to use log collection to Amazon CloudWatch, and this is where you can set some of those log stream IDs. I'm going to leave these default, I don't need to change them, but the big thing to remember is that log collection only works with the correct task role. Now in addition to CloudWatch, there are other options, so you can send to OpenSearch, S3, custom, etc. Again, very common on the exam is CloudWatch, and you need the correct IAM permissions to do so. Now I'm going to skip down here and skip over these other optional settings, they're out of scope for the exam. I'm going to skip over storage for now, and we're going to click on Create. All right, it's all done, we have our task definition, now we need to use this. So from here, we have to first create a cluster. Our clusters host our services, and our services create and run our tasks. So I'm going to create a new cluster here, I'm going to go ahead and call it Pluralsight, and then under infrastructure, I want to make sure this is serverless, so I'm going to choose Fargate. However, you could also use EC2 instances with an Auto Scaling group, which could be a mix of on‑demand and spot. Again, I'm choosing Fargate for simplicity, so I'm going to use that, and we're going to move on and I'm going to create. Perfect. Now, what's happening, as you can see, is this is creating a CloudFormation template with the resources that we need. So what I'll do here is I'm going to go ahead and jump forward, and once this is ready, we will resume. Okay, so I cut forward, it is now ready to go, I'm going to select my Pluralsight cluster. Now, within the cluster, remember, this is where we create our services, and again, our services can create our tasks. So I'm going to create a new service down here under the Service tab, and for this compute option, I'm going to choose launch type. For launch type, we're choosing Fargate because, again, we want serverless. However, you could choose EC2 if you used ECS on EC2, or, if you were using ECS anywhere, you would choose external. That's important to remember for the exam. External is for ECS anywhere. Again, I'm going to choose Fargate, we'll leave platform version the latest, and let's get down to deployment config. Under deployment config, we can launch a task, which is basically for launching, performing an action, and then closing, for example, like they say, a batch job. For us, we want a service because we want this to be running at all times and restart it if possible because it's a web application. So I choose Service, and then we get to choose our task definition. Now, for me, I'm going to go to Family, select pluralsight‑app, and we only have one revision. If you wanted to, though, and you had more than one, you could specify a specific revision by using this. After selecting this, we give our service a name. So I'm just going to call this pluralsight, and let's go ahead and move down to the service type. Now, because of our configuration options, we can't choose Daemon, but we can choose Replica, and this is perfect. We want to place and maintain tasks across the cluster whenever possible. Now for us, I'm going to do three desired tasks, and I'm going to turn on AZ rebalancing. I want to see if this service can rebalance and spread these tasks across all three AZs that we have for our subnets. After I've configured three tasks and AZ rebalancing, under deployment options, this is important as well, you can perform rolling updates or blue/green deployment updates. This comes down to your scenario, we're going to choose rolling because that's the default, and I'm not going to perform one anyways. Let me go ahead and minimize, we'll scroll down here and I want to get down to networking. So for networking, remember, we need to deploy to my custom VPC, and I'm going to only deploy these containers to our private subnets. So I'm going to choose Private C, Private A, and Private B, and there we go. Remember that we're fronting these containers with a public load balancer, so that's what we're going to do here in a moment. From there, we can choose or create a security group. I'm going to create a new one, we'll leave the default here, name and description, that's not a big deal, but I do need to change the inbound rule. So I'm going to choose HTTP, I'm going to choose source group here, and I'm going to select my ALB security group that we've already created. So this is allowing HTTP traffic on port 80 only from resources with this security group assigned. After we've done this, I need to turn off public IPs, these wouldn't work anyways, and we get down to load balancing. So I'm going to go ahead and use this, I'm going to turn it on, and let's check this out. For the type, it supports both Network or Application Load Balancer. Of course, we're going to choose ALB, and then it auto populates our container based on the container information we used above, via that task definition. Within here, you see we have our container, and then we have the host port and the container port. Remember, with Fargate, you're limited on the host port, but you can change the container port. Moving down, we can create a load balancer or use an existing. Of course, I want to use an existing, I'm going to choose my EcsTesting, I'll give myself a 5‑second health check grace period, and let's get down to listeners here. Now, in here, we can create a new listener or we can use existing. We already have one, so I'm going to choose existing, select it, and we see that default listener that we've created earlier with the default rule. Scrolling down, I'm actually going to use an existing, so I'm going to specify the default target group. Remember, if this was not an IP type for your targets here, this would not work. So that's something to remember. If you use the default options, this is going to say, hey, you can't do this, you have to create a new target group. But since I set this as an IP target type, this works. So I select the existing target group, I'll leave the defaults, and we're going to go down here and click on Create. Awesome. So the deployment's in progress, and this deployment is what's creating our service and eventually our tasks. So what I'm going to do here is I'm going to go ahead and cut forward once this is ready, and we're going to review and then test. Okay, so, we can start seeing our service here, it's still deploying, which is fine, we can still check this out. Under services, I'm going to select my pluralsight service, and we see we have one task pending. Remember, tasks are your containers, so, we're going to have three of these. Under here, we see some of our different configs, but the thing I want to look at here is our tasks, and we see, there we go, we got three that are being provisioned; one, two, and three, we see the task definition, etc., some more information, and under here, if I select one of these, we can go to logs, and we'll start seeing logs generated, which is perfect. So this is why we configured that task role. This is allowing us to push CloudWatch logs from our container. Now, under this, if I look at networking, we're going to see it's in this subnet, we have this security group, here's our private IP, and now, if I go back to my service, and I load up my ALB here, eventually we're going to start seeing some of these in the target group. So if I go to listeners, let me select my target group here and load it in a new tab, here we go, three healthy targets. Here's the IPs, the port, they're across all three AZs, so this worked out perfectly. All righty, well, let's go ahead and test this out then. What I'm going to do here is navigate to this DNS name, and boom, Hello from your Task! Now, this is going to go ahead and round robin, you can't see it as I'm refreshing here, but trust me when I say this is hitting each ECS task individually. Now, that's going to do it. We just created a load balanced ECS cluster with a custom service in three different tasks spread across three different availability zones. Let's go ahead, we're going to wrap up this demonstration and move on to another clip.

Amazon ECS Storage
In this clip, let's go ahead and start talking about storing data for your Amazon ECS clusters and your workloads. There are several storage options you have to be familiar with for this exam when you're using ECS. Let's go ahead and run through each of them, and we're going to talk about some use cases for each as well. First up, of course, is EBS volumes. You can leverage EBS volumes to store data from your containers on the underlying hosts. These are going to be perfect if you have containerized workloads or applications that might run something like a relational database, so a PostgreSQL or MySQL database. It's perfect for persistent storage to make sure that the database data does not get lost when a container is stopped or may be restarted. There's also Amazon EFS. This is perfect for shared storage between containers. For instance, maybe something like a data analytics platform, or maybe you're hosting a CMS application like WordPress where you want to share data on the back end between a ton of different front‑end containers. Thirdly is FSX for Windows File Server. This is going to be specific when you have .NET applications that might need Windows file storage that is shared between containers. This is a very specific use case, and anytime you see .NET applications and shared Windows file storage systems, I would immediately think this particular storage option. Fourthly, we have Docker volumes. Now, this is specific to Docker itself and not ECS, but you can specify Docker volumes for your container workloads. These are good for sharing volumes on different containers on the same host. So if you're running on EC2, this is a perfect option. Now you might use this option when you're running a microservice architecture for maybe an e‑commerce application. You could set up one of the services to be a caching layer like Redis or Memcached‑D, and the other services might depend on that layer for fast access to frequently requested data. So you can set up the Docker volume to be a caching layer, essentially, for other containers. And then the last one here is a bind mount. Again, this is specific to Docker, and you can specify this in ECS, and this is where you mount a host volume for data persistence on your containers. A use case for this would be you having an application that's running your container that processes data and it generates application‑specific logs. Well, maybe you want to ensure that the logs and the process data are accessible outside the container long‑term, even if the container is stopped, replaced, or restarted. That's a perfect use case for bind mounts. Let's talk about an exam scenario for storage. Let's assume you have to migrate a Linux‑based CMS application to Amazon ECS. The CMS servers must access files in a shared file share. Well, this is a perfect case for leveraging EFS as the data storage platform with your ECS tasks. Remember, EFS is perfect for Linux‑based NFS shares, and it spreads across multiple AZs for high‑availability and redundancy. And then to wrap things up here, let's have a quick exam pro tip. If you need a fully serverless solution for your container applications and storage, AWS Fargate and Amazon EFS offer that fully serverless solution that you're looking for. This allows you to offload any operational burden and just focus on the application code itself and the storage itself. With that being said, let's go ahead and wrap up here, and then I'll see you in the next clip.

Amazon Elastic Kubernetes Service (EKS) Overview
All righty, let's talk about running Kubernetes workloads in Amazon Elastic Kubernetes service, or otherwise known as EKS. Before we dive into the service itself, let's have a quick overview and review of important components within Kubernetes that you must be familiar with. I will say we're not going to review Kubernetes super in depth in this course because it's out of scope for the exam, but we have to understand basic components, so let's look at those now. First up is a cluster. This is going to consist of a control plane and a set of different worker machines that are actually hosting your containerized applications. We then have nodes. Nodes are what make up your clusters. These are virtual or maybe even physical machines that are going to host the applications within your containers themselves. A perfect example in AWS would be an EC2 instance. Within that, we have pods. This is going to be the smallest deployable unit of computing that you can create and manage in Kubernetes. This is what is going to host all of your workloads themselves. Now typically you have one container within a pod, but there are some very special use cases where you might host multiple containers in a pod. That's as in depth as we need to go for this exam, but understand a pod hosts your containers. Next up is a service. So a service is going to be a feature for exposing your application that's running on one or more pods within your clusters on your nodes. Typically, you have several pods that belong to a service, so it's very similar to ECS in that instance. Moving on to part two, three more big components. First up here we have jobs. These are one‑off tasks that are going to run until they are completed and stop. Now, they're going to create pods, and they're going to retry execution until a specified number of the pods are deemed successful. So in other words, if you keep getting failures or pods are crashing, well it's going to retry over and over again until it meets your specifications. We also have an ingress, so this is very, very important for load balancing. This is an object in Kubernetes that sets up external access to your services that live within your cluster. Typically, this is going to be done via HTTP and load balancing via an ELB, specifically an ALB within your clusters. Now they do offer TLS and SSL termination as well, especially if you're using a managed load balancer. And then lastly, you have to be familiar with the storage. Persistent Volumes, or PVs, are probably the most common version on the exam, and what these are are objects for storage in your cluster that are completely independent of your pods and your other resources within the cluster. In other words, they are persistent. With that review out of the way, let's talk about EKS itself. This is the AWS‑managed service for any Kubernetes workload needs that you need to run in the cloud. It is extremely similar to ECS. However, it is specific to Kubernetes workloads only. Kubernetes is typically far more complex of an orchestration tool. So if you need something simple for Docker containers, I would not use EKS. And speaking of that, let's look at some exam tips for EKS. This is going to be perfect if you have Kubernetes‑specific workloads that are on‑prem and you want to move them to the AWS cloud. It's also good for if you're hosting Kubernetes in another cloud, like Microsoft Azure or maybe GCP, and you want to implement a multi‑cloud design. The big takeaway here is it's specific, again, to Kubernetes. If you just need basic Docker container orchestration, I would look at ECS. Let's go ahead and explore a very high‑level cluster architecture diagram in EKS. On the left side here, we have an AWS owned and managed VPC, and then on the right side here, we have a simple, highly‑available VPC that we've created. When you create a cluster in EKS, everything within Kubernetes is done via an API call. Now this is going to be made to the EKS control plane, which is created and deployed to an AWS‑owned VPC. With that, you get an access server endpoint, which can be public or private, which we will talk about securing later on within this module. So what you do is you would make calls to your control plane to control any of your underlying resources. Now when you create your cluster, EKS actually works to inject managed ENIs into each subnet that your nodes and pods are configured to use in your VPC. So this is how it communicates with your actual workloads between the two different VPC types. Now, if we have consumers of an application, well, you can set up an ingress or an ALB specifically to communicate with your services and your pods via HTTP or HTTPS. Now, I know this is very high‑level, but for the exam, you don't need to know super in depth about the architecture much more than that. Now that we have that understood, let's talk about the node types that go along with EKS clusters. There are three node types you have to know. The first is a managed node group. This is going to allow AWS to go ahead and provision and manage your nodes, which will be EC2 instances, and these are used for your EKS clusters, and they get attached to an EC2 Auto Scaling group that's also managed by EKS. So this is exactly what it sounds like. You're allowing EKS to completely manage the scaling, deployment, provisioning, etc. of your nodes and your node group. We then have a self‑managed node. This is what it sounds like. You manage it yourself. So your nodes or your EC2 instances that you deploy using an AMI are configured to work with EKS, and they must contain required components, which are a little out of scope for this exam, but understand you do have to configure them. After that, you're also responsible for registering them to the cluster within an Auto Scaling group, so you're self‑managing your nodes. Self‑managed nodes are good for when you need more control. Managed nodes are good for offloading some of the operational burden. And then lastly, the most operational‑efficient version is AWS Fargate. This is their serverless offering, again, where you don't have to provision, configure, or scale any nodes that your pods are running on. Everything is actually managed for you, and all you have to do is specify what pods and services you want to deploy, and EKS handles everything else for you. Now, Fargate is going to be by far the simplest option in terms of operational management. Moving on, let's talk about cost optimization. Being a solutions architect requires that you can optimize your architectures, so let's go ahead and talk about the two node types that support spot instance pricing. Managed node groups and self‑managed nodes both support spot instance pricing for non‑critical workloads. So if cost optimization is the only thing you care about and your workloads can get interrupted, you can use those two different node types. Last major thing here, let's talk about scaling EKS. There are three components for scaling your EKS cluster workloads. The first thing here is the Kubernetes Metrics Server. By deploying this resource into your cluster, it's going to enable you to use the metrics API to access resource usage metrics like CPU or RAM, and then you can efficiently use those metrics for Kubernetes Auto Scaling suggestions and even actually making adjustments. Now, to make adjustments, one of the methods is a Horizontal Pod Autoscaler. This is another Kubernetes object that is meant to update your workload resources to automatically scale the workload to match the incoming demand. This works very closely together with the Metrics Server to set up appropriate scaling adjustments. And then lastly, there is a solution called the Cluster Autoscaler. This is a managed solution that's focusing on making sure that your cluster has enough nodes to schedule your pods without wasting any resources. The big thing to take here is that it focuses on the nodes and not the pods, so it focuses on a different portion of your cluster for different reasons. Now last thing here, an exam tip before we wrap this up. Kubernetes is a perfect cloud‑agnostic solution for container orchestration. So if you need to shift Kubernetes workloads between clouds, think EKS immediately. That's going to do it for this clip. Let's go ahead and end here and whenever you're ready, we'll pick back up with another one.

Amazon EKS Data Storage
All right, up next, Amazon EKS data storage. For the exam, you must know how you can store data locally within your clusters. Now, a lot of these will look similar to ECS, and that's because they are. Let's go ahead and dive through each of these now. For your EKS clusters, you can store your data on EBS volumes for data persistence. You can use Amazon EFS for shared file systems on Linux‑based containerized workloads. You can also use FSx for Lustre, FSx for NetApp ONTAP, and FSx for OpenZFS. These are all very specific to your workload needs, and I do recommend that you review each of these storage types. Now, I'm not going to review FSx in depth, it's in another course that you should look at, but understand you can use them for your EKS containerized clusters. And lastly, if you absolutely need to, you can configure your clusters and your pods to work with S3 for data storage and data retrieval. Moving on, let's look at some data storage concepts specific to EKS. First up is StorageClass. You specify this to configure the default settings for your EBS volumes that get attached to your cluster nodes. Some examples of what you might configure in your StorageClass object here is a volume type and the encryption settings, like a KMS key that you want to use for default encryption of your secrets and your data. Another thing you can specify, of course, in addition to volume types is IOPS. If you want to set a provision set of IOPS for high‑performance, you can do that as well by defining your storage class. Now, to use EBS in your StorageClass objects here, it uses the Amazon EBS Container Storage Interface, or otherwise known as CSI, and all this is is a driver for Kubernetes that allows support for several of the different storage types we looked at. And then lastly here, an exam pro tip. If you need a highly‑available, scalable, shared file system for your EKS pods that are running on Linux, Amazon EFS is a perfect option. Let's go ahead and wrap up our EKS data storage lesson here, and I'll see you in an upcoming clip.

Securing Amazon EKS
All right, we know how to deploy our EKS clusters and we know how to set up storage, but now we need to secure our clusters. So let's go ahead and talk about securing Amazon EKS. First things first, EKS roles. A service account is a Kubernetes service account that gives your pods the ability to assume an identity. Now, in AWS, of course, this would be something like an IAM role. With service accounts, you also have EKS pod identities. This is going to be how you specifically assign an IAM role to your service account so that they can make API calls to AWS for interacting with other services. An easy way to think of this would be something like an EC2 instance profile; however, it's specific for your pods themselves. Exam pro tip. If you have pods running different services like a front end and a back end, and you need your application and their services to have secure access to different AWS services, well, you can create and assign different IAM role for service accounts for each of those services. These are very similar to task roles in ECS, but these are specific to Kubernetes in EKS. The long story short is you use these for allowing your pods to interact with AWS services. Now, moving on, we looked at this diagram earlier, but let's go ahead and build off of it slightly to show you how this would work for customizing access to your cluster. Remember, each time you create a cluster in EKS, you're going to get a brand new endpoint for your Kubernetes API server within your control plane. This API server is how you work and communicate with your entire cluster and all of your pods. By default, the API endpoint is going to be publicly accessible. What that means is that it is open to the internet and anyone can attempt to access it. You control access via IAM or maybe Kubernetes role‑based access control, which offers a little bit more internal and customized access. Now, if you want to, you can enable private endpoint access. This is an optional configuration that allows you to restrict your API server to only be accessed to internal VPC traffic. This is perfect if you have a VPN in place between your data center on‑prem and your AWS VPC and your accounts, and you want to control access so no one external can even hit it at all. Last thing here, when you enable this private access for your endpoint, what happens is EKS actually creates a private hosted zone in Route 53 and automatically associates it with your custom VPC where the ENIs get injected into. So with this, your pods, your clusters, etc. can communicate with your control plane automatically via DNS, which is a best practice. Now that's going to do it for securing your EKS clusters. We talked about the roles and we talked about enabling private endpoint access. Let's go ahead and wrap up here, and I'll see you in an upcoming clip.

Amazon EKS Distro
In this clip, let's talk about Amazon EKS Distro. EKS Distro, or EKS‑D, is a Kubernetes distribution that's based on and used by Amazon EKS. With it, it has the same versions and the same dependencies that are deployed by EKS. Now a key difference between the two is that this is completely managed by you, which is completely the opposite of EKS itself. Because of this, you can run EKS‑D anywhere, like on‑prem, in another cloud, or even on your own laptop if you can support it. However, with this flexibility, again remember, everything is managed by you, literally nothing is managed by Amazon EKS at all, they're just having this to enable you to go ahead and run EKS‑style workloads on your own. An exam pro tip with EKS‑D here. It's good for self‑managed Kubernetes deployments, but you need to understand the deployment requirements. This is a very, very operational‑heavy workload and deployment method, and typically not going to be a good choice. When we say it is self‑managed, we mean you're upgrading your nodes, you have to deploy your nodes, deploy your pods, etc. There's a lot of work that goes into this. Now an exam pro tip, kind of. This is going to rarely appear on the exam, but you really need to be aware of what it is. And with that out of the way, let's end here, and I'll see you in an upcoming clip.

Amazon ECS and Amazon EKS Anywhere
Up next, we're going to talk about Amazon ECS and Amazon EKS Anywhere. First up, Amazon EKS Anywhere. This is meant to run on‑premise EKS workloads using the same practices and engines as Amazon EKS. It is like EKS Distro, where it allows for deployment, usage, and management methods for your clusters, but specific to your data centers. The nice thing about this and the key difference between this and EKS Distro is that this manages a lot more of the overhead for you; so, managing clusters, spinning up nodes, etc. You're essentially just leveraging EKS outside of the service itself. Now with EKS Anywhere, you get lifecycles. What that means is you get full lifecycle management of multiple Kubernetes clusters, and it operates independently of using AWS. Now let's talk about some EKS Anywhere concepts you should be familiar with. When you use EKS Anywhere, the Kubernetes control plane management is operated completely by yourself, AKA the customer. What that means is that the control plane is contained entirely within a customer data center location, so you're not hosting in AWS, you're hosting on‑prem or somewhere else. In addition to that, because this is outside of AWS and outside of managed Amazon EKS, any cluster updates like patching, etc., need to be done entirely via manual CLI or something like Flux. Now if you use Kubernetes, you know what Flux is for, you don't need to know that for the exam, the big thing to understand here is that any updates and modifications to the clusters have to be done by you, it's not going to be handled by the service. One of the nice things is that it does offer curated packages, which are meant to offer you extended core functionalities of Kubernetes clusters. What these do is they offer you essentially quick ways to get started and start enhancing your cluster workloads. However, one thing with this is if you're using any of these curated packages or want to use them, it requires an Enterprise subscription, which means you're going to be paying money. Now let's go ahead and compare EKS Anywhere to Amazon EKS. EKS Anywhere is managed by the customer. EKS is managed by AWS, it lives in the cloud, they handle a lot of the infrastructure. With Anywhere, the control plane, again, lives on customer‑managed infrastructure, as opposed to an EKS where it lives in only AWS. To run your actual workloads, it can run on things like vSphere, you can run it on a bare metal server, you can actually install and run it on Snowball Edge devices, you can run it on CloudStack, or even something like Nutanix. EKS, of course, is going to run on EC2 and AWS Fargate. Now the last thing here is the hardware. Hardware on the Anywhere platform is customer‑managed. Again, you're responsible for updating, patching, securing, etc. Hardware in EKS is all AWS‑managed. You really want to be sure that you weigh the pros and the cons and the different trade‑offs for using this as opposed to the managed solution in the cloud. Onto the next topic, ECS Anywhere. This is just like EKS, but it's specific to the ECS platform. This is an ECS feature that allows the management of container‑based apps on‑prem. With it, you have no need to install or operate local container orchestration software because it's meant to do it for you. Because this is operating on the same framework as ECS, it enables standardization of your container management across environments. So you can run ECS in the cloud and maybe run this on‑prem, and it allows you to have a standard on how you're managing your containers, even though they're in different areas. Now, it's very important to call out. With ECS Anywhere, there's no built‑in ELB support. What we mean by that is you can't spin up an ALB or an NLB and load balance your services because it's outside of the actual AWS cloud. What this means is that it could be less efficient because you have to manage it yourself. You have to spin up some type of load balancer, or an Nginx server, or something of the sorts, and configure it for load balancing. And lastly, there's a new launch type. Now we talked about EXTERNAL for the previous ECS clips, this is where it comes in. You can use the EXTERNAL launch type for creating services and running tasks under ECS Anywhere. Now let's talk about an exam scenario. Let's say you want to run Amazon ECS and run on‑premise applications in a hybrid environment in addition to that. And with that we're going to say it currently runs on containers and you want to maintain that standard. Well, you can use Amazon ECS Anywhere EXTERNAL launch type for the on‑premise containers. So it allows you to go ahead and manage it using the ECS platform essentially, but you get to control the launch type on your own hardware. Moving on to something called EKS Connector. This is kind of rare to pop up, but it is important you understand what it is and how you use it. EKS Connector is a tool that's meant to register and connect Kubernetes clusters that meet specifications to AWS to be viewable in Amazon EKS, the service. What an example would be is let's say you're running Kubernetes workloads in EKS already, and you have some running on‑prem. Well, you want to go ahead and view all clusters in one location, you can do that using EKS Connector. By configuring it and setting it up, you can now view and essentially manage at a high level, all of your different Kubernetes clusters all within Amazon EKS, so it provides that centralized platform for you to view everything. Now, that's going to do it. Let's go ahead and wrap this clip up here, and when you're ready, I will see you in the next one.

AWS App Runner
Okay, in this clip, let's dive into AWS App Runner. AWS App Runner is a fully managed AWS service that's meant to deploy from your source code or your container images directly to a scalable and secure web application. Let's jump into some concepts on what is important and what you should know regarding this service. The big thing here is it allows you to focus on your code and just deploying or creating your images by directly connecting to the repositories. How it works is after you connect, it will automatically start managing the following. It'll manage your operational deployment requirements, so it's going to automatically deploy your code updates for you. It will work on automatically scaling and even load balancing your workloads for performance. And it implements security, so it's going to implement security groups, etc., to lock down your application for least privileged access. The big thing here is it's meant to be a one‑stop shop for developers or teams that don't have a lot of experience. Now, speaking of experience, let's look at two use cases. First up is a developer use case. It's really good for simplifying deploying your updated versions of codes or your Docker container images. And then we have a DevOps use case. So this can work for automatic deployments every time a commit is pushed to a repository that you're in charge of. Now let's actually look at a high‑level architecture for the service types. There are two different service types: GIT repos and ECR repositories. In this first portion, we're going to assume we're using a GIT repo, and what you do is you create an App Runner connection to connect to your repository and look for different pushes and updates, etc. So now when a developer makes a git push and they update their code here, and it actually gets sent to the repo, that's going to trigger our App Runner connection to start working on deploying our brand new service, which it handles for us completely. With a few configuration steps, it says, hey, I just saw a git push or a git merge, I'm going to pull that, I'm going to deploy that, and here's your new service at service‑a.awsapprunner.com. Now the other service type is an ECR repo. With this, you have to grant an access role to App Runner to give it permissions to pull images from ECR. And in this example, if we have a DevOps engineer that is going to push an image with the latest tag, well that's going to trigger, again a connection, and it would do its best to deploy to its own separate service with its own separate unique URL. So now, we can easily have our users reach each individual service via a friendly URL and all of the deployment, security, and load balancing is handled for us, all we have to do is configure connections to our repository. Now real quick before we wrap this up, an exam pro tip. This is a very good option for when there's an inexperienced team that needs to quickly deploy to production. If they don't have DevOps engineers or they don't have senior software engineers that know AWS, this is a good solution or a good starting point to get them up and running. And with that being said, let's go ahead and wrap up this App Runner clip, and we're going to move on to the next one.

AWS App2Container
All right, let's get going talking about AWS App2Container. App2Container, or A2C, is a command‑line tool that's strictly meant for lifting and shifting your applications into containers within one of the following services. It works with Amazon ECS, EKS, and even AWS App Runner. Now currently it supports the following application types. You can use it for Java applications or ASP.NET applications. So immediately, if there are any other languages besides those two, this is not going to be a solution, that's the easiest way to eliminate it as a possibility. Now let's actually look at the process of using this. How it works is it starts by creating a list of all compatible running ASP.NET and Java applications. Once it has its list, it's going to go ahead and start to analyze the runtime dependencies of those supported applications. Once it has that dependency list in place and the application list in place, it works on starting to extract application artifacts in order to containerize them, and it will work on generating a Dockerfile for later use. After the Dockerfile is generated, it's going to start builds for the application container and work on now generating AWS‑specific artifacts to use within the cloud. Now, these artifacts include several things like CloudFormation templates, ECR images, task definitions, and many, many more. Pretty much anything that has to do with deploying containers in the cloud within AWS is generated, including the Infrastructure as Code to quickly and easily do so. Now an exam pro tip here. You can leverage A to C to create a CI/CD pipeline via AWS CodePipeline to automate your builds and your deployments. CodePipeline is a code deployment pipeline service that we cover later on within a different portion of this exam prep, but understand it does work with it to create a pipeline. And finally, let's look at an exam scenario. Let's say you want to modernize and containerize an on‑premise ASP.NET application that's currently running on a bunch of virtual machines. In addition to this, you want to ensure that you have the least amount of operational overhead and you have to run it in AWS. Well, App2Container can containerize the application, and then the CloudFormation templates can deploy the application to Amazon ECS using the different artifacts. Okay, and with that out of the way, let's wrap this up, and we're going to move on to a module summary and exam tips lesson, coming up shortly.

Module Summary and Exam Tips
All right. Way to hang in there once again. Let's go ahead and have a quick module summary and talk about some exam tips you should take with you into your exam. First up, let's review ECR. Remember, this is an AWS service that allows you to store and manage your Docker images, and you can implement public or private access to your images. It easily integrates with ECS and EKS to deploy containers. You can also leverage lifecycle rules to automate expiration and cleanup of old images as well to save on space and to make it cleaner to look at. And finally, Scan on Push. Remember, you can turn on Scan on Push, which is going to enable scanning for vulnerabilities within your images as they get pushed to your repository. An exam pro tip. If you need to store Docker images or OCI‑compatible artifacts within AWS, you should immediately think ECR. Moving on to ECS. Remember, this is the go‑to orchestration choice for launching and managing Docker containers that run on AWS compute. You need to be familiar with the launch types that go along with this service, they include EC2, AWS Fargate for serverless, and ECS Anywhere, if you want to connect on‑premise ECS‑managed containers. Remember that ECS Anywhere is an external launch type. Now, one of the nice things is that ECS very easily integrates with your Network Load Balancers and Application Load Balancers for load balancing your different services. In addition to that, Service Auto Scaling will automatically handle scaling requirements for you based on demands. And then lastly, remember the following config options. You need to understand the difference between a task role and a task execution role. Task execution roles basically give the agents and the service permission to do things like pull an image. Task roles get assigned to your tasks within your containers, or your applications, in other words, and they allow your applications to actually perform AWS actions, things like putting objects in S3, getting objects or items from DynamoDB, etc. And lastly, remember how to pass in sensitive information and environment variables. You can use the valueFrom scenario to specify you want to pull from Secrets Manager or Parameter Store. Now, moving on to an exam scenario for this. Remember, if you have to run containerized applications and you want to completely avoid unnecessary operational overhead, ECS on AWS Fargate is a great choice. Next up is EKS. This is the go‑to orchestration choice for launching and managing Kubernetes‑specific workloads in AWS. Remember, this is going to be more complex than typical Docker. Kubernetes adds some overhead, but if you need Kubernetes, this is the choice to go with. The control plane endpoint that gets created can be set to public or private access depending on your requirements. If it's public, that means it's internet accessible. If it's private, you have to have internal VPC traffic only. With EKS, understand the different node types. Understand managed, self‑managed, and Fargate for serverless. Managed is managed by AWS, self‑managed is mostly managed by you, and then Fargate gets rid of all underlying infrastructure worries, and you only have to worry about your tasks. In addition to the node types, remember the different storage options and their use cases: EBS, EFS, S3, and even FSx offerings. And then the last thing here, IAM roles. An IAM Role for Service Account, or IRSA, is used to grant API permissions for AWS services to your individual pods. You have to grant your pods the different permissions via these roles for them to have access to your AWS services. Now, three big exam tips for the exam for you to remember. If you need to encrypt EKS secrets anywhere within your cluster, specifically, let's say, for example, etcd, well, you can use a KMS key and specify that for the entire cluster, which will then go through and encrypt that data at rest. You can also leverage Fargate nodes if you need to simplify operational overhead. So if you have a scenario where you have to use EKS, but you don't want to have to worry about the operational overhead of managing the nodes, Fargate is a great choice. And lastly, you can use spot instance pricing if your nodes are going to be using EC2 instances. So whether it's managed or self‑managed nodes, you can use spot instances. Now exam pro tip for EKS. If you need a solution to host Kubernetes workloads in the AWS cloud, I would immediately consider EKS as part of the solution. Moving on to ECS and EKS Anywhere and EKS Connector. Remember, ECS Anywhere allows the management of your container‑based applications outside of the cloud, while still benefiting from the standardization of the container management system across environments. This is the external launch type for Amazon ECS. EKS Anywhere is an on‑premise way to manage Kubernetes clusters with the same practices and same orchestration methods used for EKS in the cloud. And lastly, EKS Connector. You can use this to offer full lifecycle management of multiple Kubernetes clusters, especially if they're operating independently of AWS. You set this up, you connect your clusters, and you can now view them all from EKS. App2Container and App Runner. App2Container, or A2C, remember, is a CLI tool for lifting and shifting your apps into containers into one of the supported services. It supports ECS, EKS, and App Runner. An A2C exam scenario. Maybe you have an on‑prem application that uses Java on Linux machines, and it needs to be containerized and deployed to AWS, and you really don't have a lot of experience. Well, A2C, or App2Container, is a great choice. Then we have App Runner. This service is meant to simplify code and image deployments to the cloud. So again, this is really good for getting right to market very quickly for an inexperienced team. An exam scenario for App Runner would be having a development team using AWS that has no infrastructure experience, but they really want to simplify the deployment of their code and get up and running very quickly. Well, that's a perfect time to use App Runner. Okay, and with that out of the way, that is going to do it for our module summary and exam tips. Feel free to review the material, but for now, let's go ahead and take a break, and I'm going to see you in the next module.

AWS Lambda
AWS Lambda Overview
In this module, we're going to cover the most popular serverless offering AWS has to offer, AWS Lambda. First up, let's have a quick overview of what this service is and where it came from. A long, long time ago in a galaxy far away, we actually used to put computers in a giant warehouse with a bunch of air conditioning and racks, and we called this a data center. Now, back in those days, we would have a ton of administrators and engineers that had to go and take care of those servers, in addition to actually spinning up the applications on the servers themselves. Eventually though, someone a lot smarter than me realized, hey, we can use this technology called virtualization, and we can virtualize computers, and we can run a bunch of different computers on one individual server. So, for a long time, virtualization was really good, people were running things like Oracle, VirtualBox, VMware, they were using vSphere. Life was good, and then, lo and behold, eventually something comes out called the cloud. Now, the cloud was a groundbreaking invention and people were a little skeptical at first. What it offered was the ability to essentially get rid of our physical hardware and just leverage someone else's using their Infrastructure as a Service to host all of our applications, run our VMs, etc. And then shortly after the cloud was popular, someone went ahead and invented something called serverless. Serverless is a feature and a concept within the cloud that allows us to focus on only our code and some small configuration options, and we don't have to worry about managing any of the underlying compute. With that very brief history out of the way, let's look at some benefits of what serverless offers. One of the biggest benefits is ease of use. There isn't really much for us to really do besides bringing our code, turning on a few different configuration settings, and letting our code run loose. AWS is going to handle pretty much everything else for us. It's really, really easy to get started. Next is event‑based. What serverless allowed us to do is implement an architect around an event‑based design. With a serverless design, your compute resources can only be brought online when they are needed in response to a particular event happening. They can run their code, process the event, and then shut down, and you're good to go. And then thirdly, billing. This is a true pay‑as‑you‑go feature in the purest form. You only pay for your provisioned resources and the length of runtime that they're actually being used. For instance, once you spin up a Lambda function, which we're going to cover here in a moment, you don't have to pay for it just sitting there, you only pay when it actually is running and performing some type of action, and once it's shut down and stopped, you're not getting billed for anything else. Now, speaking of Lambda functions, let's look at the primary serverless service within AWS. AWS Lambda is an AWS service that allows you to run code as virtual functions within AWS, and you don't have to provision or manage any of the underlying servers that execute the code. What this means, I'm sure you've realized, is less administrative overhead. I don't think I've ever heard of an admin or an engineer complain of having to do less hardware administration, it's always a good thing for us. Now, in addition to the less administrative overhead, it allows for a very easy way to implement your event‑based architectures. So we just covered that, and Lambda is one of the perfect solutions to achieve that design. It works by only having to invoke your functions on demand. So what that means is only when they're needed, they spin up, they run, and then they spin back down and you're good to go. Now one thing to keep in mind for the exam, and when you're designing around Lambda ‑ they can only run for up to 900 seconds, which is 15 minutes. What that means for you is to remember that these are best for short‑running workloads. Anything more than 15 minutes is directly not going to be a fit for Lambda functions, you need to find something else. However, what it does mean is that they're very good for very quick responses like API calls. And lastly, in addition to being very good for very short‑burst workloads, AWS actually handles the scaling for us. This is one of the biggest benefits. In addition to not having to manage any underlying infrastructure, that also means we don't have to scale our Lambda functions. If they keep getting invoked and they keep executing, AWS will handle scaling out for us. Now that we know what Lambda is at a high‑level, let's look at some features and benefits it provides. With Lambda, you have a forever free tier of 1,000,000 requests and 400 Gb of compute per month for your functions. This is a very generous free tier, and again, like I mentioned, it's a lifetime. With Lambda, when you're using it, it integrates very easily with services like S3, DynamoDB, EventBridge, and SQS and SNS. These are some of the more popular event‑driven design architecture integrations, and you do need to be familiar with them for the exam. Third thing here, logging and monitoring. You can easily log and monitor for metrics using Amazon CloudWatch. In fact, it's natively built into a dashboard for you when you're in the console. In addition to logging and monitoring, by default, these functions run in an AWS‑owned VPC that allows public Internet access. Now, we're going to talk about controlling that access later on, but remember, by default, they have internet access. And then lastly here, you can quickly and very easily scale function resources as you need to. What I mean by this is you can increase RAM and then directly increase virtual CPU power as well. Now moving on, let's talk about deploying functions. There are three ways you can deploy code to your Lambda functions in the cloud. The last two that we're going to cover on this particular section here are considered deployment packages. Now, the first option is directly in the console. When you're first learning this, this is probably what you're going to do a majority of the time, this is where you can directly upload all required code and dependencies via the AWS console. In fact, this is what you're going to see within the demonstrations within this module. The second option is a zip file. You can upload an archived zip that contains your function code and all of the required dependencies for it to work. When you upload a zip, the compressed zip files can be up to 50 MB in size, and once they're uncompressed, they can be up to 250 MB in size. If you have anything larger than 50 MB, when you're uploading your zip file, you're going to want to upload it to S3 first, and then you can reference it when you're creating your function within AWS Lambda. And then thirdly here we have a container image. Now, container images are perfect for building an image locally, uploading it to ECR, and having Lambda pull it. What you're going to use these for are very specific use cases. In fact, one use case could be a machine learning model inference container image where you need to control the runtime environment and bundle all of the dependencies that aren't available in a standard runtime. Honestly, these are really going to be used only when you have very heavy dependencies to package up, and they're not supported by default with Lambda. On the exam, I will say typically container images don't appear as much as typical Lambda functions, and a majority of the time if you're running a container, you're probably going to use something like ECS or EKS. In addition to that, you have to understand the service integrations that are supported by default with Lambda functions. All of these right here are very commonly appearing on the exam, and they easily integrate with your functions to trigger workflows. In addition to understanding that these are some of the more popular services to trigger your functions, you need to be familiar with the following terms. A trigger, which is a resource or configuration option that invokes your function, and event source mapping. Now, event source mappings are Lambda resources for reading items from streams and queues specifically, and they allow you to process messages in batches. Now if I go ahead and I organize these for you, this top group of services here are triggers. The bottom group of services would fall under event source mapping. So you can see SQS, Amazon MQ, Kinesis, etc. These are all considered message queues or streams, so that's why they fall under event source maps. Now real quick before we wrap things up, one exam tip here. If a scenario requires any compute processing greater than 15 minutes, you should immediately think something else other than Lambda, it's not going to be fit for Lambda functions. Let's go ahead, we're going to end this here, and I'll see you in an upcoming clip.

Configuring Lambda Functions
All right, let's talk about configuring your Lambda functions. With Lambda configuration steps, there are several concepts you have to understand to use them properly. The first thing we have is a runtime. This is the language‑specific environment that your code is going to be executed in. What you have to do is you have to pick from an available runtime, or you can bring your own, which we'll look at here in a moment. You also have an execution role. If your function is going to need to make any type of AWS API call to interact with another service, you must create and then attach an IAM execution role to grant those permissions. An execution role is similar to an instance profile, it allows the function to assume the role and then execute those permissions as needed. The third thing is the networking. Remember, by default you have public internet access. However, you can optionally define a VPC, subnets, and security groups for your functions to access private resources. And then fourthly, function resources. This is equally important to everything else. When you're defining your function and configuring it, you have to define the amount of memory that is allocated to your function in addition to the timeout. Now remember, the timeout can go up to 15 minutes, but when you're defining your memory, there's also a scale that we'll cover here shortly, coming up. The big thing to remember here is you can scale memory for your functions. Now moving on to runtimes here. Runtimes are meant to relay different invocation events, or otherwise known as triggers, the contextual information with those, so info about the invocation like the ID, the timing, the source, etc., and responses between the actual service and your actual function. Now, this top group of languages here are all popular runtimes you need to be familiar with, and they're supported by default within Lambda. You can choose them by default very easily when you're configuring your functions and gain access to their specific runtime environments. On the bottom here, we have custom runtimes. So if none of the ones on top fit your use case, you can create your own runtime using any language that you want, and this provides endless flexibility for creating functions. However, you have to understand you have to include the entire runtime in the deployment package or in something called a Lambda layer, which we'll cover later on in this module. Typically speaking, most people use one of the default runtimes unless they have a very specific use case. Moving on to compute and storage config options. The default quota allows for 1000 concurrent executions per region. We'll cover concurrency a little bit more in depth in its own clip, but at a high‑level, it's the number of requests that your function is serving at any given time. In other words, you can have 1000 functions running at the same time by default. Next up is memory. When you're configuring the memory for your functions, you can configure anywhere from 128 MB to 10,240 MB, or 10 GB. You can scale them in 1MB increments. You can also configure environment variables. So if you want to pass in environment variables for reused functions, you can do that. You can pass in up to 4 KB of information to each function. Fourthly here, you also have access to a /tmp directory by default for any storage needs that you might have that are temporary or transient. This /tmp storage can be anywhere from 512 MB, all the way up to 10 GB. So you have a lot of flexibility, but you do get billed for storage if you're using it, so keep that in mind. And then lastly here, it allows for integration to mount EFS file systems locally, if you need it. So, if you have some use case where you're having to use a shared file system, EFS with Lambda could be a great choice. Now exam pro tip here. Your virtual CPU power scales automatically based on the amount of memory that you assign your functions. At 1,769 MB with your function, that means your function has the equivalent of 1 vCPU. In other words, if you scale your memory up, then you're also scaling your vCPU power up. In addition to this, let's go ahead and cover a quick exam scenario. Let's say you're leveraging Lambda functions with environment variables and you want to protect them so only certain users can view their values. Well, one of the big benefits of using environment variables with your Lambda functions is you can enable encryption helpers using KMS keys and encrypt those values so they're no longer plain text. What this means is if someone doesn't have access to decrypt using that key, then they can't view those values at all. Now let's actually look at some architecture examples before we wrap this clip up. These are three common scenarios you have to be familiar with for the exam. In this scenario, let's say you're hosting an application that uploads images to S3 buckets. Once those images are uploaded, you have to process them, resize them, and then store them in a different S3 bucket, and you also want to store metadata in a different key value store. So to start things off, we're going to say your application is configured to allow S3 uploads for your user photos. So once they put an object into your source bucket, and in this case we'll say a 2MB image, that PutObject call is configured as an event notification, and a Lambda function is set as the destination. So once that object is in place, it's going to then trigger our function based on that event notification we configured. Once that function is triggered, it can assume an execution role that you created, assume and get its permissions, and then execute its code. That code can perform many different things, it can put the image in your destination bucket, which is now resized to 500 KB, and it can put an item in the DynamoDB table to track the metadata. This is a real‑world example that can come up, and you need to be familiar with it. Next, we have data processing. In this exam scenario, let's assume you're working on a banking application in AWS and you have hundreds of thousands of users during your peak hours. What you need to do is implement a scalable, near real‑time solution to share the details of the different transactions with other internal apps. You also need to process them to remove sensitive data before storing them in long‑term, cheap storage. Well, again, let's assume you have an application and it's configured to stream hundreds of thousands of data points to an Amazon Kinesis Data Firehose. So your application is configured, it's streaming transactions into your delivery stream because it's Auto Scaling in near real time; well, that Firehose delivery stream directly integrates with Lambda to perform data transformations on streaming data. In other words, as the data is coming in, it's invoking a data transformation function that you've created, and that's going to go ahead and execute code to scrub or remove sensitive information as the data is being streamed. Once the data transformation is complete, it can then be placed into the destination bucket, and you can have it there for long‑term, cheap storage. And then the last scenario here, CRON scheduling. Let's assume you have an application that's only used for 8 hours during each workday. Currently you have an RDS DB instance that's used to store information that the application uses. How can you minimize costs? Well, in this one, we would create an EventBridge rule. So you can set up your EventBridge scheduled rule to run at the end and at the beginning of each workday. For the target, you would set it as a Lambda function. So once you have your CRON or rate‑based schedule in place, it can then trigger and invoke your function based on that schedule. So in this scenario, it would be twice per day, once in the morning and once at night. In addition to that, you can pass in the database instance to be referenced using environment variables, so your function can go ahead and get its environment variable and know, hey, I need to look for this RDS instance. Once that function is invoked, it pulls in that environment variable value, and it can run its RDS commands, Start or StopDBInstance. This is another real‑world example on how you can minimize costs using event‑driven workloads leveraging Lambda functions. Now that's going to do it for this clip. Let's go ahead and wrap things up, and whenever you're ready, we can move on.

Demo: Creating and Invoking a Lambda Function
Okay, let's take a break from theory‑based lessons, and let's dive into a demonstration where we're going to create and invoke a Lambda function. Before we jump into the console, let's have a very quick high‑level overview of the architecture. What we're going to implement here is a common exam scenario. We're going to set up a source bucket where we're going to put an object. That source bucket is going to be set up as an event trigger to invoke our function, which will then trigger the function to go ahead and perform some data transformation. Once that data transformation is complete, we're going to go ahead and upload the new object to a different destination bucket, and it should all happen very quickly in an automated fashion. So, with that out of the way, let's jump into the sandbox now and begin. All right, I've loaded up my playground here in my sandbox environment, I'm logged in as cloud_user in us‑east‑1, let's go ahead and begin. I've already loaded up the Lambda functions list here, and before I begin creating a function, I want to show you the resources I've already created. Under S3, I've created two buckets, our source bucket and our destination bucket. These are going to be what we use within our Lambda invocation code to go ahead and trigger and act as a destination. So with that understood, I'm going to go back to Functions here, and let's create a new function. Now for this, I'm going to author from scratch. However, one of the nice things is you can use a blueprint, so they have stuff that's already designed and ready to implement, or you can use a container image, it's all going to depend on your use case. For this, I'm going to author from scratch so we can look at all of the settings that we've kind of covered already in theory lessons so that we truly understand how to make these. The first thing we have to do is give our function a name. I'll call it ConvertFiles, and then we choose the runtime. So remember, the runtime is the environment that your functions are going to execute in. This is very important. You have to choose the one that supports your language. Now for me, I'm going to choose Python 3.13, and that's because I'm going to use Python. Now, if you want to write Node.js, Ruby, etc., you can choose other supported runtimes, and the nice thing here is they include older ones in case you're using libraries that are not supported on the newest version that's supported. So for me, I'm going to choose Python 3.13, I'm going to leave the architecture the same here, and then under permissions you can see it creates a default execution role for us. What this does is creates a brand new role with basic permissions, which we will look at here in a moment, but you can also use an existing role, or you can generate a new one based on a policy template, and they give you a list of stuff that you can use to pre‑populate your permissions in your policy. What I'm going to do is say create a new role, and then we're going to go back in and edit that. So notice down here, Lambda will create an execution role named ConvertFiles, and then role with a random string at the end of it with basic permissions. Now I'm going to skip the additional configurations, we'll cover some of this in a different particular clip, and I'm going to create my function. All right, perfect. So we created our function here, I'm going to go ahead and clear my banner, and now we get down to our configuration screen. So by default, it loads the code. So this is where you can upload code by copying and pasting. However, you can also upload from a zip file, so if you have dependencies you have to upload. Or you can load from an S3 location, so you point it to an S3 URL that contains your zip file. Now we're just going to upload to the console here to make this simple. So let me go ahead and I'm going to jump into my code editor. This is going to be the Lambda function code we use. So what I'll do here is I'm going to copy and paste this in, and then I'm going to click on Deploy down here. Now deploying is how you actually set your latest function. So now our Lambda function here, with this function ARN, will go ahead and leverage this code when it's invoked. Now what I'll do is go back to my code here and I'll go at a very quick pace to review what this does. First thing I want to say is all of these files will be available for you if you want to go ahead and do this on your own, so feel free to grab them from the module assets. But at a very high‑level, all I'm doing is creating a logger to log information to CloudWatch, I'm creating an S3 client to interact with S3, and then the Lambda handler is where all of the magic happens. We're parsing our event to get the bucket name, we're getting the object key, and then we're logging some information to CloudWatch, so the event itself, and the context object. After we log this information, we then do a try accept block here. What we're doing is we're going to use a CSV file to trigger our workload. We're then going to convert that CSV to JSON, we're going to get our destination bucket and then save that new JSON file to that destination bucket. So, this is a simple Python function, but it's good to demonstrate event‑based workloads. So with that out of the way, let me go back to my console, we have our code deployed, and let's go ahead and go down now. So I'm going to click on Configuration. Let's run through some of the general stuff you need to know at this point in the module. First up is general config. So under here we can give our function a description, we can alter the memory, so remember it can go from 128 to 10,240 MB, and they go up in 1MB increments. Another important thing to remember is that as you scale up your memory, you also scale up the virtual CPU that your function gets. So, if there's ever a scenario where you have to increase virtual CPU for your functions, that means you have to increase your memory. We then have ephemeral storage, which we're going to skip for now and leave default. We're going to skip over SnapStart because we cover that later on. And then we get down to timeout and execution rule. So remember, for timeout we can go all the way up to 15 minutes, so as soon as I go past 15, well, you can't do that, the max is 15 minutes. So for this, what I'm going to do is set it to 0, and then 30 seconds. This should be more than enough time for our function to spin up, convert our simple CSV file, and then send the JSON copy to a different bucket. If your timeout is too low, your function will error out, and you will see that in the monitoring. And then lastly here, we have execution role. Now we already created our role via that first screen where we created our function, and you can see that here. So what I'm going to do is click on Save, and now we've edited our general configuration for our function. The next thing I want to skip down to here is Permissions. So remember, like I just said, we have our role defined here, and all it does is allow CloudWatch interaction. So it's allowing us to create a log group in us‑east‑1, and then we can CreateLogStream and PutLogEvents for our particular Lambda function's log group itself, nothing else. Well, this is not going to work for us because we need to trigger our function with S3, so we need S3 permissions. So what I'm going to do is select this role, and once this loads, let's go ahead and edit this execution role. We see our basic Lambda policy execution permissions here. So for this, I'm going to create an inline policy. Under JSON, I'm going to go back to my editor, and I have a simple JSON document here that I'm going to go ahead and paste in. Again, this is available for you if you want to test this. First thing we're doing here is getting an object from an S3 bucket. So this is going to be our S3 bucket ARN. So under S3, I'm going to copy and paste my source bucket name, and there we go. So now we're getting all objects from this ARN. The second one is putting an object. So we're putting objects within our destination buckets. So I'm going to do the same thing, but for our destination. So copy that, paste this in here, and there we go. So now I can click on Next, give it a name, and click on Create policy. So now we have all of the permissions that are required for this particular use case. So if I go back here, I refresh, we're going to see we have S3 in here as well now. So this is perfect. The very next thing I want to show you here, if you remember under code, we're pulling an environment variable that contains our destination bucket name. So because of this, I have to create an environment variable for our Lambda function. So under Configuration, Environment variables, I'm going to click on Edit and add a new one here. So for now I'm going to paste the value, and then its destination bucket is the key. So now we've made this a referenceable environment variable within our code. Now one thing I want to call out, you can enable encryption in transit via helpers, which are going to use KMS keys to encrypt our environment variables. The one thing to notice with this is when you do this, you have to decrypt this environment variable in your function, which means you need permissions to decrypt using the KMS key. Now I'm not going to do that, I'm going to disable this, but this is a good thing to know for the exam. If you need to encrypt environment variables, you need to leverage helpers for encryption in transit. So what I'll do is go ahead and click on Save, we've created our environment variable here for our function to go ahead and reference, and now we can get on to creating our event notification. So right now we have no triggers, so let's go ahead and set that up. I'm going to go to my source bucket, I'm going to click on Properties, find event notifications down here, and create a new one. Now for this, we're going to go ahead and go to the top here and give it a name. I'm just going to call this csv. We're not going to give it a prefix, but we're going to only look for .csv files, nothing else should trigger this event notification. So when we upload a CSV, it should trigger this, and we're going to configure it to send to our Lambda function. So for event type, I'm going to say all object created, so any put, post, copy, etc. should trigger this. However, it's good to know the other S3 event notifications are also supported. Now I'm not going to cover these, these are in a different course within this learning path, but be sure you're familiar with these different options. For now though, I'm going to skip down to destination. Under destination, I'm going to choose Lambda function, and I'm going to choose our convert files function. I'll click on Save changes, and there we go, we now have set up our event notification for our Lambda function. So what I'll do here is go back to buckets, load source bucket, and under my functions here I'm going to go ahead, I'm going to go to triggers and refresh, and now we'll see our S3 trigger here. Perfect. Now one thing I want to call out, this likely doesn't come up on the exam, but this is a very good thing to know when you're actually doing this in the real world. What this has done since we're in the console is it's created a permission for S3 to invoke our function. What that looks like here under permissions, if I go down, you're going to see a resource‑based policy statement. So this is attached to our Lambda function because it's resource‑based. What it's done, if I look at it, it's saying, hey, the principal S3 is allowed to invoke our function as long as it's from this source account and the ARN is like this. So it's essentially saying, hey, our source bucket can trigger this invoke function as long as all of these conditions are met. This is created for us in the background when you do it in the console. If you're doing this as Infrastructure as Code, you will have to create this separately. Please remember that. But, with that out of the way, we should have everything in place, so what I'm going to do here is I'm going to go to source bucket, I'm going to go ahead and drag and drop a CSV file in. So what I'll do here is on the side of my screen here, I'm going to drag in dataset1.csv, I'm going to go down and click on Upload. So now if I close this, I go to my destination bucket, it should probably already be there, and it is. That's how quick it converted and reuploaded that file. So just to show you, if I go to my IDE here, this is what the CSV file looks like. However, if I go ahead and download this JSON file, and I open it up in my IDE as well, so I'll go over here, let me drag and drop it in, dataset1.json, I'll go ahead and format this, it's converted our CSV file to a JSON object. You can see all of the data is the same, so it worked perfectly. Let me go ahead and go back to my console, and I want to show you the CloudWatch log. So under Monitor in our function screen, you can see all of the metrics that are measurable. Now these haven't populated yet due to the amount of time that's passed, but what I can do here is click on View CloudWatch logs. So what this'll do is it's going to take us to our ConvertFiles log group that was created by our function based on our permissions. Within it, it created a log stream, and within this log stream it put log events. Now let's run through what this looks like. We see the init, so it started up, we see our runtime, and the runtime version ARN. We see it found credentials in environment variables. What this is saying is saying, hey, I found credentials because you attached an execution role to this Lambda function, and I'm using those credentials to perform AWS API calls. And then we get to the good stuff. So it started, and now we can see events received. So I printed this out as a JSON dump, and you can see this is what the event looked like. We see our records, and then within records we have our actual event here. We can see the event source is S3, we see the region is us‑east‑1, and we see other information like eventTime, eventName, etc. Now within this S3 object down here, we see where we pulled some of that data. We got the bucket name itself, we got the object key, and then that's what we use to get that object, convert it, and reupload it. Now in addition to the event, we also printed the context. So this is a specific object for Lambda functions, this is not a JSON object. But you can see it contains some important information. It gives you the log group name, the log stream name that it created, which is 4a81, as you can see here, 4a81, and then a few other information tidbits, so the memory limit, function version, etc. This is just good to reference if you need information about the invocation and the function at a high level. And then the last thing here is I logged out that I transformed the dataset from CSV to JSON. So now since this function is running, it's considered warm, so it's ready to go. What that means is if I go back to buckets, I'll go into source bucket, let me drop in dataset‑2, I'll go ahead and upload, this should already be done again, so under our destination, we'll see our second object, and we're going to see in here that it's going to log to the same stream, and that's because this function is still warm, we just executed it, so it stays here. Now if this execution environment was shut down, we would have what's called a cold start. A cold start would start a brand new log stream, and it's going to take a few more seconds to start up compared to this. However, we get the same information, we see the events, we see dataset‑2.csv, etc., and we've converted to a JSON document. Now, that's going to do it for this demonstration. Feel free to play around, use the module assets to do this yourself, but for now, let's go ahead and wrap things up, and I'll see you in the next one.

Lambda Function Networking
All righty, let's talk about networking for your Lambda functions. Remember, this is important. By default, your Lambda functions get deployed to an AWS‑owned VPC that has Internet access enabled. So, that begs the question, how can we provide our Lambda functions easy access to private resources in our VPCs? Well, luckily for us, we can configure VPC access with our functions. In this diagram, let's go over an exam scenario that could come up. Let's assume you're in a company and it has a private application that's living on‑prem that your functions need access to. In addition to that, let's also assume you have an RDS database. The RDS instance is living in a separate private subnet in your VPC, and of course the application is located on‑prem. Well, when configuring your Lambda functions, you can configure them for VPC access. How it works is you specify a VPC, you specify a security group, and you specify the private subnets that you want to grant access to or deploy your functions in. Now it's important to call out just for the sake of knowing in the real world, I don't think this will come up on the exam, but you have to deploy these to private subnets. Please remember that if you're ever attempting to do this. Now, to access the on‑prem API or application, you can leverage the Direct Connect or maybe a VPN connection that's in place via that VPC connection in that VPC config. So if you have the routing in place, you have the Direct Connect or VPN connection in place, the Lambda function can easily use that private subnet and route traffic back and forth over that connection. In addition to that, you can then leverage the security groups for your RDS database instance, and you can configure it to only allow that specific engine traffic from the specific Lambda security groups in your private subnets. This is a best practice, and you need to know how to do that for the exam. You want to reference source security groups whenever possible. One last thing to call out here that's very important. If you need internet access from these functions, you have to deploy and use a NAT gateway in a public subnet. Now there's some instances where you want no internet access. Well, on that point, then, you would not want to route to a NAT gateway, and you'll want to make sure you pick a subnet that's truly private with no route to the internet at all. Now, moving on, let's talk about VPC use cases for your functions. It's very useful if you require access to a private RDS or Aurora instance or cluster that lives in your VPCs. It's also good for accessing EC2‑hosted applications, or, like we just looked at, on‑premise applications. And then thirdly, it's really good if there's ever a security requirement to restrict all internet access for your services. So again, remember, you deploy these into private subnets, and if you don't want it to have any internet access, you can easily control that via route tables, network access control lists, and your security groups. Now, with that being said, that's going to do it for talking about VPC access for our Lambda functions. Let's end here, and we'll move on whenever you're ready.

Demo: Configure a Lambda Function for VPC Access
All righty, time for another demonstration. In this demo, what we're going to do is we're going to create a new Lambda function, and we're going to configure it for VPC access. Real quick architecture overview before we jump in. What we're going to do is we're going to have a two‑tiered architecture already in place for us, we're going to have public and private subnets. We're going to use the private subnets to do two things. We're going to explore a previously deployed EC2 instance that is hosting a web server. This web server is locked down to only allow VPC traffic. After we verify that EC2 is up and running, we're then going to create a new Lambda function, and we're going to configure it to leverage private subnets within our custom VPC, as well as a new security group. Once these are all in place, we're then going to test connection between the resources. With that understood, let's jump into the console now. All right, I'm in my playground environment here in us‑east‑1, let's go ahead and review the instance before we begin creating our function. What I've done here is I've created a private server in a custom VPC. You can see it has no public IP, and it's only running a private IP. What this is hosting, and you'll have to take my word for it, but you will see once we successfully test, is that this is hosting a simplified web page that you've probably seen before in other demonstration lessons that I've created. So, with that understood, I'm going to jump into my Lambda functions list here, and I'm going to create a new function. I'm going to author from scratch and give it a name. I'm going to select my Python runtime because that's what I'm good at writing. We're going to leave the execution role the same because I don't need any specific AWS permissions besides writing to CloudWatch logs. And then under Additional Configurations, I'm going to find Enable VPC and select it, and then once here, this is where you start to select your VPC configuration. So I'm going to choose my custom VPC here. You can see we can allow IPv6 traffic if required for your networking architecture, and then we choose the important parts. So you choose your private subnets and your security groups. So for subnets, I'm going to select every private subnet that exists within my custom VPC. So A, B, and C, and you can see private, private, and private, and I'll show you what this does here after we create the function. The next thing we do is select a security group. Now for this, I already pre‑created one that allows all traffic from the VPC called Lambda functions. Obviously, you would probably want to be a little bit more selective with the traffic that's allowed, but for this demonstration to make this quick, I allow all VPC traffic. You can see here all TCP on all ports from our VPC CIDR. Now, I will say, in addition to this, really quickly, if I go back to my instance, under security, the security group attached to this instance is allowing all HTTP from our VPC only, so no external traffic is allowed. Now going back to my function, I'm going to click on Create. Now, this is creating, and I'm going to give you a heads up, I'm going to go ahead and fast‑forward here, so I'm going to jump forward so you don't wait, but when you're creating a VPC‑configured Lambda function, it's going to take quite a bit more time as compared to when you create a default function. The reason being is it's creating several network interfaces based on our network configuration here. So while this is creating, before I pause, I'll go to EC2, I'm going to load up network interfaces, and you're going to see we actually have our Lambda ENIs being created. So this is creating Lambda ENI's for us, for us to use with our Lambda function, which is going to allow us to actually communicate within our VPC. So you'll see here I have some leftovers from a different test, but if I sort, we can see PrivateFunction, PrivateFunction, and PrivateFunction here. So these three lines were just created, so it just spun out three brand new ENIs, which are what are going to be used by our Lambda function, and it lives in each of the subnets that we specified. So, what I'll do is go back to my Lambda function here, I'm going to pause, and I'll fast‑forward once this is actually created and ready to go. Perfect. So we're now ready to go, I'll tell you that took a couple of minutes, I would say, for this to be successfully created, so keep that in mind if you're going to do this on your own. So, let me go ahead and clear this, and the first thing I want to do is I'm going to copy and paste my code in here. So let me go ahead, paste this in and deploy it, and we're ready to go. So, what I'll do really fast is I'll review this at a very high level before we continue. All I'm doing is using urllib3 to create a pull for an HTTP request, and we're going to pass in the URL that we're trying to get via an environment variable. Now this code will be available for you within the module assets if you want to use it yourself and play along, so feel free if you want to. But, for now, I'm going to do a few things. First thing I want to do is create my environment variable here. So under Configuration, Environment variables, I'm going to add a new one called URL, and what I'm going to do is point to the private IP. So under my EC2 that are running here, let me go ahead and find it, I'm going to copy my private IP, so let me copy this, I'll paste it in here under HTTP, and I'll save. So that's going to be the URL that we use for right now. Next thing I want to do is under General config. I'm going to increase my timeout. So I'm going to give it 30 seconds of timeout. Now, typically speaking, when you're using a VPC‑configured Lambda function, it takes a little bit more time to execute, so to be safe, I like to add a buffer of timeout here. So I'm going to say 30 seconds, which should be more than enough. I'm going to click on Save, and there you go. Now let's go ahead and test. So what I'm going to do here is I'm going to click on Test, and I'm just going to click the test button. We see succeeded. If I go to Details here, we see the response. So this is the response based on the code, and you can see it's a non‑escaped version of our HTML. You can see my public asset, you can see the headers, etc. So this is a raw capture of that web page. And if I go to my logs here, it should have logged to our CloudWatch log group and log stream. So under Log streams, you can see fetch URL with our HTTP IP, and then our response object. So this is an actual object that I didn't actually parse, I just printed it out. Perfect. But, what if we want to use private DNS? That's a very realistic exam scenario. Well, we can do that. So what I'm going to do here is I'm going to go to Route 53, I'm going to create a brand new private hosted zone and associate it with my VPC. So under hosted zones, create new one, and let's just call it pluralsight.internal. We're going to say it's private, I'm going to associate my us‑east‑1 private custom VPC here and click on Create. All right, so now we've associated our private hosted zone here. The next thing I want to do is I'm going to create a simple record. So, I'm going to go ahead and call this app.pluralsight.internal, it's an A record, and I'm going to paste in my private IP. I'll click on Create, and there we go, we now have our record. All right, so now that we have it, we can see if it's propagated, it's INSYNC, let's go ahead and actually test this out. So I'm going to go ahead and click on this. Let me go ahead and copy the record name, I'm going to go back to my function and change my environment variable. We'll paste this in, click on Save, and now let's test. So I'm going to click on Test, we get the same response code. So now, we're using our actual URL with a private hosted zone DNS name. So under our log stream, you can see it fetched our private DNS, and this is working as anticipated. Now this only works because under our VPC we're allowing private hostname DNS resolution, so remember that, that's a very important feature for the exam. But, one last thing, that's going to do it for majority of the demo, I do want to cover one very important concept here. Remember, our function lives in a VPC now. So in order to interact with CloudWatch, it needs a connection to the service. So you either have to have a VPC endpoint for CloudWatch logs, or, like what we have is we have a NAT gateway in place, so it can communicate with CloudWatch through the NAT gateway. That's more of a very important thing to know, and it doesn't directly relate to the demonstration, but I like to point that out because that's a tricky thing that can come up. You have to have access to these services, whether it be through a NAT gateway for your function subnets or a VPC endpoint. However, with that understood, let's go ahead and wrap up this demonstration, and I will see you in an upcoming clip.

Lambda Function Concurrency
All right, I promised we would talk about concurrency a little bit more in depth, so let's go ahead and do that now. Concurrency, in terms of Lambda functions, are the number of in‑flight requests that your function is serving at any particular given time. What this means is that a function gets invoked while an existing request is still being processed. Let's actually look at some concepts that you need to understand for the exam. First up, provisioned concurrency. This is going to be the number of pre‑initialized execution environments that you allocate to your function. Really what this means is that your functions are immediately ready to respond to incoming requests, and they're essentially kept hot. This is very useful for reducing cold‑start scenarios, but, you do incur charges because Lambda, the service, is constantly keeping your functions up and running and ready to go. Next is reserved concurrency. This is where you set the maximum number of concurrent instances that are allocated to your function. When you use this type of concurrency, no other function can use that concurrency, so you're essentially taking some concurrency from that pool of 1000 and saving it for a specific function. This is useful for any scenarios where you need to ensure that your critical functions in your application can always run and they never get throttled. And thirdly, speaking of throttling, functions will drop requests and fail if you run out of available concurrency. This is called throttling. Now, with throttling, you can request a quota increase via support, so that 1000 concurrent limit is a soft limit. If you ever need to for emergency purposes, you can actually manually throttle functions. So if you suspect someone is executing a function maliciously, you can go in and with a click of a button, you can throttle that function to go ahead and kill it. How this works is it sets the reserved concurrency to 0. So you're saying, hey, you don't get any of my concurrency, go ahead and stop executing. Real quick, let's go over an exam scenario. Let's assume you're hosting a serverless application using different serverless technologies like API Gateway, Lambda functions, and DynamoDB tables. You need to ensure that latency is as low as possible with the fewest number of operational changes. That's a very big keyword to look out for, fewest number of changes. Well, from the compute standpoint, you can configure provision concurrency for your functions, and this is going to ensure that they are immediately available and they avoid any potential cold‑start times. Now, with that out of the way, let's go ahead and wrap this concurrency clip up, and I'll see you in the next one.

Lambda SnapStart
Okay, let's go ahead and get started looking at Lambda SnapStart. Lambda SnapStart is a feature that provides as low as sub‑second startup performance, with zero code changes for your Lambda functions. The one thing to keep in mind with SnapStart is that this feature is only available for specific runtimes at this point in time. It's compatible with Java 11 or greater, Python 3.12 or greater, and .NET 8 or greater. Those are the only three runtimes that are supported. When you use Lambda SnapStart on your Java functions, you actually can do so at no extra cost. Provision concurrency costs extra, but this doesn't. Fourthly here, how it works. It works by caching snapshots of the memory and the disk state of your functions that you've configured. What that means is that after the first run through, it starts to skip the init phase of the environment lifecycle for all of your future function invocations. Now, that could be a little confusing, so let's go ahead and look at it from a diagram perspective. On the left side here, we have our normal triggers, and we're triggering our Lambda function here that we will assume is written in Java. With the first initial execution or invocation of your function, this is the typical lifecycle flow. First up is the init stage. The init stage is going to start different extensions, it bootstraps the runtime and all of the required libraries and dependencies that are built in, and it's going to run static code within functions. After the init phase, it goes to the invoke stage. The invoke stage is where the actual function you've written gets invoked, and it has to respond within the assigned timeout. After the invocation is done, it moves on to a shutdown stage at some particular point. AWS Lambda shuts down all runtimes of your functions after they've been sitting there for an allotted amount of time. There's nothing necessarily specific, it's just kind of random after functions haven't been invoked, and it could be minutes, it could be an hour, etc. Now, if we enable SnapStart instead, the only big difference here, which is really key, is the init phase is now cached, so it's skipped, essentially. Lambda takes an immutable encrypted snapshot of your memory and disk state, and it caches it and replaces that init phase completely. This allows your function version to essentially skip future inits. What that does is it makes startups much faster. Now, there are two particular use case categories where SnapStart is important and good to use. First is a latency‑sensitive API, and the second is latency‑sensitive data processing tasks. I'm sure you've noticed here the keyword is latency‑sensitive. So if you're running a Java function, a Python function, or a .NET function, and it's latency‑sensitive, you might consider SnapStart. Now to round things out and end it here, let's look at an exam scenario before we move on. We're going to assume you work for a company that's using event‑driven design with Lambda functions. What they need to do is reduce their startup times for those functions running Java 11. This should immediately flag in your mind, Lambda SnapStart. I would immediately configure SnapStart for the function, and you're going to gain immediate performance optimization. Now, that's going to do it for this clip on SnapStart. Let's go ahead and wrap up here, and then whenever you're ready, I'll see you in the next clip.

Important Lambda Features
Let's go ahead and begin looking at some important Lambda features that can come up on the exam that you should be familiar with. First up, let's talk about versions within AWS Lambda. A version is a feature that allows you to essentially publish copies of your current functions and then manage the deployment of the functions without affecting users for testing, production, etc. What happens when you publish or create a new version is that the version's code, runtime, architecture, memory, etc. all get essentially snapshot into an immutable copy of your function for reference that is completely separate than the current function that you're working on. Now when you take a snapshot, there are several things that get included in it. What happens is the current version's code is snapshot, the runtime, the architecture, the memory configuration settings, etc., all of these things and many more get essentially set in stone and they become immutable. Versions themselves are all based on the current unpublished version of your function, so it's what you see in the console when you're playing around and you're uploading, changing configuration settings, etc. This current version is known as $LATEST. So you'll see this a lot of the time within the console, it probably won't show up on the exam that much, but it's good knowledge to know this is how you would reference the current version that's not published. Each time you publish a new version, it's all based off of the settings of the latest one. Moving on to aliases. Just like versions, you can also create aliases to reference your functions very easily. The difference is that these are pointers to a function version that you can change and you can update as you see fit. You use aliases to access targeted versions of your functions using an alias ARN. Now if that sounds confusing, we're going to look at an architecture diagram here coming up shortly, so don't worry, we'll explain it more in depth; however, understand you target versions using aliases. One of the benefits of using an alias is that you can easily update it whenever you need to. So if you publish a new function version and you want to reference that new version with your current alias, you can easily do that. And, one thing that's very good to know, you can actually implement traffic splitting for what is called canary testing. So you can set a certain amount of weighted values to send traffic to different versions on the back end using the same alias. Now I told you we would look at a diagram, let's go ahead and look at how versions and aliases play hand‑in‑hand and how they play together. Let's assume we have version 1.0.0 with semantic versioning of our Lambda code. Well, we perform our CI/CD pipeline, our testing, our building, etc. and we publish it to our latest version, which is the unqualified ARN that is typically referenced in the console. Once you say, hey, I'm happy with this, this code looks good, you can then publish a brand new version, which is going to increment in numbers. So it starts off with version one. Remember, these versions are immutable copies of your code for you to reference. That means you can't change the code, you can't change the basic config, etc. Once your version is published, you can also start creating aliases. When you create the alias, you can name it pretty much anything that you want, and once you name it, you can then select which function version that you want it to point to for invocation, so it can be $LATEST, version 1, 2, etc. Now, instead of pointing directly at the function, you can set up all of your triggers or your streams, etc. to actually point to your alias ARN instead of the latest ARN. This allows you some flexibility where you don't have to reconfigure the event notification, like in this diagram, and instead, you can control the back‑end code that your alias is actually pointing to. So that pointer is always the same for the actual invocations. Now let's assume we go ahead and we build version 2 of our code. Well we published that to latest, so this is the current unqualified ARN version, and after we do some simple testing, we say, hey, this is good, let's publish version 2, version 1 now gets moved over. Well what we can do is, assuming we're promoting code to production, we can then create a brand new alias called PRD and reference the previously‑tested version, which is version 1, to maintain stability since we've already tested it and we made sure it works. So now people can point to the PRD alias, which points to version 1 on the back end. Now that our PRD alias is set up with our stable version, we can now repoint our DEV alias to version 2, which is our newest code. So that's how easy it is to repoint your aliases to a different back‑end version. Before we move on, let's look at one thing called a Lambda weighted alias. So I talked about canary deployments, and this is what it looks like at a high level from an architecture diagram. We're going to assume we have the same version 1 and version 2 deployed already, what we're going to assume is we create a new alias called testing. Now we're going to use this alias to set up a weighted alias to split traffic between our different function versions. This is perfect for canary deployments. What it means is we've set up 25% of the traffic to go to version 2, and then on the bottom, we've set up 75% of our traffic to go to version 1. So now, for any of our traffic that's going to invoke our Lambda function, like say through an API Gateway for example, well, it's going to split that traffic for us, so three of the requests go to version 1, one of the requests go to version 2. Again, this is very good for canary testing and canary deployments because you can very quickly shift weights as needed or slowly change weights as needed. One last topic before we move on, Lambda layers. Lambda layers are a zipped file archive that contain all supplementary code or data that you might want to use in addition to your Lambda functions. Typically, these are going to be used to share common code or common dependencies that are not available by default across your Lambda functions. Some benefits and uses include you can reduce the size of your deployment package. For instance, maybe you have a common set of libraries that all functions need to use, and instead of installing them via the deployment packages for each function, you can instead create a layer and then reference that layer from your functions as needed. It also helps you separate core function logic from your dependencies. Similar to reducing the size, you can take out the shared dependencies and just focus on only the core logic within each function. Thirdly, you can share dependencies across functions. Let's say you have 100 functions in one account and they all share a common dependency or library that's not default, well you can reference the same layer and not have to recreate it each time. And lastly, probably one of the biggest benefits, you can easily share them with other AWS accounts. For instance, maybe you have a central account where you like to manage a lot of organizational resources. Well what you can do is create a new Lambda layer and then share it with all of your organization accounts so that they can easily reference it within their own functions. In addition to that, you can also make the Lambda layer public. Now, I don't think that's in the exam, but it's a good thing to know. If you want to create your own and then share it with the world, you can do that. And last thing here, a potential use case that you might see on the exam. You might have a scenario where you have a common set of internal libraries that are proprietary or custom, and you need all of the functions in your organization to use this set of libraries. Well then a Lambda layer could be a very good solution or part of the solution that you should look into. With that being said, that's going to do it for important Lambda features you should be aware of. Let's go ahead, we're going to end here, and I'll see you in an upcoming clip.

Module Summary and Exam Tips
All righty, way to hang in there once again. You've reached another module summary and exam tips clip. Let's go ahead and review what we just covered that I think is pretty important for you to take with you into your exam regarding Lambda functions. First things first, three questions you should be asking yourself during your exam. First thing, do you need servers? If you do, then you probably want to avoid Lambda. If you don't, well then maybe you want to start looking into using Lambda because it gets rid of a lot of operational overhead, complexities, etc. Second, is the application a better fit for containers? Again, if that's true, then maybe you look at something like ECS or EKS. If it's not true, then Lambda could be a perfect solution. And then thirdly, how long does the code need to run? If your workload is going to be anything longer than 15 minutes, immediately you should be ruling out AWS Lambda. Remember, the execution time for a function is up to 900 seconds or 15 minutes. These are best for short‑running workloads. With those three questions out of the way, let's review Lambda itself. Remember, you use IAM execution roles to grant AWS permissions for your Lambda functions. Some common examples are getting objects from S3, putting items in DynamoDB, or maybe receiving messages from an SQS queue. And speaking of all of those resources, make sure you're familiar with common Lambda triggers and event source mappings for these event‑based workloads. Some very popular ones are S3 bucket event notifications, Kinesis Data streams and Data Firehose, SNS or SQS, and even EventBridge. Again, these are all some of the more common triggers for Lambda functions on the exam. Thirdly here, remember, again, functions should be short and they should be concise. They allow for up to 15 minutes of runtime maximum, and what that means is they're very good for short‑running workloads. If something only needs to run 5 minutes, then this is a perfect choice. Fourthly here, functions can be configured to run in a VPC if you need to connect to private resources. For instance, maybe you have an API hosted on an EC2 instance that you need to go ahead and interact with, or maybe you need to actually reach an RDS database cluster in your VPC. That's all doable by configuring VPC information like subnets and security groups for your Lambda functions. And then lastly here, you can use Lambda layers to share common code and dependencies across functions, even across different accounts. Exam pro tip number one: If you need to connect to an RDS instance with your functions, you should also consider RDS Proxy. This is one of the major use cases for this feature. It allows you to pull connections, which is extremely important since Lambda functions can go up and down over and over again, and you might get a throttling error for a connection limit. Moving on to some exam scenarios. These are three popular scenarios, at least at a high level, on how you might use a Lambda function. First up, files uploaded to an Amazon S3 bucket. You need to then convert the files right after they are uploaded in a cost‑efficient manner. Lambda functions are perfect. You can use event notifications, trigger your function, and then execute your code. Second, let's say you're getting data ingested via a Data Firehose delivery stream for near real‑time streaming. While the data is being streamed, you also need to perform some type of data transformation during the stream. Well, delivery streams within Data Firehose directly integrate with Lambda functions to perform this type of workload. And then thirdly, let's assume you have messages coming in from an application that gets sent to an SQS queue for decoupling and resiliency. You need to find a way to asynchronously process the messages as they come in. Again, you can easily receive messages off an SQS queue, and your Lambda functions will scale automatically to quickly do so. Moving on to exam pro tip number two. Remember, Lambda functions can be targets for Application Load Balancers. This makes them very good for very short API calls where you can host your own API within a Lambda function and trigger it via your ALB DNS. And lastly here to wrap this all up, let's go over an exam scenario. If you ever have an exam scenario where you need to help a company optimize costs for applications that are running on both Lambda functions and EC2 and you have predictable long‑term utilization, you should immediately consider a compute savings plan. Remember, compute savings plans cover EC2, they cover Fargate, and they cover Lambda functions after the free tier. So this is a perfect and valid option to optimize costs. Now, that's going to do it for this module on AWS Lambda. Let's go ahead and wrap things up. We can take a break, grab some coffee, grab some water, and I'll see you in the next module.

Event-driven Architectures
Amazon RDS Events
All righty, let's jump into the next module within this course. In this module, we're going to start looking at event‑driven architectures. These are very important for you to understand when you're designing your architectures in the AWS cloud. The first topic we're going to cover here is Amazon RDS events. An Amazon RDS event is a near real‑time notification that contains a bunch of different information about events that occur on your monitored RDS resources. This can be your instances, your clusters, etc. A very important thing to know here is they do not contain actual database data, so nothing within your tables is contained within these events. There are several categories for RDS events. Now, you don't necessarily need to know these at an in‑depth level, but be aware that they do exist. There's categories like DB instance, snapshot, security group changes, etc. When these events actually do occur, you can configure them to send messages or publish messages to an SNS topic, or you can send them to Amazon EventBridge, which allows you to perform or trigger different workloads. A common scenario is to trigger a Lambda function from either of those two services to perform some type of custom logic, whether it be alerting a team, restarting an instance, creating a snapshot, etc. Now, some examples of an RDS event are whenever a snapshot is created, whenever an instance is shut down, or even when a cluster has to failover. These are all real‑world examples you should be familiar with. Now moving on, let's look at an RDS event architecture exam scenario that could occur. Let's say you have an application that leverages RDS for storing online sales transactions, OLTP workloads. With RDS, you've went ahead and you provisioned the storage capacity at 200 GiB, but a recent online sale has resulted in a spike, which is going to start consuming more capacity. Because of this, you receive an RDS event, we'll call it RDS‑EVENT‑0221, which is a real event, and an example of that is when your free storage capacity for your instance hits a certain low point of the allocated storage. Now I'll tell you really quickly, you don't need to exactly know the event numbers. I can promise you that, just be aware this is a possibility to capture. You don't need to know the explicit event number, but understand the overall event that triggered it. With this all in mind, how can you automate increasing the RDS provision storage as well as alerting admins? Well, let's look at that now. Remember, we're using an application database hosted on RDS, and we assume capacity has reached a specific threshold, which generated our event in the first place. Well, we've configured it, so when that event occurs, It's going to go ahead and send a message to our SNS topic. From there, once the message is published, we can use SNS to perform a bunch of different things. We can send a message or an email to an admin email list so they can immediately be notified when this is occurring. After that, or in addition to that, I should say, you can also have a separate subscription for a Lambda function, so our SNS topic can invoke our function, which can go ahead and increase our storage capacity in almost near real time. So this is how you can use one event with a topic in SNS to trigger multiple workloads. In addition to RDS events, there's also a capability within PostgreSQL for RDS where you can invoke a Lambda function based on data events. At this time, RDS for PostgreSQL supports this function. What it does is it allows you to actually work with database data events and not just the resource events. However, with this, you have to have traffic connectivity in place for the AWS Lambda function service. What that means is you have to allow internet gateway traffic or you have to have a NAT Gateway in place, or even better, you have a VPC endpoint so you contain all traffic within the AWS network to trigger your Lambda functions. In addition to network connectivity, it also requires IAM permissions to invoke the functions, so you're essentially granting RDS the permission to go ahead and invoke the Lambda function to perform your workload. An example of this is maybe you want to trigger a snapshot whenever a specific insert is performed within your tables. Let's look at a very high‑level architecture. Now I'll say this, you just need to be familiar with the process overall, you don't have to know the very specific nuances to get this set up for this exam. So we're going to assume we're running an application database using PostgreSQL on RDS. We're also going to assume our users or our application runs a specific insert within one of our tables that we're monitoring. Well, what we've done is we've set up the assigned permissions for RDS and within our table, and it's going to go ahead and invoke a Lambda function, which can then trigger specific workloads like creating a snapshot once that insert is completed. Again, you don't need to know this super in depth for the exam, just understand that you can do it. With that being said, let's end this clip here, and I'll see you in the next one.

Amazon S3 Events
Okay, up next, let's look at Amazon S3 events. Now we covered some of this previously in a different course within this path, so this is going to be a quick run through and review because it's very important you know how these work. First up, remember, Amazon S3 offers the ability for you to receive notifications whenever specific events happen in your buckets. What I've done is I've went through and I really went ahead and highlighted here the ones that more commonly appear on the exam. Now that's not to say you shouldn't understand or know about the other ones, but I can tell you for the most part these ones here that we've highlighted are much more common. In addition to the event notification types or the sources, remember the supported destinations. You can send a message to an SNS topic, you can publish a message to an SQS queue, you can invoke a Lambda function, and you can even send events to Amazon EventBridge. Now for this clip, what we're going to do is review specific event exam scenarios you should be familiar with. First up is SQS. Let's assume you have a web application that's uploading images to your S3 bucket, and you have EC2 instances that run through and process the images that were received each day. Well, the jobs only take roughly 5 minutes, and you're going to say you don't need more than 256 MB of memory to complete the job. How can you implement better resiliency and lower cost of operation? Now with this, it's kind of combining a few services, but this is important you understand at a high level how this architecture functions. On the left here we have our applications and our users, and we're going to assume that they are uploading an image to our S3 bucket, so they're creating an ObjectCreated:Put event, which we can look for in our event notification configurations. Currently, S3 supports SQS queues to send messages to. That being said, it only supports standard queues. So once that event notification is triggered, SQS can then receive that message. Now, with Amazon SQS, which we will cover much more in depth later on, at a high level, it provides a highly‑scalable and cost‑effective way to decouple the upload process from the processing process. So if we get spikes in uploads, SQS can queue and buffer the messages, essentially, for a Lambda function or something else that's pulling the messages off the queue. What we can do is then set up that SQS queue to actually trigger or invoke our Lambda functions. So what we can do is transfer that EC2 application to a Lambda function because each image is only taking 5 minutes and 256 MB of memory to complete. Because of these requirements, that means we can easily use Lambda functions for the compute portion of the architecture. That means we're not incurring costs whenever the functions aren't running, and in addition, it's event‑driven, which means it's going to happen almost immediately once an image is uploaded. Once we invoke our Lambda function via that message, it can say, hey, I received the message, go ahead and hide it, once it's done, it says, okay, delete that message. Again, we're going to cover SQS much more in depth later on, but understand at a high level it offers this buffer and decoupling mechanism for this architecture. Moving on to the next exam scenario, SNS. We're going to assume your team must use EC2 instances right now to receive data from a third‑party vendor, and then they go ahead and upload that data to an S3 bucket. With that, the same EC2 instances are used to receive and then upload the data, as well as send notifications to an admin after everything is completed. Well, recently, you've noticed that you're getting slower performance, so you want to improve it, and you also want to shift to components that have the least amount of operational overhead. Well, with this exam scenario, let's talk about re‑architecting this to be much more efficient. In this, we have our SaaS, or our third‑party vendor, where we're syncing our data. For this, what we can use is we can shift to a managed service called Amazon AppFlow, which we will cover in depth as well, but this is meant for easily transferring data between SaaS applications and your AWS services, specifically things like S3. So, once we go ahead and set up this connection in place, we can start automatically syncing our data via AppFlow, and AppFlow can sync that data to our Amazon S3 buckets, which can then, of course, initiate or trigger an event notification. Well, these event notifications can be configured to go ahead and publish a message to SNS topics whenever an event occurs, so essentially whenever data is synced to our bucket. So once that event notification is triggered, it sends a message to our SNS topic, which can then send notifications to any subscription. So it could be an SMS or text message, it can be an email, etc. What we've done with this architecture is reduce any need for custom notification logic and any processing logic on Amazon EC2. The third and final example here is triggering Lambda. Now this is going to be a very quick review because we covered this in the path already within the Amazon S3 lessons within another course. However, with that in mind, it's very important you understand how this works, so we're going to review it. We're going to say, hey, you host an application that uploads images to S3. Once they're uploaded, you have to process them, resize them, and store them in a different bucket. You also want to store object metadata in a key‑value store for separate processing. Well, this is a perfect architecture. Now, again, I'm not going to review this in depth, we've covered this in a different course, but essentially what you're doing is using the CreateObject whenever an object is put into our bucket, it triggers an event notification, which invokes our Lambda function, and that Lambda function can perform different aspects of our workflow. It can put an object that's processed into a different bucket that's much smaller now, and it can also put items in our DynamoDB table to track that metadata. Again, I'm not going to cover this in depth, feel free to look at the other course with S3 covered in depth there, but for now, just understand this at a high level. Real‑world pro tip here: For Amazon SNS topics, Amazon SQS queues, and Lambda functions, you have to grant S3 permissions to interact with those services; it doesn't just work right out of the box. With that being said, let's look at some of the event notification permission requirements. On this one, we have a queue policy. You'll notice we're allowing the S3 principal to send a message to SQS. With this, this gets attached to the actual SQS queue itself. On the bottom of the policy, we have a condition saying that we're looking for a specific source ARN, and that StringEquals must match a SourceAccount. So this is a real‑world example of an S3 event publishing to an SQS queue. Here we have SNS access policies. These get attached to SNS topics. Again, we're going to cover SQS and SNS much more in depth later on, but these tie into S3 events specifically, so I'm covering them here. Similar to SQS, we're allowing the S3 principal to publish a message, so SNS:Publish. We specify our topic on here, and again, we have the same condition block. And then lastly, our Lambda resource policy. We looked at this when we covered Lambda, but remember, it creates an event source permission policy to get it attached to our Lambda functions to allow it to invoke the function. So the only difference here, really, is the action. These are the big differences between all three. The action itself is specific to the resource, and of course you attach these policies to the specific resource. Finishing up with EventBridge. This was the last supported destination here. EventBridge is going to be a perfect choice when you need to trigger workflows using any other AWS services besides the one we covered. Some examples could be SageMaker Pipelines, Kinesis data streams, or maybe AWS Batch. All events get sent to EventBridge once you enable this destination type. From there you create EventBridge rules to route the events to different targets based on criteria. An exam scenario for an example, let's say files get sent to S3 daily and you want to replicate them and use Lambda functions to run code on those copies. In addition to that, you want to automate sending the copies to pipelines in Amazon SageMaker Pipelines to perform some type of analytics and machine learning workloads. Well, this is a perfect scenario to enable EventBridge S3 event notifications. Now that's going to do it for this lengthy clip. Go ahead and review those architectures, if you get a chance, but for now, let's end here, and I'll see you in an upcoming clip.

Amazon EventBridge
Next up, we're going to start talking about a very important service called Amazon EventBridge. We looked at this very briefly in a previous clip, but in the upcoming clips, we're going to start diving into this much deeper. This is a very important service. EventBridge, which is also formerly referred to as CloudWatch Events, is a serverless event bus that actually allows you to pass events, or triggers, or different things that happen in your account from a source to an endpoint. Here are some important concepts when we're talking about EventBridge. First is an event. This is a recorded change within an AWS environment, a SaaS partner, or one of your very own configured applications and services. There's also rules. Rules are simply going to be the criteria that you use to match incoming events, and then you say, hey, I want to send you to this specific target. Targets can be many different things. They can be a third‑party application, they can be your own application, or an AWS service. These rules are based on either event patterns, which we'll look at, or scheduled patterns, which we will also look at. The last major component here is an event bus. An event bus is simply a router that's going to receive your events, it looks at your rules, and then it delivers your events or your transformed events to your different targets or destinations. Every single AWS account has a default bus within EventBridge that you can use for pretty much anything. Now, I told you we would talk about trigger types for your rules. There are two different kinds. The first is an event pattern. This is where you define an event source, so say things like S3, SQS, DynamoDB, etc., and an event pattern specific to that source that's going to go ahead and trigger your rule once that event pattern occurs within your monitored source. You also have scheduled. So this is exactly what it sounds like, you set up a recurring schedule to trigger your rules using Amazon EventBridge Scheduler. So this is perfect for recurring events. So maybe you have to perform workloads at the beginning and the end of each workday, well a scheduled trigger is perfect. Speaking of EventBridge Scheduler, there are two different ways you can set up a scheduled event. The first is a rate‑based. So this is perfect for setting specific rate‑style triggers. So for instance, you could say, hey, I want this rule to be triggered every 12 hours. In that case, you would say rate(12 hours). Now you can do minutes, you can do days, you can do a lot of different customization options, you don't need to know the minimum and maximum for this exam; however, understand you can use rate‑based options. The second is a cron‑based option. So this is similar to a Linux cron schedule. You set different fields that are specific to the day, minute, day of the week, month, etc. for a recurring scheduled basis. In this specific example, this is saying to go ahead and trigger the rule every first day of every month at midnight UTC every single year. Now you're not going to need to know how to read a cron‑based schedule for the exam, just understand you can schedule it to be recurring. That's the big takeaway for this clip. Speaking of the exam, let's look at some scenarios for each type of pattern or trigger type. First, for event pattern. These are common scenarios that could come up that would be useful to use EventBridge for. Maybe you need to send some type of notification or alert or perform some type of action when a user or role terminates or stops an EC2 instance. Maybe you need to send an SNS notification whenever a KMS key deletion is scheduled. Or, maybe you need to alert based on Amazon Macie finding sensitive data within your S3 buckets. These are all real‑world examples that can come up on the exam, and you can easily set up this workflow using EventBridge. These are all API calls that occur or events that occur within your account. For scheduled events, maybe you need to invoke a Lambda function every day to check for expiring imported custom ACM certs. Maybe you want to start or stop an RDS database instance during a work week, so you want to start it in the morning, stop it at night. And lastly, maybe you need to schedule ECS tasks using Fargate Compute to complete a recurring job that only occurs at a certain particular point in the day each day. These are all, again, real examples that can come up. Now speaking of examples, let's look at an architecture example for Amazon EventBridge. In this example, we're going to assume a user or role stops an instance within Amazon EC2. Well, stopping instances is an API call, so any API call in AWS can be captured using EventBridge. Using this captured event, we can then set that up to trigger a rule. Here on the right side of this diagram is an enhanced version of this example event. We have EC2 Instance State‑change Notification, EC2 service, and then the resources, the state, etc. Using this example event, what we could do is we could set up our EventBridge rule to trigger based on this event source pattern or event pattern. So what happens is EventBridge captures that event and it triggers our rule. Now, the cool thing about EventBridge is you can send or use one singular event to send your customized events or the same event to multiple targets and destinations pretty much at the same time. So in this example we could perform two different things. The first thing we could do is trigger a Lambda function or invoke it, which then can go in and restart the instance. We can also send or publish a message to an SNS topic, which can alert an email administrative group to let them know that, hey, you actually just had this EC2 instance restarted, you might want to make sure everything is running appropriately. This is a common use case or scenario or at least something like it that can come up on the exam, so be familiar with this. The big thing to take away here is EventBridge allows you for basically near real‑time notification capabilities and invocations of automated workflows. With that being said, that's a good coverage of an overview of EventBridge, let's move on to some more specific deep dives into some of the components here, coming up next.

Amazon EventBridge Event Buses
All right, we just covered EventBridge as a service, let's go ahead and dive into some of the components. In this clip, we're going to look at event buses. First up, there are three different event bus types within EventBridge. The first is the default. We covered this at a high level, and this is included with every single AWS account. This default event bus, when it's enabled, automatically receives every single possible event that is compatible with the service from AWS services within your same account. You also have a custom event bus. You can create a new, completely separate bus for your own custom workloads and your own separate systems. And then lastly, we have a partner event bus. These are specifically meant to receive events from SaaS partner applications and SaaS services. So for instance, you could use it for things like Datadog, Sumo Logic, things of that nature. Let's look at an architecture and an example of where you might use each of these event buses. First up, the default. This is perfect for any and all events that need to be captured within the primary AWS account for any type of easy setup. So every single service in AWS within your account can send events to your default bus and you can trigger many different rules to send to different targets and destinations based on the service event. For the custom event bus, this is perfect for centralizing events from other services and other accounts, like within an organization, for example, or maybe you need to divide event traffic that's separating PII and non‑PII events to make sure that the data is completely separate for processing. So these are two real‑world examples that can come up on the exam. They're good for centralizing within an org or separating data workflows. A big thing to note here is these require setting up a resource policy on the event bus to allow your custom applications or other accounts to actually send these events and messages to this bus. And thirdly, the SaaS partner event bus. Again, this is perfect for receiving custom SaaS events from different supported partners. You can use these events to trigger workloads, so maybe you have SIEM events, maybe you have logging events, or even security alerts from a third‑party monitoring system. Now speaking of EventBridge partner bus supported partners, this is a comprehensive list of what is supported currently based on documentation. Now you shouldn't have to know these in depth; however, I like to highlight here the ones that I've more commonly seen whenever I've worked with accounts within an organization that's using a third‑party toolset. The big takeaway with this entire list is that you can use several different supported partners to send events to your partner bus. Now that's going to do it for this clip on event buses. Let's wrap up here, and we're going to move on to another concept here, coming up next.

Amazon EventBridge Events
Okay, let's look at events within EventBridge. Let's talk about events and rules really quickly before we dive into some examples. First up, remember, in events, you have to know this, this is a change in an environment, so it can be something within your account, it could be a partner service, or even your own custom application that you've configured to send events to EventBridge for. Events will always be a JSON object that follow a similar structure and have the same top‑level fields. Now the details within those fields obviously will change, but they always follow the same structure. It's important to note, some AWS services that you use with EventBridge are considered best effort, so they do their best, but it's not guaranteed, and then some others are actually considered durable, so they will always make sure that notifications are sent. For rules, remember, you use these to specify which events that you want to send to which specific targets for your own processing needs. So maybe some events you want to send to a Lambda function, and then others you just want to send an SNS notification. You can use rules to route your different events based on your own custom logic. One cool thing to note is a single rule can actually send one event to multiple targets. We looked at that in a previous clip where we had an example of an EC2 instance being stopped and we sent the event to a Lambda function, as well as an SNS topic. Moving on, let's look at some sample events. Now, you don't need to know these super in depth, but I like to include them so you understand what to look for when you're reading through these in your own real‑world environments. In this event, we're using an AWS health event, so this is warning us, as you can see, that our EC2 instance store drive performance has been degraded. You can see it here with the event type code. Now, equally important are the resources. So you can see at the top here we're saying, hey, this is the list of resource ARNs that are affected by this AWS health event. So you might want to go ahead and check these out, make sure that they're resolved, etc. Another sample event is something from trusted advisor. So, this is saying, hey, we're going to go ahead and see that the password policy based on the check name here in the detail field is actually disabled. You can see the reason, password policy not enabled, and that's an error. That's a bad thing, we want to go in and enable that. So in theory, we could have a notification be set up for when this event comes in, we can alert our cloud engineers or our IAM security engineers, they can go in and they can resolve this issue. Moving on to three more important concepts here: replays, archives, and sandbox. An archive is something that allows you to create a collection of different events so you can easily replay them later on within your day or at a later time. You can use archives to go ahead and replay events to maybe test error recovery processes, or maybe you want to validate new features that you've added to an application. Speaking of replays, with replays, you can specify what archive you want to pull events from, you can specify instead the start and end time for the different events you want to replay, you can choose the source event bus, and you can even specify one or more rules to replay for the events to work with. One big thing to understand here is when you replay an event, it might not replay those events in the same order if you're replaying multiple. That's not a big exam thing, that's just more of a gotcha that I like to point out because this has caused me headaches in the past. Big thing to note here is replays leverage archives or start and end times. And then lastly, we have our sandbox. So this is a newer feature in Amazon EventBridge that allows you to experiment with creating event pattern matching, rule configs, and even transformations of input without actually having to implement a design. This should be pretty easy to remember, you play in a sandbox, this allows you to play with the service. Next up, schemas. This is an important concept, it can come up on the exam. A schema defines the structure of events that get sent to EventBridge. So, most services follow the same schema by default, but you could have custom ones. With schemas, you can create your own, you can use the defaults that we just talked about, and you can even infer schemas from previous events. So if you think this is going to be a repeated pattern, you can infer that schema for quickly setting up future process workloads. There's also something known as the schema registry. So this is going to be a container for your schemas, and it just collects and allows you to organize them into logical groups, so this is really good for testing of your applications. On the exam, it might come up, and the big thing to note here is it's very helpful to speed up development. So if you're adding a feature like we talked about and you want to test it, well, you can infer that schema, add it to your registry, and you can use that to create new custom testing events. Now that's going to do it for this clip. Let's wrap up here, and I'll see you in the next one.

Demo: Trigger Workloads Using Amazon EventBridge
All right, let's go ahead and get started with this demonstration clip. In this demo, we're going to work on triggering a workload using Amazon EventBridge. In this demonstration, we're going to implement this simple architecture that we looked at in a previous clip. We're going to set up an EventBridge rule to invoke a Lambda function to restart a stopped EC2 instance. In addition to that, we're also going to set up a secondary target as an SNS topic to send us an email to alert us what's going on. Now with that understood, let's jump into our sandbox now. Okay, I'm in my cloud sandbox as my cloud_user, let's go ahead and review what I've already created before we jump in and start creating the other parts of this architecture. First things first, I've created this very important server, as you can see by the name here, and this is the EC2 instance that we're going to use to trigger our EventBridge rule, and then we're going to restart it using Lambda. I've also created a Lambda function here. Now we've called it EC2RestartingFunction, and I'll briefly overview the code. Now this code will be available for you within the module assets, so feel free to use it, play with it, change it, whatever you want. What we've done here is I'm importing, of course, my libraries, I'm setting up my boto3.client for EC2, and within the Lambda handler where I actually perform the workloads, we're doing a few things. We're getting the instance ID from the event. Remember, the event is an object that triggers or is sent into our Lambda function with all of the details that we want. So using it, we're parsing it, we're looking for the instance ID in one of the fields, and then from there we're describing that instance, we're looking at the state of that instance, and if it's in stopped, we're going to go ahead and restart it based on this try block here. Again, that's as deep as I'm going to go on this, feel free to play with this yourself, but that's what the Lambda function does as a whole, so let's go ahead and continue on. Now in addition to this function, I've attached an already configured IAM role, which is here under permissions, called eventbridge‑demo‑role. Now this also is in the module assets, so you can go ahead and copy and paste this and create your own role to use. What it does is, of course, allow me to log to CloudWatch, and it allows me to describe and start instances in EC2. So it's least‑privileged access as far as API calls go. With that out of the way, the next thing I want to show you is Temp Mail. So this is just a temporary email service that I like to use in my demos, and we're going to use this to receive our alerts from SNS. With the review out of the way, let's begin. First thing I want to do here is I'm going to set up a brand new SNS topic. Now I'm not going to dive into this in depth, this is covered in its own module later on, but for now, I'm going to create it and fly through setting this up. So I'm going to create a new topic, I'm going to choose Standard, I'm going to give it a name, we're going to go ahead and skip down here, I'm going to click on Create topic, and there we go. Now the next thing I have to do here is create a subscription. So under Subscriptions, I'm going to create a new one, I'm going to choose my protocol of email, and I'm going to copy and paste my email in here. So under here, paste it, create, and then now what I have to do here is I have to confirm this subscription. So under my email service I get a subscription confirmation message, I click this link here, so let me open that up in a new tab, and it's confirmed. So now when I close this, I go back to topics, and I refresh, we're good to go, we're confirmed. So now I can start sending messages to this email via this topic. All right, so now let's set up the actual triggering. So under EventBridge here, I've loaded this up, I'm going to go to Rules. I'm going to select the default event bus for this. Remember, you can create a custom, etc., but for this, for simplicity, we're choosing the default. I'm going to click on Create rule here, I'm going to give my rule a name, we can give it an optional description, we choose the event bus that we want to go ahead and attach this to, and we are going to enable it. Now for rule types, remember, we can do a schedule where you can build it via EventBridge Scheduler or we can do event pattern. Now in the sandbox, I will tell you, the Scheduler does not work, so keep that in mind, if you play around with this, you might get some errors. So for this demo, we're going to choose event pattern because I want to trigger it on that API call. So I'm going to click on Next, and then we get to our event source. So do you want it to be an AWS event? Do you want it to be a custom one from one that you send or maybe a partner sends, or you can look at all events that come in. Now this is not recommended, as you can see, it sends every event to your event bus, so, there's a lot of different events coming in. For this, I'm going to choose AWS event because this is an AWS event. In fact, under sample events here, I can actually look for EC2 and then look for an instance change here. So if I look for this, Instance State‑change Notification, this is what the event will look like. So this is actually nice to play around with, you can test all of your different options, configurations, etc. So one of these actually does stopped, and this is what it will look like for us. Moving on, we go to creation method. So do you want to use a schema, which we talked about? Do you want to use a custom pattern where you build your own event pattern? Or you can use a pattern form. Pattern form is the easiest to get started with, so I recommend starting with that. Under here, we choose the event pattern, so, remember, it's going to be an AWS service, we're looking for EC2 here, so I'm going to select EC2, event type will be Instance State‑change Notification. As I select this stuff, notice on the right here that it's filling out the event pattern. Now the first thing I have to do is what state are we looking for? Do we want to match all states or do I want to look for a specific state? Personally, I want to look for stopped, so I'm only going to filter and run this rule when a stopped state is recognized. The next thing we can do is we can look for any instances, or we can look for specific instance IDs. Now for this, I'm going to treat this as a prod account and I'm going to say any instances that stop should trigger this rule, so any instances that hit the stopped state from this service. I'll click on Next, and now we select our target. So what do we want to send our events and notifications to? So we can send it to another event bus, we can send it to an API destination, or we can send it to a service. So for this, I'm going to choose service, I'm going to find my Lambda function, and I'm going to select my Lambda on here, so EC2 restarting function, and if we wanted to, we could choose a specific version or a specific alias. Now I'm going to use default because I didn't create any versions or aliases, so I want the latest. After that, there's more settings. So this is where you can send the input or configure the input. Do you want to just send the entire matched event? Do you want to parse only part of the event? Do you want to send a constant JSON text, so a static text that is passed regardless of the event that comes in? Or, do you want to transform it? So this is pretty cool, you can set your own static string using the different configuration options in the fields within the JSON. I do suggest you play around with this, it's very neat to send custom events in messages. Well I'm going to say matched event, I'm going to go ahead and leave these defaults, and I'm actually going to add another target. So remember, we can have multiple targets for a single rule. For this, though, I'm going to search for SNS. I choose my topic here, and I'm going to use the same settings for additional settings. I'll click on Next, Next, we review, so we have our targets, etc., we click on Create rule. And, there we go. So now we have one EventBridge rule that will send it to two different targets, which is perfect. So now we can test this out. So to do that, let's stop our EC2. So I'm going to go to EC2 instances, under Instance state I'm going to stop my instance. I'll click on Stop, and while this is stopping, I'm actually going to go to my Lambda, go to Monitor, and I'm going to skip to CloudWatch logs here. Now we're going to notice eventually this is going to hit a stopped state, and it's going to go ahead and trigger our Lambda function via Eventbridge, and we should receive an email within our email address that we subscribe to our topic. So what I'll do here is I'm going to briefly cut forward until this is complete, and I'll show you how fast this really was. So, I've got my notification here. So if I select this, we're going to see we have the event. So this sent the exact event that triggered our EventBridge rule. If you notice, we have the state of stopped here, we have the EC2 Instance State‑change Notification, we have the source, etc. So also under, if I go to CloudWatch and I refresh my log group here, we should see a log stream, there it is, and we see our logged info, so Instance ID, current state, starting it, and then we got the response here, which is a JSON response. So now if I go back to my instances, it's running again. So it restarted it automatically. Now it's important to remember this is not real time, it took roughly a minute, I would say, to get all of these pieces moving and connecting and restarting this instance. So that is a very important distinguisher on the exam. This took roughly under a minute to complete from end to end. Now, that's going to do it for this demonstration. Hopefully, you saw how easy it is to use EventBridge to capture an event in your account and then trigger an automated workflow. Let's end here, and I'll see you in an upcoming clip.

Module Summary and Exam Tips
Okay, welcome to the end of this module. As usual, let's have some module summaries and some exam tips before we move on. Remember the two event‑driven sources that we talked about in the beginning of this module. The first one was an RDS event. Remember that these are near real‑time notifications with information about different categories of events that have occurred on your RDS resources. This can be an RDS instance, a database cluster, snapshots, etc. A big thing to remember is it does not contain database data, so it's not your actual data, it's just information about your resource. The second was an S3 event. Again, we look at S3 events in detail in the course where we talk about Amazon S3, but in this module, we reviewed at a high level some of the important integrations. Remember that you can monitor for different events like putting objects, deleting objects, redundancy restoration events, etc. that happen in your bucket, and then you can trigger workloads via SMS, SQS, Lambda, or for any other need you can send it to EventBridge. Speaking of EventBridge, remember, you should think EventBridge if you want to trigger an action based on something happening in your account. A common use case is to trigger a Lambda function for back‑end logic whenever an API call happens, so you're automating responses. You need to make sure that you're familiar with the different rule configurations, the different targets or destinations, and the two different scheduling options. There are schedule‑based rules and there are event pattern‑based rules or event‑triggered rules. This is going to be one of the quickest and easiest ways to respond to things that happen in your environment, so API calls, resources terminating, etc. And lastly, remember, you can use the default bus that's in every account, or you can create a custom bus for cross‑account access, and you can even use a partner bus for supported SaaS partners. To wrap things up, let's have a very quick exam scenario. Let's assume you're running a Python script or job using Amazon EC2. It runs every 2 hours for only 30 seconds, and it's on a scheduled interval that uses 2 GB of memory to complete. You really want to optimize the cost and the overhead since the EC2s are not really being utilized fully outside of when the job is actually running. Well, you can move the job to a Lambda function due to the limited or small amount of required resources, and you can use the EventBridge to schedule the invocation of your functions every 2 hours. That's going to go ahead and do it, I think that does it all for this module. Let's wrap things up, take a break, and I'll see you in the next module whenever you are ready.

Amazon API Gateway, AWS AppSync, and AWS X-Ray
Amazon API Gateway Overview
Hello, and welcome to the next module in this course. In this module, we're going to cover several different services, including API Gateway, AppSync, and X‑Ray. The first clip in this module is going to be an overview of a very important service called Amazon API Gateway. So, what is API Gateway? Amazon API Gateway is a fully managed service that allows you to publish, create, maintain, monitor, and secure your own custom APIs. Essentially, it allows you to put a safe front door or an entrance for your applications that you're hosting within AWS. One of the very nice things about API Gateway is that it integrates very easily with Lambda functions. It also supports custom HTTP endpoints on the back end, and it actually supports pretty much every other AWS service. If you remember, every time you interact with a service in AWS, it's just an API call. So what you can do is front these custom services or these other services with your own custom API, which you can host in your own application. So you can essentially front other services with your own abstraction. Two key things to look out for on the exam. API Gateway is compatible with Swagger and OpenAPI API frameworks. If you see either of these two indicators in an exam scenario and you need to create an API for it, well, then API Gateway is a great choice. Also, in addition to that, API Gateway as a whole allows you to transform and even validate incoming API requests and the responses that are being sent back. It is very customizable. In fact, there are many different ways to set up different integrations. You can customize pretty much every step of the way ‑ the response, the request, the invocations, etc. For the exam, you're going to have to know the three different endpoint types that API Gateway offers you. First up, we have an edge‑optimized API Gateway endpoint. This is going to be the default option. This is a global API option. Now when I say global, it means it's deployed at different edges around the globe on the network. But understand that API Gateway is a regional resource, so if you create it in us‑east‑2, even though it's an edge‑optimized API, that resource still lives in us‑east‑2. When you choose this endpoint type, your requests are going to get sent through a CloudFront edge location, so a point of presence. These are generally going to be best for your global user base, and that's because it's going to improve connection latency usually. The next type is a regional API type. This is exactly what it sounds like. You deploy it for specific region interactions. So this is good for clients that reside in the same region and they don't really go anywhere else, there's no user base outside of that region. Now, if you want to, you can leverage CloudFront with them; however, at that point, I would recommend you just do an edge‑optimized API. The nice thing about these is that they're also compatible with latency‑based routing. So if you have three or so APIs that are regional, you can set up latency‑based routing via Route 53 routing policies with different records, and with that you can route your clients to the endpoint that has the least amount of latency. The benefit of regional is that they bypass CloudFront by default, so they could be a little bit quicker for that regional traffic. And then the third one here is a private endpoint. These are only going to be accessible from VPCs using interface VPC endpoints. These are meant to be completely isolated from the public internet, and that's why they're called private. They're perfect for private API calls that you have for proprietary software that you only need to use within your organization. Now, be sure you understand each of these endpoint types. They can come up on the exam, and honestly, they come up quite often. Understand the benefits and the use cases for each. Next up is the API type. We talked about the endpoint types, well, there's also API types. The first and the most well‑known one is a RESTful API. These allow you to do pretty much everything possible within the service. It's a collection of the HTTP resources, the methods, everything else that's integrated with the different back‑end services. So it supports Lambda function invocations, you can pass data to an HTTP endpoint that you're hosting on something like ECS or an EC2 instance, or you can directly interact with other services like SQS or SNS, etc. These are typically going to be the most complex, but they offer you the most customization. We then have an HTTP API. These are simpler in configuration compared to a RESTful one. Now, don't let it fool you, this is still a RESTful API, but they call it an HTTP API because it's meant to be simpler. It abstracts some of the customization options from you. These are meant to integrate with custom endpoints on the back end and Lambda functions. They tend to be cheaper and they offer minimal features compared to the REST API type. And then lastly, we have the all‑powerful WebSocket API. WebSockets, if you're not familiar, essentially allow for two‑way communication sessions between a client and a server. With WebSockets, it allows your servers or your clients to essentially receive responses without having to pull one another for a reply. These types of APIs allow you to integrate with Lambda, HTTP endpoints, and other services like a RESTful type. However, they're invoked via front‑end WebSockets as opposed to a standard API call. So keep that in mind. Now, real quick, two use cases for Amazon API Gateway. If you need to deploy a RESTful API in AWS that's managed, this is a perfect solution. You can go ahead and deploy it in front of Lambda functions to perform custom data transformations within your workflows. The second use case, you can deploy it in front of Amazon Kinesis for data ingestion. So you can have your default API that you set up, and the users wouldn't even know that they're actually sending messages to Amazon Kinesis data streams on the back end because they would interact with your front door, in other words, your API. These are two solid use cases or possibilities with this service. Now that's going to go ahead and do it, let's wrap this clip up, and moving on, we're going to dive into some more specifics about API Gateway.

Amazon API Gateway Authentication and Security
All righty, we just got done reviewing API Gateway as a service, and now let's dive into authentication and security. First up, resource policies. Resource policies are resource‑based policies, which you should be familiar with, but you use them to control which principals can call an API. Now, typically principals are going to be AWS resources, but they don't have to be. The big thing to remember is a resource policy controls who can make a call to your custom API. Now within your policies, similar to conditions for IAM policies, they support specifying specific account users or roles, you can make conditions for source IP ranges, or you can even require that calls come from VPCs or VPC endpoints. And speaking of private traffic, when you're using a private API, these work in parallel with endpoint policies. So typically you want to use them together to really securely control who can access them. Now, we know a private API can only be accessed via endpoints and via VPCs, but to truly secure them fully, you want to make sure you're locking both types of policies down. Now a use case for a resource policy is maybe you want to deny API traffic based on source IP addresses or ranges. So in other words, maybe you know you have some bad actors or malicious traffic and you want to deny those IP ranges, you can implement that with a resource policy. Moving on to authentication options, you have to know how you can control access to your APIs via authentication and authorization. The first type is publicly accessible. So if you deploy an edge‑optimized API, typically they're going to be publicly accessible by default, which means anyone on the internet with your endpoint address can hit it. In addition to that, you can also control access to your APIs with IAM permissions. So you can say, hey, I only want cloud user to hit it, or I only want this EC2 role to be able to make an API call. These are valid options. The third option is a Cognito user pool. Now we'll talk about Cognito later on, much more in depth, but understand this is a way for authentication to be handled to control access to your API. This will generally be useful for mobile applications that need to scale very easily, and you don't want to handle that back‑end resource for handling your Active Directory or your user authentication. And then fourthly, you can use a custom authorizer, which is built using a Lambda function. Now on the exam you might see Lambda Authorizer, but regardless, it's the same thing. You control access to your API by implementing this type of authorizer. What happens is your client requests the method, API Gateway then calls your Authorizer, and that Lambda Authorizer can validate the identity of your caller and make sure everything is as it says it is. These are complex, but they're useful whenever you need custom authorization in place. Next up, we have API keys and usage plans. An API key is similar to an access key, but it's specific to our APIs. What these are are sets of alphanumeric string values that you essentially provide your customers to go ahead and grant them access to specific APIs and specific methods. So what you can do is in the resource policy, make sure that an API key is present before these can actually be called. So that's a way to restrict access. Now, with API keys, you can also set up usage plans. Usage plans allow you to set up who can access your API stages and methods, and it allows you to throttle the number of requests that are being made. That's a popular exam scenario. How can you throttle or control the number of API calls? Well, you can use usage plans and API keys. Moving on to more important security features that can come up. Now, you can use a custom domain name to front your API Gateway. When you do this, however, you have to leverage a TLS cert within ACM to deploy it via HTTPS, that is a requirement. Now, when you use an edge‑optimized endpoint and you want to implement custom domain names, the ACM certs that you deploy have to be created in us‑east‑1. That is very important to understand. Anytime, regardless if you deploy it in us‑west‑2, and it's an edge‑optimized API type or endpoint type, you have to create the cert in us‑east‑1, that's the only way it will work. On the other side of that, if you're just using regional endpoints, well then the certs have to be created in that same region that the API Gateway lives in. So that's a very key difference between those two endpoint types. I can promise you this is common on the exam, and you have to remember this. Now, moving on, you can set up your DNS for your custom domain name very easily using Route 53. If you do this, you would need to create either a CNAME or an alias record that points to your API Gateway with that alias value. Of course, you don't have to use Route 53, however, it's going to be the easiest because of all the integrations that are supported. And then the last point here, you can leverage AWS WAF, which is a web application firewall, in front of your API to prevent SQL injections and things like cross‑site scripting attacks. Now we dive into the security stuff here with WAF and some of the other things later on in a different course within this learning path, but just remember right now you can implement WAF in front of your API. Before we wrap things up here, one quick pro tip in general, not necessarily specific to the exam, but it's a good thing to know, Amazon API Gateway does allow for mutual TLS support. So if that comes up on the exam or you need to do that in real life, understand that API Gateway does now support this. Now, that's going to do it for this clip on security and authentication methods. Let's wrap things up, and I'll see you in the next one.

Amazon API Gateway Integrations
One of the biggest benefits of API Gateway within AWS is its ability to integrate with several other back‑end services. So let's look at some of those scenarios now. During this clip, I'm going to exclusively cover some common exam scenarios and the integrations that are possible with API Gateway. The big thing to take away from these is that API Gateway is not the only portion of the scenario, but it's a key part of the scenario. First up, let's talk about API Gateway and Lambda. In this scenario, let's assume you're creating a dynamic website with some static content and you want to minimize all server maintenance and patching. This website that you're building has to be highly‑available, and it needs to scale its storage read and write capacity as quickly as possible to meet that demand. Well, how can you achieve this manner? Based on some of these key indicators, we're leaning toward a lot of serverless services due to the minimization of maintenance and patching and the scalability of storage. So let's look at some now. A potential way to solve this could be hosting an Amazon S3 web page behind an Amazon CloudFront distribution. Remember, one of the big benefits of S3 is hosting static websites. You can do it for a very, very low cost, and it can handle scaling automatically. In addition to S3, you can stand up an Amazon CloudFront distribution, which we cover in a different portion of this learning path, but a big thing here is that it's a CDN that you set up in front of your S3 bucket's website to increase performance via caching. In addition to that, it also helps increase security using what is known as AWS Shield, which we cover in a security portion of this learning path. Now with this CloudFront distribution, you can easily get your static materials and static content from Amazon S3. In addition to the static content, you can then set up Amazon API Gateway to front anything on the back end from a logic processing standpoint. So for instance, if you have PUTs, POSTs, DELETEs, etc., well you can stand up Amazon API Gateway to then trigger an AWS Lambda function. Lambda functions easily integrate with your gateways to perform any back‑end logic or processes for dynamic content. In addition to that, remember the storage aspect. They wanted serverless storage, so in this case we can use Amazon DynamoDB. DynamoDB offers on‑demand capacity, which allows you to automatically scale your reading and writing capacity. So here is a quick summary of why you would choose this. Now I'm not going to read this out, I just included this so you can study it later on if you want to go through these slides on your own. The big thing to take away here is we eliminated server maintenance and patching with the serverless technologies, and we also designed a highly‑available service using CloudFront, Lambda, etc. Moving on to the next scenario, API Gateway and Kinesis. Let's assume the following scenario. You have thousands of remote IoT, or Internet of Thing, devices that send data to a RESTful web application that's currently running on EC2 instances. The instances that it's running on, or the singular instancem really, receives the raw data, it transforms the raw data, and then it stores all of the data in an Amazon S3 bucket. Now, let's assume you expect the number of devices sending data to grow into the millions very soon. Because of this, you need a better and highly‑available scalable solution that minimizes operational overhead. In my mind, immediately I'm thinking of some type of managed service. EC2 is good; however, if you want to get rid of operational overhead, there's better choices. So let's look at an architecture example here. On the left side here, we have examples of our IoT devices. Well, what you can do is you can stand up Amazon API Gateway, and your devices can put all of the data in real time to your REST API that you deploy globally or regionally. Behind that API Gateway, you can set up other AWS services. So in this case, we can stream to Amazon Kinesis data streams. Because of this direct integration, we can essentially stream the data coming in from the devices in real time to our back‑end data stream. As that data is coming into our data stream, we can then set up a Kinesis Data Firehose, which is feeding off of the data in the data streams. So the Data Firehose is ingesting data from the data stream, which is receiving data from our API Gateway. Amazon Kinesis Data Firehose allows you to scale automatically, and it directly integrates with Amazon S3 directly to put your raw data. Once that data is automatically delivered to a configured bucket, you can then use a service like AWS Glue, for example, to go ahead and crawl and then process that data that lives in the bucket. Remember that AWS Glue is an ETL process or ETL service that allows serverless crawling, processing, transformations, etc. The big benefit is that it directly integrates with S3 buckets. Again, here is a summary of how we would go ahead and approach this or why we would approach it and use these technologies. The big thing here is we use serverless and scalable services using Kinesis, Data Firehose, S3, and Glue, and it really minimizes all of the operational overhead for us. Awesome. Moving on to the next example here. Now this is just going to be high‑level because it doesn't appear on the exam too often, but I want you to understand from an architectural standpoint how it would work. Remember, you can set up custom HTTP endpoints behind your API Gateway. So we can create a standard API for our users to use, and on the back end we can host our own application, so maybe it's running on EC2 or maybe you have an Application Load Balancer that's fronting Amazon ECS tasks. The big takeaway here is if you have a custom HTTP web application already running and you don't want to get off of those services that are hosting them, you can easily set up API Gateway in front of those services to go ahead and create a standardized interface. Overall, the big thing to take away from this lesson and clip in my opinion is that you can use API Gateway to set up a friendly standardized front end for interacting with almost all other AWS services. So instead of invoking a Lambda function, your users can make a REST API call that then invokes it automatically. Let's go ahead and wrap up here, and then when you're ready, we'll move on to some more clips.

Amazon API Gateway Deployments and Versions
All righty, now that we understand a little bit more about API Gateway, let's talk about some specific features called deployments and versions. To publicly leverage your new RESTful API that you create, you create what is called a deployment, so you actually deploy the API for use. These deployments then get associated with a stage that you get to name. A stage in this service is basically just a snapshot of the API itself with all of the methods, the responses, the different integrations, etc. Pretty much all of your settings and configuration options get captured within a stage. With stages, they offer you the ability to perform optimizations and customizations. What I mean by this is you can do things like enable caching to speed up latency, you can customize throttling for usage plans, you can enable logging to CloudWatch to make sure everything's working as expected, and you can even enable canary testing. So if you want to do a small test with a new release, you can control how much traffic is hitting that new API compared to the old one. Now, that's going to do it for this very short clip. Understand deployments and stages at a high level, you shouldn't have to know them too in depth, but you do need to know them for the exam in general. So let's go ahead and wrap up here, and we'll move on when you're ready.

Demo: Creating a REST API Using Amazon API Gateway
Okay, let's get started with a demonstration here. In this first demo, we're going to create a brand new RESTful API using API Gateway. Before we jump into the sandbox, I just want to do a high‑level overview of what to expect. We're going to create a very simple API using a Lambda function on the back end to perform all of our business logic. So with that in mind, we're going to create a few different resources. We're going to create our Lambda function, which I will provide code for you for within the module assets, and then once we verify the Lambda is in place, we're then going to work on creating our API and setting it up to integrate and send requests and receive responses from our Lambda function using a regional API endpoint type. So, with that out of the way, let's go ahead and jump into the console now. All right, I'm in my console here in us‑east‑1, before we go ahead and create our API here, I want to show you some of the underlying functions or resources that I've already created. Probably the most important thing as far as being already created is our QuoteFunction. This is the Lambda function we're going to use to perform our workload. Now this code that we're about to review here will be available for you to use within the module assets, but at a high level, all this does is it prints out the event that it receives, and then it gets a quote from a website and returns the quote and the author. So it's very simple, we'll demo this after, of course, we implement this with our API, but understand this is created for us already, and I provide the code for you to use. So, with that out of the way, the next thing I want to do here is go to API Gateway, and the first thing we do is choose our API type. So do we want HTTP? Do we want a WebSocket, RESTful, or RESTful Private? For this, I'm going to choose Build REST API because I want to see all of the feature options. From here, we can select creating a new one, we can clone an existing, we can import from an OpenAPI definition file, or we can use an example one to go ahead and play around with. For this, I'm going to create a new one, and then I give my API a name. We can then give our API a description. And finally, we choose the API endpoint. So we have Regional selected, which means it gets deployed into the current region, we have Edge‑optimized, which uses CloudFront to lower latency globally, or there's Private for things that only want to be accessible from VPCs. For this, I'm going to choose Regional, and I'm going to create my API. There we go, we now have our awesome API here, we've selected it, you can see the API ID here, which is what gets referenced in Infrastructure as Code typically, let's go ahead and start configuring this. Now that we have the overall resource, we need to actually create an actual resource for the API. So what I'm going to do is create resource here, we'll choose the default resource path, and then I'm going to give my resource a name. So since we're working with quotes, I'm going to call my resource quotes. From here, I create it, and there we go, we now have our quotes resource and we can start creating methods for it. So I'm going to create a method, and from here this is where we kind of get some more configuration options you need to be familiar with. First up is for the method itself for our quotes resource here, we choose the method type. So, do you want any HTTP method, GET, POST, PUT, etc. For this, I'm going to select GET, so I only am going to allow GET requests to this resource. From there we have a lot of integration type options. So I'm going to skip Lambda for now because we do that in the end, but you'll notice we can select an HTTP endpoint where you select the back‑end method, as well as the URL, you can set up a mock for testing, you can integrate with other AWS services like SQS, DynamoDB, etc., or you can integrate with resources within a VPC link. Now for this, what I'm going to do is choose Lambda function, and then let's go ahead and set this back end up. You're going to notice there's a proxy integration flag here, so I can turn this on or off based on my requirements. All this does is it reformats the event that's getting sent in in your message body, and it sends it to your back‑end Lambda in this structured event that it's used to seeing. So when you print an event with Lambda, this is going to try and reformat everything coming in to follow that structured event. Now we're not passing anything in, so I'm going to disable this, but if you're actually trying to pull data, you would likely want to turn this back on. So for me, I turn it off, and then I select my Lambda function, so my region, and then I choose the ARN, so QuoteFunction. Next up we have the integration timeout. So, you can set either 50 or 29,000 ms, so 29 seconds. This is a service quota that you can raise, however, if you're having an API that takes more than 29 seconds, there's probably something else you need to worry about. So for me, I'm going to leave this default, if it ever hits 29 seconds, it will time out, we shouldn't have to worry about that, so I'll leave it. The next thing I want to show you is the method request settings. So under here is where you can require authorization to hit your API. So right now I have it set to None, which means it's publicly accessible. However, I could say I want IAM permission requirements in order to call this, so you have to have a valid signature to hit this API that's signed by IAM. We could also require an API key, so you can say, okay, maybe this is public, but I'm going to say only someone with a valid API key that I create and give to them can actually call my API. So for this we're going to skip the rest, you can set queries and parameters, request headers,etc., and I'm going to create my method. And, there we go, just like that, we now have our GET method on our quotes resource. On this console here we can review everything, so there's four different portions of an API call within API Gateway. We have the method request, the integration request, the integration response, and then the method response. All of these have their own settings that you can configure. Now for the exam, we don't need to do a deep dive into this, so I'm going to show some of the ones that are more popular and actually required. First thing I want to show you is authorization. So we said none, but remember, we could have chose IAM or API keys, but within your API you can also create a custom authorizer. So under Authorizers here, I can actually create a brand new one, and it could be a Lambda‑based or a Cognito‑based. Now this is out of scope in terms of actually creating and using, but understand you can create your own Lambda custom authorizer or use a Cognito user pool. We'll cover Cognito in a different module within this course. For now I'm going to click on Cancel, go back to my Resources here, select my GET method, and the next thing I want to show you is I just want to test. So under testing here, I can click Test, Test, and let's see what happens. Perfect, we get a status code of 200, it took 991 ms, and we get our response body. So, quote, author, everything seems to be working, and you'll also notice with testing, we can get our response headers and some logs for this execution. So we see the method, we see the paths, we see all of the other optional stuff that we did not set, and then we see some of the invocation specifics, so the URI, we can see the Lambda, status code, etc. This is working perfectly. So now if I go to my function and I go to Monitor, I'll click on View CloudWatch logs here, let's see what's going on with our logs. Okay, so we have our log group, I see a log stream, if we go in here, we can see it's actually executing. Now in the code I printed out the event and then whatever event was passed in, and you'll notice there's nothing because we're only getting, we didn't pass any message body yet. Okay, so our Lambda is invoking as expected. What if we want to log our API Gateway calls? Well, we have to turn that on. So under my API Gateway here, I've selected my awesome API, and what I want to do is I want to deploy and then create a stage. So under Deploy API, I'll click this, and we choose a stage. Remember, you create a stage, which is a snapshot essentially of your API configuration options. So I'm going to click on New stage, I'm going to call this test, and then I'll click on Deploy. So it's created a new deployment and a new stage. You'll notice now that we created a stage, we get a specific URL that we can test with our own software or locally. In addition to that, this is where we turn on logs and tracing. So notice here that everything is inactive. You can't do it at the resource itself until you create a deployment and a stage, so really remember that for the exam. Now in here, what we can do is turn this on. Now I'm going to tell you really quickly this is going to error out, and I'll show you why, it's because we don't have a role attached. I would select my level of logs, maybe I want to do data tracing, metrics, turn on X‑Ray, and I would click on Save. Now this should error out because I didn't set a logging role yet, which is required, this is very important to remember. What you have to do is set a logs role for API Gateway to use. So I'm going to click on Cancel, and under Settings down here, I'll click on this, ignore this, this is a bug, what I'm going to do is load IAM. I'll look for my role. I'm going to look for my API Gateway role, select this, copy my ARN, and under our Settings, I'm going to go in, replace this, and I'll click on Save changes. Now that was just a bug, so ignore that previous role ARN, I don't know why that was in there, sometimes the UI is just buggy, but now what we've done is set our log role. So under this role, all it's doing is allowing API Gateway to log to CloudWatch logs. So now if I go back, select my API, select my stage, go down here and turn on my logging, click on Save, we've now integrated or enabled logging and tracing whenever this stage is called. So to test this, what I'll do is I'll copy this, I'm going to jump over into my Postman application, we have my GET method, I'll paste in my URL, I have to make sure I give it my resource, which is quotes, and then I can click on Send. And there we go, we now get our response. So we have our quote, we have our author, and now each time I invoke this, we should get a different quote. Perfect, so our API is working, we're using our regional deployment and stage that we created for our original resource. And now what I can do is if I go back to my console, I'm going to load up CloudWatch here, let's go to Log groups, we're going to notice, first of all, that it created a default welcome log group to test. In addition to that, we now have our API Gateway execution logs here, we'll have log streams eventually, and now it will log to CloudWatch logs any time this is invoked. Now we're not going to go ahead and wait for this, this could take several minutes, it's not real time, the big thing to takeaway here is when you're doing this, you must grant API Gateway permissions via an IAM role in order to enable any CloudWatch logging. Now, one last thing before we wrap this up, I wanted to show you the other authorization method for API keys. So under API keys here on the bottom left, this is where you can create an API key specific to customers or testing, etc. So what we can do is create one, I'll just call this client, you can give it a description, it'll auto generate, and it creates an API key that they can pass in as a header when they're hitting your API. So this is how you would use that restriction option in authorization that we looked at earlier on. In addition to an API key, you tie them to usage plans, so we can create a usage plan called client, and in here you can create a plan for usage, you can create throttle limits, you can create quotas, and you can enter the total number of requests that someone can make using this usage plan per month, per day, or per week. So maybe we want to say, okay, you're a premium user, you can have 1000 requests per month, I'll turn off throttling, and I'll click on Create usage plan. So with this, what we do is you have to then tie your API key to this usage plan. So we associate it to a stage, which could be our awesome stage that we deployed on test. We assign it to there, and then from there you can associate API keys. So maybe we want to say our client that we created earlier now falls under this usage plan, and they get 1000 requests per month for our associated stage here, which is test. Now this is also a very popular exam scenario. Big thing to remember, you create an API key for a client, and then with that API key, you can create usage plans to either throttle or open up quota bursting for that client. Now that's going to do it. Let's go ahead and wrap this API Gateway demonstration up. We created a regional API Gateway, we deployed it to a new stage, and we just invoked it using the URL with our resource and our method. With that being said, let's go ahead and wrap up, and I'll see you in an upcoming clip.

Demo: Using TLS and Custom Domain Names for API Gateway
All right, welcome back to another demo. In this demonstration, we're going to use a previously created API. I've added a few resources and methods to it, and we're going to deploy a custom domain name using a TLS cert. Before we jump in, here is a very high‑level on what we're going to accomplish here. We have our API Gateway already created, and what I've done is I've integrated Amazon SQS on the backside, which we will review, and we're going to set up a brand new custom domain name with our own custom TLS cert, and we're going to test an API call. So, with that out of the way, let's jump into the sandbox now. All right, so we're in the sandbox here. Real quick, I want to review some of the resources I've created already in the background that we're going to use for this demo. First things first, what I've done is I've used an existing API that we created in a different demonstration, and I just added on to it. I created a /messages resource and a POST method for that. Now with this POST method on this resource, what I've done is I've integrated it with a service called Amazon SQS. So what's going to happen here is this API is allowing us to post a message to an SQS queue that we've granted our API Gateway permissions for. So in here I'll do this at a very high level, we've set it up for SQS, it's posted, we set up the path, etc., some of the other settings. The big thing to notice here is the execution role. So yes, you can allow API Gateway to interact with AWS services, but remember, it needs an execution role granting permissions. So this role allows us to send messages to my queue. Now with that being said, what I've done is I deployed this updated API to a test stage, and you can see the resource in the method here, and you see our Invoke URL. So if I copy this, I jump over to Postman, I'm going to set this to POST, I'm going to paste this in, and I'm going to copy and paste a message body in here. So under Body, I'm going to go ahead and say raw JSON, paste this in, Hello default URL, I'll send it, and we get a response. So now if I jump back into my SQS queue and I start polling for messages, we'll see the message here. Now, SQS is out of scope for this module, but we do cover this service later on much more in depth, but for now, just understand it's a message queue service in AWS. We see our message here, and we see the body that we put in. Perfect. So I'm going to go ahead and delete this message off my queue, and let's begin testing with a custom domain name. So under API Gateway, what I'm going to do here is find custom domain names. I'm going to add a new one. We're going to give it a name. So what I'm going to do is call api. public hosted zone that is available in this account. So I'm going to copy this in, and there we go. So this will be the domain name we use to invoke our API resource. It's a public API, and now we get down to our public domain config. So, very big thing to remember here for the exam, there's regional and edge‑optimized for custom domain names. When you choose to use a custom domain name, you must select an ACM certificate for the secure communication. When it's a regional API endpoint type, the certificate gets deployed to that same region that the API resource was deployed to. If it's an edge‑optimized API, then you must always deploy your cert to us‑east‑1. Now, I can't do much in other regions than a sandbox, so I have to just call this out. Edge‑optimized APIs require certs in us‑east‑1 always. Regional APIs require the regional cert to exist. So we are in us‑east‑1, so I'm going to select Regional, I'll leave the defaults here, TLS 1.2 as the minimum, we're not going to enable mutual TLS, and you'll notice, well, I don't have a TLS cert, so let's create one. I'm going to go to Manage certs here, and let's create a new cert, so I'm going to request a new public certificate, we're going to call it api. domain name, DNS validation, accept defaults, and request. Now we cover ACM much more in depth in a different course within this learning path. High‑level, this is a certificate manager service for public and even some private TLS certs. The big thing for the exam is to understand you can issue public certificates and assign them to supported AWS resources. Now for this cert, we still have to say, hey, I own this domain, you can go ahead and create it. So, I have to validate ownership. Now since we're using Route 53, there's a shortcut here, so I'm going to create records in Route 53, and if I load my hosted zone, I refresh, it should have created our records for us, as you can see here. So now, if I go back and look at certs, it should be issued here in a moment, and after a few refreshes, it's now issued. So now we can use this for our custom domain name. So I'm going to go back to API Gateway, refresh, select my ACM cert, and then click on Add domain name. Now, there's just a bug here, so you can ignore this, it disappears, it's the wonderful world of AWS UIs, but it did work. So now you notice we have our domain name, we have our settings, it's available, and, a very important thing to note here, an endpoint configuration. So what this has done is saying, hey, we're going to allow you to map this custom domain name with your supported cert, and it's essentially pointing to this gateway domain name. This is created and managed for us by API Gateway. Now, to actually reference this, a few things we have to do, we have to create a DNS record, and we also have to map this custom domain name to an API. So let's start with the Route 53 record. I'm going to go ahead and go to Route 53, I'm going to create a new record, I'm going to call it API, it's an A record type because it lives in AWS, and it's an alias. So for the alias, I want to route traffic to an API Gateway API in us‑east‑1, and the endpoint is our endpoint that we saw over here, d‑mcdj, d‑mcdj, so it's mapping an alias to that endpoint that was created for us via the service. So I create my records, it now is in our hosted zone here, and I can reference this. Now the next thing I have to do under API Gateway here is we set up the DNS, we now need to map this custom domain to one of our APIs. So we're going to map it to our awesome API. So I'm going to configure mappings, add new, select my API, you select the stage that you want, so we only have test, and then we give it a path. So I'm going to call this v1 for version 1, click on Save, and we now have our mapping in place. So what this is saying is that path here, version 1, is going to be used for our custom domain and it's going to point to our awesome API's test stage. So, now we can test things out. So what I'll do here is copy my domain name, I'll go over to Postman, I'm going to create a new here, make it a POST, paste in HTTPS. API, we're going to say /v1/messages. Remember, the custom domain name with this path is pointing to our test stage for our API, so it's a mapping. The messages was the resource and we had a POST method. So now if I go in here, I'll go ahead and copy and paste in a different message body, make sure it's set to JSON, we're saying Hello custom URL instead, I click on Send, and we get that response again, so this is working. Now if I go into my SQS queue here and I poll for messages, we have a new message, Hello custom URL. And, there we go, we've now implemented a custom domain name with custom TLS certificates for our RESTful API. Big thing to take away here is to remember the certs, where they go. Regional must go into that region. Edge‑optimized require them to live in us‑east‑1. Now, that's going to do it for this demonstration. Hopefully you saw how easy it is to create a custom domain, map that to your API, and then test it. Let's go ahead and wrap up here, and I'll see you in an upcoming clip.

AWS AppSync
Okay, we're done with API Gateway, let's look at AWS AppSync. AWS AppSync is a robust, scalable, and serverless GraphQL interface for developers. That is going to be the thing to watch out for, GraphQL. If you're not familiar with GraphQL, it's basically just a specific query language for APIs that enables your applications to fetch data quickly. It works by combining data from multiple sources. In AWS, that could be sources like Lambda, DynamoDB, etc. With AppSync, this also supports API keys like Amazon API Gateway does, it supports IAM control, you can leverage Cognito for user authentication or an OpenID connector, or you can also even use Lambda authorization. In other words, it's very similar to API Gateway in terms of controlling who can actually hit your GraphQL API. One thing to know for AppSync, you can only use JavaScript or TypeScript at this point for creating your back‑end logic. So if it's any other language than that, you're going to unfortunately have to use something else like API Gateway. Now real quick exam pro tip to wrap things up here. If you have a specific requirement for setting up GraphQL in AWS, you should immediately think AWS AppSync. That is going to be the key indicator and key use case for any scenario within AWS. With that being said, let's go ahead and wrap this short clip up, and we'll move on to the next one.

Debug Applications Using AWS X-Ray
Okay, let's move on to talking about debugging applications in AWS using AWS X‑Ray. AWS X‑Ray is a service that collects data about requests within your applications, and then it gives you tools to view, filter, and even gain insights about that data, which are there to help identify potential issues and even opportunities to optimize your applications. With X‑Ray, there are some important concepts you need to understand regarding the exam. The biggest thing is that it's perfect for insights. It collects application data for you to view, filter, and gain those insights we talked about about both requests and responses that are handled by the back‑end components. By enabling X‑Ray, you can easily view all of the calls to any downstream AWS resources and even other custom microservices or APIs that you're hosting and database storage. Pretty much anything within a request and a response can be tracked and easily viewed. After you enable it, each of those individual portions of the request or response workflow provide what is called a trace. These traces provide an ID that you can use to gain further insights, so you can see which portion of your application workflow is taking longer, which can go faster, which is breaking, etc. Now when you use it with integrated services in AWS like API Gateway, Lambda, etc., they can actually add tracing headers, they can automatically send trace data, and they can even automatically run the X‑Ray daemon. Now, the X‑Ray daemon is something that runs on static compute. So, for instance, you might run this on an EC2 instance if you're running an application that's hosting APIs there. By default, it's going to go ahead and capture all of the important trace information, which you can then visualize to see how long certain aspects are taking. Now, on AWS there are several built‑in integration options that are very easy to use. In fact, this is the list here. What I've done here is I went through and selected what I would consider the most common on the exam. So these commonly appear on the exam when we're talking about AWS X‑Ray. The most common out of all of these, which I've highlighted, I would probably say is API Gateway. If there's ever a scenario where you need to trace, think AWS X‑Ray. And, speaking of exam scenarios, let's go over one now where this is a good choice. Let's assume you're running a microservices app on ECS, and the different microservices interact with each other. Well, you need to ensure that there's some type of observability to identify performance issues that might come up. You can do two things here. You can send app logs to CloudWatch, which is a logging service, and you can also configure X‑Ray to trace requests between the microservices. This is a perfect use case for X‑Ray. It can say, hey, microservice A took about 400 ms to respond, and then two, microservice B took about a second. So, at that point you can see, well, microservice B is taking longer to respond, maybe I should troubleshoot that. Moving on to an exam pro tip here. You can simply toggle X‑Ray on for X‑Ray support when you're using Lambda functions. It's a configuration option, and it allows you to very easily trace your serverless components. Again, this is a very common scenario on the exam. You front Lambda functions on the back end, and you have API Gateway in front of those that your clients interact with. When they make that API call to your API Gateway, it invokes a Lambda function, and you can trace all the way from end to end the requests and the responses. Now, that's going to do it for this clip. Big thing to take away here, if you need observability from end to end for all of your downstream resources in an application, go ahead and think X‑Ray. Let's go ahead and wrap up here, and I'll see you in the next clip.

Module Summary and Exam Tips
Okay, once again, way to hang in there, we've reached the end of this module. Let's have a quick summary and exam tips clip before we move on. First up, let's review Amazon API Gateway. This is a very popular service that appears on the exam quite a bit. Remember, API Gateway is perfect if you have a scenario where you need to create, maintain, publish, monitor, and even secure your own custom RESTful APIs. One of the very nice things about it is its integration abilities. It very easily integrates with Lambda functions for back‑end processing, you can configure it to connect to your own custom HTTP endpoints that you might be hosting on EC2, or EKS, or ECS, and, honestly, you can hook it up to pretty much almost every other AWS service. It allows you to create your own RESTful interface for clients, customers, and applications, and they don't even know they're interacting with AWS services on the back end. For instance, maybe you want to publish messages or stream messages, you can send those to SQS or Kinesis, and it's all done via a verified, easy‑to‑follow framework using a RESTful API. Next, you have to know the endpoint types and their use cases. Remember, there's edge‑optimized, these are deployed to regions, but they're called around the world at points of presence, so all of the requests go through a CloudFront distribution to speed up latency. Edge‑optimized is the default option for RESTful APIs in API Gateway. You then have regional. These also are deployed to a region. When you use a regional API Gateway, this is going to be usually specific to regional traffic, so maybe you have customers that are in a particular region that you want to focus on. Remember, with regional, you can actually create several regional APIs, so maybe us‑west‑2, us‑east‑1, and you can use Route 53 latency‑based routing to direct traffic to the lowest latency. And then lastly here we have our private APIs. These are deployed and only accessible via VPC traffic or through something like a Direct Connect and VPN. You control access to these via resource policies on the API and VPC endpoint policies. In addition to the endpoint types, you also have to know authentication options. Do your best to remember the four different ways you can control access to your APIs outside of the network aspect of things. You can make them publicly accessible so anyone with the endpoint URL can hit them. You can control access via IAM authentication so they have to have a valid signature from an IAM role or user in order to hit it. Or, you can also use Amazon Cognito. Now we cover Cognito in a separate module in this same course, so if you're not familiar with it, don't worry about it, just understand you can use it for authentication with API Gateway. And lastly, if you have any type of custom requirements for validation and authorization schemas, well, you can use a Custom Authorizer via AWS Lambda. Again, these might be called Lambda Authorizers on the exam now, but it's the exact same thing, they just changed the name. Now, two more things with API Gateway. Remember, if you use a custom domain name, it requires ACM certs for TLS. You have to have a publicly‑trusted cert for a custom domain name to work. In addition to that, this is very tricky. If you're using an edge‑optimized API Gateway, even if you deploy that resource into, say, us‑west‑2, or eu‑west‑1, well, no matter what, your cert has to be deployed into the us‑east‑1 region. On the other side of that, if you deploy a regional API Gateway resource, well, then the TLS cert has to be deployed in that same region. So, again, if you deploy in us‑west‑2 and eu‑west‑1 and you want to use custom TLS, well, you have to make sure that you deploy a cert in each of those regions within ACM. This is a common scenario on the exam, it gets very tricky. Just remember, edge‑optimized goes to us‑east‑1, regional goes to the same region. Wrapping things up here with AppSync and X‑Ray. AppSync is pretty easy to remember, just look for anything related to deploying GraphQL‑supported applications, that's going to be a direct indicator for AppSync. And then we have X‑Ray. This is perfect if you have any requirements where you have to enable observability, or, maybe you have to implement traceability into your applications. This easily allows you to trace end to end all of your requests and all of your responses. This is going to be perfect if you need to identify performance issues, or it's perfect for if you want to look where you can actually improve and optimize your application. Now with that being said, let's end this module here. Thanks for hanging in there. Go ahead, take a break, grab some water, take a nap, and I'll see you in the next module.

Serverless Authentication with Amazon Cognito
Amazon Cognito Overview
All right, you've deployed your application, but now you need to set up authentication mechanisms. Well, let's talk about serverless authentication using Amazon Cognito. First things first, let's review what this service is at a high level before we dive into deeper components. Amazon Cognito is an AWS‑native identity provider that offers you authentication, authorization, and user management. Amazon Cognito allows you to easily integrate with your web and mobile applications, and they say it's all doable without needing custom code. Now, I'm not sure how entirely true that really is; however, the point that they're making is it's very easy to integrate with. So remember that for the exam. When you use Cognito, and you have users within your authentication service, well, these users can be granted the ability to sign in directly using a username and password, or they can use a third party, so Facebook, Amazon, Google, or even Apple. Essentially, they can use those social third‑party identity providers to validate and log in to prove they are who they say they are. One of the biggest offerings that Cognito has is the easy scaling for userbase growth. It allows you to not have to worry about the underlying infrastructure, and you can scale with your user demand. Another important key indicator on exam scenarios, this is a serverless offering. If you have a serverless requirement for authentication, you might consider Cognito. Now let's talk about three exam tips for Cognito. First up, it is entirely serverless, so if you need an entirely serverless design for a web or mobile app, you might consider using this. In addition to that, it can scale, and it can scale very well. It can scale for hundreds, thousands, and even potentially millions of users if you need it to. And then thirdly, it is perfect if you need any type of mobile user sign‑up or mobile user authentication. If you have a mobile app, I would also consider Cognito for user authentication and authorization needs. Moving on to the last two points here, Cognito pools. We're going to talk about each of these in upcoming clips. First up is a user pool, and you have an identity pool. Now I'll tell you really quickly, this is why I actually cannot stand Cognito sometimes is they're naming conventions, these can get very easily confused on the exam when you're reading through scenarios. So what we're going to do is we're going to break here, and we're going to dive into a user pool and an identity pool in their own clips to really dive in to understand what they do.

Amazon Cognito User Pools
All right, are you ready to be confused with some naming conventions? Well, I hope so. Let's go ahead and dive into user pools first. User pools are very important on the exam. You have to understand what they do, what they offer, etc. A user pool is simply a directory of your users that provides a sign‑up and sign‑in option for your application users. It provides your authenticated users a JSON Web Token that has permissions and grants to perform actions within your applications. Now, authentication options include basic auth, of course, that means username and password, and you can use federated identities using third‑party IDPs like we discussed in a previous clip. So Google, Facebook, Amazon, etc. User pools also allow you to enable and even require multi‑factor authentication for the users. You can also allow your users to reset their own passwords, and you can implement requirements for email or phone verification to make sure you're not signing up a bunch of bots or a bunch of spam users. The fourth thing here which can come up on the exam, I've been seeing it more and more, is adaptive authentication. Adaptive authentication is a feature in Cognito that allows you to essentially block suspicious sign‑ins or add another second factor authentication method based on identifying an increased risk level for users. So, if a basic user pool is not enough and you need an extra layer of security, you might look at adaptive authentication. Now let's look at an exam scenario for a user pool. In this, we're going to leverage API Gateway. Let's say you have an application running in AWS and you've decided to use Cognito to manage your mobile users. This application hosts a RESTful API that performs some custom back‑end logic. What you need to do is implement an AWS‑managed solution to control who can access and call your RESTful API. Well, within Cognito, your user pools verify your authentication, and then they provide the users a JSON Web Token that grants downstream permissions. So they sign in and they get a validated token. Now, with your sign‑in in place, Amazon API Gateway can actually leverage Cognito user pools as an authorizer to restrict access to your APIs. If you remember in the demo, we actually looked at setting one of those up at a high level, you can use Lambda or user pools. So, in theory, these users, now that they've received their token, can now make the RESTful API call, and they pass in that token within an authorization header. What happens is API Gateway validates that token, and then it can start making the back‑end service calls based on your application setup, so now your users can make the authenticated calls and they can interact with your back end, or maybe you're hosting your own custom app via API Gateway. Either way, the big thing to take away here is that user pools allow interactions within your applications. Next up, let's look at an Application Load Balancer. Let's assume you have an app and it's running in AWS in a VPC. You currently front your web application using an internet‑facing ALB that listens on 443 for HTTPS traffic. What you want to do is set up serverless authentication via AWS services to control access to your application. Well, in this example, we can assume that your ALB rules listen for HTTPS on 443, like we looked at, and you can configure it to have an authenticate Cognito action in place, which offloads authentication workloads to the load balancer instead of your application. So now when your user runs a GET command on our ALB, well, that's going to go ahead and offload the authentication to Amazon Cognito. So now Cognito performs the authentication flow to validate that user's authentication and authorization, and it uses a grant code sent by the ALB. After that valid token is returned, it now gets sent to your back‑end web applications. So an authentication session cookie is validated by the ALB, and then user information is sent to the back‑end targets using a specific headers set that you see here. Now, I'll tell you, you don't need to know the exact header set name, the big thing to take away for the exam is that you can use Cognito for authentication via an ALB. The other big thing to take away is user pools allow you to configure authorization and access into your applications. Please remember that for the exam. Now, with that being said, let's wrap up here, and then we're going to move on to the other confusing concept known as an identity pool.

Amazon Cognito Identity Pools
All righty, we just looked at user pools. Let's look at the other pool type called identity pools. Cognito identity pools are very important. They allow you to give your users access to AWS services via temporary credentials. So this is a drastic difference from a user pool. Identity pools allow them to access a service, user pools allow them to access your applications. How these work is users authenticate via a third‑party IDP or even a Cognito user pool. Once they're authenticated and validated, what happens is those users can leverage IAM roles with their assigned policies to go ahead and get the required user permissions to interact with AWS. Now with these, you can assign a single IAM role for all authenticated users, or you can base the role on user characteristics. So, in other words, maybe you have premium and free users. Well, maybe the free users have limited permissions to access some files in an S3 bucket, and then your premium users can access all of the content in all of the buckets. That's a perfect use case for assigning different roles to different pools. Another key concept here is guest access, also known as unauthenticated access. You can set unauthenticated access up. What this means is that users don't authenticate, but they can still have an IAM role with heavily‑restricted permissions. So again, like that previous example I just talked about, maybe you have a free tier and you want to allow everyone in the world to get a free document, but if they pay for a premier access or a subscription, well then you can grant them more permissions with a different role. I know that can be a little confusing, so let's look at an architecture with an exam scenario. Let's assume you're hosting an application in AWS and you're already using a third‑party IDP for user auth. What you need is a secure and scalable method to allow users to access their assigned objects in Amazon S3. So with this, the application users would go ahead and authenticate against their chosen IDP, which can then return a token. After that validation and authentication is in place, Cognito then validates that token that was received against the configured identity provider. So it exchanges the token for temporary creds, and it validates that the token is even valid in the first place. Once it's validated and the exchange is complete, what happens is each authenticated user will receive a set of STS credentials based on their assigned IAM role with their specific permissions that are required to access S3 objects. After the credentials are returned to the user or within the mobile application, they can then use them to get an object. So in this case, they can get their user PDF file using the temporary credentials from STS. The big thing to take away from this architecture is that identity pools, once they validate authentication, provide STS credentials for temporary access to an actual AWS service. Again, this gets very confusing on the exam, so really understand how this is used. Moving on, let's look at authenticated versus unauthenticated access. At a high level, you have to understand both of these. When a user is authenticated in an identity pool, that means they're validated and they receive temporary credentials based on an assigned role that you set. With unauthenticated or guest access, that means they have not logged in and they're not authenticated, and you need to assign a default IAM role. Now, you can assign a custom IAM role for this as well, but at worst, you have to assign a default role for unauthenticated access if you allow it. Last thing here, a pro tip. Guest access, or unauthenticated access, is useful for showing limited parts of an application's content to all of your users. With that out of the way, let's go ahead and wrap up identity pools. Big thing to take away, please remember they grant temporary access to AWS services. Let's end here, and we're going to move on and wrap up our module.

Module Summary and Exam Tips
Okay, once again you hung in there, you made it to the module summary, let's talk about some Cognito exam tips and summary reviews before we move on. First things first, Cognito. Remember, for the exam, this is an AWS native serverless identity provider service. It offers you several capabilities, including authentication, authorization, and even user management. One of the benefits of this service is that it's said to integrate with your web and mobile applications without needing custom code for authentication and authorization. So if you see that as a requirement, I would immediately consider Cognito. For user pools, remember, these are user directories that provide sign‑up and sign‑in options for users of your application directly. In other words, you use user pools to grant access to your applications. Within a user pool, you can require MFA, you can require email or phone verification, and you can even turn on adaptive authentication if you need additional protection. And then identity pools. These allow you to give users access to AWS services. That access is granted via temporary credentials that are generated by STS, and they're based on roles and permissions that you set. Again, you have to understand the difference between identity and user pools. Identity pools grant access to AWS services. In addition to that, remember, authentication is done via third‑party IDPs, or you can use Cognito user pools as well. Those are the two common scenarios and options for the exam. Now, with that being said, let's go ahead and wrap this up, very short summary and exam tips clip. Go ahead and take a break if you need it, but whenever you're ready, I'll meet you in the next module.

AWS CloudFormation and Infrastructure-as-Code Services
Infrastructure as Code Review
When you're working within AWS, deploying your services and your Infrastructure as Code is a critical concept you have to understand. So with that in mind, let's actually begin this module with a review on what Infrastructure as Code is and the benefits that it brings. Infrastructure as Code as defined by AWS themselves is simply the ability to provision and support your infrastructure using code instead of manual processes and settings. Now with Infrastructure as Code, or IAC for short, there are many benefits that go along with using it. When you have your infrastructure defined as code, you can quickly duplicate and deploy your environments across different regions and different AWS accounts. Just imagine all of the time that you're saving when you don't have to click around within the console or run some type of script or CLI command. Another huge benefit is it helps you avoid human errors. No matter how careful everyone is, you're always going to make a mistake when you're manually doing something. Now it might not be a lot of mistakes, but they're still mistakes. When you set something into a templated file as code for resource deployment, it's going to help you easily avoid those errors because everything is being templated and deployed from the same set of files. In addition to avoiding human errors, you can also integrate with source control for iterations and controlled upgrades of your environments. So what this means is you can use something like CodeCommit, or GitHub, or GitLab, and track the changes to your environments based on those code files. So if developer A makes a small change to a resource setting, well, you can see when that occurred and who actually made that change, and you can control whether or not you want to deploy that change. And, lastly here, it helps you implement a controlled and planned resource tagging approach. When you use the tagging approach within your templated files, it allows you to easily track cost of the resources and the applications that belong to a specific environment. Now all of these on here are best practices and benefits for using Infrastructure as Code. So if you're still thinking, well, why would I use this? Well, Bill Gates said it best. He chooses a lazy person to do a hard job, and that's because the lazy person is going to find the easiest way to complete that job. Now, of course, when you use Infrastructure as Code, you're not being lazy, but that's just a nice way to think about it, you're making it as easy as possible so you don't have to worry about errors, you don't have to worry about manual provisioning, etc. When you're using Infrastructure as Code, it usually goes hand in hand with automation. So let's talk about automating your builds and your deployments using your templates. Again, manual builds are going to be a gamble. The best‑case scenario when you manually deploy is that you're going to build it correctly. However, all of the other scenarios mean that you could very easily make some costly errors. For example, maybe when you're deploying an autoscaling group, you set the minimum number to too many instances or you use too large of an EC2 instance type. Those could all very quickly add up from a cost perspective. Also, time is a factor. We have way more important things to do at work than manually deploy things. So using automation with your Infrastructure as Code helps you speed up your deployments and offload the operational burden. In addition to time, security is there. This offers a very easy method to prevent security incidents. You can control your templates via version control, lock access down via IAM or authorization with another platform, and if you happen to see some incident occur within your template, well, you can easily fix it and quickly deploy that update. And lastly, consistency. Of course, when you automate your templates, you're going to get the same result every time. This is because there's no guesswork, everything is well‑defined and declared within your files. Now exam pro tip here to wrap things up. Whenever possible on the exam, you're going to want to select an answer that does not include manual steps. Think automation and Infrastructure as Code, if possible. Now, with that understanding and review out of the way, let's move on to some technologies and services within AWS, starting off with CloudFormation.

AWS CloudFormation Overview
All right, when you're working with Infrastructure as Code within AWS, one of the primary services that you're going to need to know for this exam is CloudFormation. AWS CloudFormation is a service that allows you to essentially declare your Infrastructure as Code for repeatable, automated deployments via what are called stacks. Now, with CloudFormation, while a lot of AWS resources and services are supported, not all resources and different types of services are supported. If something is not supported by default, you can leverage what is called a custom resource to go ahead and deploy those different types of resources. Now, we're going to talk about those much more in depth later on, but understand not everything is supported by CloudFormation. With that understood, however, this is going to be the perfect choice for creating immutable architecture across multiple regions and even across multiple accounts. If there is a scenario where you have to create the same architecture and deploy the same resources across environments, accounts, and regions, this is the service to use. Within CloudFormation, it supports some of the best practices that the well‑architected framework lays out. One of those best practices is tagging your resources. You can very easily specify a tag key and a tag value for every single resource within a stack. In addition to custom tags, they also automatically tag resources with a stack identifier. This stack identifier lets you know that, hey, these groups of resources were deployed by this particular stack. What these two solutions offer is a much more efficient way to track costs across multiple deployments. And then the last major point here from an overview perspective, this is free to use. That is one of the biggest benefits to using this service. Now, while the service itself is free, the resources that you spin up, you're going to have to pay for, which makes sense, right? You can use their automation tool for free, but when you're spinning up EC2, DynamoDB tables, RDS instances, etc., well, you pay for those underlying resources. Moving on to template languages. Remember, we deploy what are called stacks, and to actually create a stack, you write a template. The template is what contains all of your resources that you're declaring. At this point in time, and probably for the long foreseeable future, CloudFormation supports two languages, it supports JSON‑formatted documents and it supports YAML‑formatted documents. Now, I'll go ahead and tell you, in all of the demonstrations you're going to see, and in a lot of the examples, you're going to see YAML. That is simply for the sake of readability. It's a little bit easier to format and read when I am presenting it, and it's a little bit easier to understand when I'm breaking it down. With that being said, just remember you can do either/or. Continuing with templates, we know the languages, we know now that templates are used to define or declare your resources. Well, for the exam, you have to know some important sections that make up a template. Now, what I'm about to show is not an exhaustive list, so please remember that, feel free to dive into templates a little bit more; I'm just covering the important parts that you should know for this exam. First up, the required sections. The only thing that's required within a template is the resources section. This is where you declare every single resource that you want to provision as part of your stack. So, if you want to deploy an autoscaling group with a load balancer and your VPC with security groups, well, it all belongs within the resources section. Now, in addition to that, there are several optional sections you have to be familiar with. The first is a parameter field. So parameters allow you to leverage the same template and then use a parameters input value to essentially customize the templates with different custom values whenever they are created or updated. Parameters are a best practice to use when you're trying to deploy to different regions, different accounts, etc. There's also outputs, so an output is exactly what it sounds like, it's the value being output from the resource that the CloudFormation can show you so you can reference it later on. We also have mappings. So, mappings are a special type of section where you have key‑value pairs that you use to set values based on certain conditions or dependencies. We dive into these a little bit deeper later on, but at a high level, think of it as a selection based on a specific condition. For instance, maybe you want to deploy a certain instance type if the region is us‑east‑1, and you want to deploy a different instance type if the region is use‑west‑2. You can use mappings to automatically look up the values based on other specific values that the CloudFormation stack can reference. We also have conditions. Conditions are something you set to say, hey, do I want this resource to be created or configured? An example could be a security group. If you're going to end up not creating an EC2 instance because maybe the environment is getting flagged as development, well, maybe you don't want to create the security group for that instance because there's no need for it. Well, you can put a condition and say, hey, this doesn't exist, so don't create this security group resource. And then lastly, we have a transform section. Now, this is kind of getting phased out, but I like to bring it up just in case. These are essentially macros that you can put in your CloudFormation template, and the service uses them to process your template further. These are generally used for custom resources, and they're just a good thing to know. Again, I haven't really seen them come up on the exam a lot anymore, but, you do need to be familiar with them just in case. Moving on, let's look at a template example. Again, this is written in YAML, but you could take this and write it in JSON, and it would work just the same. You'll notice here at the top of the template, we have two fields, we have the template format version, and this value will always be the same, it never changes. And then right below that we have the description. So the description is just a human‑friendly name or string of text that you can put in there so you can easily see what this template does. So in this case, it's deploying instances across multiple regions using a mapping. Below that, we then have parameters. So remember, you can enter custom values to customize your templates in the end. So in this case, we're saying, hey, we want an instance type parameter, it's a string type, and it has a default value. However, even though we have a default value, we can enter any other valid EC2 instance type in that field, and it will get passed down later on, which I'll show you here in a moment. We then have mappings. So this is a perfect example of using a mapping. In this case, we have a resource called a region map that we've named, and all it's doing is looking up the region that we're deploying in, so you see us‑east‑1 and us‑west‑2, and depending on the region that we're deploying to, so us‑east‑1 or west‑2, it's going to select that specific AMI ID that you can see listed. After mappings, we get down to the resources, which, remember, is the one required section in a template. In this template, we're defining two instances, one in each of our supported regions. So we see the EC2 instance type, and then we have some properties below that so we can specify and customize the instance. You'll notice the first thing we specify is the imageId, and we're running a FindInMap intrinsic function. So this is saying, hey, find in our map, RegionMap, wherever the key is us‑east‑1, and then select the AMI value. So in this case, we're selecting that first AMI for this instance. In addition to that, we have the InstanceType right below that where we're referencing the InstanceType parameter. You reference a parameter using the flag that you see here, !Ref. After that, we specify our availability zone. Now, this could be customize or randomize, we just hard coded it, and then we have tags. Tags, remember, are a best practice, so we're naming our instance, as you can see here. Some other valid methods of using tags are for cost center, so you can bill appropriately to the correct department. Now, the second instance is similar, the only thing we're doing differently is deploying in the us‑west‑2 region. So you can see we're finding in the map wherever the key is us‑west‑2 and selecting that AMI ID. Now this is just a condensed example, it doesn't necessarily mean it's going to work, I just wanted to show you how some of these sections work together. And finally here on the bottom we have our outputs. So what we're doing is referencing each EC2 instance resource and outputting the instance ID. Now, moving on, let's look at the JSON example. This, what you see here, is the exact same thing that we just looked at in YAML. This is why I like to reference YAML more than I like to reference JSON. It just tends to be a little bit easier to read. As you can see, this is a very, very long template because of JSON formatting. Now, moving on, since we understand how templates work, let's have some exam tips before we wrap this clip up. First and foremost, stacks are regional resources, so you deploy them to each region where you need them. So if you need them in two different regions, you're going to have to have at least two different stacks. And then secondly here, use case. On the exam, anytime you're going to need to easily deploy repeatable infrastructure across either regions, accounts, or even just for different environments in the same region, I would immediately think AWS CloudFormation. Now, that's going to do it for this introduction into CloudFormation as a whole, make sure you understand the different template sections, the different formats, and some of the different intricacies that go along with referencing the different sections within a template. Let's wrap up here and we'll move on.

AWS CloudFormation Service Roles
All right, so we just reviewed CloudFormation as a whole. Well, when you're deploying templates using CloudFormation, you can control the permissions allowed using what is called a service role. A service role within CloudFormation allows the service to make API calls to the different resources and services for you. So if you're creating EC2 instances or creating DynamoDB tables, well, CloudFormation is making those API calls to spin those up. Now, if you don't set a specific service role when you're deploying a template, well, then CloudFormation actually uses temporary credentials that are based on the user deploying the stack. In other words, if you have admin credentials and you don't set a service role, well, then you're essentially granting CloudFormation the same permission set that you have, so it can complete its work. Now, if you want to restrict the permissions, then you set the role during stack creation. A perfect example of this is to use it to follow the least‑privilege principle for deploying resources. You can make sure either users or roles don't have enough permissions or overly permissive permissions, and you can use this to control exactly what they can create. A use case could be limiting CloudFormation to only perform specific EC2 API actions. Maybe you don't want it creating VPC resources, you don't want it creating database, etc., you only want it to create an EC2 instance and it's specific security group, well, you can limit that using a service role. Now, that's going to do it for our clip on service roles with CloudFormation. Big thing to remember, you set a service role if you want to limit permissions when deploying the stack. If you don't set it, then they get the same credentials of the user that is deploying the stack itself. Let's go ahead and wrap this up, and we're going to move on to a demonstration where we're going to write and deploy a template.

Demo: Writing an AWS CloudFormation Template
Okay, let's get started with our demonstration. We're going to look at writing a CloudFormation template, and then we're going to explore deploying that template to a sandbox account. Before we jump into this sandbox, let's just do a quick overview of what we're going to do. What we're going to do is deploy a multi‑AZ architecture using two different custom VPCs. Within the template, we're going to explore some of the high‑level sections you should know, like resources, parameters, etc. Before we deploy the template, we're also going to show how you can use what is called the Infrastructure Composer view within the console to visualize what's going on. With that being said, let's go ahead and jump into our console now. All right, I've loaded up, first of all, my CloudFormation template that we're going to use to start this demonstration. What I want to do is I'm going to walk through this within my IDE because it's a little bit easier to read, as opposed to the console. So at the top here, you're going to see we have our TemplateFormatVersion. Remember, this will always be the same value, 2010‑09‑09. We then have our description field here. What I've done is I've given it a very simple description, however, formatting with YAML allows you to do a multi‑line description if you want to. So mine is simple, I'm just saying, hey, this is two VPCs, we're creating a peering connection and some security groups. The next thing we have here is the Parameters section. So, these are user‑defined. What I've done here is I've given the first one VpcAName, and then the second one, VpcBName. These are simply the logical identifiers that we want to reference them as later on, and I'll show you what that means here when we look at the resources. For now, though, let's look at VpcAName. I give it a description saying, hey, this is the name of the first VPC, it has to be a string, and we're going to give a default. So if you don't fill it out, we're going to fill it in for you. We create another one called VpcBName that we're going to use as well. Now, next up down here, we have the Resources block. Remember, this is the required portion within a CloudFormation template. If you don't have resources, well, there's no need to use CloudFormation in the first place. So, what I'm going to do is I'm not going to walk through every single resource. I will make this available for you to discover and look through on your own, but I want to cover some of the high‑level stuff. What we're doing, the first thing is we're creating a VPC, we're logically identifying it as VPC, and you can see the type here. Now, this is the syntax that all resources follow. AWS, we see the namespace and then the resource type. Now within a VPC, we have a block of properties. Now there are many more properties, however, these are some of the popular ones. And, in addition to that, these are specific to a VPC. We have our CidrBlock, which is a /16, and we're enabling both DnsHostname and DnsSupport for the entire VPC. The next thing we see here are the tag values. So we can see we're passing in a key of Name, and then with Value, we're doing something called an intrinsic function. We're referencing our VpcAName parameter. So what this is doing is saying, hey, we're going to look up the value that was passed in up in our parameter, and we want to reference that as the value. So this is one way you can use a parameter within your template. Now within here, we're starting to create some public subnets, some private subnets, etc., and you're going to notice a few things. We're again referencing a logical identifier, but in this aspect for this public subnet, we're referencing VPC. So what this is doing is referencing our VPC resource here, and by default, when you use reference on a VPC resource, it gives you the VPC ID. Now, I'll tell you, for this exam, you don't need to know this super in depth, I just like to point it out for when you're actually doing it yourself. So we're creating a public subnet within that VPC, we pass in some other properties, and then another thing I want to show you is a get function. So an intrinsic function here is we can get the AZs within the local region. This is a pseudo parameter within CloudFormation. Now, I'll give you the links to the documents to explore these on your own, but there are some intrinsic pseudo parameters that you can use to easily reference data within the region. So in this case, we're looking up the region, we're getting the AZs, and we're selecting the second one out of that map. Again, this is way more in depth than the exam needs, so I'm not going to dive into it, just understand the concept of using parameters and intrinsic functions within your CloudFormation template. Now what I'm going to do here is I'm going to skip down, we create some more subnets, we create another VPC with its own subnets, and at the bottom here, we have outputs. So you'll notice I'm outputting four different things here, two from VpcA and two from VpcB, as you can see. So, the first thing we're doing is we're outputting our VPC ID and we're exporting the name. So we're saying, hey, this will be the name, following this syntax. We're then outputting the CIDR block, and then we're doing the same for the second VPC. Now, at a high level, that's what this does, it's creating two VPCs, it'll create a peering connection that we can accept, and then it will output some information. So what I'm going to do is let's deploy this. I'm going to go to my console here, and I've already loaded up CloudFormation. Now before I create a stack, I want to show you under VPCs in my region here, we have no other VPCs. So we're going to see two additional ones here in a moment. So, what I'll do is I'll go to CloudFormation, and I'm going to click on Create stack. Now from here, you can choose an existing template, which can point at an S3 URL, you can upload it from a local file, or you can actually sync from a Git repository. Now, syncing from Git would be the recommended way, but for this demo we're actually going to build it. So what I'm going to do is select Build from Infrastructure Composer. I'm going to create, and what this does is it loads a graphical interface for us to use to visualize what's going on. So if I wanted to, I could go in and I could drag and drop resources, I can connect them to other resources, etc. However, what I want to do instead is I want to copy and paste my template in. So what I'll do is I'm going to go to my template, copy this, go back, I'm going to paste it in, and there we go, we now have our template within this user interface. Now, quick pro tip here. This is in YAML, but if you wanted to, and you know this is valid, hit this button here and it converts it to JSON for us. Now I'm not going to leave it in JSON because it's hard to read and present, so I'm going to go back to YAML. Once we have this in here, what we can do is we can click Validate. This is going to say, hey, is my template valid? Will it work to deploy? It says yes. Now one thing to note here, this is only validating the syntax, so if you have any errors that are not specific to CloudFormation, well, that's not going to catch that, it's only catching essentially the syntax and formatting of your template. But, with that being said, it's a valid template, and now if I click on Canvas, it automatically creates a map for us so we can see all of the resources that are getting created. We see our SecurityGroupA and B, they map to the different VPCs, you can see the VPC here, and then our second one over here, and a peering connection. So this is just an easy way to view from a graphical standpoint what your template is doing. So what I'm going to do here is I'll validate one more time, it's valid, and I'm going to create my template. You're going to notice when we do this, what's going to happen by default is it's going to create a bucket for us, as you can see here, to store this template file. However, you can specify your own bucket if you have one. For me, I'll accept the default, confirm and continue, and now we have our template. You can notice here the Amazon S3 URL that it's referencing. So, we have our stack template in place, I'll click on Next, and now we specify our details. So the first thing we have to do is give it a name. I'm just going to call this TwoVpcs, and then we get to our parameters. So remember, we can pass in parameter values for our VPC name. So for the first one, it's VPC‑A as default, second one is VPC‑B as default. Well let's say I want to change this, so I'm going to change this to Z; I don't want the default, I'm going to have A and Z. I click on Next, and now we get to tags. So what we can do now is we can tag, remember, via custom tag key‑value pairs. So maybe for this, I want to add one called Environment, and I want to call it dev for the value. And then in addition to that, I want to track costing, so I'm going to say Cost center, and we're going to call this development. So now all of our resources are going to get tagged with these tags so we can have a better cost tracking and cost analysis in place. After that, we have permissions. So remember we talked about service roles. Well, this is where you can assign that service role. But, since we're not going to pass one in, remember that since we're not doing that, that means CloudFormation gets a temporary set of credentials that mimics the same permissions that we have. So if you want to restrict that or control that, you would pass in an IAM role here for CloudFormation to use. Next, we have stack failure options. Now I'm not going to dive too much into this, it's not on the exam that much, but you can either roll back or preserve successfully‑provisioned resources based on stack failure. In other words, if the stack gets about halfway through deploying and then hits a failure, it can roll back and delete, or it can preserve what was already created and just let you know. We also have the Delete newly created resources during rollback. Again, do you want them to delete all new ones, or do you want to reference a deletion policy, which we talk about in a different clip within this module? We're going to skip this for now. Moving down, we have advanced options. Now, again, I'm not going to cover these, these are out of scope for this particular clip and this particular exam, but we will cover stack policies in its own clip, but rollback we just covered at least at a deep enough level for the exam. Now notifications you can notify via SNS whenever certain resources or stack events occur. Now, I'm going to skip over the last one, I'm going to click on Next, and now we have a review. So we see our template, we see the description, stack name, we see our parameters, etc. So what I'll do now, I'll click on Submit, and there we go. So now it's going to start creating these resources, and we can see timestamps, logical IDs, statuses, etc. So what I'll do is I'm not going to sit here and wait, this'll take a couple of minutes, but I'll cut forward once this is done and we'll continue. All right, so, I've now fast‑forwarded, we can see it is CREATE_COMPLETE status, the stack itself is done, and I would say that took roughly 2 minutes. So that's not bad considering if we look at the resource list, there are 39 resources that were deployed. Now, this is a very simple template, and you can create very complex ones. So hopefully you see how easy and efficient it is to use Infrastructure as Code to deploy a bunch of resources programmatically. So now if we go back up, we see the list of resources, we see the events, there's a timeline view over here to see how long each thing took, but what I want to look at here is our outputs. Well, there we go, VpcACidrBlock is 10.0.0.0/16, VpcBCidrBlock is 10.0.0.0/16. Now I can tell you, we made a mistake in this template, so, this would not be the same CIDR. Well, this is easy to diagnose. If I go to Template here, I go all the way down, we're going to see our outputs. So let's see what happened here. VpcACidrBlock is referencing VPC.CidrBlock, VpcBCidrBlock, oops, it's referencing the same thing. So that's how easy we can identify, using outputs, what's going on. So in reality, if I go to VPCs now and I refresh, we're going to see our two VPCs, and they actually have different CIDRs, so this is perfect. We just made a mistake in the template, and that's easy to diagnose using CloudFormation. Now one more thing that was created here, remember, is our peering connection. So if I go to Peering connections, we should see it, it's active, there we go. So, the actual peering connection is in place, and that's because this is in the same account and I have permissions to do so. Okay, well, that's going to do it for our CloudFormation demonstration here. Feel free to use this template, deploy it, play around with it, change some of these settings, etc. In addition to supplying this, I will also give the link here so you can see just how many resources are actually supported within CloudFormation. It is a bunch. Just about every resource, but not all resources, are supported. Now, with that being said, once again, thanks for watching, let's go ahead and wrap this demo up and we'll move on.

Deleting AWS CloudFormation Stacks
Okay, we're diving a little bit deeper into CloudFormation stacks. Sometimes you're going to want to delete your stacks to quickly delete and clean up all the resources, but, what happens if you don't want to delete them or you don't want to allow another user to delete your stacks? Well, let's talk about some of the concepts that we can use to protect ourselves from that scenario. There are two specific resources or features within CloudFormation that we can use to protect our stacks from deletions and updates. The first is a termination policy. The second is a stack policy. Now let's look at each of these more in depth now. First up, let's look at termination policies. A termination policy is a feature that you can enable for each individual stack. You use these to prevent your stacks from being completely deleted, so it stops any deletions from occurring. Now, it's good to know, they are disabled by default. So what that means is you have to go in there and manually enable these if you want to use them. One thing to keep in mind is this is not the same as what is called as disabling rollback. By default, when you create a stack and a resource fails, let's say, halfway through the deployment, the default action is to roll back all of those previously successfully deployed resources. So that's what rollback is. It's good to know in addition to termination policies, you can disable rollbacks, you can stay at the exact point where your template broke. Now, moving back on, let's talk about the second concept here, which is a stack policy. When you create a template and deploy a stack, by default, all updates are allowed for all deployed resources within that stack. Stack policies prevent specific resources from being updated or even deleted, so you can specify what you want to allow or not allow to be updated. Within a stack policy, you allow updates on resources by specifying an allow statement for that resource type. It's very similar to an IAM policy. And, when you're using stack policies, you can only have one stack policy per stack. This is an important thing to remember, it's not necessarily huge on the exam, but this does trick people; you can only have one policy per stack. What that means is you have to define all of your resources within that stack policy. So really be careful when you're using these. One of the biggest benefits of using a stack policy is that it's going to apply to any user attempting to update the stack. So even if they have admin permissions, if the stack policy says, hey, you cannot update that resource, well then the stack policy wins. And speaking of stack policies, let's look at a quick example here. This is a real example of preventing updates to all EC2 instances that exist in our stack. So you'll notice the top here, we have a deny effect, and then we have an update with an all asterisk there, so all update actions. We're saying, hey, any single principal within AWS on any of the resources in this stack cannot be updated if they meet this resource type condition, which is an EC2 instance CloudFormation resource type. And then on the bottom we're saying, okay, besides the EC2 instance, you can update everything else. So we're allowing all update actions for any principal on all resources within this stack. So this is a real‑world example on how this would work. We're allowing all updates besides anything related to EC2 instances. Now, let's wrap this up real quick with some exam tips. First things first, if you need to prevent stack deletions entirely, just enable termination protection. This is the fastest way to protect your stacks. If you need to prevent resource updates or specific resource deletions, well then implement a stack policy. Remember, you use stack policies to explicitly allow or explicitly deny any type of update or delete action for the resources in your stack. Now that's going to go ahead and do it, let's wrap up here. Please do your best to remember the two different types of protection methods for your stacks. And with that being said, I'll see you in the next clip.

Organizational Deployments via StackSets
Okay, we just got done looking at some components and some features within CloudFormation. Now we want to look at how we can deploy across organizations and different accounts using what is called a StackSet. A CloudFormation StackSet is a feature within the service that allows you to create, update, and delete stacks across multiple accounts and multiple regions, all using a single operation. When you're using StackSets, you leverage a delegated administrator account to centralize template management and deployments. With the delegated admin account, you specify target accounts where the stack instances are going to get deployed to. Now, since you have to specify target accounts, and this is going to be cross‑account more than likely, that means you have to allow permissions for these to work. Now we'll talk about permissions here coming up shortly, but just remember, you do have to allow the centralized admin account to be able to deploy the stacks. And then lastly here, remember, this is important, stacks are regional. What that means is you have to deploy them to each region that's required to host your infrastructure. That's where StackSets really shine. You can deploy them to multiple regions and multiple accounts with the single click of a button. Now I told you we would talk about permissions, so let's do that really quickly. When you're using this feature, it requires the use of service roles within each target account that you plan on deploying to. With these permissions, there are two methods for managing them. The first is called self‑managed. So, when you use self‑managed permissions, you have to create the IAM roles that get used by the admin account CloudFormation service that deploy the StackSets to the different accounts and regions. What that means is you create the source role, and then you create all of the different target roles in the target accounts, and you allow those target accounts to be assumable by the centralized admin role. The next type of method for managing permissions is called service‑managed. This is where you use a service called organizations, which is covered in a different course within this learning path, and you use that to create the required roles for you that are used to deploy the stacks. This is going to be the easiest way to do it because it allows for automatic deployments for any new accounts that join your organization. So if you have a set of infrastructure that you want to require in every account at all times, well, this would be the way to do it. Now, a quick exam pro tip here. If you need to centrally manage stacks across an organization, whether it be across regions, across accounts, etc., immediately think StackSets. These are the easiest way to accomplish that. With that being said, let's wrap up here, and we're going to move on to a demonstration where we deploy our very own StackSet.

Demo: Deploying a StackSet
Okay, let's have a quick demonstration on deploying a StackSet across an AWS organization. First things first, let's have a quick overview of what we can expect. One thing I did want to call out here is we are using a service called AWS Organizations, and within Organizations, this is not going to be supported in the hands‑on environment. So I'm using a separate isolated account for this demonstration. With that note in mind, what we're going to do here is log in to the main account within the organization. Now, Organizations is covered separately in a different course within this same learning path. What that means is don't worry about it too much right now, we're going to cover it much more in depth, but understand that it is a service that allows you to easily accomplish a multi‑account environment. I'll show you what I mean when we're in the console, and I'll do my best to cover some of the supported features that are important for this particular demo. Within that account, we're going to work on deploying the StackSet, and we're going to deploy that StackSet to our target account in multiple regions. Now for this demo, we only have one member account, but it will work with however many accounts you have, so hundreds, tens, etc. What we're going to do is deploy the same stack to multiple regions with the click of a single button, and I'll show you exactly how easy it really is. So with that in mind, let's actually jump into the console now, where we're going to demonstrate this process. All right, welcome to the demonstration. I'm in my sandbox environment here, and just another quick reminder, this is not supported within the provided hands‑on playground. So to accomplish this, I'm having to use a separate set of isolated sandbox accounts just for me personally. Now, with that being said, let's review some of the infrastructure that we have in place already before we begin. First up, we have AWS Organizations. So I'm in Organizations here in the main account, and again, just a reminder, we cover Organizations in its own clip in its own lesson in a different course within this learning path. For now, I just want to show you the organization we have. I have my main account that I'm in here, as you can see at the bottom, and then right above that I've created an organizational unit called Sandbox, and this is where I invited and added a completely separate child or member account, as you can see here. So what we're going to do is deploy a CloudFormation template from the main account here, and when I cut over to my member account, we can see that I'm logged in here as well, as you can tell by the account ID at the top right. Now you're going to notice in this member account we have no stacks, and if I load my VPCs here, we're going to see in us‑east‑1 I have only the default VPC right now. And with that review out of the way, let me jump back into my main account, and let's get started. Now I've loaded up CloudFormation, and what I want to do is under StackSets here, I'm going to create a new StackSet. Now, before I do that, however, whenever you're creating a brand new organization like I have, you have to activate trusted access, as you can see at the top. So what I'm going to do is click on that, and we're going to talk about trusted access much more in depth in the actual Organizations clip, so don't worry about that right now, only thing you need to understand is you need to activate that for this to work. So, I've activated it, and now we see at the top here I can actually go ahead and create a delegated admin account. We talked about this briefly in a previous clip, but what this does is it allows me to select an account from the org, and by doing that, what happens is I'm saying, hey, this account ID within my organization can actually go ahead and deploy StackSets across the organization itself. So you're delegating admin responsibilities for this particular service. This is very common if you have a centralized account where you want to control who and what is getting deployed. Now with that being said, I only have one other member account, so I'm not going to choose that, but please understand for the exam you choose a delegated admin account if you want to control which account has permissions to perform this if you don't want to use the main account. So we're not going to delegate an admin, instead, I'm going to create a new StackSet. In here we see permissions. Remember, they have service‑managed or self‑service. If we choose self‑service, you have to create the roles that are used in the target accounts. Now, what I'm going to choose instead is service‑managed because I want Organizations and CloudFormation to handle this for me. So it's going to create the roles in the target accounts, it's going to assume them, etc. So I choose service‑managed, and then we see the prerequisite for preparing our template. Well, I'm going to choose Template is ready because I have my template, and under here we can choose an S3 URL, or we can upload. So what I'm going to do is choose upload, and then choose my file, and I chose a very simple VPC file here for CloudFormation that deploys public subnets across three AZs. This is going to speed up the deployment, so that's why I chose it. Now I go ahead and choose on Next, and now we get into some of the details about the StackSet. So the first thing we do is we give it a name. After the name, we can give it a description. So I do highly recommend you provide a description that is easy to read and really understand what this does. So let me go ahead and put that in here now. Okay, so I put my description, and then the next thing we see here is a parameter. So, this is because we have parameters within the actual template itself. So what we're doing is passing in the VPC name. Now I will leave this as is, PublicOnlyVpc, but it is best practice to have as many parameters as you can so you don't hard code values into your StackSets. So I'll leave this, I'll click on Next. I'm going to give it a tag, so let me go ahead and call this pluralsight true, and then we get to execution configuration here. So what this is doing, it's a little bit more in depth than you need to know, but I like to cover it, is it's saying, hey, can this process perform non‑conflicting operations? So in other words, can it create resources and stacks that are not dependent on each other at the same time? So what we can say is inactive, no, just perform one single thing at a time, or yeah, go ahead, if it's non‑conflicting, you can create it concurrently and just make sure you don't have any conflicts. Now what I'll do is I'll set this to active because we're just creating a VPC, and there shouldn't be any issues, so this will speed up the actual process. I'm going to click on Next, and we get to deployment options. So with this, you can deploy new stacks, which is what we're going to do, or you can import stacks to the StackSet so you can centrally control them. This is out of scope, so we're not going to cover this in depth. Instead, I'm going to deploy new stacks. Next, we see deployment targets, so you can deploy to the entire organization, as we've selected here, or if you want, you can deploy to a specific OU. So what I want to do is I want to target only a specific organization. So I'm going to go back to Organizations here, I'm going to copy my OU ID, I'm going to go back to CloudFormation, paste that in, and there we go. Now any account within this organizational unit will get this StackSet. I'm going to skip over filter types, that's out of scope for this exam, and let's get down to auto deployment options. So, this is a very, very handy feature for this service. With this option, what happens is any time an accountant is added to the organizational unit that we put up here at the top. This service is going to automatically deploy a new stack instance to that account. So for instance, if we created a brand new development account, we added it to this OU, well then this will automatically deploy a new stack. It's very, very handy. On the opposite side, if we remove that account from the organizational unit, it deletes the stack, so it's automatically creating and deleting as necessary. The next thing we see is account removal behavior. Essentially, do you want to retain or delete stacks? So by default, like I was saying, we're deleting the stack, but you could also retain it if that target is removed from the target OU. We're just going to go ahead and delete by default because it's a single account, not a big deal. Big thing to remember here is that it does allow for automatic deployment using StackSets. Please remember that for the exam. Next up we have our regions. Remember, you can use StackSets to deploy cross‑region, and this is handy because CloudFormation templates are regional resources. So what I'm going to do here is I'm going to choose two regions that I want to deploy the stack to. I'm going to choose us‑east‑1, and then I'm going to choose us‑east‑2. This is saying, hey, deploy a separate stack instance to each of these regions in any of the accounts within this organizational unit. So we should have at least two stack instances deployed. Next up we have deployment options, really quick high level, you can set the maximum concurrent accounts, so how many accounts do you want to deploy to at one time, and you can set failure tolerances. So do you want to tolerate any or no failures? After that we have region concurrency. So do you want to deploy StackSets one region at a time, or do we want to say deploy these in parallel? So deploy to both regions at the same time, I'll choose parallel. For concurrency mode, we'll say strict failure tolerance, we want to make sure we don't exceed the failure tolerant number, and I'll click on Next. And there we go, we get a review, so we're deploying our template URL, we see our description, it's a service‑managed permissions, and then we see some more details like our parameters, we see some tags, etc. So what I'm going to do now is click on Submit, and we see that it's starting to run, so this is an operation under StackSets. Now when I select this, it's going to take us into that StackSet info, and we're going to start seeing some of these stack instances deploying here in a moment. So what I'm going to do here is I'm going to keep refreshing until we see stack instances, and there we go. You'll notice we're deploying two different stack instances to the one account that we have in that OU. So if we compare these IDs here in our regions, if I go to my other console window with this other account, we see it's the same ID, and if I go to confirmation stacks in this member account and I refresh, here we go, it's creating a stack for us within the account from a centralized management account. We see our description, I can click on this and view the actual template, the resources, etc. Pretty much anything you could do with a normal CloudFormation stack, so now this is creating, in my VPCs, I should see a new VPC here, and I do. Perfect. Now, remember, I also deployed to us‑east‑2, so let's make sure that's working. I'm going to go to Ohio here, and we should eventually see a VPC in this region as well, and again we do. So it's deploying the same VPC across regions. So in addition to this, obviously that means it created a stack within the region as well, so if I go to us‑east‑2 within CloudFormation in the target account, we should see the StackSet, and we do here. Awesome, this is working perfectly. So we see the StackSet, it got created within both regions, if I jump back into my main management account here, we see eventually this is going to be listed as succeeded, and we're good to go. Perfect. So that's how easy it is to create a StackSet and deploy it across accounts and across regions from a centralized location. That's going to go ahead and do it for this demonstration, if you were following along or doing this on your own, make sure you delete stack instances from the StackSet first, and then you can delete the StackSet overall. That's a little bit tricky for some people, so I just like to call that out. But with that being said, let's go ahead, we're going to wrap this demo up, and I'll see you in the next clip.

Preview Infrastructure Changes Using Change Sets
All right, using CloudFormation is great for automating changes and deploying infrastructure across environments, but, what happens if you want to make a change to that existing template in the resources? Well, that's where change sets come in. A CloudFormation change set is a feature within the service that basically allows you to preview how any and all proposed changes to an existing stack might impact those existing resources. When you leverage change sets, they include any and all resource replacements, resource deletions, and any updates to your resources that include their properties as well, so tag values, etc. A great example is maybe you're changing an autoscaling group to have larger instance sizes for the scaling, well, that would be shown within the console or the output of a change set. Now when you use change sets, you are allowing CloudFormation to make the changes to your stacks only if you perform an execution of the change set, so you're actually approving it and you're executing it. If, however, you decide, hey, I actually don't want to execute this change set, well then nothing happens, you can erase it, delete it, and that change never actually gets put into place. In addition to that, if you decide, well, I do want to make a change, but I want to alter this change that I've put forward, well, you can do that. You can make a different change set based on the current one if you don't like those originally proposed changes. So maybe going back to our autoscaling example, you say, uh well, this instance size is a little too big, but I want to make it bigger than the original. Well, you can make a change to that change set, and once you get it ready, you can say, okay, this looks good, let me execute this one instead of the previous one. Now a pro tip here for real‑world scenarios, these allow you to have a layer of protection against accidentally changing important resources. You should use these any time you want to make an update to your existing stacks. It allows you to see everything that's going on, and you might catch an accidental deletion or update that might break something. Now, with that being said, let's go ahead and end this clip on change sets, and we're going to move on to a demonstration, coming up next.

Demo: Creating a Stack Change Set
Okay, let's get started with another demonstration. In this demo, we're going to look at creating and deploying a simple CloudFormation template, and then once it's deployed, we're going to go through and we're going to create a change set with some different settings changed and we're going to preview how that would affect our current resources. So let's go ahead and jump in the sandbox now. Okay, I'm over in my sandbox here, we're in us‑east‑1, and one thing to remember is CloudFormation stacks are regional, I just like to point that out, that can come up on the exam, you have to deploy these in each region that you want resources in. So with that being said, what I've done here is I deployed a very, very simple CloudFormation template that we're going to test. All it does is creates a few VPCs with some connections. Now, in VPCs up here, if I refresh, we're going to see our VPC, as you can see here, VPC‑A, and we have our CIDR of 10.0.0.0/16. So let's say in this case that we're not really happy with this CloudFormation template, we want to make some changes, we did some testing, etc., well, this is not where we want it to be. So let's test that out. I'm going to go back to CloudFormation, and I'm going to click on Stack actions. From here I'm going to create a change set. Now, before I click this, yes, you could just update your template in place; however, the best practice is to create a change set so you can visualize what's going on. So for this, what we're going to do is we're going to select Edit in Composer so we can view this. However, you could directly replace your template by uploading or pointing to an S3 URL, you could use the existing one and edit parameters for settings that you can change, or like what we're going to do, like I said, we're going to edit in Composer. So I'm going to go to Edit in Infrastructure Composer here, and this is going to load our graphical user interface. Like I said, you can see it's a very, very simple template. Let's test out a few things. The first thing I want to change or test changing is a non‑destructive action. So, let's say for instance, our naming convention, well, this doesn't fall into our governance rules. I need to follow a specific format. So what I'll do is I'll call this Production‑VPC, and that's it, we're only changing the value of our name tag for our VPC, everything else we will leave as is, I'm going to validate, it's valid, let's create our change set. Again, it creates an updated template in a bucket for us, just like when we created an initial demonstration using a template, I'm going to confirm, and there we go. So I'll click on Next, and we see our stack details. Now for this, it's actually really specific to the change set. So you can see the change set name, and it gives us a random identifier. So ChangeSetD, and then a bunch of different randomized characters. I'll leave that as is. You can give it a human‑friendly name if you want, we don't need to do that. Next thing we can do is give it a description. So, let me go ahead and change this to match what we're doing. Perfect, we don't have any parameters to change, but if we did, we could change them here. I'll click on Next, we'll leave everything else as is, and I'll go down here and click on Submit. Okay, so what this is going to do is it's going to go through and create a pending environment for us. So you'll notice here, changes have already been flagged to our VPC, and it's not going to replace the resource, you can see here, replacement false. So what I can do now is after refreshing this, we see it's complete, we can now execute it or we can delete it, and you can actually see a bunch of stuff, it's pretty cool. So we see we're modifying our VPC, we can view the property level details, which is saying, hey, this is the portion in the JSON or the template that is being changed, it's highlighted. So that's perfect. What I can do is I'll go back, we can look at the input, we can look at the template, just like we would normally, again, we can view the JSON changes, and evaluations. Now, this is way out of scope, I'm not going to cover evaluations, big thing to know here is that change sets allow you to view what modifications are going to occur. So, since this is not a replacement, let's go ahead and execute this. I'll click on this, we can set some of our settings. Do we want to roll back in case of failure? Do we want to preserve if some are successful? We'll select roll back, and we will use the deletion policy, which is default, which is nothing, in case of the bottom portion. So I'm going to execute this, we see it's now updating, based on the update in progress here. And what I can do is I can refresh, we see it's cleaning up, and eventually this is going to be update complete, and there we go. So now if I go back to VPCs and I refresh, our name has changed, but nothing else has. So this is perfect. Okay, awesome. So that was non‑destructive. Well, let's go back, and let's create another change set here. So Stack actions, Create change set, I'm going to edit again in Composer, and what I'm going to do under Template here is I'm going to change the CIDR. So I'm going to do 10.1, I'll click on Validate, I'll click on Create change set, and Confirm and continue. Now, I'm going to tell you, this wouldn't work when I deploy this because I didn't change the subnets, which are hard coded, but that's a topic for another day where you would want to avoid hard coding values like I did, but for sake of simplicity, that's what I did. So I'll click on Next. We'll leave the defaults here, I'll go Next, and then Next, and then we'll go ahead and we're going to submit. Okay, you'll notice now, as it's going through and it's complete, we have a lot more changes, and you're going to notice these are all triggering replacements. Well, that's because these all rely on that VPC CIDR that we changed. You can see subnets, route tables, etc., all of these rely on that CIDR to be the same. So we would have to destroy our VPC and then recreate a new one in place, as you can see, ReplaceAndDelete. Okay, so, let's say, oh, I don't want to do this. Well, easy enough, we just delete the change set, and we're back to normal. Nothing was actually changed, as you can see here, and we're good to go. Okay, that's going to do it for this demonstration on change sets. Make sure you understand that they are primarily used to view what kind of updates and modifications will occur, and you can control or add a layer of protection in order to do so. Let's end here, and I'll see you in the next clip.

CloudFormation Custom Resources
Okay, moving on to the next topic with CloudFormation. Let's talk about using what is called a custom resource to reform actions that aren't natively supported with the service. Custom resources are essentially methods within CloudFormation that allow you to write custom logic into your templates. You would use these whenever you have logic that is considered too complex or not supported and it can't be deployed or completed via built‑in resource types. Now when you're using custom resources, they're great for things that are not infrastructure related. And, an example of that would be executing a database migration script or an initialization for your database after you create an RDS instance. That's a perfect use case for a custom resource. Now I'll go ahead and tell you custom resources are starting to get phased into more of the developer‑related exams; however, they are still on the exam guide and they could still pop up, so really just remember that they're used to perform custom logic that are not natively supported within CloudFormation. That's going to do it for this short clip, let's wrap up here, and I'll see you in the next one.

CloudFormation cfn-init
Moving on to the next important feature within CloudFormation, CloudFormation init. Cfn‑init, or CloudFormation init, is basically a helper script within CloudFormation that allows you to define initialization tasks. Now, this is usually going to be defined in the metadata section of an EC2 resource. Now you might be asking me, well, what's the difference between this and something like user data? Well, let's go ahead and answer that. User data within an EC2 instance, while very usable and very handy, when you have it in a template and you update that user data, it triggers a complete replacement of that instance. If, however, you use CloudFormation init, it allows you to update the instances in place, so you're not replacing that resource. That's a very key thing to understand. It performs similar capabilities or allows similar capabilities to user data, but it's a little bit better for programmatic access and Infrastructure as Code. Now, let's look at some use cases for init. It's very useful for getting metadata from CloudFormation itself, so when you're trying to input data onto your instance, it can grab metadata from the CloudFormation template and the CloudFormation service. It is commonly used to install packages on your EC2 instances. Again, this is very similar to user data, but it's CloudFormation specific, so remember that please. Thirdly, you can use it to write files to your EBS volume at launch. So, similar again to the last one, when your instances are launching, it can perform this init helper script and write some data, create files, etc. And then lastly, it's also very commonly used to either start and stop or enable and disable system services on your operating system. Now with that being said, let's go ahead and wrap this clip up on CloudFormation init. Big thing to remember here is it operates just like user data does on instances. However, be sure to remember that it does not instigate or trigger a replacement of an EC resource, unlike how user data would. Let's go ahead, we're going to wrap this up, and I'll see you in the next clip where we're going to start exploring some other services.

Sharing Templates Using AWS Service Catalog
All righty, we have a very solid understanding of when and why to use AWS CloudFormation, let's move on to another service called Service Catalog. First up, what is this service? This is a service that allows organizations to create and manage what are called catalogs that contain IT services. Now, the point of this service is you use it to essentially deem and approve uses of these services within AWS accounts. By setting up a catalog within the service, you essentially allow end users to easily deploy pre‑approved items. These items within these catalogs containing this service are basically just CloudFormation templates. So that's why it's very important you understand how CloudFormation works. This is how this service works on the back end. What happens is your team that decides what can and can't be used can create these templates, it can then share them with an organization, and these end users can have permissions to easily deploy these pre‑approved templates. In Service Catalog, it includes the following. You can use things for approved AMIs, you can use specific EC2 instance types in your templates, you can say, hey, I want you to only use specific RDS instances and engine versions, etc. The big thing to take away from this is that it enables companies to basically centrally manage commonly deployed services in AWS, and it allows them to more easily adhere or stick to governance and compliance requirements. You can essentially just give end users the ability to only deploy these pre‑approved catalog items, and those catalogs can be deployed using service roles, and you can easily control what is actually getting deployed. So you can say, hey, you can or cannot deploy these services based on our guidelines. Now, within Service Catalog, there are several benefits and use cases. So let's talk about using it really quickly. First up, it allows for easily achieving standardization of infrastructure within an organization. You can approve assets and restrict what can and cannot be launched, including instance sizes, types, etc. It also allows you to enable self‑service for teams within an org. So if you have a bunch of development teams with their own accounts within an organization, well, you can allow them to browse the listings of your catalogs and products and then deploy those on their own, and it's all controlled centrally. Thirdly, it also allows for fine‑grained access control. So you can grant access to your catalog templates using IAM permissions, you can share it via RAM, and you can control what's getting deployed via the catalogs themselves. And fourthly here, it allows you to use version control systems. So when you write your templates, you can version control those, make small updates, and when you push an update, it's going to push it automatically to any catalogs referencing that original one. So, if a development team uses a catalog and they deploy their services, well, any updates that may get propagated to those products in that portfolio. Before we wrap up, let's have a quick scenario that could come up on the exam where you might use Service Catalog. Let's assume you're working as a consultant for many different customers and you're wanting to provide solutions to speed up the process of data, ETL workflows in AWS. You need a way to centrally manage and deploy common sets of solutions for the customers, and you want to enable self‑service. Well, this is perfect for Service Catalog. You can create and then share your catalog products or your catalogs in general, and define the required services for those customers. Now, that's going to do it for this clip on Service Catalog. Big thing to remember here is you create catalogs of pre‑approved services, which are templates, and you can share those for self‑service via an end user. Let's go ahead and wrap up here, and we're going to move on to another service called AWS Proton.

Self-Service Templates with AWS Proton
All right, let's move on within our Infrastructure as Code journey, and we're going to look at something called AWS Proton. Proton is a service that's meant to create and manage infrastructure and deployment tooling for users. This includes serverless and container‑based applications. Now, using Proton, the big thing to remember is it is also meant to help automate IaC provisioning and deployment, so not just the resources, but the deployment methods as well. One of the big use cases that you would use this for is to define standardized infrastructure for serverless and container‑based apps. Similar to CloudFormation, you also define templates that define and manage application stacks that contain all of the components within an application. One of the benefits with Proton is it's meant to automatically provision your resources, it automatically helps configure CI/CD pipelines, and it will also help you actually deploy the code itself. So it's meant to be a little bit more of a one‑stop shop option. Two big things to look out for is it supports CloudFormation templates and Terraform providers, so those are the two things it supports, and that's where it's commonly used. Now, I know it's confusing to talk about this, CloudFormation and Service Catalog, but coming up next in our module summary clip, we're going to talk about when you would choose one over the other. So let's go ahead and wrap up here, and I'll see you in the next clip.

Module Summary and Exam Tips
Okay, once again, way to hang in there, pretty long module here. Let's have a quick summary and go over some exam tips. First things first, four questions to ask yourself on the exam. One, can you automate this process? And if you can, should you? Not everything should be automated, but I will go out on a limb and say a majority of things should be automated. If you can and should be automating it, well, what kind of automation works in that particular scenario? You need to understand some of the small intricacies and small indicators that require you to pick one service over another. Thirdly, is the automation repeatable and does it need to be? For instance, do you need to deploy the same infrastructure across several different accounts and environments and different regions within an organization? Well, in that case, maybe you want to use something like a StackSet. And fourthly, how would this work cross‑region or cross‑account? And, does it even need to work like that in the first place? These are all four valid questions to ask yourself when you're reading through a scenario and the different answer choices. Now moving on, let's start reviewing some of the services that we learned about and some of the important information for each one. First up is CloudFormation. Please remember, this is the service for leveraging version controlled templates for automating repeatable deployments in AWS. This is considered the actual backbone for Infrastructure as Code within AWS. With this service, you define what is called a template, and you can define it in JSON or YAML languages. Using those templates, you automate your infrastructure creation, updates and deletion. In addition to automating infrastructure deployments and deletion, this is perfect for using to implement tags for easily identifying different resources and their associated costs. Remember, it does support stack identifiers, but you can put your own custom tag values in there. For instance, maybe you want to track costs across different applications, different environments, or even different cost centers. You can easily and quickly deploy the same tag across all resources within a stack. Now when you use this, you don't have to be a coding expert, but you do generally need to know what parameters are, what mappings are, and you absolutely need to know the resource section of a CloudFormation template. Please be sure to review that clip if you need to, where we break down the required and optional sections of a template. It's important you understand how it works. And lastly, remember that mappings and Parameter Store can be useful to help make your templates more flexible. You can look up map values, and you can pull Parameter Store values. Now, exam pro tip: Do you need to codify and do you need to automate infrastructure deployments? Well, right there, I would immediately begin thinking AWS CloudFormation. This is the whole purpose of this service. Now let's discuss a few features within this service that you need to remember. It has several built‑in features and capabilities that enable you to accomplish different required automated tasks. So again, please remember these for the exam. First up, remember a stack policy. You use these to restrict and allow which specific resources can be updated or deleted within a stack. You also have change sets, so you can generate previews of what changes are going to occur if you were to deploy an update to that stack. And then we have StackSets. You use these to centrally deploy cross‑account stack instances using the same template. This is very good for organizational deployments. Now, in addition to these, remember termination protection as well. You enable termination protection to completely disable deletion of the stack itself. Now, that's enough CloudFormation. Let's review the other two services we talked about. First up, let's talk about Service Catalog. Remember, this is a service that allows you to create, manage, and share catalogs of IT services, which are basically just AWS resources that have been deemed to be approved for using within AWS. Using Service Catalogs enable self‑service, so users can deploy by themselves approved catalogs that include approved resources. So you can use this to restrict things to specific AMIs or certain ECT instance types, etc. If you need to restrict what's being deployed, this is a perfect choice. AWS Proton allows teams to leverage templates to automatically provision resources, configure pipelines, and even deploy their code. So it's more of a one‑stop shop for someone that's a little bit less experienced. Now, I'm going to go out on a limb here, and for an exam pro tip, it's much more likely that you're going to use Service Catalog for leveraging managed templates with approved services. It falls more in line with the solutions architect exam, and it is much more useful for controlling what's or what is not deployable within different organizational accounts, so it's perfect for compliance and governance. Now with that being said, let's wrap up this module. Thank you for hanging in there, let's go ahead and move on to the next one whenever you are ready.

AWS Elastic Beanstalk
Reviewing Platform-as-a-Service (PaaS)
Okay, in this module, let's start talking about the Platform‑as‑a‑Service offering in AWS called Elastic Beanstalk. First up, before we dive into the service itself, we need to review what a Platform‑as‑a‑Service offering really means and what it is meant to offer you. Platform‑as‑a‑Service, or PaaS, or P‑A‑A‑S, is a complete development and deployment environment that is hosted within the cloud. It's designed to be able to enable you and developers to essentially deliver everything from simple applications all the way up to even complex cloud enterprise apps. Again, this is meant to be a one‑stop shop for everything platform related. When you're using a PaaS, the hosting provider is meant to handle all of the infrastructure, including other tools like even database management, middleware, and potentially CI/CD deployments. The big takeaway here is that you are meant to manage the applications and you are meant to focus on just developing. You're using the PaaS provider to handle literally everything else for you. Moving on, let's compare the three popular offerings. There's Infrastructure‑as‑a‑Service, Platform‑as‑a‑Service, and Software‑as‑a‑Service. You need to understand the differences between these three. First up is Infrastructure‑as‑a‑Service. Everything in the blue, or in other words, the top five boxes here are managed by the customer. So in other words, you're in charge of managing these. Examples of Infrastructure‑as‑a‑Service would be things like Amazon EC2. You control the applications, you're responsible for encrypting data, and you have to patch operating systems. On the bottom, the bottom four boxes, which are orange, AWS handles all of that for you if you're using EC2. They handle the hypervisor, the underlying hardware, networking, etc. In other words, you're just using the infrastructure to do what you need to do. Then we have our Platform‑as‑a‑Service, so you'll notice there's a little bit less that we have to manage as a customer, the top two boxes here. You focus on your application and your development and even your data. So you are responsible for making sure your data is secure and correct. You'll notice that everything else is managed by the provider, the runtime, operating systems, and the infrastructure underlying that stuff. And, all the way on the other end of the spectrum, we have Software‑as‑a‑Service, or SaaS. When you have a SaaS, you're only using the application, you're not managing it, so you don't have to worry about pretty much anything. You just use the app, you log in, you do what you need, and the provider is literally handling everything else for you. So with that review out of the way, let's move on to three key benefits of using Platform‑as‑a‑Service. First up, it offers much easier automation since you just have to worry about development and coding and managing your data. Well you can allow the toolsets and the providers to help you automate deployments and updates. And, speaking of deployments, it's going to enable faster deployments, again, because you're not focusing on as much stuff as you would with Infrastructure‑as‑a‑Service. You worry about your code, you worry about your data, and you push it out and let the provider handle the rest. And then lastly, usually they offer simple integrations. When we're talking about AWS‑specific services, you're going to see just how many integrations Elastic Beanstalk offers. But for now, let's go ahead and save that for the next clip, we can end here, and I'll see you in the next one.

AWS Elastic Beanstalk Overview
All right, we're going to start diving into the Platform‑as‑a‑Service offering known as Elastic Beanstalk. Beanstalk is a managed service that is meant to be your one‑stop shop for everything Platform‑as‑a‑Service‑related within AWS. So, because of this, remember, it's going to allow your developers and your teams to really only worry about development. They're not going to have to worry about that underlying infrastructure. If you recall that comparison where we compared Infrastructure, Platform, and Software‑as‑Services, Platform‑as‑a‑Service kind of fell in the middle. You have to worry about application and data, and everything else is good to go. Within Beanstalk you create basically templates of how you want your environment to appear, and then the Beanstalk service will go ahead and automate all future deployments for you. With it, you're going to easily upload your code, you can perform testing of your code, and you can deploy to multiple environments, which is a best practice. So for instance, dev, testing, and production. The big takeaway with Beanstalk is it handles building out the application on your compute for you. So you say, hey, here's my Java application, deploy this to the underlying EC2s that are required in a load balancer and handle all of that other stuff for me, I just want to handle and worry about my application. Moving on to some important concepts with Beanstalk, it supports the following languages and platforms, so Java, .NET, Node.js, etc. You do need to be familiar with these for the exam, as these are common indicators for using this service when you want to go with Platform‑as‑a‑Service. The service itself deploys multi‑tiered, highly‑available architectures for you. Now it's going to include things like EC2 instances, Application Load Balancers, and autoscaling groups, and if you want it to, it can also deploy an RDS instance, or a relational database. Now typically speaking, you're not going to want to include RDS in your environments due to best practices. However, you can use that option. So if you want it to manage every little thing, it can do so. Now there are several components that are very important for you to understand within this service, and these are going to be talked about coming up in their own separate clips, but we're looking at applications, versions, environments, and deployments. These are all actual individual components and terms that we will look at. Now for Beanstalk, at a high level, for an exam scenario, maybe you have a scenario where you're migrating or creating a Java or PHP‑based application to AWS. Well, you need a highly‑available, managed solution with minimal operational overhead. Well, that means AWS Beanstalk could be a very good choice for that scenario. It handles all of the underlying infrastructure and you only worry about your app. Now, with that exam scenario wrapped up, let's go ahead and wrap this up, and we're going to start looking at some of those upcoming components that we just talked about a little bit more in depth.

AWS Elastic Beanstalk Environments
All right, let's go ahead and dive into some of those important components we just looked at in a previous clip. We're going to start things off with environments, applications, and application versions. First up, an application. An application in terms of Beanstalk is going to be a logical collection of all of your components, including the environments, versions, and configurations that you've made. The easiest way to think of an application is a folder structure. All of the different resources, configs, etc., go into an application. We then have application versions. So, when you have multiple versions, you have to start labeling them. So a version is a specific labeled iteration of your code for a web application. Versions themselves are essentially pointing to an S3 object that contained all of your code and your configuration. In addition to versions, we have environments. This is going to be the collection of AWS resources that are actually running your version that you've specified. With environments, you run a single application version at a time per environment, so it's a 1:1 relationship. And then lastly, we have environment tiers. Tiers are meant to allow you to essentially know the type of applications being ran. In addition to that, it determines the underlying resources that get created by Beanstalk. The two tiers are web server or worker, and we're going to look at those much more in depth coming up. First up, let's look at a web server tier or a web server environment. In a web server environment, you get assigned a CNAME URL that's managed by Route 53, and it points to a managed Elastic Load Balancer that is also created by Beanstalk. This URL is easily usable and publicly accessible based on Beanstalk's configuration settings. Now in addition to the URL and the Elastic Load Balancer, Beanstalk is going to create an autoscaling group that's multi‑AZ and is fronted by this managed load balancer. All of the EC2 instances deployed to these autoscaling groups are going to have a host manager software that basically allows Beanstalk the service to deploy, update, monitor, and scale applications as required. Again, you're offloading the burden of handling the infrastructure to this service. You only worry about your code. Now, like we mentioned previously, you can also manage databases in your environment, so you can specify in your application code and your environment config that you want an RDS instance. However, it's best practice not to do so. The reason I say that is because if for some reason you tear down this environment, that's also going to delete and tear down your RDS database instance. So it's best practice to create that separately and reference it within your code via some type of environment variable or config file. So, that's a web server environment at a high level. This is the type of stuff you need to be familiar with for the exam. You get a CNAME URL, it points to a load balancer, which is fronting a group of autoscaling instances. Next we have the worker environment, so this is the other type of tier. On the top here, we're going to assume we also have a web server environment spun up and created, and we've just minimized this and compacted it to make it simple. So we have that URL at the top, and with a worker environment, what happens is it creates an SQS queue. Now you can use an existing queue or you can have the service create one for you. Long story short, SQS is a message queueing service that we cover much more in depth within this same course. The big thing to know here is that it's a message queue, and you send messages to it and pull them off to process workloads. Once your queue is configured or created for you, what's going to happen is Beanstalk creates an IAM role with permissions that are required to pull messages off of that queue and assigns it to the same type of autoscaling group of instances. This autoscaling group of instances scales in and out based on CloudWatch metrics that are looking at the number of messages within the worker queue. In other words, if there are several hundreds of messages in your queue at one time, well, then your instances will scale out. And, on the other side of that, if your message queue is empty or near empty, then Beanstalk is going to work on scaling in your autoscaling group automatically. Again, the big thing to take away here is you scale based on your queue length, and Beanstalk handles all of the infrastructure for you. You just create your application that processes the messages. Now let's look at some exam tips here. Two big things to take away from this particular clip. Web server environments. Remember, these deploy managed autoscaling groups of web servers on EC2 instances that are behind an ELB with a CNAME URL assigned. All of these components are managed for you. The worker environment is going to deploy a managed autoscaling group of EC2 instances that scale based on messages within an SQS queue. The best way I like to compare these two is a web server is an Application Load Balancer or a Public Load Balancer, and I like to think workers are working to pull messages off of a queue. Now with that being said, let's end this here, and we're going to move on to some other important components within the Beanstalk service.

AWS Elastic Beanstalk Deployments
Okay, up next in our journey, we're going to start looking at deployments. Now before we look at deployments, we have to understand what an environment type is. We looked at environment tiers and those different types of environments, worker and web server, but now we need to look at an actual environment type. First, we have a single instance environment type. This is going to be where you deploy a single instance, thus the name, and you get an associated Elastic IP address for connecting to it. These are going to be really only useful for development and very simple testing. So if you want to start off with this service just to see if it works, this is a good way to start. The other type is load‑balanced. So this is going to be meant for production in more complex workloads. This is a scalable environment where you get your managed ELB and your managed autoscaling groups for high‑availability and automatic scaling. Typically speaking, again, single instance is only good for development, load‑balanced is good for everything else including production. Moving on, let's look at deployment policies. There are several policies you have to be familiar with at a high level for this exam. The first is an all‑at‑once deployment policy. What this is going to be is the quickest method to deploy updates and deploy applications. When you do this, it's going to result in a brief amount of downtime because it's deploying all application versions to all instances at one time. The next type of deployment policy is rolling. So with a rolling deployment policy, it's going to take a little bit longer to deploy your application updates. However, even though it takes longer, this is better for avoiding any downtime. How it works is your versions for your applications get deployed one batch of instances at a time. So Beanstalk is going to work on saying, okay, instances one through four, here's the new version, and then five through nine, you guys can just wait for a little bit and I'll update you later. Once that first batch is complete and up and running, it will then work on deploying to the next set of batch of instances. The third is immutable. This is a slower method, but not the slowest method. This is where your versions for your applications are going to be deployed to new instances that replace old ones, and they're all done via a second autoscaling group that gets created and eventually pushed into place to replace the old one. So you're creating a new autoscaling group, it's pushing the application update to the new set of instances, once those are all ready, it replaces the original autoscaling group. And lastly, there's traffic splitting. So this is going to be specifically for canary testing. This is great for testing the health of new versions while also still serving traffic to old versions. You can very quickly say, hey, I want to send 10% of traffic to the new version, but keep 90% of traffic to the old one, etc. Big thing to take away here, just try and do your best to remember these four deployment policy types, and just remember a deployment is literally just deploying an application version that's been updated, changed, etc. Now, exam pro tip here. Do you need to avoid downtime with your Beanstalk deployments? Well, then you'll want to lean towards immutable or rolling. There will be some other key indicators you should keep an eye out for, but these two are good options. A secondary exam pro tip here. Do you need to deploy as fast as possible? Well, in that case, you want to lean towards all at once because then it replaces every single instance version all at the same time, and it should be immediately ready as soon as possible. Now, the next type of deployment policy we want to look at here, this is a very common scenario on the exam. It's called blue/green deployments, or in other words, URL swapping. Beanstalk allows you to perform a blue/green deployment methodology to really avoid any downtime within your applications. To do this, you have to deploy the new version to a separate environment, and then you essentially swap the CNAMEs of the two environments. So remember, with a web server tier you get a CNAME to reference with your application, and it's all created and managed by Beanstalk. Well, using that, you can swap your environments, and that's going to allow you to redirect traffic to the brand new version immediately. Now we say immediately, really it depends on TTL of your records for your CNAME values, but this is going to be the fastest way to do this. On the exam and in the real world, this is commonly referred to as URL swapping. You're swapping the CNAME URLs to point to different environments. So, on the bottom here you can see on the left in our blue environment, we have our staging Beanstalk environment running version 2 on our EC2 instance. On the right we have our green environment, which is prod, and it's running version 1 on our EC2 instances. Well, by performing a URL swap, it's literally this easy, you just say, hey, prod, I actually want to move you to point to version 2, and staging, I want to point you to this version 1 environment, which was the original production. Now it can be tricky in the real world because your environment names stay the same, so you might want to name them something else besides staging and production. However, I just did that for simplicity. The big thing to take away here for the exam is that URL swapping is great for testing new features frequently while also avoiding downtime. You just swap the environments that the URLs point to. Let's go ahead and wrap up here, and we're going to move on to the final clip in this module where we have a wrap up and review some exam tips.

Module Summary and Exam Tips
All right, very good job hanging in there. We went over Elastic Beanstalk, so let's do a quick summary and review some exam tips that I think are important for you to take with you into your exam. Remember, Elastic Beanstalk is going to be your one‑stop shop for all things related to deploying and hosting web apps on AWS. This is the go‑to Platform‑as‑a‑Service offering. If you have questions looking for a simple solution to bundle and deploy applications, then you should likely favor this over other things like CloudFormation, Proton, Service Catalog, etc. For the exam, be sure to remember the deployment policies. There's all‑at‑once, you have rolling, immutable, etc., and really, really be sure to remember the concept of URL swapping for blue/green deployments. That comes up a lot on the exam. In addition to understanding and remembering the deployment policies, you need to know when to choose a web server environment or a worker environment. Web server environments are for hosting web applications, that's the name, and you get a CNAME URL, a managed‑load balancer, and your autoscaling group of instances. In addition to that, you can also have Beanstalk manage and host and control an RDS database instance, which again is not a best practice, but it is doable. With a worker environment, it creates your autoscaling group, and that autoscaling group hosting your application reads messages off of an SQS queue. That queue can be pre‑existing or created for you, and the autoscaling group itself scales in and out based on the number of messages that are on that queue. And then lastly here, understand single‑instance versus load‑balanced environment types. Single‑instance, you get a single instance, thus the name, with an EIP, and this is very good for development only. Load‑balanced is better for pretty much everything else. You get autoscaling groups of instances, a load balancer, and a public URL, and it's going to handle a lot more complex workloads. With that being said, let's wrap up this module on Elastic Beanstalk. Big thing to take away here, this is the Platform‑as‑a‑Service offering, be sure to remember that. In addition to remembering that, also remember the different supported platform types and the engine types: Java, PHP, etc., all of those are supported. Let's go ahead and wrap up here, and I'm going to see you in the next module whenever you're ready.

AWS Systems Manager
AWS Systems Manager Overview
All right, let's get talking about another very, very important service within the AWS suite of services, AWS Systems Manager. Let's start things off with an overview of what the service is at a high level, and then we'll start diving into deeper components and resources within it that are very important for you to know for the exam. First up, what is AWS Systems Manager? First and foremost, this is one of my favorite services within the AWS cloud. It offers so many capabilities, and it can be very powerful for operational procedures. What this is at the highest level is essentially a suite of tools and capabilities that are designed to let you view, control, and automate nodes and compute within AWS, other cloud providers, and even on‑premise. In other words, it works with things like EC2, it works with Azure VMs, and it can even work with your own VMware VMs on‑premise. In order for your nodes to work with Systems Manager, however, they have to be known as what is called managed instances or managed nodes. When we say managed in this particular aspect, we mean we have what is called the SSM Agent installed and connected. Now, the SSM agent you might be familiar with if you took other courses within this learning path, it is used to connect to your EC2 instances securely using Session Manager. In addition to having the agent installed, configured, and connected to the service, you also need to grant correct IAM permissions for that agent to connect and for you to use the managed nodes. So essentially there's a two part, you have to have network connectivity and you have to have IAM authorization and authentication in place. Now when you're using this service, the common example that you'll see a lot of the time within the console and in the CLI is using this particular AWS‑managed policy. By attaching this policy to a node that is going to be managed, you can easily grant all of the required permissions that are needed to interact with Systems Manager. And, speaking of the Agent, let's talk about it a little bit more in depth. This is a critical component, so you have to understand how this works and why you use it. The SSM Agent is essentially just a required Amazon software that's going to run on your nodes, and it allows Systems Manager to connect and then update, manage and configure any resources that are running it. A large majority of the time it's going to be typically pre‑installed on a majority of the official AWS AMIs. So what we mean is the Ubuntu, Amazon Linux, all of those officially released AMIs typically have this pre‑installed, and you just have to make sure you configure it and allow it or authorize it to connect. Now, again, very important, I bring this up because it is extremely important. You have to ensure you grant IAM permissions for it to connect when it is running. If it has network connectivity, but you don't allow the IAM permissions, well, it's not going to work and you're going to have a huge headache. One of the benefits of using the SSM Agent is that it's essentially portable. You can install it on your own compute, so on‑prem, in Azure, GCP, etc., and even on edge devices, so IoT edge devices. By installing it and then configuring it correctly, it allows all of your compute to be managed and interacted with by AWS Systems Manager. So it allows for a one‑stop shop or one place to actually configure and manage all of your compute. Within Systems Manager for this exam, you have to know several of the capabilities. Now you don't need to know all of them, but I went ahead and picked out the ones that are very important for you to remember going into the exam. So let's look at those really quickly. First, we have Automation. We have something known as Run Command. There's Patch Manager, we have Parameter Store. There's Maintenance Windows and then Session Manager. Now Session Manager we're not going to cover in depth within this course because we cover it way more in depth in a previous course within this learning path. So if you want to learn about Session Manager and you haven't done so yet, please reference the course that you can see here, go back, check it out, and see how you can connect to your EC2 instances securely using that capability. But for now, we're going to end here and we're going to start moving into some of these important capabilities a little bit more in depth.

Patch Manager and Maintenance Windows
All righty, let's dive into the first set of capabilities, Patch Manager and Maintenance Windows. Patch Manager is a capability and toolset within the Systems Manager service that is going to allow you to automate patching of your managed instances and managed nodes. It's very important to realize you can patch both operating systems and your applications using this capability. Maintenance Windows are another capability that allow you to define essentially a schedule for when you want to perform actions on your managed nodes. Now let's dive into Patch Manager a little bit more in depth here. With Patch Manager, it supports Windows operating systems. With Windows, it supports installing service packs on Windows nodes. So if you need to install service packs and patch your OS on Windows, you can do that using Patch Manager. From the Linux side of things, it supports a little bit more in my opinion. It supports minor version upgrades on Linux nodes in addition to all of your small packages that you might update using yum, apt‑get, etc. In addition to the operating system support, it also is supported on EC2 instances, edge devices, on‑prem servers, and even on‑prem or third‑party VMs. Essentially, if you can run one of these supported operating systems and you can configure and install the SSM Agent, then you can use Patch Manager. When you use Patch Manager, you're going to use what is called a patch policy and a patch baseline to configure your patching rules, and we will explore these a little bit more in depth here coming up shortly. And then the last point here, it supports both scan and then scan and install patching operations. When you choose scan, it's only going to scan and then generate a report on missing patches that can be installed. If you choose scan and install, well, it does the scan and reporting, and then also goes ahead and installs supported patches based on policies and baselines. Now, speaking of baselines and patch groups, AWS offers pre‑defined patch baselines. A patch baseline is going to say which patches you want to actually look for and potentially install based on the operating system. If the AWS predefined ones are not good enough, you can also define your own. So you can say, hey, I only want to look for security updates or I only want to look for minor version updates, etc., and you can create that patch baseline and assign it to a patch group. Because you're customizing it, you're customizing and configuring the categories, you can also customize and configure auto approvals. So you can say, hey, I want to auto install these minor versions automatically after 7 days if they're not manually approved, or you can completely ignore entire categories at all. So maybe security updates, maybe major version updates, etc. And finally, if you use custom patch baselines and custom patch groups, you can set up custom alternative patching sources. So if you have a Linux EC2 instance and you want to set up custom repos for patches, you can do that using custom patch baselines and custom patch groups. Let's have a quick exam scenario on when you might use Patch Manager. Let's say you have a fleet of thousands of both Windows and Linux EC2 instances. You need a way to automate operating system updates, and you want to be able to scan for patch compliance and report on that compliance. Well, AWS Systems Manager Patch Manager does this for you. It's going to greatly reduce the amount of operational overhead and complexity, and it offers it all in one single service. Now moving on to Maintenance Windows. This was the other capability we wanted to cover. First off, what are they? What these are are a capability within Systems Manager where you can define your own schedules for when you might have to perform possibly disruptive actions. So maybe something like an install that's going to cause downtime. Now, these don't just support Amazon EC2 and compute, you can actually base them around other resources like S3, SQS, and OpenSearch Service. Now I say that because it supports it, but to be honest, for the most part, you're going to use these with Amazon EC2 instances or some other type of managed compute. However, since this is a solutions architect exam, just understand they can reference other resources if needed. Now with Maintenance Windows, an exam pro tip, if you need to define any schedule for avoiding impact while patching or installing application updates on a managed node, you're going to want to think Maintenance Windows, plain and simple. Easiest way to think of this is you're performing maintenance within a specified window. Now let's go ahead and wrap this clip up, and we're going to move on to the next set of capabilities that you need to know.

Automation and Documents
Okay, let's talk about automating workflows and processes via automation and documents within Systems Manager. First up, automation. This is a tool and capability within the Systems Manager platform that's going to help you simplify common maintenance, deployment, and remediation tasks. Automation itself supports several different resources. It can automatically and easily work with EC2, it can work with RDS, it supports S3, and many, many more. Again, just like last clip, however, a majority of the time you're going to use automation on the exam to interact with managed compute and managed nodes. There might be a few regarding RDS, and maybe very little with S3 and others, so really be sure you understand this works very closely with EC2 and other managed nodes. Now when you're using automation, you can create your own automations or you can use AWS predefined ones. A large majority of the time I tell people to use the predefined ones and then create your own based off of those where you can customize some of the smaller configuration options. Now, you might be asking, okay, what is an automation? Well, an automation, by terms within Systems Manager, is any set of tasks or tasks that are defined in a runbook that get performed by the service within Systems Manager. Some example uses of an automation are disabling public access for security groups. You can use them to restart EC2 instances with or without approval. And you can even use them to configure S3 bucket logging. Now, with these example use cases, we need to talk about triggering them or kicking off the automations. There are four primary ways to use automations. You can actually schedule them using Maintenance Windows, which we just covered in a previous clip, or you can trigger them via EventBridge, so you can trigger them on a scheduled basis with EventBridge, or maybe you want to look for an event type that triggers the kickoff of one of these automations. For instance, maybe you have a change in a public security group that allows public access, well, you catch that with EventBridge, trigger an automation, and it can automatically fix it based on a document. Thirdly, they work very, very closely with AWS Config rule remediations. Now we cover Config much more in depth in another course, but at a high level, Config essentially monitors for compliance and historical configurations of your resources within AWS. So if it sees something and it says, hey, this is out of compliance, it can kick off a remediation effort, which leverages automations within Systems Manager. And then the fourth option here, you can manually trigger these via console or CLI or SDK. So if you want to just test them or you want to play around with them or you have a one‑time requirement, you can do that very easily manually in the console or via some type of API call. To be honest, the two very popular options or methods on the exam are going to be these middle two here, triggering via EventBridge or using with AWS Config. Now moving on, let's talk about documents. Documents are what make up automations essentially. These are going to define the actions that are actually getting performed on your managed nodes. With your documents, there are three types. There are owned by Amazon documents, which are offered by AWS, and they're preconfigured and you can't customize these directly. You can also create your own, and these are going to be referenced as owned by me. These are where you create your very own custom documents with all of your own options. Again, I like to reference people to use or start based on the AWS offered ones and then customize those as I see fit. And then lastly, there is a shared option. These are where you can create and share your own documents within AWS Organizations. So if you have a centralized account for compliance, they can create their own documents and then share it out to the org so all of the member accounts can reference those. Moving on from documents, let's wrap things up with an exam scenario. Let's say you have to automatically identify EC2 instances that do not have encryption at rest enabled, and if it's not enabled, you need to enforce compliance by essentially turning it on automatically. Well, this is a two‑parter. You can use Config to detect the non‑compliance, and then when it says, hey, EC2 instance one is not in compliance, it can trigger an automation to go ahead and remediate that issue. Now, with that out of the way, let's go ahead and wrap up this clip on automation and documents, and we're going to move on to another important capability called Run Command.

Run Command
Up next in our Systems Manager journey here, we're going to look at another tool and capability called Run Command. AWS Systems Manager Run Command is a tool that's offered that allows you to remotely and securely manage your managed nodes. It's going to be used to automate common administrative tasks and perform one‑time configuration changes at scale. Now, if you want, you can perform it on a single managed node, but it's very useful for performing a single command one time on hundreds or even thousands of managed nodes, all with the click of a button. Using Run Command allows for several different possibilities. You can execute scripts, so PowerShell or maybe Bash, etc., or you can even execute one‑time commands. So if you don't want to run a script and you don't want to pass in a script format, you can literally just pass in a single line with a one‑time command that's supported by the operating system. To use it, of course, it connects via the SSM Agent. So that's why we said this is such an important and critical component within the architecture when using Systems Manager, pretty much everything is done via this Agent. An example for using it could be updating installed software where you don't want to bring down any of the instances, you don't want to replace any of the instances, etc. For example, maybe you need to post a hot fix to your application, well, you can quickly and easily do that using Run Command. Moving on, let's look at some important concepts to know regarding this tool. First and foremost, it supports logging, so this is very important for compliance. You can send all of your command logs to be saved to Amazon S3 buckets and Amazon CloudWatch logs. However, keep in mind you need to be careful with sensitive data. If there's sensitive data in there in plain text, well, it's going to be logged in the same way it was ran, so it will also be plain text. For triggering, you can manually trigger via console, SDK, or CLI, or you can use Amazon EventBridge rules. So you can set EventBridge rules up to go ahead and look for either a schedule basis or an event type, and then trigger your Run Command workflow. The third important concept here is regarding security. All API activity that is completed using Run Command or even to trigger Run Command gets captured via AWS CloudTrail. CloudTrail will be covered much more in depth in a different course within this learning path, but at a high level, it's used for auditing and compliance purposes to capture all activity within an account. So, your Run Command trigger gets captured and anything it does as far as API calls go also gets captured. You can restrict access to Run Command and Systems Manager in general using IAM permissions, of course. And then lastly here, an important note. This is going to be best for one‑time tasks, and of course this requires the SSM Agent, so make sure it's configured, make sure it's connected, and then you can use it for your one‑time configuration, one‑time maintenance, etc. Now, that's going to do it for this clip on the Run Command. Let's go ahead and end here, and we're going to move into some demonstrations where we start demoing some of these tools.

Demo: Executing Remote Scripts via Run Command
Let's go ahead and have a quick demonstration where we're going to use the Run Command tool within Systems Manager to execute some remote scripts on some EC2 instances. All right, I'm over in my sandbox environment here, logged in as cloud_user, let me zoom in to make this a little bit more readable. And before we dive into doing this demonstration, let's have a quick review of some of the infrastructure components I've already populated for us. I went through and I manually created three different EC2 instances here, and I gave them the name of Web Server. Now each of these instances is the same, it's running the same AMI, it has the same IAM role, etc. For security, if we go to this IAM role here, I'll open this in a new tab, what this has done is it's given us the required permissions to do a few things. I've attached the SSMManagedInstanceCore permission policy, which is AWS managed, and this gives us the exact required permissions to interact with Systems Manager. Remember, you have to have the SSM Agent running, and you have to have network connectivity and IAM permissions for it to actually work appropriately. I've also went through and gave it some permissions for CloudWatch so it can log, if we choose so, when we are executing some of these commands. Now, the CloudFormation template I used to deploy these resources, everything except for the EC2, that is, will be provided as a module asset, so you can go ahead and use that if you want. In addition to these servers, for the tags, I gave them a name, and I also tagged them with Application Web, and I'll show you why that's important when we start executing these scripts. Okay, so let's go ahead and get started now. I'm going to go ahead and navigate to Systems Manager, which is here on another tab, and feel free to enable the new experience if you want in your own account; for this, I'm just going to focus on our particular tool, which is Run Command over here on the left. So, once we get in here, we can start selecting commands we want to run. So I'm going to find Run command, and this top portion here are the different available documents that are supported via Run Command. Remember, SSM docs are how all of these automations work, so the automation, Run Command,etc., leverage these documents to perform what they need to do. So what we can do is if I just skip through here, I'm not going to look at all of these, just understand there are a ton of different documents for us to use. So what we could do here if I go back to the first page, we could filter, we can look for document names, we can look at the owner, so is it owned by me or Amazon, etc? Or, we can even look by platform types. So let's say I want to filter on Linux because my EC2 instances are running Linux. So now we get a filtered view of only Linux‑supported documents. So if it was only Windows, this wouldn't show up in this list, it has to have Linux as well. Perfect. So let's go ahead, I'm going to actually search for ShellScript. Now once I filter that down, you're going to see we have this document here called RunShellScript. So what I'll do is I'll select this, and before we do this, let me open this in a new tab here, and I just want to show you at a very high level what this is doing. It's not important that you understand the contents of this document, but I just like to show you so you understand at a deeper level. This is what a document looks like. This one's very simple because it's literally just taking in commands, running them out of a working directory, and then giving us output. So this is what this document looks like on the back end, and we're going to use it to run a shell script. Now, the next thing we do here is we can choose the version. So, if you have your own documents, you can have multiple versions, you can use defaults, latest,etc., but for this, we have 1, so I'm going to leave it as the default. Next we have the parameters for this Run Command. Now this document only has our commands, working directory, and timeout values, so that's all we have to worry about here. So what I'm going to do is all of these instances have a web server running, HTTPD, so I'm going to actually go ahead and jump into my IDE, and this is the command we're going to run here, it's a simple bash script that restarts the actual service. It captures the output of that restart command, adds a time stamp, and then echoes it into a temporary file that we're going to verify by connecting to one of these instances. So, I'll copy this, I'm going to go back into my console, and then I'll paste it in. So now that we have our bash script in place as our command, I can go down and I can specify the working directory if I wanted to. Now we don't need this, but if this was specific to an application, you might have to point that in here. For instance, maybe you're having to update a config file or update a specific third‑party app, well then maybe you need to reference it via its path in this particular field. We then have execution timeout, which I will leave as default here because this should not time out, but you can see there are some options here. Moving on to the next important section here is target selection. This is where we specify what managed instances and managed nodes that we want to run this on. So you'll see we have three options. We can choose a resource group, which we don't have, but if you had a resource group, you could just pick that one out by itself. You could actually choose instances manually, and you're going to see here at the bottom, all three instances are online, as you can tell by ping status, they're running, and then we have the information for each of these particular instances. We see the SSM Agent, etc. So if we wanted to, we could just go ahead and pick one particular instance and continue that way. What I'm going to do instead, though, is pretend we have a ton of EC2 instances, and I want to actually go ahead and use tag key values. So I'm going to go ahead and copy and paste the tags here, we have Application as the key, and then we have Web as the value. So let me copy that, I'll paste that into the tag value here, and then I add it. So what this is doing now is saying, hey, any managed instances that are online that have a tag key of Application with a value of Web, I want you to run this command on. So hopefully you can see how handy that is, since maybe you might have thousands of production instances, etc. You can just specify tags and let this run with it. Next up, we have other parameters not specific to this exact document. We can give it a comment, and we can specify a timeout, so I'll leave this as default. After that, we have rate control. So if I go ahead and open up rate control, we can set concurrent targets or percentages and a number of errors that we're willing to handle. This is out of scope for the exam, so I'm not going to cover this, but the next thing I want to do is talk about output options. Remember, in Systems Manager with Run Command and some of the other automations, you can output documents and you can output logs to buckets and CloudWatch logs. So, if there was output with this command, which there really shouldn't be, we can enable it to go to S3, and then we can choose a bucket here that we want to save the output in. We can give it a prefix, and we can even enable CloudWatch logs. So what I'll do is I'll leave the default here, but I'm going to enable S3 and CloudWatch log, I'm going to scroll down past alarms here, and everything else is out of scope for this exam, so we're not going to cover it. I then click on Run, and we get our statuses. So if I start refreshing this, we should see all three instances, and we do. If I refresh again, these are going to go successful because it's that easy of a command. So we see now in a very easy‑to‑read format, we have overall success, success, we see the number of targets we aimed at, the number completed, etc. Now what I can do here is if I go ahead and click on this, we'll look at this instance here, 83e90, if I find that and I select this, it's going to take us to the output for that specific command ID, and you can see that ID here at the top, which is a unique identifier. Within this, we see the response codes, the step names, start and stop, and there is no output and there is no error, so, we're not going to get any logs within S3 because there was nothing to output, but it's important you understand you can configure that to do so. But, with that being said, if I go to my instance and I connect to this via Session Manager here because I have the Agent installed and configured, and I go ahead and ls in the tmp directory, we see our command_output_log. So if I cat that, there we go, this is exactly what our script did, it ran that HTTP restart command, it output that format into a log file for us, and at the bottom here you see our Hello Pluralsight new line that it also printed to test. So using the SSM Agent, we were able to remotely execute a very simple shell script on our managed instances. Now that's obviously a very simple example, but you can use this for far more complex scenarios like patching, updating, etc. With that being said, let's run one more here, and we're going to use a different Run Command document. So under Run command, I'm going to run again, and I'm going to look for Agent. So this is a better example, and let's just assume we want to update software on our instances. So I'm going to select this one called UpdateSSMAgent, I'm going to go down here, we'll leave default, we'll leave this blank for version, otherwise you can specify the version of Agent you want to install, and you can say, hey, I want to allow downgrades, so I'll change that to true, just to switch it up a little bit. And we're going to do the same thing here, we're going to say Application Web, I add that tag, and then I scroll down, I'll select my bucket name, I'll enable logging again, and then I'm going to click on Run. Okay, so we see these are in progress here, eventually, we'll see all three, and if I keep refreshing, at some point, these are going to be successful or they're going to error out. So what I'll do here is I'm going to go ahead, I'll pause very briefly, and then I will resume once these are done. Okay, so I fast‑forwarded, we see all three completed successfully, which is great. We see the start time and we see the end time, which roughly took 40 seconds or so. And now what we can do is I can click on, again, the same instance, and under this output, we see the steps, etc., we get output, and you'll notice that there are several steps within this one command document. So this one is saying, hey, I skipped this because this is not a Windows platform. So step two is update the Agent. So if I go to output here, well, this has success output. So it's skips step one based on some conditions, and it's saying, hey, I am updating the Agent, we can see it successfully updated, and we got no errors. Now, if we wanted to, remember, we actually logged this to CloudWatch and S3. So, just for this, I'm going to select CloudWatch Logs, this will take me to my log stream and log group, and let's see what happened here. Perfect. So we got our one output, successfully downloaded the manifest, downloaded the updater, etc. So this is logging our Run Command for this instance to our CloudWatch Log here. Now you're going to notice, if I go back here to the log group itself, we're going to have three different log streams because it's logging each instance, and you can see in the middle of the log stream here the instance ID. Now, that's going to do it for this demonstration on the Run Command tool within Systems Manager. Remember, for the exam, you can use this to execute remote scripts and commands on managed nodes and managed instances to perform things like updates, deletions of files, application patching, etc., all that scale with a single click of a button. Let's go ahead and wrap up here, and I'll see you in an upcoming clip.

Parameter Store
All right, next in line is Parameter Store. This is another very, very important tool within Systems Manager that you have to understand how to use. So let's go ahead and look at it now. Parameter Store is an offering and tool within Systems Manager that's meant to give you secure, hierarchical storage for configuration data and even some secrets. The whole point of using Parameter Store is to easily reference it in scripts, Systems Manager documents, and even automation workflows. For instance, maybe you want to store the parameter of an AMI ID or some type of credential, you could easily do that in Parameter Store and then reference it using a parameter name and getting that value programmatically. Within Parameter Store, you can grant granular access control via IAM, and we're going to explore this with an architecture example here, coming up very shortly. The big thing to just realize here is that IAM is used to control access, which pretty much goes for any AWS service. Some examples of using Parameter Store are creating parameters for passwords, usernames, and even AMI IDs. Now, these are all pretty simple strings for the most part, but you can also use it for something more complex like a license code. So you can have a massive block of text, or you can even Base64‑encode it and store that encoded text. In addition to something like that, you can also store application configuration data. So maybe you have a development config file or config parameter, you have a test and you have a production version of the same type of parameter, all with different values, but you reference them via the parameter name. With those examples out of the way, let's look at the parameter types. You have to understand the three parameter types within this tool. The first parameter type, and the most common and easiest would be a string. A string will literally take any literal block of text that you enter into the value field. Examples of using this could be for usernames, it could be for HTML code, and it could even be for Base64‑encoded strings. Again, pretty much any string of text that you want to enter, you can enter, and it will store it. You also have a string list. This is going to be a comma‑separated list of values. Examples of using this could be things like values for holding white‑listed IP addresses, maybe you have several account IDs, or maybe you have days of the week where you want to schedule certain automations for and you pull the parameter value to see which days those are. And then lastly, we have a secure string. This is for any data that you need to be encrypted and stored within this service and tool. This directly integrates with AWS Key Management Service, or KMS, to encrypt the values of the parameter. KMS is covered much more in depth in a different course within the learning path, but for now, understand it allows you to perform cryptographic operations on data within AWS. A common example of using a secure string could be something like a database password. Now, let's move on to an architecture example. In this example, we're going to demonstrate how you can use hierarchies or paths to implement permission controls for different values. On the left here, we're going to assume we've created two different roles for our users to use or maybe our EC2 instances, etc. There's a dev_role and then there's a prod_role. Now, in AWS Systems Manager over here on the right side, we have four different parameters, we have two for dev and two for prod, as you can tell via the pathing structures. So you'll notice on the top we have /dev/db_hostname and db_password, and on the bottom we have /prod for the same values, hostname and password. Well, you can leverage IAM policies attached to those roles to say, hey, this particular principal can access specific parameters based on those paths. For instance, maybe the dev_role can only get parameters in the /dev/ pathing, and then prod can only get the /prod/ pathing. This is how you can use granular controls within the same account to restrict access to different parameters. So now, when they make a GetParameter API call, they can call their respective parameter path and get those values. It's also important to call out that you can mix and match parameter types as needed to host both sensitive and non‑sensitive data. So, for instance, let's look at another example here where we're just going to use the /prod pathing. If we wanted to use a secure string for the db_password, which would make sense, you can specify it as a secure string type and then you can use a custom KMS key for that parameter. So what happens is the parameter references that KMS key and it encrypts the value via the KMS encrypt call. If we or some IAM principal wanted to actually get that value of that secure string parameter, they have to have both GetParameter permissions and the ability to decrypt using that KMS key. So there are two specific permissions that have to be granted there. The big takeaway here is secure strings use KMS for encryption, and you have to be able to both get the parameter and decrypt using the key. Let's have a quick exam pro tip to break things up here. When you're using a SecureString or path‑based parameters, again, ensure you grant the correct IAM permissions. You always want to grant the least amount of privileges that are possible. Moving on, we're going to compare something called Secrets Manager with Parameter Store. Now AWS Secrets Manager is something we're going to cover in a different course as well within this path, however, it does directly relate or compare to Parameter Store, so I'm going to cover it at a high level here. Secrets Manager at a high level is a service strictly meant to store and potentially automatically rotate secret values for your workloads. So with that being said, Parameter Store does offer KMS encryption. We looked at that with SecureStrings, it also very easily integrates with AWS services. You can easily reference parameter values within ECS task definitions, you can use them in Lambda functions, etc. It also offers CloudFormation support, so with parameters in your templates, you can reference Parameter Store parameters and automatically pull in those values as well. When you're using Parameter Store, there are two classifications of parameter types, there's Standard and Advanced. With the Standard tier, you can store up to 4 KB of data. With the Advanced tier, you can actually store up to 8 KB of data. In addition to that, Parameter Store is going to be best for broader use cases. It can store things like App Config data, usernames, etc. There's just a lot more you can do with it when we compare it to Secrets Manager here in a moment. One thing to call out, however, is there are no automatic rotations of Parameter Store parameters. Believe me when I say this is a key indicator on an exam, you do not have automatic rotations, and one more humongous indicator to keep in mind ‑ this is free to use for Standard parameters. So if you're using a Standard‑tier parameter, this is a free service. So if there's cost optimization, think here. Now if we compare this to Secrets Manager very quickly, you can see there are a lot of similarities, but when we get down to the bottom, this is where the differences kind of start to kick in. Secrets Manager supports larger amounts of values, it's specifically meant for secrets, it does offer built‑in automatic rotation, which is a huge feature, and, it is never free, you are always paying to use Secrets Manager. To be very honest, those last two points there on each of these are the big things to keep in mind for the exam. Now, to wrap things up here, a quick exam pro tip, just to keep it in mind and keep it fresh, if cost efficiency is the key indicator in your exam scenario, you're likely going to choose Parameter Store over Secrets Manager. With that being said, let's go ahead and wrap up here. Be sure to go back and review the different parameter types, the use cases, and we're going to move on to a demonstration where we're going to use this tool.

Demo: Creating and Using Parameters in Parameter Store
All right, in this demonstration, we're going to jump back into Systems Manager, and we're going to create and then use parameters within Parameter Store. So let's go ahead and jump into the console now. All righty, I've jumped into my cloud sandbox here, I'm logged in as cloud_user in my us‑east‑1 region. Before we jump in, I want to overview some pre‑created infrastructure pieces that I've already deployed, and these will be easily deployable for you if you want to do this yourself via the supplied CloudFormation template. A few very important components here. Obviously, we have cloud_user, which is a default thing when you load our hands‑on playgrounds, but I've also created a KMS key that we're going to use for encrypting our secure strings. Now I've aliased this as parameters, and we'll see that when I create our parameters later on, and we're going to use this for production‑related parameters when we're testing. I've also created this UserA IAM user. This user is who we're also going to use in addition to our cloud_user, and I've created a permissions policy here that allows it to GetParameters, GetParameter, and GetParametersByPath for all dev‑specific pathed parameters in Parameter Store. In addition to this policy, I've already set up some security credentials here, some access keys, and we're going to test this in the CLI, here coming up in a moment. Now with that being said, let's go ahead, I'm going to jump over to Systems Manager, I'm going to find Parameter Store here on the left‑hand side, and I'm going to load the service. So, once I get in here, we can start creating our parameters. So let's go ahead, and the first thing I'm going to do is I want to create a bunch of dev parameters, so specific to development. So the first thing we do is we give it a name. So the name we're going to start off with, /dev for a path, and let's just call it username. We can give it an optional description, I'm going to skip that for now, we can then select the tier, so Standard, which is free; or Advanced, which is not free. We're going to choose Standard, and then when we scroll down, we get to the parameter type. Remember, there are three types of parameters, as you can see here. For username, we're just going to give it a string. So I select the string type, and then we can see data type. This is a nifty feature here. If we wanted to, you could create your own AMI, and you can specify that via this data type here, and it will do some validation to see if this is a valid value or not based on the data types supplied. We're not going to do that, just keep that in mind that this is a supported validation option. I'm going to select text, and I'm going to go ahead and give my value for my parameter. Perfect. So I go down, I'm going to create my parameter here, and now we have our /dev/username, and if I go in, we see the value here in plain text. Awesome. So let's go back and let's create some more here. I'm going to create two more dev parameters. So I'm going to do /dev, I'm going to call this patch_days, and we're going to pretend this is a list or a StringList here of days that we are allowing dev to be patched. So I'm going to go ahead and I'm going to put in my separated list. I go down, I create my parameter, and there we go, we now have patch_days here, it's a StringList. Perfect. So let's go ahead and create one more dev parameter called password. So Create, /dev/password. I'll scroll down, I want to make this one a SecureString because I want this to be encrypted. Now, this is where we get a little bit more specific and you need to understand this for the exam. We're going to select our current account for the KMS key, and we're going to use the default KMS key with the alias of aws/ssm by using this default managed key. As you can see here in this blue box, all users in this accounting region have access to it, so it's encrypted, but any user, essentially, in the account can decrypt whatever is being encrypted with it. So even though our user here in IAM doesn't have the permissions to decrypt KMS keys, as you can see here, based on how this works, we will be allowed to do it by default using UserA. So, with that being said, let me fill in my value. We have a SecureString, it's encrypted with the default key, and I'll click on Create parameter. Perfect. So now we have all three parameters. If I go into password here, SecureString, you're going to notice it is encrypted and hidden within the console. So what we can do is show decrypted, and by doing that, that runs a decrypt command to KMS, finds that key and says, okay, give me the plain‑text value. So now we see our password. If I deselect, it re‑encrypts it. All right, so let's go ahead and test this. I'm going to jump over to my IDE really quickly, and I'm in here with my simple script, and there's three scripts that we're going to test. This first one is the dev script that should work. What we're doing is we're running three commands. We're saving username, patch_days, and the secure_password based on the ssm get‑parameter command, we're specifying the name of the parameter, we're getting the value and outputting it. Now, I need to call out what I've done here is I went ahead and I already configured my CLI to use UserA within IAM. So to show you that, I can run this get‑caller‑identity command, and you can see right here, UserA. So these are the default credentials. Let me clear this, and let's go ahead and execute this script. So, aws_cli_dev.sh, and there we go, we get our username, Patch Days, which is our list, and our decrypted value. So this worked perfectly. We were able to decrypt that SecureString using the default alias key. Well, that's fine and dandy, but let's try encrypting with a KMS key that's not default. So I'm going to jump back into my console, I'm going to go here, I'm going to call it /dev, and we'll call this custom_password. So I'm creating a /dev parameter still, so permissions‑wise, we should still be able to get it, and it's custom_password. I'm going to make it a SecureString, and I'm going to select my custom KMS that I created called parameters. I'll give this a new value, and then I go ahead and create it. So now since I'm logged in as cloud_user, I should be able to see this. So if I go to /dev/custom_password, it works, I can decrypt it because I have essentially admin credentials with the cloud_user. But, if we jump back into our IDE, and I use this break command here, you can see we're trying to get the parameter, custom_password, we pass in with decryption because it's an encrypted version and we're trying to output it. So, using the same credentials here, now I'll run this other command. Oh, and there we go, we get an error. So you'll notice, hey, I could see the parameter because it's /dev, but I cannot decrypt the key being used for this parameter value. So even though IAM allows me to get /dev parameters, well, this one's encrypted with something I can't use. So IAM and KMS are working appropriately. With that being said, however, let's create some prod ones. Since cloud_user has admin permissions, I should be able to go ahead and perform a similar act with that custom key for /prod. So what I'll do is I'll jump into the console, and I'm going to create three new parameters really quickly to basically mimic the first three that we created. So I'm going to go in here, do /prd/username, we'll give it a text‑based value here, and I'll create it. Now I'm going to fast‑forward here and create password and patch_days for prod. Now, I'm in the middle of creating my prod password here, I'm going to select my parameter string, and I'm going to give it my value. I create it, and there we go, we now have our three prod values and parameters here, as you can see, let's test this out now with a different set of credentials. I'll jump back to my IDE, we're going to run this prod script, which is using the cloud_user profile, which is our cloud_user sandbox user, and I'm going to go ahead and execute this. So aws_cli, prd, let's run it. And, there we go, we get all values successfully because this user has both the ability to look at path‑based parameters and it can decrypt that KMS key value. Perfect. So that's how easy it is to use Parameter Store to control via pathing different parameter types and access, we created each parameter type to view how it works. Let's go ahead and wrap this up, and we'll move on whenever you're ready.

Module Summary and Exam Tips
All right, and that's going to do it for this module. Before we wrap it all up, let's have a quick summary of what we learned, as well as some important tips for the exam. First things first, and probably the most important thing to remember in this entire module is what Systems Manager is. Remember, this is a suite of tools designed to let you view, control, and automate managed nodes within AWS, other cloud providers, and even in on‑premise data centers. Any interactions with Systems Manager and your managed nodes are going to require that the SSM Agent be installed and running. In addition to having the Agent installed and configured, well, you have to have IAM permissions in place as well. Remember that there are two parts to this scenario. You have the Agent that is configured, installed, and running with its network connection, and then you also have IAM permissions in place. There are two major portions to the puzzle. Now, we didn't cover Session Manager in this course, but it is covered in another course, as you can see here, which I just listed on screen. So make sure you are familiar with that as well. Very high‑level, it's a secure way to connect to your EC2 compute without having to open up SSH or RDP ports. Now real quick, let's run through the different tools that we learned. You need to review these and understand where you would use them within an architecture. First up, Patch Manager. For this, remember you use this to automatically patch, update, and configure managed nodes, including both Windows and Linux machines. Remember, with Patch Manager, you can scan and you can perform scan and install operations. You also have maintenance windows. These are definable schedules for when you think you might need to perform potentially disruptive actions on your managed nodes. In other words, maybe you're going to install patches or application updates that might cause downtime and you don't want to impact your customers. We also talked about automations. Automations leverage documents or SSM documents and automation documents to perform common maintenance, deployments, and even remediation tasks. And speaking of documents, remember, you have custom documents that you get to create and own and manage, or you can leverage predefined AWS‑owned versions. These documents are how Systems Manager performs automations. One of the most common scenarios on the exam with an automation is using this with AWS Config to perform remediations. If you recall, one of the examples we talked about was having a security group that might not meet compliance for public access, and when AWS Config marks it as noncompliant, it can execute a remediation action using an automation document. Moving on to the rest of the tools that we looked at. Remember, Run Command. This is a tool offered to remotely and securely manage your managed nodes. By leveraging this built‑in tool, it allows you to execute either entire scripts like PowerShell or Bash, or even just one‑time commands, so anything supported by the operating system that you're executing it on. One of the biggest benefits of Run Command is that it integrates very, very tightly with Amazon CloudWatch logs and AWS CloudTrail so you can log all of your activity and capture any audit trails. We also talked about Parameter Store. Remember, this is a secure hierarchical storage for configuration data, and even if you need to, some secrets. You need to be aware of the three types of parameters, as well as the requirements for using a SecureString parameter with a custom KMS key for encryption. To wrap this clip up, let's have one last exam tip. Typically, Secrets Manager is going to be best for managing secrets, especially when it's integrated with other services and you need to rotate secrets. The primary time you're going to choose Parameter Store over Secrets Manager on the exam is when cost optimization is the primary focus. Remember that Parameter Store is free, and that's one of the big benefits. But, for a majority of other use cases, especially regarding secrets like passwords and usernames, you're probably going to want to lean Secrets Manager. But, with that being said, let's end here. We can take a quick break, and then I'll see you in the next module, and we'll pick back up right where we left off.

Amazon SQS and Amazon SNS
Decoupled Architectures
All right, let's jump into another module. In this module, we're going to review two very important services for decoupling your architectures. Now, the first thing we want to talk about here before jumping into the services is we need to review decoupled architectures themselves. This is a best practice, and it falls in line with the well‑architected framework. So, to begin this, let's look at a tightly coupled architecture. Usually when you have a tightly coupled architecture, the components are going to depend heavily on one another. So in this case, if our front end is impacted, well, we don't have any other way to reach our back end, so, essentially the entire system is going to be impaired. So if we were to assume that something happened to our front‑end application and it goes down, well, unfortunately that means our users are going to have a bad experience. One of the benefits of using a tightly coupled architecture is that it will leverage synchronous systems design, which can be very quick because the components are directly communicating with each other and they tend to interact at the same time or in real time. However, that one benefit is very easily outweighed by the fact that we are so heavily dependent between the two components. If our back end goes down, well, then the same thing could be said for the front end, vice versa. If we shift to a loosely coupled or decoupled architecture, well, the first thing you might notice is it is slightly more complex in the design itself. However, by taking the time to properly design a loosely coupled architecture, that means the components are far less dependent on each other, so you're going to minimize the impact of changes or failures within the system. A common methodology for decoupling architectures itself is to use what is known as a messaging queue, which is this layer here near the bottom of the architecture. These are such a critical component in decoupling architectures because they offer things like resiliency and asynchronous communication. With asynchronous communication, if there were something to go wrong on one side of this application, well, we still have a happy user, it's not necessarily a major component factor because we are loosely coupled. What this means is if we do have a failure or some type of downtime, whether it's expected or not, well, the application can continue running, and we still have our happy user here at the top right of the diagram. The big thing to take away here is that you want to always decouple your architectures in the cloud. With this type of architecture design, there are two different types of messaging systems that you have to be aware of for this exam. The first is a push‑based message system. This is where messages are sent by a producer to a server, and then from that server they are immediately sent to a consumer. You'll notice I highlighted topic and pushed because these are going to be very critical coming up in some future clips. You then have pulll‑based or queue‑based architectures. This is where messages are sent by a producer to a server, and those messages are queued up, and then they get pulled off by consumers when those consumers are ready. The big word highlighted here is queue. You have a queue of messages or a line of messages. Now with these two types of messaging designs, there are two very important services, which we briefly talked about in the beginning of this clip. For push base, there's Amazon Simple Notification Service, or SNS, and for pull‑based, we have Amazon's Simple Queue Service, or SQS. Let's go ahead, we're going to break here, and we're going to start diving into some of these services at a much deeper level now.

Amazon SQS Overview
All righty, first up on our journey through decoupling our architectures, let's look at Amazon's Simple Queue Service. Amazon SQS. What is it, why would you use it, and how would you use it? SQS is the go‑to messaging queue service within AWS that allows for asynchronous processing of workloads. You will want to use this to implement a decoupled approach within a distributed system. Now, typically you're going to have multiple and different queues for different purposes within the same architecture. To review, this is a pull‑based system. So a producer sends a message to an SQS queue, and then another resource called a consumer pulls that message off the queue to process it. With this approach, it allows you to build a buffer between components. So if producing messages is much faster than consuming, that's okay, it can queue those messages up and they can be consumed and processed whenever they get the time, or in other words, whenever your consumer processes can catch up. Now I will say this ‑ Amazon SQS is heavily featured on the exam for implementing both resilient and decoupled architectures. So believe me when I tell you, you have to know this service. Let's go ahead and review an example. Now, this architecture diagram was used in our previous clip where we talked about tightly versus loosely‑coupled systems, and you might remember I briefly talked about message queues. Well, that's where SQS comes in. You can add this messaging queue layer to get more resiliency and you easily decouple the architecture layers. So these top two subnets here, private_subnet_1 and 2, can be producing messages based on incoming events from our load balancer. They post a message to the queue, and then our resources in our private_subnet_3 and 4 can go ahead and consume and pull those messages as they're able to. Again, a big purpose for SQS is to decouple and add resiliency and buffering between layers. Let's talk about some components regarding the service that you have to know for the exam. When you're using SQS, you send messages to the queue via the SendMessage API. That's an important IAM policy permission you have to know. For the consumers, they pull that queue, and then they receive messages when they're available via the ReceiveMessage API. The important thing to know here is you're never just pushing a message to a consumer, consumers are always pulling and polling for messages on the queue. We'll talk about more specifics on how to configure that stuff a little bit later on. Now when they receive a message off the queue, the consumers also have to perform what is called a DeleteMessage call. That's because, and we'll look at this much more in depth, when you pull a message off, it's really not pulled off of the queue, it's just hidden from every other consumer. So if there's an issue with processing that message at some configured point in time, that message becomes visible again so it can be processed again by a different consumer. Again, we'll dive into that much deeper coming up here in a bit. On the exam, and really in general in the real world, SQS is commonly integrated with Lambda functions and API Gateway. Typically speaking, Lambda functions are the consumers of the queues, and API Gateways are usually the producers of the queue. And lastly here, key indicators on the exam to look out for to immediately consider using SQS. If there's some type of design that you need durability, resiliency, availability, and scalability while minimizing operational overhead, SQS is a perfect choice to consider for that architecture scenario. Moving on, let's talk about access control, authorization, and security. Of course, you can use IAM and what are called access policies. IAM is used for pretty much everything, and because of this, you want to make sure you are properly implementing the correct access controls. Remember the principle of least privilege. In addition to IAM policies, you can also leverage what is called a queue access policy. Within this queue access policy, we're going to look at an example here shortly, but you can define specific to the queue, what accounts, users, and roles can even use it in the first place. In addition to access control, they also support server‑side encryption on the queues for your messages to protect your data at rest. You can use either an SQS‑managed encryption key, or you can use keys in AWS KMS. So, it really just comes down to the specifics on what kind of compliance requirements are required for your encryption needs. And the last thing here, public endpoints. It's very important for you to know on this exam that SQS is a public service. Because of this, by default, your queues that you create receive a publicly accessible URL in order to actually reach them. That's why it's very important that you implement correct IAM policies and access policies for your queues. Common exam scenarios and use cases that can come up involve restricting access using VPC endpoints. So really make sure you review how to use a VPC endpoint within a network architecture, and remember that SQS has its own endpoint type that you can use for it. Breaking things up here with a general best practice ‑ you're going to want different queues for separate functions within a workflow. With that being said, understand that queues themselves are not bi‑directional. What that means is if you need to send a message back, well, you need a different queue, you can't use the same queue for the same producers and consumers. Again, it's not bidirectional. Now let's look at two access policies before we wrap this clip up. Remember, you use access policies in addition to IAM policies to control access to your queues, so who can consume and who can produce. In this example, we have the same account. So you'll see we're allowing the principal, which is pointing at the root user. So in general, the account itself. We are allowing all SQS actions, so SendMessage, ReceiveMessage, DeleteMessage, etc., and we're specifying this particular queue resource, so you see the ARN at the bottom. What this means is we're relying on IAM at this point to go ahead and control access to our queues. This next example is a good scenario on the exam to keep in mind, cross‑account access. Using queue policies, you can allow other AWS accounts to send messages, receive messages, etc., all using the access policy within the service. So in this case, we're allowing an AWS account within the principal to send messages to our queue, so it can't receive, list, etc., it can only send a message to our public queue URL. And with that, I think that's enough for this clip, it's getting a little lengthy here. Let's go ahead and wrap things up, and we're going to move on to talking about queues much more in depth.

Amazon SQS Queues
All right, we just got done talking about SQS as a service, let's start diving into the important resource known as a queue. Within SQS there are two queue types that you have to know. The first and the default is a standard queue. They also offer something known as a FIFO, or First‑In‑First‑Out, queue. Let's go ahead and dive into standard really quickly and then we'll move on to FIFO. Remember that this is the default option, and it supports nearly unlimited number of API calls and messages. What that means is it's very good for scaling all of your workloads for decoupled architectures. When you use an SQS standard queue, you get what is called at‑least‑once message delivery. What that means, and you have to remember this, is that you could get duplicates within your architecture workflow. So because of this, you have to either handle duplicates or leverage a different queue type, which is the FIFO type, and we'll look at that here in a moment. Big thing to remember is that standard queues are at‑least‑once message delivery. In addition to delivering at least once, messages are also not guaranteed to be delivered in order. The service does its best effort to do so, but messages can come in out of order. These are two very important component concepts to understand for a standard queue. With this service, however, one of the best things about it is it does allow for redundancy and high‑availability. Your messages get stored within multiple AZs within the region you deploy in. In other words, when you create a queue, it gets created within a region and it leverages the AZs within that region. And then the last thing here, when you're using this service, and I can promise you this is important, there is a maximum message size of 256 KB. Now we're going to talk about bypassing that in a very specific way later on, but for now, let's move on to FIFO queues. FIFO queues have very similar capabilities to a standard queue, but you get slightly more options for more enhanced messaging requirements. You're only really going to want to use these if order of operations is critical, so message order really matters, and duplicates cannot be tolerated. That's because these are guaranteed to maintain the exact same order for sending and receiving messages, and in addition to the order being maintained, you get exactly‑once processing. It's able to offer exactly‑once processing via message deduplication ID, so it can reference and say, hey, this dedup ID already existed, I don't want to deliver this message again, otherwise I'll get a duplicate. And for the ordering, it uses a message group ID to maintain message order within that specific message group. Make sure that you are familiar with these two terms and these two concepts; there's exact‑same ordering and exactly‑once processing, and they leverage dedup IDs and message group IDs. Now let's actually compare throughput for FIFO queues. They don't support the same number of messages as a standard, which was basically unlimited. When you're using a FIFO queue, if you don't batch your messages together, you can send up to 300 messages per second. So you can see how there is a drastic difference compared to these in standard. If you do batch your messages, which is a specific operation, you can send up to 3000 messages per second. Now, while this is really good, remember, you're only really going to use these if you have very strict requirements for those two use cases we talked about. The first being message ordering being maintained, and then the second being exactly‑once processing being maintained. Other than those two specific use cases, I would tend to lean toward a standard queue. Now let's look at an exam scenario on where you might use a FIFO queue. Let's assume you have an application where data messages are being written in a specific order like an online retail shop, and they have to be maintained throughout the processing. Based on this scenario alone, there are a ton of key indicators in there to say, hey, I should lean toward a FIFO queue based on those indicators, message order, dedup, etc. Now let's wrap things up here real quick with an architecture example on message ordering. Remember, standard queues are best effort. So as messages get put into the queue, they're not necessarily going to be in the exact same order when they're getting pulled off. Again, this is critical to remember, please remember it, it is best effort for message order and deduplication. FIFO, however, while you give up some of the performance aspects of standard queues, is going to maintain exact same ordering, and there will be no duplication. I know I keep bringing these points up, but this is going to be the easiest way to distinguish between what queue you should go for. Other than those two key differences, they both offer high‑availability, redundancy, and decoupling. So, again, just make sure you're really paying attention to the scenario. With that being said, let's wrap up this comparison on the queue types, and we're going to move into attributes for configuring your queues.

SQS Queue Attributes
All righty, when you're deciding on standard or FIFO queues, regardless, you have to configure them using specific attributes that are important for the exam. So let's go ahead and look at those here now. First up is the concept of delivery delay and message timers. You use these to delay the delivery of a new message for a specified number of seconds. By default, that number is 0. So what that means is when a message gets produced, it's immediately available on the queue for a consumer to pull off the queue. If you need to, though, you can configure it to be up to 15 minutes, so you can wait for the message to become visible for 15 minutes. The next important concept here is message retention. This is going to be the amount of time that SQS will actually retain the message in the queue. What that means is it will just sit there if it's not being processed and pulled off. The default for this is 4 days, but you can specify up to 14 days. So, depending on your requirements, you can have a message live there up to 2 full weeks. The next concept, which is very important, is short‑polling. This is the default option where ReceiveMessage calls to SQS don't wait to pull the queue. In other words, it's constantly looking for messages, and because of this, this is actually a common cause for empty messages and increased costs. That's because every time you make an API call, you're getting hit with a charge. So when you're trying to receive a message, even when the queue is empty, well, you're still getting billed. So, on the exam, if you see that type of issue, I would immediately think about changing away from short‑polling. When you change from short‑polling, you go to what is known as long‑polling. Long‑polling is the complete opposite. It's when you set any non‑zero wait time for a ReceiveMessage. This helps reduce the number of API calls being made, as well as the number of empty responses because you're waiting for a few seconds to actually look on the queue. The maximum amount of time that you can configure for long‑polling on a queue is 20 seconds. Really be sure you know the difference between short‑polling and long‑polling and why you would actually enable long‑polling. And speaking of that, exam pro tip: you're going to likely want to favor long‑polling over short‑polling in pretty much every scenario. It's pretty rare that you need short‑polling unless there is some extreme requirement for speed. The next topic here is visibility timeout. This is a configuration option where you can set the length of time that a message can be received from a queue. What that means is once it's consumed and pulled off a queue, the message will remain invisible to other consumers during that time period. If you remember, in an earlier clip I mentioned messages get pulled off, but you have to delete them from the queue once you successfully process them. Otherwise they become available again for other consumers. Well, that's exactly what this is meaning to actually solve and help out with. If the message is not deleted, or in other words, successfully processed by a consumer before this timer expires, then the message reappears on the queue. This is going to be typically good for long‑running tasks and implementing retries in case of failures. So if your component fails, doesn't process the message, but it pulled it off, well then once this visibility timeout goes away, that message is available again and it could get retried. The minimum number for seconds in visibility timeout is 0, which is pretty rare, but you can also set a maximum of 12 hours. So you can have very short or very long windows. The default is going to be 30 seconds. So in other words, you have 30 seconds for your consumer to pull a message, process the workload, and then delete the message before it's visible again. Now, that being said, remember when I talked about this earlier, the max message size for a queue is 256 KB. You might also remember that I said there's a slight way around that. Well, you can use the Amazon SQS Extended Client Library if your message payloads are greater than 256 KB, but less than 2 GB. Let's look at how this would work from an architecture standpoint. On the left here we have our producer. Now this producer is going to use the Extended Client Library to send a message containing essentially a pointer to the object that exceeds the message size quota. So, for example, let's say we have a 50 MB object here and we want to send a message with it. Well, how it works is it puts the object into S3, and then it sends a message to our SQS queue, essentially referencing that bucket object URL within the message itself. So now this queue has our message, and our consumer pulls that message or receives the message off, and it can look up and find the reference to that object and download that object from S3, and then delete the message from the queue. The big takeaway here is you use this if you have any messages that are technically bigger than the quota size. It uploads the object to S3 in a bucket, and it creates a pointer within the message that is referenced by the consumer. Now, with that being said, let's wrap this up here, I think this is a good ending point, and we're going to move on to some more architectures here revolving around SQS, coming up next.

Example Architecture: Scaling ASGs Using Amazon SQS
All right, in this clip, we're going to go through an example architecture that is commonly appearing on the exam itself that you need to be aware of. So, let's go ahead and dive in. In this example architecture, we're going to start talking about scaling autoscaling groups within EC2 using Amazon SQS. For this, let's just assume you have an API set up in API Gateway, and you have a bunch of producers or customers that are sending messages through API Gateway into your SQS queue. Typically, you can say, hey, I know the amount of predicted throughput that's going on during normal operations. So a standard day, we can estimate how many messages are coming in, and we can set our desired number of instances to go ahead and pull those off. So, as the messages are being sent into our SQS queue here, we can start pulling them using the autoscaling group every so often. So these EC2 instances are performing ReceiveMessage, ChangeMessageVisibility to make them invisible, and then delete them once the processing is complete. Of course, don't forget they can do this via IAM role and instance profile permissions. In addition to the autoscaling group instances pulling those messages off and processing them, you also can create an Amazon CloudWatch alarm. Now CloudWatch will be covered in depth in another course within this learning path. But understand, this is the go‑to solution in AWS for logging and metrics and alarms. So in this case, we could create an alarm that monitors the ApproximateNumberOfMessages queue metrics. So this metric is specific to the Amazon SQS service. During normal operation, well, it's all good to go, right? There's no breaching, it's not triggering any alarms, etc. However, let's assume we have a big holiday event, maybe we're running a retail store. So because of this, there's a sudden spike in incoming messages that are being sent to our queue now. What happens with that is all of these messages are going to start piling up because we don't have enough consumers on the back end to pull those messages off and start processing them. Well, with this happening, our alarm could now be triggered, so it could breach, and using that alarm with that breach, you can schedule or trigger an autoscaling action to scale out your autoscaling group. So once we trigger this scaling action, well, we can start creating more instances, which are essentially the consumers within this workload, and they can scale out and start processing more messages based on the amount of compute available. And with that, eventually they're going to catch up. So our queue is going to shrink back down, we're going to be good to go, we won't have that many messages sitting on there anymore because either we're going to catch up or that event is going to pass. And after that, our CloudWatch alarm will no longer be breaching, so it's not triggering any scaling actions, and we can scale our autoscaling group right back down. This allows us to dynamically respond to the influx of messages being sent to our queues. And the nice thing about it is it adds a layer of resiliency, durability, and elasticity, all while still optimizing costs. We're only growing and shrinking when we absolutely need to. Now the big takeaway here for an exam pro tip, remember this CloudWatch metric for the exam, and be sure to remember the overall architecture design and idea that we just looked at. You can scale autoscaling groups based on the number of messages in an SQS queue. And with that, we're going to end this architecture discussion, and we're going to move on to some more resources within SQS.

Dead Letter Queues
When you're implementing Amazon SQS queues within your workloads, every once in a while you're going to have a failed processing of your messages. So how do you handle that within AWS? Well, that's where dead‑letter queues come in. So let's go ahead and break down what these are. Dead‑letter queues, or DLQ for short, are targets for messages that cannot be processed successfully from a previous queue. How it works is you actually set up as a target for an existing queue this place where those failed messages can go. So you have your source queue, which is where your primary consumers pull from, and if a message fails continuously, it gets placed on a dead‑letter queue. The big benefit that these bring is a way to save and store messages for further processing or investigation later on, and we'll look at some use cases specific to these here coming up shortly. Now if you use a DLQ with a FIFO SQS queue, that means the DLQ has to also be a FIFO queue. That's an important thing to remember. Another additional point to keep in mind, a dead‑letter queue within SQS can work with, of course, SQS itself, but it can also work with a service known as Simple Notification Service, which we're going to discuss later in this module. For now, just remember, you can use DLQs with SQS and SNS. And really, even though these are called dead‑letter queues, they're technically just other standard or FIFO queues, there's nothing really inherently special about them, it's just the way that they function and how they are configured or set up to receive failed messages. But in the end, these are just a standard resource within AWS. Let's look at some use cases and concepts where you might use a dead‑letter queue. These are especially useful for debugging applications or messaging systems because it allows you to analyze message contents. That message that fails can be stored in the dead‑letter queue, and then later on when you have time, you can send an ops team in, they can pull that message off and investigate just why it didn't process in the first place. In other words, it basically grants you an ability to isolate those unconsumed messages to troubleshoot. Think of it sort of like a sideline queue. You have a message that fails over and over again, it gets placed in this sideline queue, and then you can come in whenever you're ready and investigate. Another use case is you can use these to see if you've given your consumers enough time to process the messages on the queue or not. Remember, there's a message visibility timeout setting that can be configured by you, and you might not be giving your consumers enough time to actually consume and then process the message. So because of that, it might keep reappearing on the queue, and it really has nothing to do with the application failing, it's just not getting it done in time. And lastly, you need to understand the concept of redrive. This is a capability that allows you to move messages back into a source queue or into a different queue of the same type to reprocess. So, if you get a message that lands in a dead‑letter queue, you go in and you troubleshoot and see, well, there's nothing wrong with this message, maybe this was just a temporary or transient error, you can go ahead and place that message back into the source queue, or you can move it to a different queue and process it again. So that's what the process of redrive is, you redrive that message back into a queue. Real quick exam scenario that you might run into. Let's assume you have a system that's processing messages, and any messages that fail to process have to be retained for future investigation, and they want to reprocess them if possible. This is a perfect use case and scenario for a dead‑letter queue, whether it be an SQS queue or an SNS topic, which again, we're going to look at later on, DLQs are perfect for capturing those failed messages for future investigations. Let's go ahead and wrap this up here, and we're going to move into a demo within Amazon SQS.

Demo: Sending Messages to an Amazon SQS Queue
I think that's enough talking about SQS, let's look at a demonstration where we're going to create some queues and send some messages to the queues. In this simple demonstration, we're going to do a few things, let's have a quick high‑level walkthrough. We, of course, will be the user, and we're going to send messages to some newly created Amazon SQS queues. What we're going to do is poll for those messages, and once we verify that works, we're going to set up a Lambda function to consume the messages off the queue and then perform some simple logic. So with that being said, let's jump over into our sandbox hands‑on playground now. Okay, I've jumped into my hands‑on playground here, I'm in us‑east‑1, before we begin creating our queues, one thing I want to review here is some pre‑existing architecture, specifically my Lambda function. Now I'm going to make this code for these functions we're going to use available for you later on, so you can use that if you want, they'll be in the module assets, but for now, let's break down what this does at a high level. The first version of this function, all it does is it triggers via an SQS standard queue and then prints out some formatted information based on the messages that it receives. Again, this code is available for you, feel free to use it if you want to. In addition to this, I created, of course, a permissions policy here with this execution role that allows CloudWatch Logs permissions, and SQS. So you can see we're receiving message, allow DeleteMessage, and we can GetQueueAttributes for any matching ARN you see here. Now you might notice, right here, this little asterisk, I'll show you why that is here when we start working with FIFO queues. For now, though, what I'm going to do is jump back to code, I'm going to load up SQS here, and let's begin. First thing I want to do is let's create a queue. I'm going to create a new standard queue, we'll do FIFO later on, and I'm going to give my queue a name. I'm going to call it OrderQueue, and this is important, two things ‑ it's case sensitive, as you can see right here where they call out, and in addition to that, it has to match my permissions policy. So this is the exact formatting and casing that I have in my policy. After this, we skip down, we look at configuration. I'm going to tell you right now, you need to understand every single one of these configuration options for this exam. With that being said, let's go through left to right, top to bottom and talk about each of them. Remember, visibility timeout. This is the amount of time that a consumer has to process and delete a message. So with this setting here, we're saying, hey, all consumers, when you receive a message, you have 30 seconds to process it and then go delete it off of my queue, otherwise I'm going to put it back onto the queue for other consumers to use. On the right side, we have message retention. So what this is saying is saying, okay, your messages can live on my queue for currently 4 days if they're never consumed, received, deleted. It is important that you understand you can do this for up to 14 days, that does come up on the exam, so be aware of the minimum and maximum here. After that, we have our delivery delay over here on the left. What this is saying, if you recall, is we're giving right now 0 seconds for delivery delay. However, you could add time, maybe you want to add a 10‑second delivery delay, and what that does is it's saying, okay, when a new message is sent to me on my queue, I'm going to wait 10 seconds before I make it available for any consumer. So it kind of adds in a built‑in buffer for messages coming in. I'm going to leave it at 0, but that's what you would use it for. After that, we have the max message size. Now, I've never really seen use cases for shrinking this, I'm sure there are a lot of different ones, but people usually leave this as default, and you do need to remember it's 256 KB as the max. And then lastly here, very important for designing with SQS, receive message wait time. This is where you change from short to long‑polling. So right now we have 0, which is short‑polling, but if you wanted to, anytime you make this a non‑zero number up to 20 seconds, you've now enabled long‑polling on the queue. Long‑polling is useful if you're using short‑polling and you're receiving a lot of empty queue responses, and you're just incurring a lot of API charges. If that's the case on the exam, look into long‑polling. We're going to leave it as 0 because we don't have any use case for this, but just, again, remember you can change this up to 20 seconds. Perfect. Let's move on, but make sure you do remember those config options. Next we have encryption. So we're going to leave this enabled, and we're going to use the default SQS key, this is managed for us, it doesn't incur any additional costs, so this is definitely something you want to turn on. If you wanted to, though, you could enable a custom KMS key, and KMS is covered in a different course in this learning path, but when you do this, you have to allow people to use this key and you incur more charges. So keep that in mind. So what we're going to do is use the default SQS key, and let's move on to access policy. All right, remember, the access policy goes on an individual queue, and it tells you, in addition to IAM policies, who can do what with the queue. So for this, we're going to leave it as the default, and we're saying, okay, anyone in our account can perform all SQS actions on this order queue. So now we're just leveraging IAM to control that policy permission outside of the service. I'll leave this as default, and let's move down here to the next two major options. First, redrive allow policy. What this is saying is, hey, I want to use this queue as a dead‑letter queue for a different source queue. Now this is the actual source queue, so I'm going to disable this, but we're going to look at this here in a moment when I create a dead‑letter queue. And speaking of dead‑letter queues, We can specify here what dead‑letter queue we want to enable. Now, we don't have one, but I'm going to create one later and we'll edit this right after we test this. So with those options filled out, I'm going to create my new standard queue, and there we go, we now have our order queue, and all of our settings are visible. Now, let's test out that dead‑letter queue. I'm going to go back to queues, I'm going to create a new queue, we'll choose standard, and I'm going to call this OrderQueue, but DLQ at the end. Big thing to remember here, dead‑letter queues are just other standard queues, they're just used to sideline messages. So, remember that for the exam and when you're designing this on your own. So I give it a name, I'll skip through the default options here, but I do want to change the redrive policy. I'm going to select Enabled, we can allow all queues to use this, we can deny all queues, which doesn't really make sense, or we can specify a specific queue. Now I'm going to do this because I want to lock it down to our order queue that we created earlier. So now we're saying, hey, OrderQueue can use this DLQ for its dead‑letter queue policy and dead‑letter queue configuration. So I'll scroll down here, I'll click on Create queue, and there we go. Now, for our old queue here, OrderQueue, I'll edit this, scroll down to dead‑letter queue, I'm going to enable this, and we're going to choose our new dead‑letter queue we just created. After that, I'm going to set maximum receives to 2. What this number here is saying is that, hey, if I receive a message on this queue more than two times, I want you to sideline it and put it into my dead‑letter queue that I configured above. How this would work is if a message gets polled off of a queue or made invisible on a queue, and maybe it fails a process or it doesn't process in time, well then it gets put back onto that same queue, remember. The nice thing is SQS keeps count on how many times it's received that same message. So as soon as we receive this three or more times, it's going to get pushed to this dead‑letter queue. So what I'll do is I'll leave that, I'll click on Save, and there we go, we now have our dead‑letter queue set up, let's begin testing. I'm going to go up here to Send and receive messages, I'm going to put in a test message body, we'll leave everything else the same, and I'm going to send this message. Now you'll notice, immediately, messages available has been updated to one, and now what I can do is poll this queue in the console. So we see here we have the message ID, we see the attributes, body, details, etc., and it's receive count of one. Typically, what would happen is you would take this message, parse the body, perform the work, and then you would delete it, so I would click on Delete. However, what I'm going to do is stop polling, and then once it's done, I'm going to re‑poll. You'll notice here on the bottom right, the receive count has gone up to two. That's because, well, we read this message before when we polled the first time, but we didn't do anything with it, we didn't delete it, etc., so it's back on the same queue, and it increased that receive count. So now if I stop and then start again, you're going to see, well, it disappeared, it's not there anymore. That's because it got sent to our dead‑letter queue. So if I go to Queues, let me go ahead and refresh, select my dead‑letter queue, we'll go ahead and send and receive, and we see we have one message available. I'll poll, there it is, it's the exact same message we just looked at; we see our body, attributes, details with the receive count, and there we go. So that's how a dead‑letter queue functions. The primary queue and its consumers weren't able to finish and delete that message, so eventually it was received too many times and it got parked over here for us to look at later. So what I can do is I can say, oh yeah, I know what happened, let me delete it, and we'll go ahead and go back to our regular workflow, etc. Awesome. So that's how a dead‑letter queue works, let's go back in and let's set up a Lambda trigger for our source queue. So in here what I'm going to do is select Lambda triggers, I'm going to configure a new one, and I'm going to select our MessagePollingFunction and click on Save. Now you see here on the bottom, it's creating that trigger, and eventually it will go to enabled, and if I go to my function, and I go up here and reload, we should start to see the SQS trigger in the console, here in a moment. Perfect. We see our trigger here, I'll select it, we see all the details. This is good to go. So, let's go ahead and test. What I'm going to do is go back to my Queue details, let's refresh, we see it's enabled, let's send and receive some messages. I'm going to go to my IDE really quickly, copy this JSON in, paste it, and it's just a very simple example JSON. I'm going to leave everything else attribute‑wise the same, and I'm going to send my message. Perfect. So now if I view details, I'm going to keep this up for later, I'm going to go to my function, monitor, I'm going to load my CloudWatch Logs here, and once in here, we see a log stream, and we see in plain text, the output. We see our init, our start, we see the function printing everything that we configured, we see the message ID right here, it ends in da4ea, if I go back here, da4ea, so this is the same message. Now if I look at the logs, we get some other attributes, right? The ReceiveCount, timestamps, etc., and then we print the body of the message. Awesome, so this is working as expected. This is great. We've polled this message off the queue, processed it, and deleted it, so it's no longer there. Okay, well, let's try a FIFO queue. Remember, I wanted to create a FIFO queue and test that as well. We'll use the same function, I'll just change the code and change some settings. So the first thing I'm going to do is I'm going to start tailing this. Now this is in place, I'm going to go to my function, and I'm going to copy in a new set of code here. So let me go to my IDE, scroll back, and again, this Lambda function code will be available for you to use in the module assets. I'll deploy my updated function here, and it's just printing a few more attributes and a little bit more information because FIFO offers enhanced configuration options. So we have this in place, I'm going to go to Send and receive here, go to Done, go back to Queues, and let's create a FIFO queue. So under Create queue, I'll select FIFO. Now, you're going to notice here, it has to end in .fifo. So I'm going to name this OrderQueue.fifo. Now that we have the name in place, it might make sense why I have the permission policy set up the way it is. Remember, I added an asterisk to the end of this, and that helps match the .fifo here. So that's why I added that asterisk at the end of that ARN in the IAM policy for the Lambda function. And with that being said, we have our FIFO queue here, I'll accept the defaults, not a big deal, let's just get down to queue settings. First thing, I want to enable content‑based deduplication. So this allows you to dedup based on the content of the message and not just the message ID itself. So this is something you can turn on or off, it's up to you, we'll just leave it on. You could also turn on high throughput. Now, we're not going to do that, but this does allow you to increase the number of messages sent per second, but again, we'll leave it disabled. The next thing I want to show here that's important for the exam is deduplication scope. So you can dedup based on the specific message group, or you can dedup based on the entire queue for all messages. I'm going to select Queue. After I have this in place, I'll scroll down here, we'll leave everything else the defaults, and I'll create my new FIFO queue. Okay, that's perfect. So now what I'm going to do here is create a new Lambda trigger. You can ignore this, this is a bug, I don't know why it shows up, but what I'm going to do is configure a new one, select my MessagePollingFunction, click on Save, and there we go. Now let's test it out. I'm going to send and receive messages, we'll copy and paste that same JSON in, we see it's the same exact JSON as before, we can give our message group an ID for unique identification, and we'll leave message dedup ID optionally blank. This will fill in for us. Now, I'll send the message, we see it's been sent, let me view the details, I'll go to my CloudWatch tail, and wow, it's already there, great. So we see, it was Lambda‑triggered by our FIFO queue. Message ID ends in 35a9c, 35a9c, so this is the exact same message, and you'll notice there's a sequence number as well. This is used to know the exact order that messages were sent in. That's a little out of scope, the big thing to take in mind or remember is that you do use this to maintain order of operations. Now if I go back to my tail here in my log, we see it prints out some more info. The dedup ID, the message group ID, the other attributes, and then it also has the body like before. Perfect, so this is working. We've now tested both a standard queue and a FIFO queue, and we tested a dead‑letter queue for any failed messages. Let's go ahead, we'll wrap up this demo, and move on when you're ready.

Amazon SNS Overview
All right, we wrapped up Amazon Simple Queue Service, let's go ahead and look at now the other decoupling service that's very popular on the exam, Amazon SNS. First, before we dive into the service, we need to review push‑based messaging. Remember, this is where messages are sent by a producer to a server, and then from the server, they are immediately sent or pushed to a consumer. With that review out of the way, let's look at the push‑based messaging service that is native to AWS, Simple Notification Service. Again, this is the go‑to push‑based messaging service when you're building cloud‑native applications. It works by proactively delivering messages to different endpoints that are subscribed to it. So you subscribe endpoints as consumers. This service is going to be very useful for setting up alerting systems or alerting people, and even triggering event‑driven workloads in a near real‑time mechanism. The benefit of Simple Notification Service is you can implement either a one‑to‑one or one‑to‑many system design. What this means is you can send one message to several consumers, and they can all get the same version. Remember, with SQS, that's not the case. If you pull a message off, well, that message is gone and no other consumer can get it. Now to accomplish this, producers send their messages to what is known as an SNS topic, and we're going to dive into topics much more in depth here in a little bit. Think of it for now as the topic being the server within this push‑based messaging system. Producer sends messages to topic, which is the server, and the server pushes the message to a consumer. Now, really quickly, first thing to note, if you need to send a notification or alert as soon as possible in AWS, you should consider Amazon's Simple Notification Service. This is the sole purpose of this service, immediate messaging using one‑to‑many or one‑to‑one relationships. Switching subjects to some important concepts for the exam, your consumers subscribe to your topic to receive messages. By default, all incoming messages are going to be sent and pushed to all confirmed subscriptions. So if you have 100 consumers subscribed and you have one message sent to the topic, well then all 100 of those subscriptions receive the same message. The messages are encrypted in transit by default, of course, using TLS, but you can also add at‑rest encryption on the topic themselves via KMS, if required. It's going to come down to the compliance requirements that you need to fulfill. In addition to encryption, you're going to need to know the subscriber protocol. So how can the consumers subscribe and receive messages? Well, there are several built‑in subscription protocols. You can subscribe Kinesis Data Firehose, you can subscribe via email addresses, you can set up SQS queues to receive notifications, you can trigger Lambda functions, you can send them to custom HTTP or HTTPS endpoints, you can send SMS text messages, and you can set up what is known as a platform application endpoint. In addition to the protocols, these are pretty similar to SQS in terms of a message quota. Messages can be up to 256 KB of text in any of the supported formats. So that could be JSON, plain text, any type of raw data, etc. It can only be up to 256 KB to be sent if you want to send it directly through SNS. I talk about this message size, but you can use the SNS Extended Library to send messages up to 2 GB in size. Now, if you remember from SQS, this functions the same as that Extended Library. It creates a message to send, it has a pointer to an object in S3, and then the consumer references that pointer and pulls the object out of S3. Essentially, this is a way to kind of bypass that message limit. Of course, you're not sending that oversized item and data through the topic, but you're pointing at it using the topic. Now moving on, let's talk about securing the service. Of course, just like every other service in AWS, you can leverage IAM to control access to your topic resources. In other words, who can send, who could subscribe, etc. You can also use an access policy. So this is just like a queue access policy, but it's specific to topics in SNS. These get assigned to individual topics, and they allow you to essentially further customize your access via a JSON‑formatted document. So again, it's just like a queue policy, but it's specific to this service. You'll use this in conjunction with IAM. And finally here, there are some exam scenarios to keep in mind. You can use these access policies to allow cross‑account message publishing, you can specify specific principals who can even subscribe to the topic, or you can actually allow other AWS services to send messages and receive messages. Some common examples are EventBridge and S3 as producers of messages. Now building on the topic of topic access policies, here is an easy access policy example. In this, there are two primary portions. At the top here, we're allowing any AWS principal to perform the list of actions you see here: Publish, DeleteTopic, Subscribe, etc., to our resource, as long as they belong to this AWS account. So based on the ARN here, this is the same account that owns the topic. So we're saying, hey, any IAM principal within my account can perform this action based on this topic access policy. On the bottom of the policy, we have another very critical component. We're allowing anybody, thus the whole point of the principal with the star and the asterisk, to subscribe to our resource, which is our topic ARN. Now, this is a real policy, so really do your best to understand it. Very simplified, we're allowing anyone within the same account to perform the top actions, and in the bottom we're saying anybody in the world can subscribe if you know the topic ARN and the topic URL. Now, that's going to do it for this introduction clip on Amazon SNS. Let's go ahead and wrap it up, and we're going to move on to some more important components within the service.

Amazon SNS Topics
All righty, we just got done having an overview of Amazon's Simple Notification Service, let's dive into what is known as a topic. This is a critical resource type in this service. First and foremost, similar to queue types within SQS, there are two topic types within SNS. You have a standard topic and you have a First‑In‑First‑Out, or FIFO, topic. When you create a new topic, standard topics are going to be the default option. So when you're creating it, if you just run through it and accept the defaults, well, then it's going to be a standard topic. With that being said, standard topics are typically going to be good enough for a large majority of infrastructures. You can, however, if required, leverage a FIFO topic for special requirements. Now let's actually compare standard and FIFO topics so you can see when you might use or not use one of the options. First up, let's look at the number of topics per account. With the standard, you can create 100,000 per account, with FIFO, you can only create 1000. In addition to that, the number of subscribers is drastically different. You can have up to 12.5 million subscriptions per topic, so that is 12.5 million potential consumers per one individual resource. But with a FIFO, you're only going to have 100, so these are, again, far more specialized for smaller use cases. Now one big benefit of a FIFO is that it does offer message archiving capabilities and the ability to replay messages that might have failed to been delivered. Standard does not offer that. But if we look at performance, standard topics, similar to standard queues, are going to have the highest throughput option. It's almost nearly unlimited messages that you can send through there per second. With FIFO, again, just like SQS FIFO queues, well, a FIFO topic also is limited in the throughput. You can send 300 messages per second per topic. Now this next point here is probably, in my opinion, one of the most important things to keep in mind for the exam. With a standard topic, you have at‑least‑once delivery with best‑effort ordering. So this is, again, very similar to an SQS standard queue. At‑least‑once means you could have duplicates, and best‑effort means, well, it could fail, it's not guaranteed. With a FIFO topic, however, you have strict message ordering, thus the name, First‑In‑First‑Out, and you get exactly‑once delivery, so you get rid of duplication using built‑in deduplication methods. Again, this is probably the biggest point to remember for this exam when picking what kind of topic type you want. And then to finish things off here, the max message size is the same for both. Now, we looked at in a previous clip how you can sort of bypass this using the Extended Library, but the message itself that goes through the topic can only be this big. Now building off of that, a quick exam pro tip: FIFO topics actually can only send messages to FIFO queues. I know it seems weird, but we're going to discuss something called a fanning‑out architecture later on, and just keep that in mind. So, FIFO topics only send to FIFO SQS queues. Now let's look at four more important topic concepts for the exam. SNS topics do support cross‑region and cross‑account delivery. So, subscribers can be in different accounts, different regions, etc. In addition to that, we did talk about dead‑letter queues previously, well, you can use dead‑letter queues in SQS for failed delivery, even with an SNS topic. I know I covered that in depth a little bit earlier on, but I'd like to call it out again, you can use a dead‑letter queue with your SNS topics. Please remember that for the exam. Thirdly here, you can actually implement what is called a filter policy, which allows you to define which messages a subscriber gets. In other words, maybe you have a filter in place where you only want a subset of subscriptions to receive a message, well you can do that using a filter policy and metadata on the messages. And then the last thing here, topics integrate with a majority of AWS services fairly easily. Again, this is the go‑to solution for push‑based messaging within AWS cloud‑native solutions, and that's why it's so easy to integrate with a lot of services. Now, that's going to do it, let's go ahead and wrap this up, and we're going to move on to that next solution and scenario we were just talking about where we're going to discuss fanning out using SNS and SQS queues.

Fanning Out with SNS and SQS
Now that we've covered both Amazon SQS and Amazon SNS at a deeper level, let's go ahead and begin looking at a very critical design called fanning out. SNS Fanout is when you have multiple subscribers on the same topic so that messages get sent to multiple different endpoints at the same time. In other words, you're fanning out the message. By using SNS Fanout, this allows for a fully decoupled, parallel, and asynchronous processing approach for messages. By using this, it allows you to process the same message differently based on the endpoints that are subscribed. A perfect example could be for an online ordering system. Now let's actually look at some important architectural patterns before we move on to the next clip. First up in this example one, let's just assume we have an inventory management application, and in the middle here, we have three different queues that belong to three different applications. We have Store A, we have Store B, and then we have an AnalyticsQueue at the bottom that we use for analytics. What we're going to assume is that anytime a price update is made within our custom application, it's going to go ahead and publish a message to a single SNS topic so that we can track immediately when that price change occurs. Now using fanning out, we can allow these three different SQS queues to subscribe to this single topic that we've created. By doing this and allowing these subscriptions, the back‑end applications can perform their own custom logic and their own custom processing, all based on the same message that was received by the original application, so we've fanned out that original message. Once that original message is sent to all of the endpoints, in this case, all three different SQS queues, well then the different applications for each queue, where they're consuming from, can go ahead and perform whatever they need to do. So in this case, maybe we have Store A is a basic wholesale store which offers wholesale pricing. Store B might be a sponsorship shop where they offer sponsored athletes or people specific pricing discounts based on the pricing. And then, Analytics could be used for an internal or external analytics platform to maybe, let's say, track prices over time. The big thing to take away here is that fanning out allowed all three of these completely independent applications to do whatever they need to do using the exact same message. We sent one message to multiple queues. Now let's actually move on to example number two. In this example, we have an ObjectCreated event on our Amazon S3 bucket. It is important for you to remember, Amazon S3 events can be sent to SNS topics to trigger event‑driven workloads. Again, just like before, the same single event and single message can be sent to multiple queues at one time. Or, in addition to queues, you can use other supported subscription types or destination types, like a Lambda function, for example. And, just like the previous example, we can now leverage other services on the back end or other custom applications to pull messages off that queue or receive messages from SNS directly and perform whatever they need to do with that single message. By leveraging S3, SNS, SQS, and Lambda within an architecture, well, you're achieving an entirely serverless and highly‑available infrastructure while also optimizing cost efficiency because you're only getting billed for what you're using. Real quick exam tip before we wrap up. If you take away one thing from this clip, just remember, fanning out provides a decoupling and scalability approach because you're allowing multiple consumers to process the same messages completely independently. And with that, we're going to end here. Be sure you review those architectures we looked at, and we're going to move on to a demonstration coming up next.

Demo: Sending Notifications through Amazon SNS
Okay, let's jump into a demonstration. We've seen enough and we've learned enough about SNS, I think it's time we jump into the console and start playing around with it. In this demonstration, we're going to work on creating a topic, creating some subscriptions, and we're going to test sending a message. Before we jump into the console, let's have a very quick high‑level architecture overview. What we're going to do here is we're going to have an S3 bucket already created for us, and we're going to set up versioning on that bucket. Now once we do that, we're going to go ahead and delete an Amazon S3 object from that bucket so we can test our versioning process. Of course, when you delete an object within S3, well, you get an S3 object‑deleted event. So we're going to create an S3 event notification, which is then going to send a notification message to an SNS topic, which is going to have several subscriptions in place, AKA it's going to be fanning out. What we're going to do with this single fan‑out approach is we're going to just simply have two subscriptions. We're going to have an ops team email subscribe to receive an email on what's going on, and we will have a Lambda function that's going to work to restore the object immediately after we accidentally or purposely delete the object. And with that out of the way, let's jump into the console now where we can go ahead and test this out. Okay, I've loaded up my hands‑on playground here, I'm logged in as cloud_user, let's go ahead and begin. Before we jump into creating other resources, I just want to review what I've already deployed, and just keep in mind I will include as much as possible within a CloudFormation template so you can start off with the same resources if you want to do this on your own. And with that being said, first thing I created here was a bucket called testing and then a bunch of random characters. Within this bucket, I've created or uploaded a top_secret.png file, which is what we're going to use to restore or test restoration once we delete it. Now in here, I do have go to Properties, and I'm going to turn on bucket versioning. So under Bucket Versioning, I'm going to enable, and there we go. In addition to our bucket with our versioning enabled that we're going to use, I've also created a Lambda function. Again, this code will be available for you if you want to use it, and all I'm going to do at a very high level is review the code and then we'll look at the permissions. So the code itself is pretty simple. It gets triggered by an SNS message, it parses that message, it's going to look for the relevant data in there specific to the object key that was deleted, and then it restores it if possible. So again, this is very simple and you can use this, it'll be in the module assets, so feel free. From a configuration standpoint, the only thing I've done is I've set the timeout to a higher than default value, and we have our role document here. So this allows us to push to CloudWatch Logs, and if I refresh, we should have S3 permissions, as you see here, so we can list bucket versions and delete object version. Remember, you delete the delete marker, which is essentially an object version, so we need to give permissions to delete that deletion marker. Now with that being said, the last thing I have here is a temp mail address. So we're going to use this to subscribe to our topic, and with that out of the way, let's get started. What I'm going to do here is jump to Amazon SNS. I'm going to select Topics, and we're going to create a new topic. So Create topic, and we get our topic types. Now let me go ahead and zoom in here to make this a little bit easier to read, and you're going to notice we have either FIFO (First‑In‑First‑Out) or we have standard. For this, we're going to choose standard, because remember, with FIFO, you can only subscribe FIFO SQS queues. You can notice here where it says Subscription protocols SQS. For this, we're going to use standard. So I select Standard, I give my topic a name, and if we want, you could give it a display name. Now, this is only for SMS subscriptions, so I'm going to leave this blank for now, and let's move down to the other options. First up, encryption. Remember, you can enable server‑side encryption for your messages that live on your topic before they get pushed to your subscribers. So for this, we can use the default key, or you can specify a custom one. I'll just leave the default for now because that's going to be the simplest in terms of enabling this. We then have our access policy. So what I'm going to do is leave the default that's in place for now, you can see that here, and this is actually going to break when we try and set up our S3 event, and I'll show you why once we get there. For now, though, I'm going to leave the default access policy, and we're going to skip over some of the other stuff, as it's not necessarily specific to this exam, so it's out of scope. So what I'm going to do here is I'm going to skip down, and the last thing I want to cover, which is in scope, is active tracing. So you can use active tracing via X‑Ray to trace your different components as the workflows are being completed. I'm going to disable this, but this is important to know. Remember, you use X‑Ray to set up tracing throughout your serverless applications to see what's taking how much amount of time or where things might be breaking. Now with that being said, I'm going to go to Create topic, we're going to create it, and there we go. So, now we have our topic, but we don't have any subscriptions, so let's do that. First thing I want to do is I want to create a subscription for email. So for protocol, you're going to notice all of the supported protocols here. For this one, we're going to choose email, and I'll copy and paste my email address here as the endpoint. So I copy it, I paste it in, and we're going to have to confirm this, and I'll show you what that means. For now, though, I paste it in, and then let's look at these last two things here. First up, subscription filter policy. We briefly talked about this in our topics lesson, but you can put a filter policy in place to say, hey, I want this subscription to receive or not to receive messages that contain these attributes or message bodies that contain certain keys. You should be aware of this for the exam, it's starting to make more of an appearance, so just understand you can put subscription filter policies in place on your topics to say which subscriptions you want to receive the same message. So instead of the default where everyone receives every message, you can use these to go ahead and filter that. We then have redrive policy. Remember, redrive policies allow you to send your undelivered or failed messages to a dead‑letter queue. This is important to know as well. You can use SQS dead‑letter queues to go ahead and park your failed messages if you need it. I'm going to turn it off, we don't need it, but just understand you can do so. Now I go down, I create my subscription, and if I go to my topics here, and I select this, we're going to see this is going to be pending confirmation, as you can see here. So now, if I go to my temporary mailbox here, we see our message, we have to click a link to confirm it, and there we go. So now I'll go back to my temporary mailbox, I'm going to go to my SNS topics here, I'll refresh this page, and we have a confirmed status. So now we can start sending messages to this email. So now if I publish a message, we'll just give it a subject of testing, we'll leave the defaults, I'll go ahead and give it a message body. We can set some message attributes for filtering if we had filter policies, and I'm going to click on Publish message. So now in my temporary mailbox here, we're going to get, at some point here in the next minute or so, this message, which was sent through our SNS topic. And there it is, testing, I opened it, we should get our message body, there we go. So this is working. Next thing we want to do, though, is I want to set up my Lambda function as a subscription. So I'm going to create a new subscription, I'm going to select my protocol of Lambda, and then I'm going to select my RestoreDeleteMarkerLambda function here. We have the same options as before, I'm going to create my subscription, and when we go to topics and view this, we're going to see this is already confirmed automatically. So this is important to know just in general. By default, when you create this in the console, this subscription gets confirmed automatically for you, and what it does here is it goes into our Lambda function and it's going to create a resource policy allowing this trigger to take place. So you see our trigger here now, SNS, and if I go down to Configuration, I look at Permissions, we're going to see a resource‑based policy here, which is similar to an S3 event. It's saying, hey, Amazon SNS is allowed to go ahead and invoke our function as long as it's coming from this particular source ARN. So all of this is done in the background for us, but if you do this via Infrastructure as Code, you have to create this yourself. Please keep that in mind. Perfect, so now we have our topic in place, we have our two subscriptions for fanning out, let's go ahead and perform the last bit of setup here, our S3 event. So, under Properties, I'm going to go down here, I'm going to find Event notifications section, I'm going to create a new one, and I'm going to give it an event name. We're going to leave the defaults here, we don't need to match suffix or prefix, I'm going to go down to event type and say All object removal events. So, object deletions and delete markers being created. I'll scroll down, we select our destination. So, this is very important. SNS and SQS are both supported for S3 events. Please remember that for the exam. For this, I'm going to choose SNS so we can fan out. From here, I'm going to choose my SNS topic. Now, when I click Save changes, watch what happens. It fails. Well, that's because under our topic, our access policy does not allow the S3 service to go ahead and use or publish messages. So what I need to do is edit this access policy. So what I'll do here is I'll click on Edit, I'm going to go to Access policy, and I'm going to add in a simple piece of JSON here. Let me go ahead and copy and paste really quickly. I've copied from offscreen, I'm going to go ahead and paste in this message here, and let me go ahead and fill out some of the stuff that needs to be filled out. We're allowing S3 to publish to our SNS topic, and I need to specify some ARNs here. So what I'm going to do is find my resource ARN, which is here, I'll replace this, and then we are setting conditions. So, hey, my S3 bucket ARN must match within this account. So for the account ID I can copy and paste that, and for bucket name, I'll go up here, I'll copy my bucket name from the URL up here in the browser, and I'll paste this in. So now this is allowing the S3 service to publish messages as long as our conditions are met. Now what I'll do is I'll click on Save changes, I'm going to go back to S3, I'm going to try again, and there we go. So remember that for the exam, you have to use either IAM policies or access policies to grant permissions to leverage your SNS topic. So now we have our event notification in place, I can see that here, we have versioning turned on based on the top here, let's go ahead and test this out. What I'm going to do is go to Objects, I'm going to select my top_secret.png, and I'm going to delete this. I'll go ahead and type in delete, Confirm, there we go. I'll go back, and now we should start getting some messages. So what I'm going to do is go to Lambda, I'm going to go to Monitor, we're going to click on View CloudWatch logs, I see a log stream down here. So now if I select this log stream, we see our message that we've printed triggered by SNS for this bucket here, this object. It found the delete marker, it printed out the version ID, and then it deletes that marker for that object. So now if I go back to my bucket here, I refresh, there we go, it's restored our top_secret.png file. Now I should also, under temp mail here, well, I have an S3 notification. Also, check this out, there's two of them. So this is a perfect example. I'm glad this worked out this way. Remember, you could get duplication with non‑FIFO resources. So I received two S3 notifications through my email subscription. Now I will say this is extremely rare, so I am glad I caught this, but just keep that in mind, you don't get exactly‑once processing, it's best effort, and you could have duplications. In here, it just prints out the events that was received by our Lambda as well, just JSON, I'm not going to read through it. Really, the big takeaway is we've used a single subscription here to fan out to multiple resources. So by deleting this object in S3, we used an S3 event, it sent a message to our SNS topic, this SNS topic fanned out the message to two different endpoints, and we performed a workload and we received an email. All right, with that being said, what we can do here is we can wrap up. One thing I want to call out, if you are following along and doing this yourself, sometimes there are errors when you enable encryption on the topic, so just go through and disable encryption, and you can test again, and then it might fix the issue that's going on if you're failing to get the S3 events to come through. However, that's going to do it. Let's go ahead and wrap up, we can end this demo here. I hope you learned a lot on fanning out using our SNS topics here with our multiple subscriptions. When you're ready, I'll see you in the next clip.

Module Summary and Exam Tips
Okay, let's go ahead and wrap up this module with a quick summary and some exam tips specific to SNS and SQS so that you can take this knowledge into the exam. First up, four questions to ask yourself. Is this synchronous or asynchronous? This is very important when you're designing with SNS and SQS. Remember, SQS is a queue system that allows for asynchronous processing. Secondly, what type of decoupling makes sense? Do you need push‑based or do you need pull‑based messaging systems for your architecture? Thirdly, does the order of messages matter? In other words, do you need FIFO, or can you use standard? And in addition to order, does duplication matter? In that case, you also need to decide between FIFO or standard. And then lastly here, what type of application load will you see? Remember, FIFO gives you more complex capabilities, but you lose a lot of the throughput options for both SNS and SQS. So you really have to know the differences and pick up on the key indicators that are important. Now, speaking of differences and indicators, you need to know the differences between push and pull‑based messaging system. Pushing is where you have messages sent by a producer to a topic, and those get immediately sent or pushed to a consumer. Pull or poll‑based messaging systems are where messages get sent by a producer to your queue, they get queued up on that queue, and then they get pulled off by consumers when those consumers are ready. Now, let's dive into the services themselves. First up, Amazon SQS. This is the message queue service that is great for building out new applications in AWS. If you need a messaging system and you don't have one yet, this is a perfect option. You need to understand these standard values for SQS settings. Remember things like the retention period, which can go up to 14 days. Remember the message visibility timeout settings, etc. If you forget them, go back and review them, I can promise it's very important. Also, in addition to the settings, know when you should use a FIFO queue. The key things to look out for are message ordering and deduplication. If those come up with SQS, use a FIFO queue. In addition to FIFOs and knowing when to use those, understand queues are not bidirectional. A consumer can't also be a producer. That will be an endless loop, that's not how it works, so if those are ever in scenarios, avoid those as answers. Remember, these are a perfect solution for decoupling and buffering your workloads. It allows you to asynchronously process messages whenever consumers are able to. And then lastly here, remember long‑polling versus short‑polling. Short‑polling is 0 seconds, and anything above 0 is now considered long‑polling. The big thing to keep in mind here is short‑polling could cause empty charges or empty pulls from your actual queue because there's no messages, and you could incur charges that are avoidable. Long‑polling in that case would be a perfect solution. Next up was Simple Notification Service. This is perfect for scenarios that involve alerting in anything where a CloudWatch alarm has to message some type of team or email system. You should immediately consider SNS for any of those key indicators. Also, any questions about emails, texts, or any other type of push‑based notification, I would immediately consider SNS as part of the answer. Remember, in our demonstration, we showed how quickly you receive an email by using an SNS topic subscription. In addition to those scenarios, you need to be familiar with the different subscriber options, Kinesis Data Firehose, there's Lambda functions, SMS, email, etc., and a big one to remember is SQS. Please remember that for fanning out. With SNS, you can leverage FIFO topics if you need strict ordering and deduplication. Again, just like SQS, if these come up, you want to think of FIFO topics instead of standard. Those are two humongous indicators on the exam that should immediately trigger FIFO in your mind. And lastly here, exam scenarios to watch out for, anything with fanout architectures. We presented a few earlier, we demoed one in a demonstration. Really be sure you understand how they work and why you would use them. By fanning out in architecture, you can use the same message for many different consumers to process different parts of the workloads. Now exam pro tip here. Leverage SQS dead‑letter queues for saving any failed messages from both SQS and SNS. That's a big thing to remember, it works for both services. Now moving on to the last tidbit here, controlling access. Now obviously you use IAM permission policies for controlling access primarily, but you also have two policy types, and you need to remember those on the exam. You use a queue access policy to control access to the queues like publishing, sending messages, receiving messages, etc., and there's SNS topic access policies for individual topics, for sending messages, subscribing, etc., on the SNS site. You use these in conjunction with IAM to secure your resources. That's going to do it. Let's go ahead and wrap up this module, thanks for hanging in there. I'll see you in the next one here, coming up shortly.

Amazon Kinesis
Amazon Kinesis Overview
All righty, you now are well versed in decoupling architectures using SNS and SQS. Well, there's another service that's very important for this exam, which also offers decoupling mechanisms known as Amazon Kinesis. First thing we're going to do here is we're going to review the Kinesis tool suite as a whole, and then we'll dive into each individual offering a little bit more in depth. So to kick things off, let's review what Amazon Kinesis is at a high level. Kinesis is going to allow you to ingest, process, and then you can analyze real‑time and near real‑time streaming data and video. It's best to think of it as essentially a humongous data highway that's connected to your AWS account, and you get to control who can travel that highway. An important thing to consider here or understand is that this is considered a pull‑based or polling‑based systems design. You'll explore this more in depth here coming up shortly, but understand it is a pull‑based messaging system essentially. On the exam, there are going to be four important versions or offerings that fall under Amazon Kinesis that you're going to have to know. First is a Kinesis data stream, we have Amazon Data Firehose, there's Amazon Kinesis Data Analytics for SQL Applications, and finally, Amazon Kinesis Video Streams. We will break down each of these in their own portion within this module, but for now at a high level, be familiar with these terms. Moving on to some critical requirements to watch out for on the exam. If you see anything saying you need real‑time or near real‑time data streaming, Kinesis is a great option. Now, I will also say, while Kinesis is a good option for these two scenarios, the big thing here is that these are majorly different. Based on if you need real‑time or near real‑time is going to decide which Kinesis offering you choose. With that being said, let's start diving into the different offerings so we can see where we would use what.

Amazon Kinesis Data Streams
First up in our Kinesis journey, let's look at Amazon Kinesis Data Streams. Kinesis Data Streams are useful for real‑time streaming of incoming data. I highlight real time here because that is a key indicator for you to pick out that you need to use a data stream. When you use Kinesis Data Streams, what happens is you have applications, also known as a consumer, that reads the data records off of your streams. The service itself is perfect for streaming applications that require large amounts of very fast data ingestion. So again, live streaming, video, etc. One of the big benefits of using a Kinesis Data Stream service is that it's going to typically take less than 1 second for data to be ingested and pulled off of a stream. So again, this goes back to very fast data ingestion and consumption. Now, with that overview out of the way, let's go ahead and explore some important concepts that you need to know about data streams for this exam. First up, producers. These, just like SQS and SNS, are applications or sources that are pushing data to a Kinesis Data Stream. The primary difference with a producer here is that it's constantly pushing data, so it's constantly streaming data to a data stream. Your consumers are going to be your applications that are processing that data off of the stream in real time. Again, real time is a key indicator for a Kinesis Data Stream. Now we're going to explore consumers and producers more in depth later in this module, as there are some important components and concepts that you have to understand for both of these, but for now, understand producers push data, consumers pull data. Fourthly here, data streams contain what are called shards. Shards are what hold your data. They contain sequenced data records. Data records are going to be the unit of data stored in the streams. These can be up to 1 MB in size per record. Now moving on to some more important Data Stream concepts. Data Streams are very important because they offer something known as a retention period. Retention periods on Kinesis Data Streams allow you to configure the amount of time that you want your records to be available for after being added to the stream. By default, this is 24 hours. However, you can set a maximum retention period of 365 days. So you have a lot of flexibility here for retaining data. Again, a shard, more in depth, though, streams are made up of one or more shards that have a set amount of capacity. What this means is you add shards or remove shards based on your capacity needs. A shard can handle either 5 read transactions per second, or up to 2 MB of data reads per second. It's either or. From the writing side, it can write 1000 records per second, or up to 1 MB of writes per second. Again, that's an either/or. So if the first hits, then the second is bypassed. Kinesis Data Streams also offer ordering, so this is very important as well. They offer partition keys, which are required, and you put these on your data records and then your data records get added to your data stream. These partition keys get used by the service to group similar data records across different stream shards. These sequence numbers that are added are unique keys that are used to maintain data record order. So, again, if data record order is important for a real‑time streaming service, this is a perfect use case for Kinesis Data Streams. And then fourthly here, replay. Messages that get added to your streams do not go away or get deleted until they expire. That means even if they get consumed, they live on your stream. This is a very important concept to remember. While you can consume them and ingest them, they don't go away until your retention period is met. What this does is it allows for multiple consumers to replay the same data records in the exact same order. Again, this is a key feature you have to remember for the exam. Now, one big thing here too, Kinesis Data Streams do support encryption of your data within the streams using KMS keys. So if a scenario comes up where you need to encrypt your data on your data stream, remember that you can use a KMS key to do so. Now before we wrap things up here, let's have a quick high‑level data stream architecture example. On the left here, we have our producers, so this could be EC2, IoT sensors, mobile phones, etc. What happens is these producers are going to typically be pushing a lot of small real‑time data records into your data stream every second. Again, remember the key indicator of real time. Some examples of potential type of data would be application logs, IoT sensor data, or maybe something like a game leaderboard where you have to have live updates. As this data is getting pushed in, you have your shards. So you can see we have three shards or however many you really need. The data itself lives on these shards and uses sequence numbers to stay ordered. You need to remember that you create shards within your data stream to handle increased capacity, in other words, the amount of incoming data. Once your data is living on a data stream within a shard, your consumers can now consume that data, so they can leverage custom applications using SDKs, API calls, etc., to read data records, or you can even leverage many different native integrations within AWS. So, some common examples are Lambda functions, Data Firehose, which we'll explore, and Amazon Managed Service for Apache Flink. Now I'll tell you those first two are very important, Lambda and Data Firehose, and again, we'll explore those here later on. For now, though, we're going to go ahead and wrap this up, we're going to move on to consumers and producers. Be sure to remember that this is for real‑time data, and understand some of those key concepts that we looked at earlier. Let's go ahead and wrap up, and I'll see you in the next one.

Kinesis Data Stream Consumers and Producers
All right, we just got done reviewing Kinesis Data Streams at a high level and when you might use them, let's now look at the consumers and the producers for your data streams. First up, for producing data, typically on the exam, producers are going to use the Amazon Kinesis Producer Library, otherwise known as the KPL. Consumers are going to use, typically, the Kinesis Client Library, or KCL. Now let's explore each of these a little bit more in depth. For producers, when you're using the KPL, first of all, it is meant to make ingesting data to a data stream much simpler for your applications. It's going to handle a ton of stuff for you, like aggregation of your data records, metric collection to Amazon CloudWatch, and even retrying whenever there's a failure. Essentially, it's going to abstract some of the manual tasks for you. Now it's important to understand using the KPL is not the same as using the Kinesis Data Streams API to put records on a stream. You likely should not get tested on this, but I like to point this out for real‑world use cases. The fourth thing here, using the KPL is perfect for EC2 instances when you have to write thousands of events per second. Again, it handles a lot of the automatic aggregation for you, and that's one of the biggest benefits. In addition to that, when you do use the KPL, it allows you to easily leverage EC2 instance profiles and IAM roles if you're hosting your application on an EC2 instance. Now, of course, that could work for ECS as well with your different task roles there; however, just the big thing to remember here is if it's on managed compute like an EC2 instance, you can leverage built‑in IAM profiles, etc. Moving on to the consumers and the KCL. This is going to be a standalone Java software library. So if you see those type of key indicators, think KCL. The KCL, just like the other side, the KPL, is meant to simplify the process, but in this case, it's meant to simplify consuming and processing data off of a data stream. It's meant to essentially easily allow you to implement a library and then focus on the application code itself and not have to worry about the data stream aspect. You just set this up, configure it to pull off a specific data stream, and worry about your application code. One of the biggest benefits, or a couple of the biggest benefits, it offers load balancing across multiple workers, it will handle automatic failure retries, it's going to checkpoint any processed records using sequence numbers, and it even responds to changes in the number of shards. So if you're having to scale out and add shards on your data stream, the KCL can recognize that and take advantage of that. Now, another less popular option for consumers in exam scenarios is the straight up AWS SDK for Java. This is a possibility, but typically you're going to use either the KPL or KCL for either producing or consuming. Moving on to another important thing to understand and remember for the exam is the concept of Enhanced Fan‑Out. Enhanced Fan‑Out essentially turns Kinesis Data Streams into a push‑based system. Remember, by default, data streams are a pull‑based system themselves, you pull messages off. By enabling Enhanced Fan‑Out, it allows you to essentially create a push‑based system. It works by allowing you to push directly to consumers. By enabling this, your data is immediately pushed to every single consumer once it's ready on the data stream, so there's no waiting. The big benefit of using Enhanced Fan‑Out is high performance. You can get extremely high throughput while also maintaining extremely low latency. However, with that being said, you are going to incur additional costs compared to normal data streams. The key difference to using Enhanced Fan‑Out compared to typical data stream designs is that each consumer actually gets 2 MB per second for read throughput per shard. So they're getting that equally across every single shard, as opposed to the entire stream. Typically, it would be the sum of 2 MB per second per shard for all consumers. So, in other words, everything is shared in the original design, but with Enhanced Fan‑Out, you're allowed to scale because you're allowing every single shard to get its own bandwidth essentially. Now an exam pro tip: if you have a scenario where you need to scale consumers of a data stream and you need to maintain high throughput, I would consider Enhanced Fan‑Out. Again, this allows you to achieve essentially a push‑based systems design and achieve high throughput with low latency. With that being said, let's end this clip here, and we're going to move on to some more configuration options you need to know within Data Streams called capacity mode.

Data Stream Capacity Modes
In this clip, we're going to start looking at capacity modes. Remember, data stream capacity is the configuration that you get to set to say how much data your stream can manage and handle. You get charged within data streams based on capacity. So with that, you have two options. You can use provisioned capacity or on‑demand. Let's look at both of those really quickly. On‑demand capacity. This is where you're not going to have to worry about capacity planning. The service itself will automatically scale and manage your shards to handle, potentially, up to gigabytes of write and read throughput per minute. This is essentially like DynamoDB on‑demand, where you just turn it on, the service handles it for you, and it could be a cost‑savings approach depending how you use it. On the other side, we have provisioned. So this is where you actually set or provision the number of shards that you want to be created for your data stream. When you use provisioned, you can manually increase and decrease the number of shards based on the demand. Now there are ways to automate this process as well, but the big thing to understand is provisioned is where you actually set the number of shards you want, and on‑demand is where you let the service handle it for you. Now there are two primary things to understand when using on‑demand capacity modes. This is going to be perfect if you need data streams and you have highly variable or unpredictable application traffic. So in other words, you're not really sure how your traffic is going to look, you don't know how you can actually predict the amount of shards you need, well, then using on‑demand is a perfect choice. One thing to understand, though, pricing is based on the amount of gigabytes of data that is written and read from your data streams. So again, this could be a cost‑savings approach, or you could actually incur a lot of costs if you didn't understand the actual provisioning that was needed. The big thing to take away here, though, is that if you have data streams or need data streams and you have variable traffic, I would consider on‑demand capacity modes. But with that being said, let's go ahead, we're going to wrap this clip up here, and we're going to move into some of the other services that Kinesis offers.

Amazon Data Firehose Overview
All righty, we just wrapped up Kinesis Data Streams for real‑time data ingestion, let's now look at Amazon Data Firehose for near real‑time ingestion. Amazon Data Firehose is another capability and service within the Kinesis suite that's meant to offer a fully managed data streaming solution for near real‑time data ingestion. That is a huge indicator on the exam. With Data Firehose, similar to data streams, producers send data to the Firehose delivery stream, and it's going to automatically send it to the configured destination. In other words, instead of typically consuming like you would a data stream, once you configure the Data Firehose, it pushes data for you automatically to a lot of supported destinations that we're going to explore here in a moment. On the exam, Data Firehose is commonly used to send ingested data to different services, including Amazon S3 buckets, Amazon Redshift tables, Amazon OpenSearch clusters, and a ton of different supported third‑party providers. Again, these are going to be four common scenarios to watch out for for Data Firehose. Let's explore some concepts specific to the Data Firehose service. Remember, this is a fully managed serverless offering that offers automatic scaling for data ingestion. Those are a massive flag that you should look out for in your exam scenarios where you might want to consider Data Firehose. When you're using Data Firehose, you use the Amazon Kinesis Agent to typically send data to the stream. Now, there are many other ways, including API calls like PutRecord and PutRecordBatch, but typically the easiest way is to run that Amazon Kinesis Agent on your application. When you're using Data Firehose, it actually allows for data transformation, mid‑flight using Lambda functions, and that's all natively built in, you just have to write the code and configure the stream. And then the last point here, you only pay for what you use with Firehose. That, again, is one of the biggest offerings that this gives you. You pay for what you use, it's fully managed, and serverless. On the exam, you are going to have to know some of the supported destinations that this allows you to configure, and here is a list of some of the more popular ones that could come up. To be honest, though, in my experience and talking to others, these four here on the top left that I've highlighted and bolded are going to be the most common to appear on the exam. Data Firehose allows you to natively integrate with these four services, and it's extremely easy to do so. Now with that being said, let's take a look at Firehose streams, coming up next.

Firehose Streams
All right, now that we have an idea of what Amazon Data Firehose offers, let's take a look at what makes this service run, known as a Firehose stream. Firehose streams are how this service works. This is where your data lives, it's where your data gets sent to, and how your data is consumed. Currently, there are three sources that are native to use for Firehose streams within Amazon Data Firehose. Let's take a look at those three different sources now. The first is a direct PUT. So this is exactly what it sounds like, your producer applications directly put or directly write to your stream using PUT commands. Services and applications that can use this include things like the AWS SDK for your favorite language, CloudWatch can actually stream logs to the service, you can put SNS notifications there, AWS IoT data, and the Kinesis Agent, which we briefly talked about in a previous clip. All of these are direct PUTs, and you need to know that for the exam. It also supports native integration with Kinesis data streams. So you can easily configure your data stream as a data source, so that you can read data from the stream, and then ingest it and load it into your Firehose stream into a chosen destination. A common use case here is if you have real‑time data coming into a data stream and you want to automatically and easily transform and then load that data into an S3 bucket, well then you can use Kinesis data streams, to a Kinesis Firehose, to an S3 bucket. An example architecture for that scenario is actually here. We covered this in a previous module within this same course, but remember the scenario. We had an API Gateway hosting a RESTful API and we have IoT devices sending data in real time through our API. Then in the middle, we're streaming that data into a Kinesis data stream because it supports real‑time ingestion. Now we understand, Kinesis Data Firehose or Amazon Data Firehose allows us to go ahead and natively use a data stream as a data source for our Firehose stream. This allows us to easily deliver data to an S3 bucket, we can process and transform that data before doing so, etc. Again, this is just a reminder of this architecture we dove into earlier in this course. Jumping back here, the third source that you need to be aware of is Amazon Managed Streaming for Apache Kafka. Amazon Firehose allows you to read directly from your Kafka clusters, and we're going to cover this service much more in detail later on within this module, but for now, just understand that it is supported as a direct source for a Firehose stream. Moving on to records and buffers, you have to understand how your Firehose stream works and some of these configuration options. When you put records into a Firehose stream, they can be up to 1000 KB in size. That's an important number to remember for the exam. Any incoming data gets buffered before being sent to the configured destination. Now there are two ways to set a buffer limit. There is a buffer size, which you specify in megabytes, and there's a buffer interval. The buffer interval is classified or configured in the amount of time in seconds that you want this to wait. Now, buffers can be a little tricky, so let's dive into these really quickly. It's important to know, if buffer size is met, so that megabyte size is met, then Firehose does not care about the interval timing. And on the other side of that, if the interval is met first, so a certain amount of time passes before the buffer size is full, then Firehose doesn't care about the buffer size. This is essentially a first‑come‑first‑served basis, so if the size is met before the timer hits, it will still send the record and vice versa. The big thing to remember is data gets sent when either of those two settings gets triggered. This is important because it's near real time and it's not real time. That buffer is what classifies it as a near real‑time solution. Moving on to some more very important concepts. This service is commonly used for sending several formats of different files and documents, including CSV, JSON, Parquet, text, and even, if required, binary data. In addition to the supported formats or the commonly‑used formats, you can leverage Firehose to convert Parquet and ORC formats, and you can use it to compress the delivered records using GZIP or Snappy compression methods. Again, this is an important thing to remember for the exam. It compresses the delivery of records, so this is important for when you're sending stuff to an S3 bucket. The third thing here is it does support easy data transformations using Lambda functions before sending data to a final destination. Once again, we brought this up before, but this is a key indicator on the exam as well. It can live transform data within the stream before it delivers it. A quick exam scenario for you to keep in mind on the exam. If you need a near real‑time data ingestion system that allows you to transform data as it's being streamed and then store the data in S3, this is a perfect service to use, it does all of those very easily. And, another exam pro tip: you can leverage S3 to store the transformed records, the original source records, and even failed delivery records. Let's actually jump into a more specific exam scenario before we wrap this clip up. In this scenario, let's assume you're working for an enterprise or organization that collects data reports from IoT sensors around the globe. The sum of these data reports that are generated by the sensors equate to roughly 1.5 TB each day. The data reports themselves that get sent in, however, are 2 KB in size each, so they're not very big. However, you need something that's highly available, it allows you to minimize costs, and you want it to have the least amount of infrastructure management possible. So, you want to avoid managing any of the overhead. And then the last requirement here, your data needs to be immediately available for at least 21 days, and then you can archive it after that. Well, let's break down a potential architecture example that leverages Data Firehose. On the left here, we have our IoT devices, and let's just say we've configured them to directly put records into our Firehose stream. So now our sensors are sending our data, and Firehose is allowing us to use a serverless offering that automatically scales to handle spikes of ingested data from all of our different sources. Within Data Firehose, you can easily configure different destinations. So on the top here, S3 can be easily configured to host all of our transformed, finalized data for any analytics purposes or just long‑term storage. In addition to that, remember, you can store your failed transformations, your failed deliveries, and even original source records in a separate bucket for backup purposes. And in addition to that, we can then implement on S3, a lifecycle policy. We can use this lifecycle policy to archive our data after it hits that 21 days where we need it to be available immediately. This is a real‑world exam scenario that you should be familiar with. The big thing to take in mind are the configuration options and the destinations. Now, real quickly too, we could easily also implement data transformation. Remember, you can natively integrate a Lambda function in this workflow so you can transform your data as it's being streamed before it hits your destinations. Now, that's going to do it for this clip on Firehose streams, let's wrap this up, we're going to move on to some other services within the Kinesis platform.

Amazon Kinesis Data Analytics
All right, we've covered Kinesis Data Streams, and we just wrapped up Kinesis Data Firehose. Let's go ahead and move on to another service within the Kinesis platform called Kinesis Data Analytics. Now, more specifically, this is called Data Analytics for SQL Applications, and that's typically going to be a big giveaway on the exam itself. You can use this to easily process and analyze streaming data using SQL commands. You use what are called applications to read and process the streaming data in real time. So, similar to data streams, etc., you have an application which is a consumer, but in this case, it's specific to using SQL commands. This service also allows you to perform conversions, transformations, you can enrich your data, and you can even filter data using pre‑processing Lambda functions. So it functions kind of similar to a Firehose, but again, this is specific to SQL Applications themselves. In addition, there are two supported sources for this service, you can use a Kinesis Data Stream or a Firehose stream, those are the two currently supported sources in AWS. As far as destinations go, you can send your converted data to Kinesis Data Streams, Firehose streams, and even Lambda functions. So the destinations offer a little bit more compared to the sources. Now, an important note here, this service is being deprecated within the next year or so; however, we included it here because you should still know what it offers, because it could still appear on the exam until it's gone. But with that being said, an exam pro tip for a key indicator: if you need to analyze streaming data specifically using SQL commands, I would consider this service, Amazon Kinesis Data Analytics. And with that said, let's wrap up here, and we're going to move on to the next offering within Kinesis Video Streams.

Amazon Kinesis Video Streams
All right, when you're designing architectures in AWS, you might run into a scenario where you need to leverage streaming of video, so, let's talk about the service to handle that, Amazon Kinesis Video Streams. This is a managed service within the Kinesis platform that is specifically meant to stream and store live video from different devices to AWS. It is very useful for real‑time video processing or whenever you need to perform batch video analytics. If any of those indicators come up, I would consider this particular service. And speaking of coming up on the exam, here's an exam indicator for you. If you have a scenario where you need to capture a large amount of live video data from around the world, I would immediately consider this as one of the potential solutions. Now that's going to do it for this very short clip. Again, just remember Kinesis Video Streams are useful for real‑time video processing or batch video analytics. We're going to end here, and I'm going to see you in the next clip where we review our module and have some exam tips.

Module Summary and Exam Tips
Okay, here we go. We learned all about different ways to decouple our architectures, more specifically in this module, we looked at Amazon Kinesis. So let's go ahead and review this module and look at some of the exam takeaways before you move on. First up, let's look at how to choose between SQS, SNS, or Kinesis for your decoupling needs. Remember, SQS is a messaging broker service that's simple to use and doesn't require much configuration. It does not offer real‑time message delivery. SNS is a push‑based message service that's good for fanning out architectures. However, it also does not officially offer real‑time message delivery. Kinesis, which we just reviewed in this module, is a bit more complicated compared to those, and it's mostly going to be used in big data applications. The big thing to keep in mind here is that Kinesis does provide methods for real‑time communication for decoupling your architectures. Moving on, let's start looking at reviewing our Kinesis data streams. Remember, data streams offer data retention for your ingested streaming data for up to 365 days. How it works is data is never deleted from a stream, it only expires based on your retention period. That is a very important thing to keep in mind. In addition to data, you can have 5 read transactions per second or 2 MB of data being read per second. That is an either/or scenario. If one of those hits, the other is skipped. When we move on to writes, you can write up to 1000 records per second, or you can write up to 1 MB per second. Again, just like reads, this is an either/or scenario. If you write 1000 records, well then it skips the size and it says, okay, let's get the next batch going. Fifth point here, Kinesis Data Streams offer a guaranteed ordering of your messages and your data records. That is a key indicator on the exam for real‑time data streaming. And then lastly, because of how these streams work, your consumers can replay the same data records in the exact same order. It can accomplish this by using sequence numbers and partition IDs and the retention period. Remember, data doesn't get deleted, it just expires after a certain amount of time. Now exam pro tip to summarize this: do you need real‑time data ingestion and data streaming? Then I would consider Kinesis Data Streams. Next up was Data Firehose. This is a near real‑time option for streaming data. Again, remember that it is near real‑time. The outputs are going to be based on buffer interval and buffer size. If buffer interval is met, then buffer size is skipped, and vice versa. One of the big offerings is that this is a serverless method that offers automatic scaling, so it can handle spikes in your data ingestion automatically and you don't have to worry about it. You only pay for what you use. In addition to understanding that it is a serverless offering with autoscaling, understand the different destination options that we're about to list, these are very important for the exam. You can easily send a data record output to Amazon S3 buckets, it integrates with Redshift tables and OpenSearch clusters. These three services are common to appear on the exam. And then the last major point here is that it offers the ability to transform your data midstream using a Lambda function. And with Data Firehose review, let's look at the other two services, and then we'll wrap things up. Kinesis Data Analytics. This is going to be used specifically if you need to perform SQL commands on streaming data. If you see that indicator, I would consider this service. And, Video Streams are used if you need to stream or store massive amounts of live video within AWS. Again, that is the key indicator for this particular service. With that being said, thank you for hanging in there. Let's wrap this module up, and we're going to move on to some other services in a different module.

Amazon MSK and Amazon MQ
Amazon Managed Streaming for Apache Kafka (MSK)
All right, welcome to the next module in this course. During this module, we're going to cover two specific services within AWS that you should be aware of for the exam, specific to messaging within your architectures. First up, we're going to cover Amazon Managed Streaming for Apache Kafka or MSK. You might have heard us talk about or briefly mention MSK in previous modules, specifically when we talked about Kinesis, so let's go ahead and break down what this service is at a slightly deeper level so you're aware of when to use it on the exam. What MSK is is a fully managed service for running data streaming applications that are specific to Apache Kafka. That should be your first indicator on the exam. If you need Apache Kafka, this is a perfect option. When using this service, it handles all of the control‑plane operations for you, and you just have to create, update, and delete clusters as required. Now you're not going to have to know Apache Kafka in depth for this exam, but again, understand this is the service to use if you need it. From an infrastructure standpoint, you're going to choose what VPC and availability zones that you want to deploy the Kafka clusters in. From there, the service is going to manage the cluster broker nodes and the Zookeeper nodes. These nodes are specific to Apache, and again, just more so understand that it is highly available and it deploys across multi‑AZs. One last reminder, you're not going to have to know Apache Kafka in depth, just understand that's what this service is for. With Amazon MSK, this is going to offer an alternative to using Amazon Kinesis. Typically, however, if you're building a brand new application and you don't have Kafka‑specific requirements, you're going to want to use Kinesis due to the simplicity compared to Kafka. Moving on, let's look at some important concepts to remember for the exam. Recovery. MSK offers automatic detection and recovery from common failure scenarios for your cluster nodes with minimal impact to system performance. In other words, if it sees that a node is going down, for instance, maybe an availability zone has gone down. Well, the service itself is good about automatically recovering, reinstalling, resetting up, etc., and getting your entire cluster back up and running full speed. It does support specific encryption requirements. So, in transit, it's going to use TLS, which is typical for a majority of services in AWS, but it also supports leveraging KMS for encryption at rest for any data that lives on your cluster volume. Remember, you create clusters across AZs, and what that means is that you're creating instances in servers where this compute lives, and with those servers has volumes. So because of that, you can encrypt that data using KMS. Another very important offering that you need to at least be aware of for this exam is MSK Serverless. This is a cluster type within Amazon MSK where you can have serverless cluster management with automatic provisioning and scaling. Typically, you're going to specify the cluster size, etc. that you want with the normal MSK offering, but if you need something where you want to be a little bit more hands‑off, then they do offer a serverless feature. And then lastly, here are some common consumers. Some common consumers on the exam that you might see are custom applications that run on EC2, ECS, and EKS, and of course, those are all specific to consuming Kafka streams. You could also run into things like Kinesis Data Analytics and AWS Glue for ETL workloads. All of these are what could more than likely appear on the exam as components for consuming from a Kafka stream in MSK. Now that's going to go ahead and do it, let's wrap up this Amazon MSK clip. We're going to move on to the other service, Amazon MQ, before we wrap up this module.

Amazon MQ
We now know what Amazon Managed Streaming for Apache Kafka is used for, so, let's look at the other messaging service known as Amazon MQ. Amazon MQ is another separate message broker service that allows for easier migration of existing applications to the AWS cloud. At this point in time, when we're recording this, it currently supports two specific engine types for messaging. It supports Apache ActiveMQ and RabbitMQ systems. If you see either of those two mentioned in an exam scenario, I would immediately consider using Amazon MQ. The service itself is commonly used to send ingested data and messages between large cloud‑native applications and services. However, with that being said, if you're creating new applications within the cloud, it is recommended to explore SNS and SQS instead due to their simplicity and their scaling abilities. Now I mention that because there are actual infrastructure components you have to worry about within Amazon MQ. It does offer highly‑available architectures with different system components and servers, but you have to plan around that. While on the other hand, SNS and SQS allow you to accomplish this without having to worry about the infrastructure. So with that being said, let's look at the different availability options that MQ offers. It's going to depend on the broker engine type that you use. For Amazon MQ, if you're going to use it for ActiveMQ engine types, it does offer active standby deployments, where you're going to have one instance that remains available at all times, and then you have another one that's standby in a separate availability zone. What happens is you configure the network of your brokers with different and separate maintenance windows, so performance will not be impacted if there's ever any type of update, patching, etc. that needs to be done. If you're using Amazon MQ for RabbitMQ, the cluster deployments are logical groupings of three broker nodes that get split and spread across multiple availability zones. These three broker nodes will be behind a network load balancer so that you can handle an influx of incoming traffic. The big thing to take away here is that depending on the broker engine type, that will affect your deployment options. Now one example pro tip here regarding Amazon MQ: if you see JMS or other messaging protocols like AMQP, MQTT, OpenWire, or STOMP, I would immediately consider Amazon MQ. Those are perfect scenarios for using this service. Otherwise, I would lean toward SNS and SQS for other messaging needs. Let's go ahead and wrap up here, and I'm going to see you in a module summary and exam tips clip, coming up next.

Module Summary and Exam Tips
Awesome, way to hang in there, that was a very short module, so let's go over some brief summaries and exam tips that I think are important for you to take into your exam. First up, Amazon MSK. Remember, this is a fully managed service in AWS specific to using Apache Kafka for data streaming. Within this service, they offer MSK Serverless. You can leverage this to use Apache Kafka on MSK and you don't have to worry about capacity or infrastructure planning at all, you allow the service to do it for you. Within MSK, it does support multi‑AZ deployments, and you can configure your nodes for public network access. So if that comes up on the exam, you can deploy your nodes in a public subnet for direct access to your Kafka applications. And then lastly here, let's compare it to Kinesis. MSK supports larger message sizes, it uses Kafka topics and partitions, and it allows for plain text data to be sent. In Kinesis, data is always encrypted in transit, it has a much smaller message size, and of course, it's not specific to Kafka. To be honest, if you see Kafka, you should immediately consider this service. We then had Amazon MQ. Remember that this is another managed messaging broker service, and it's better fit for migrating existing messaging systems to the cloud. With this service, you can only use it for RabbitMQ and Apache ActiveMQ engine types. Also, really remember that for the exam. In addition to the engine types, it does support multi‑AZ active/standby deployments. One of the big differences between this and some other services like SNS and SQS are the fact that MQ is not as scalable because you do have the actual infrastructure components you have to worry about. SNS and SQS are handled for you, and typically for cloud‑native new applications, you're going to want to avoid MQ. You really should only use this if you run into the specific engine types we looked at, or if you see anything specific to JMS, AMQP, or MQTT messaging protocols. Those are key indicators for you to immediately consider this service. Now, wrapping things up with an exam pro tip. Again, if you are designing a cloud‑native app that is brand new, I would consider using SNS or SQS on the exam instead of this service. Now that's going to do it for this module, very short, very sweet, let's wrap up. Take a break if you need it, and I'll see you in the upcoming module.

Amazon Step Functions and AWS Batch
AWS Step Functions
While working in AWS, there's going to be some times when you're designing an architecture that would fall under the asynchronous communication category. However, there might be some complexities that go along with it, or maybe some wait periods, and you'll need a way to orchestrate that workflow. Well, that's where Step Functions and AWS Batch come in, so let's start off this module looking at AWS Step Functions. AWS Step Functions are a serverless orchestration service for business applications and workflows within the AWS cloud. This service is going to primarily be used to orchestrate Lambda functions working together. Now, it does support integrations with other services within AWS, including SQS, API Gateway, and a ton of others; however, the primary use case on this exam will more than likely be dealing with orchestrating Lambda functions that work together within serverless workflows. Here is a list of capabilities you need to be aware of regarding Step Functions for this exam. When you use Step Functions, they allow for retries. So if one Lambda invokes another one and then there's some type of error that needs to be handled, well, you can do that using a Step Function. They also allow you to process workflows in parallel. So if you have to go ahead and run four different workflows at the same time, you can do that using the service. Thirdly here, you can enter and require human approval for the continuation of your orchestration of your workflows. For instance, maybe you want to enter a human check somewhere along the line to make sure that information is correct and it is approved; well, with that, you can require a human to say, yes, go ahead, this is accepted, you can continue on in your workflow orchestration. And then lastly here, it does allow for built‑in error handling. So, again, if you have an error and your retries fail, well, you can say, okay, I want to handle this error a specific way. Maybe you want to send a message, maybe you want to park that message into a dead‑letter queue, etc. Let's move on to some use cases specific to step functions for the exam to be aware of. First is data processing workloads. Step Functions are great for image and video processing, you can help perform ETL jobs using them, and you can perform batch processing or HPC workloads, or at least trigger those to complete. Another real‑world example is order processing. This is probably one of the biggest use cases. You can have a distributed application with a bunch of different serverless functions, which all handle different portions of a serverless workflow for an order completion project. For instance, if you have a retail online and you get an order, you can orchestrate the different serverless functions along that process to complete the order, fulfill it, ship it, etc. And then thirdly here, it's very good for security automation while including human approval. Remember, you can add human approval steps within your workflows, and that's especially good for security automation. Maybe you want to get approval by certain people before things continue on, like correcting a security group, for example, or maybe you need to disable some settings that were accidentally enabled, well, you can do that using these Step Functions to go ahead and orchestrate that workflow. Now, moving on, let's look at some important components of the service. First up is a state machine. These are also called workflows, and you might see that instead of state machine, so just be aware of that. State machines are essentially how this service works. This is the series of event‑driven steps, otherwise known as states, that make up the different tasks and the different flows to complete your orchestrated workflow. Within your state machines and workflows, you have task states. This is going to be the actual unit of work that another service performs during the state itself. A great example would be invoking a Lambda function to perform a process. You do need to be aware of how these work together. Task states belong to state machines or workflows within their own specific states. Now, speaking of states, I'm sure you're tired of hearing that word, but you do need to understand some of these different states for the exam and how they function. There's a pass state, which passes any input that it receives directly to the output or appends that output to a fixed dataset that you have within it. For instance, maybe you're just appending a simple value to continue on in the workflow. You also have a task state. This is going to be, again, a single unit of work performed, so invoking functions, sending messages to SQS queues, etc. We then have a choice state. This is exactly what it sounds like. You can add branching logic to your state machines. So, do you want to do this or do you want to do that, based on the content of your previous state? Fourthly is a wait state. So you can actually specify a time delay within your state machine. For example, maybe you want to wait 60 seconds because you triggered something externally and you want to give it enough time to complete before you continue on. Fifth, we have a parallel state. This allows you to perform parallel processing. You can run parallel branches of your task executions within the state machines themselves, so you can do, again, parallel processing. Next is a map. With a map state, you essentially pass in an array, or in other words, a map, and you run a set of steps based on the elements within that map. So it's going to iterate through it and perform each step. And then we have the two final ones here, succeed, which is exactly what it sounds like, you succeed in your workflow, and we have fail, which is going to stop executions and mark them as failures. Those last two are pretty self‑explanatory, but I like to include them. Let's look at map states a little bit more in depth. There are two modes you have to be aware of for the exam, as they can come up. The first is inline mode. This will be the default mode that offers limited concurrency, and it's going to iterate over the array in the order in the context that it was given. So for instance, if you have an alphabetical order within your map, it's going to go ahead and iterate A, B, C, D, etc. in that exact same order. We then have distributed mode. So this is better for large‑scale parallel processing workloads, for instance, a lot of data ETL jobs. That's a good example of a large‑scale parallel process. So just remember, inline is default and it processes or iterates inline, and distributed is good for large‑scale processing. Now, next up here, let's wrap this clip up with some workflow types. There's standard and express workflows. With standard, these have an exactly‑once execution, so it's going to execute one time and then move on and mark it as succeeded, failed, etc. With express, these are good to handle at‑least‑once and at‑most‑once workflow execution, so you can add a little bit more complexity to these. With the standard, they can actually run for up to one year, so they can sit in transition between states for an entire year, which is pretty insane, that's a very long time, but that can come up on the exam. With the express workflow types, however, again, these are for high‑performant and very fast ones, these can run for up to 5 minutes, so they're a lot shorter, way different than standard. Moving back to standard, these are useful, of course, for long‑running auditable workflows. Since they can run for up to a year, that's why they fall perfectly within this category. For the express side, these are going to be only useful for high‑event‑rate workloads. So if you have a ton of events coming in, you need to process them in parallel very quickly, then I would use express. And then lastly here, standard is billed based on the number of transitions that occur. So, when you go between different states, that's when you get billed. Now of course, you also get billed for the resources that are used, so if you're invoking Lambda functions, you pay for those separately, but as far as the actual billing goes for step functions, you pay on the transitions themselves. With express, however, you're based on the number of executions that occur, the total duration of that execution, and the memory consumed per state. So it's a little bit different from a billing perspective, and if I had to pick anything out, I would just understand that express is good for high‑event‑rate workloads that run shorter, and standard are for longer‑running workloads that you can audit. Let's go ahead and end here, and we're going to move on to a demonstration where we look at Step Functions.

Demo: Orchestrate Workflows Using AWS Step Functions
All right, I think that's enough talking, let's jump into a console within our hands‑on playground, and we're going to look at a demonstration on using AWS Step Functions. All righty, I've loaded up my hands‑on playground here in us‑east‑1. Before we get going creating our state machines here in Step Functions, I want to review some existing architecture I've already created. What I've done is I've created this SQS standard queue called OrderQue, and we're going to use this to park some messages just to demonstrate how this works. In addition to this, I created this OrderTopic standard topic with a simple email subscription to my temporary mail address, which you can see over here in this tab. And then the last two resources I created are two Lambda functions. This first one is called ApplyDiscount, and all this does is a very simple logic check. It says, hey, if couponCode is present and it matches these values, take off the respective percentage of that price. So what we're going to do here is pass in a fake order with a simplified item number, a price, and then maybe a couponCode and test out our logic. In addition to this ApplyDiscount function, I have a ProcessOrder one, which just lets the user know, hey, this order reached this Lambda successfully and we processed it, go ahead and move on. Awesome. With that out of the way, first thing I want to say is the code for our Lambdas will be available for you; however, you will have to create the queues and topics if you're doing this on your own, but I think that's good practice because this course covers those two topics. So, first thing I want to do here, I'm going to create a new state machine. Once we're in here, what's really nice within the console is look at all of the templates that are offered for you to get a very quick start on. You can process high‑volume messages, you can query datasets. There's a lot of different options for you based on use case or even just services. Go ahead, feel free to poke around this on your own, I'm going to choose Blank, and I'm going to choose Select. Perfect. So let me go ahead and minimize this menu on the left, and we come to the main design screen. So this is one of the biggest benefits of using Step Functions. You can use this graphical interface and drag and drop stuff, you can move it around, you can configure it,etc., and it configures your state machine for you. What I'm going to do, though, to get started is let me delete this, I'm going to go to Code, and I'm going to copy and paste a state machine definition file in here and then we'll edit it. So I'll go ahead, let me paste this in, and let's walk through this from a graphical standpoint and a code standpoint. So on the left side we have the JSON code that defines our state machine. You'll notice the first thing we're starting with here is a choice state. So this is a choice type of state machine, and it's performing a very simple check here. It's saying, hey, if this variable is present, I want you to run the ApplyDiscount step. Now what that means here is at this choice state, if it's present, it's going to run down here to this ApplyDiscount Lambda code. So when I click on this, it skips us down to that portion of the state machine. Now obviously this is not correct because I don't have the right function code in here. We're going to leave this as is, and I'm going to go back to design here in a moment and we'll fix these errors, but for now, I just want to run through the logic. With that being said, if couponCode is present, the next thing it's going to do here is SendOrder, so you see Next, SendOrder. What SendOrder is is a parallel state type. You can see it says Parallel state, the type here is parallel, and we're running two different branches within this parallel state. Remember, parallel allows you to run different workflows in parallel, thus the name. So within this, we have the two branches. We have our ProcessOrderLambda, which is here, and then we have our SNS Publish, which is the other branch. So what this is doing is it's triggering a Lambda function, which is going to be our ProcessOrder function, and it's going to send us an SNS message via our SNS topic, which we'll configure. It's doing both of these things at the same time. Awesome. So that's if couponCode is present, but what happens if it's not? So that's going to be the default. So if I skip up top again, you can see the default is set to a pass. What that means is, hey, if this is false, run here to the pass state. What we're doing in a pass state, remember, is you can either just pass the static input/output or you can append to the input and the output. What we're doing is we're appending, so I'm adding a result of a reasoning key with this value. You can see the ResultPath, we'll say, hey, add this to the OrderUpdates key containing this JSON, and I'll show you this in our example here coming up. That might be a little confusing, but don't worry, we're going to look at it more in depth. Big thing to know here is pass date can either pass the same values or it can append to the existing ones. After it performs its work, it's going to go ahead and go to Next, which is our SQS SendMessage here, so if I click on this, this is going to send a message to our SQS queue URL, which we will configure, and it's going to send it with the MessageBody here. So it's going to send the exact message as an SQS message. Assuming that this is successful, it then moves to our SendOrder parallel state. So we always end up to the same parallel state, just via a different path. Okay, with that being said, that is the general flow. Let's go ahead, I'm going to go to Design, and let's edit some of these errors. The first thing we see here is our Lambda here. So I'm going to click on this, and you'll see, hey, this function doesn't exist, and well, that's because it doesn't. So I'm going to choose a function, I'm going to refresh, and I'm going to choose my ApplyDiscount function here. I'll click on Done, and there we go. The next thing we have to do is our queue. So I'm going to click on SQS SendMessage, I'm going to scroll down here, and we need to enter a valid queue URL. So I'm going to refresh, I'm going to select and find our order queue. After that, the next thing we have to do here is there are other Lambda functions. So ProcessOrder, I'm going to choose my ProcessOrder function here from our dropdown, select Done, and then move on to SNS. This is the last portion for us to fix, I need to set the correct topic ARN here, so I'll select it from my list, and then I'm going to go up here, and I'm going to click on Create. Now check this out. This is very important for you to understand. In the console, this creates an execution role for us with all of the exact required permissions, and you can see the actions here for their respective services. If you are doing this in Infrastructure as Code or CLI, you have to create this policy yourself. Remember, the only way this service can interact with other services is via an IAM policy, so that's important to remember for the exam. So what I'll do is I'll click on Confirm, it creates our role for us, and then it's going to create our state machine, and there we go. So let me go ahead and get rid of these, and let's test this out. What I want to do is click on Execute. Now in here, we can paste our JSON. Now typically you would trigger this via some type of API call, maybe an Amazon API Gateway or some other way, but for sake of simplicity, I'm going to paste in some JSON here. So the first thing I'm going to do here is let's paste in a JSON. We have our item number, our price, and then our couponCode, which should be valid, I'll click on Start execution. If I scroll down here, you can see a graph view of when this is running. So green is succeeded, so it checked couponCode, it said, hey, yes, couponCode is present, so I went down my branch here, made a choice, and went to the ApplyDiscount function. Now notice here on the right we have our state input, which is the original JSON. Well, our function went through, performed its logic, and then updated that output to have the discounted price, so we took 10% off. What happens now is that output goes to our next step, which is our parallel step. So we see our input here on the right, which came from our function, and we see our output. So we see the status from our Lambda step, which is Order Lambda has processed, and then we see the output from our SNS topic published. So now if I go to SNS here, or my mail, we see a notification, I select this, and we should see that order, and we do, {"item":1,"price":90,"couponCode":"DISCOUNT10"}. So this is working. What's cool about state functions is we get this graphical face, you can see where the logic actually went, if it was successful, etc, so let's try this again. I'm going to try one more time here with a valid coupon code. I'll go to New, let's change this to 30, I'll click on Start, and it's going to run through the same process. Now notice how fast this one was. That's because everything's already warm, our Lambda functions don't need to spin up, we see our new output, and I'll eventually get a new message here, and I do; so this is working as expected. Now, if you remember, in our code, if there was no valid discount, well, it should go ahead and check that. So what I'm going to do is start another execution here, I'm going to make up a discount code of 50, which wasn't valid, and let's see what happens with this. Scroll down, it checks coupon again because that's the choice, and you'll notice, hey, well, the price isn't different. Well that's because we didn't have a matching coupon code, so this is not going to have anything discount‑wise taken off, and we get that eventually over here as well. Now I'm not going to check this, it's getting a bit repetitive, but it's just good to know you can have different logic within Step Functions and the code that's being executed. Perfect. So let's test one last thing. I want to test going to the left here. So what I'll do is I'm going to go ahead and copy and paste a new execution in here, and we're not going to include any discount code. So in theory this should go to our SQS queue. So I'm going to start, we see our choice was made, there was no coupon code, so it went to our pass state, our pass state took in the input, appended the new output here, and then it sent it to SQS. This SQS SendMessage was successful, so it went to our parallel branch, and we get another successful processing. So now since this went through our pass date, it passed it to SQS, I should be able to go up to my queue, let's send and receive and then poll, and we see our message. So now if I select this message, there we go, there is the updated order body. We see our item, the price, and the OrderUpdates key that we added with our reasoning key‑value pair. Perfect. So this is working as expected. Now, that's how cool it is to use a Step Function. It provides you a graphical interface for orchestrating different serverless workflows, and a great example is order processing, like we just looked at. Feel free, go in here, create your own state machine, you can use the one I provided, or you can create your own. And, go ahead and really drag and drop stuff. There's a lot of stuff you can do here, you can just drag and drop, you can put stuff wherever you want, edit it. Again, feel free to play with this, this is a very cool service, but for now we're going to end this here. Hopefully you learned a lot about using Step Functions for orchestrating your workflows, and we're going to move on to the next service.

AWS Batch
All right, you're now an expert in AWS Step Functions for orchestrating your serverless workflows, let's talk about the next important service within AWS called AWS Batch. AWS Batch is a fully managed batch processing service, thus the name AWS Batch. It works by running batch computing workloads within AWS that run on EC2, which can be On‑demand or Spot instances, or it can leverage ECS containers and Fargate tasks. So you do have some flexibility on the compute that you want to use, the benefit, though, is that Batch handles all of that infrastructure for you. Because this is a managed service, it's going to automatically launch any required compute as you need it once you configure the type that you want. What that offers is that it allows you to have to not worry about any heavy lifting to configure or manage any infrastructure or software that would be required for batch computing. That is one of the biggest benefits of this service for the exam to keep an eye out for. If you want to remove operational overhead for batched processes, I would consider this service, AWS Batch. When you're using it, you just submit the jobs as needed, or you can schedule them. So you can submit on‑demand or have them scheduled for later to be performed whenever you want. Again, the big thing to take away from AWS Batch is it allows you to skip the installation and the maintenance of any batch computing software, and you can just focus on obtaining and analyzing your batched results. This should be an immediate indicator on the exam for this service. Now let's look at some important concepts for the exam that you should keep in mind. First up is a job. This is going to be the unit of work in Batch that gets submitted to the service, so this can be a shell script, an executable, and you can even pass in a Docker image. You also have, with jobs, the job definitions. So this is where you specify how you want your jobs to be run. So you can think of this as basically a blueprint for the resources within your jobs. The third concept here is a job queue, so this is going to be where jobs get submitted. You submit jobs to a specific queue, and then they reside there until they get scheduled to run in a compute environment, so until they get executed. Now building off of this, lastly is the compute environment itself. This is going to be the set of managed or unmanaged compute resources that get used to actually run the jobs that you scheduled. So this is going to be made up of EC2 instances, Fargate, or ECS tasks. Now let's actually explore the different compute environment types. You need to be aware of these for the exam. There's managed and there's unmanaged. First up we have managed. So this is going to be where AWS manages the capacity and the different instance types that make up your actual compute environment. How it works is you specify the compute resource specs when you create the environment in the first place. So you say, hey, I want you to go ahead and follow these guidelines when you're managing this environment for me. If you use ECS and ECS instances, they get launched into VPC subnets that you have to configure. And with that, the default AMI that gets used is going to be the most recent approved ECS AMI by Amazon. However, you can specify your own AMI if you need to for compliance purposes. And then lastly here, you can also leverage Fargate, Fargate Spot, and regular Spot instances within a managed compute environment. Now let's compare that to an unmanaged compute environment. This is going to be where you manage your own resources entirely. That means you are responsible for everything that gets configured, launched, etc. When you do this, an AMI that you use has to meet the minimum ECS AMI specs. So you can customize it however you want, but as a baseline, it has to meet the same specs as the approved version that is listed for you. Now I'm going to go ahead and say unmanaged environments are less common compared to managed because of the complexity that goes along with them, but if you have any reason where you just cannot use the default managed environments, then you can use this type. In other words, it's going to be a good choice for extremely complex or extremely specific requirements. So maybe you have compliance you have to worry about, maybe you are restricted on the different type of instances, etc., then maybe you use unmanaged instead of managed. Typically, the best route is going to be managed, but again, I repeat this, unmanaged is good for extremely specific use cases. Building off of compute environments, let's talk about when you might use Fargate versus EC2. This is an important scenario that comes up on the exam. Fargate is going to be recommended for most workloads. This is because it's just easier to use. You tell the service to use Fargate, you don't have to worry about infrastructure, it goes up, it goes down, you're done. It's going to be perfect for anything that requires a fast start time, so in other words, less than 30 seconds. It's also good if you have anything requiring 16 virtual CPU or less, it doesn't require any GPUs, and it requires 120 GiB of memory or less. Anything above those numbers, or if it requires a GPU, you cannot use Fargate. Those are immediate indicators for you to realize I have to use EC2. And, speaking of, EC2 is perfect if you need more control over your instance selection. Obviously you have to specify types, and with Fargate, you don't do that. Additionally, it's going to be used if you require GPUs for your workloads, if you require Elastic Fabric Adapter for any storage benefits, or if you require custom AMIs. Any of those three are immediate indicators to use EC2. Now obviously, if you need more than 120 GiB of memory or more than 16 virtual CPUs, that's also an indicator, so do remember that. EC2 is also good if you have high levels of concurrency. That's because you have more resources to spare as opposed to Fargate that would have to come up, come down, etc., well, you can have EC2 up and ready to go and handle your concurrency very easily. With that being said, let's go ahead and wrap up this clip on AWS Batch. Big thing to remember here, it is perfect for managed workloads for anything batch related. Let's wrap up here, and we're going to move on to a module summary and some exam tips.

Module Summary and Exam Tips
Way to hang in there. Let's go ahead and have a quick summary and review some exam tips and scenarios that I think are important for the exam. First up, let's review AWS Step Functions. Remember, this is a serverless orchestration service that is meant for event‑driven task executions while using AWS services. It uses different states, which are elements within state machines to perform actions using the different services. So you're orchestrating services in a workflow. Now, there are many AWS service integrations that are available, but the more common ones that you'll see on the exam are going to be Lambda functions, API Gateway, and even Amazon EventBridge. Remember those for the exam. For an exam pro tip, anything related to the orchestration of workflows should consider Step Functions. That's especially true if it's a serverless workflow. This is very good for orchestrating Lambda functions to work with one another. Moving on, let's review AWS Batch. One of the big and confusing things that happens on the exam is choosing between Batch and Lambda. Remember with Lambda, you have a 15‑minute execution time limit. You also have limited immediate disk space. Well, you can use EFS, but that requires your functions live in a VPC, so you're now incurring cost on the network interfaces as well. And then lastly here, limited default runtimes. With Lambda, yeah, you can create your own, but then you're losing the trade‑off of having less overhead. By not using a built‑in runtime, well, then you're kind of having some trade‑offs, you have to decide if they're worth it or not. For Batch, there is no current time limit. So if you have a long‑running process that's considered a batch process, this is the way to go. Also, the storage is dependent on the compute used. So if you have large EC2 instances, you can have large volumes that far surpass the disk space for Lambda functions. Also, Batch can and is commonly used with Docker. So you can use pretty much any runtime very easily because you just say, hey, here's my Docker image, use this for your runtime and perform my process. Now, be sure to remember these differences and really take your time when you're reading through the scenarios for different key indicators. If I had to be honest, my best guess would be that the time limit will be the biggest indicator out of all of those we just covered. Now, moving on, let's look at a couple exam scenarios specific to Batch. Let's assume you're migrating a legacy app to the cloud. You have an hourly batch job that runs, of course, every hour, and it takes a lot of CPU power. The batch job itself takes roughly 20 minutes on average when it's running on an on‑premise VM. This on‑premise VM has 64 virtual CPU and 520 GiB of memory. What that means is you need a solution that will run the job within the 20 minutes with the least operational overhead. Now, if I had to pick anything out in this entire list, these highlighted words here would be my key indicators. Every hour, lots of CPU, 20 minutes, 64 virtual CPU, and 512 GiB of memory, and least operational overhead. This would be a perfect scenario to use AWS Batch on EC2. Remember, those limits of using Fargate; we had way too much virtual CPU and way too much memory, so we need to use something else, and that's where EC2 comes in. Now let's look at one more scenario and then we'll wrap things up. Second scenario here, let's say you have an image processing application moving to AWS. It can process thousands of images, and then it creates large files at the end of each workflow. What you need is a solution to manage the growing number of jobs and reduce any manual tasks. With this, you don't want to manage any underlying infrastructure, you want AWS to do it for you, and you need a solution that's going to meet all of those requirements with the least operational overhead. Again, if I were to select any key indicators from this scenario, it would be the ones I just highlighted: processes thousands, creates large files, growing number of jobs, and you want to reduce manual tasks, and you don't want to manage infrastructure, etc. With this, this is a perfect time to use Batch jobs to process the images, and then you can use Step Functions to orchestrate your workflows and store the final images in S3. So this is a double whammy, you get a 2‑for‑1. I think that's good for now. Be sure to remember these two scenarios, review the differences between Batch and Lambda, and remember Step Functions are good for serverless orchestration. Let's end here, and we're going to move on to another module, coming up next.

Application and Network Caching
Amazon CloudFront Overview
When you're deploying to AWS, eventually you're going to start running into latency issues or application performance issues where you're slowing down on a lot of requests or whenever you have global users. Well, in this module, we're going to talk about some of the services and approaches we can use to overcome these challenges. So let's go ahead and dive in, and the first service we're going to cover here is Amazon CloudFront. Now, before we dive into CloudFront, let's really quickly review the concept of caching. In this example, let's assume we are the user over here on the left, and we really want some ice cream. Well, unfortunately, that ice cream store is roughly 25 miles away or roughly 40 kilometers. So, what that means is that it's going to take us a while to get there, enjoy the ice cream, and get back. Well one way we can approach this to make it better is we could cache or store some of this ice cream a little bit closer to us in a freezer. So now what we can do is every time we go to the ice cream store, we grab enough to fill up our freezer, and now our freezer, which hosts the ice cream that we want, is only 10 ft away or roughly 3 m. Since we know that we're going to repeatedly get the same type of ice cream, we can cache it or store it locally or closer to us, resulting in a much faster and better experience. Now obviously, that's a high‑level abstracted view, but that's sort of an idea of why you might cache something, right? You have something that's requested repeatedly, so why not just cache it closer to the end user, and then they can go ahead and get it much faster. Now moving on here, let's actually dive into one of the primary services for application caching in AWS, known as CloudFront. CloudFront is a CDN, or a content delivery network service that securely delivers your data. It helps deliver video, applications, and even APIs to customers around the world. How it works is it reduces latency and provides higher transfer speeds using what is known as an edge location or a point of presence, and we will review those here in a moment. To leverage these edge locations, you set up what is known as an origin. An origin is what essentially hosts your content at a high level. Once you set those up, CloudFront uses that with your distributions and it says, okay, I'll go ahead and handle caching all of the most recently retrieved data, much closer to end users so we can increase their experience. Now within CloudFront, it offers several built‑in security features that we're going to explore in its very own clip much more in depth that you have to be familiar with for the exam. And in addition to that, it's also important, it actually supports you using it to front both AWS resources and external resources, so it doesn't just have to work within AWS. Now moving on here, let's review really quickly the edge locations or the points of presence. On the map here, we're about to highlight, currently, what is all of the edge locations around the world. You'll notice there are tons and tons of them. For a quick review, what happens is Amazon actually works with a bunch of different telecom carriers around the world, and it works with them to connect to their own internal network using those carriers. What it does is it allows AWS to deliver content to end users with much lower latency around the world, much easier. This is one of the most powerful features in terms of increasing performance, reducing latency, etc., that AWS offers. Now with that review out of the way, that's a high‑level look at CloudFront, the content delivery network service within AWS that's in here, and we're going to move on into some of the more specific concepts that you need to know about the service.

Amazon CloudFront Origins
All right, we just got done looking at CloudFront from a high level, let's dive into what is known as an origin, which is extremely important for you to understand what it is and how you use these with the CloudFront service. A CloudFront origin is simply the location or resource where you are storing your content that you want a distribution to cache for your users. Commonly used with this is an Amazon S3 bucket for static resources, but there are ways you can use custom origins, which we will explore later on. Now, a distribution, which we just talked about within that origin definition, is a configuration that is telling CloudFront, hey, this is the origin server I want to use, right? So this is my Amazon S3 bucket I want you to use, and I'm also going to tell you how to manage the delivery of that cache content. Some examples are configuring security requirements like requiring HTTPS or enabling logs that you want to store for anyone that accesses your content. Things of that nature are what you configure within a distribution. Now, I mentioned this briefly earlier, but you can easily front an S3 bucket with a CloudFront origin and distribution. This is a high‑performant and secure way to allow access to S3 resources. What happens is you set it up and then you grant access and customize security via what is known as an origin access control. Now we have an asterisk here because this is what is replacing what is known as an origin access identity, and we're going to talk about those later on when we talk about security specifically within CloudFront. For now, though, just understand you can front an S3 bucket static assets with a CloudFront distribution and you grant access via an OAC. One of the benefits of using an OAC is that it also allows you to upload content to S3, so it's dynamic, it's not necessarily only for getting objects, which used to be the case back in the day. And then moving on here, a custom origin. We mentioned S3 as the primary that usually gets used, but you can set up your own custom origins as well. So what happens is CloudFront supports Application Load Balancers, EC2 instances, or really any HTTP server that the service can reach. So this includes even on‑premise servers or VMs where you're hosting a web application. However, the big thing to know here, and this is an exam pro tip, is that CloudFront is only for Layer 7 traffic, it is specific to HTTP and HTTPS connections. If it's anything else like UDP or TCP‑specific, we're going to cover some other services later on, but CloudFront might not be the right choice for that scenario. Now, moving on, let's look at a quick high‑level architecture example on how CloudFront works and how it fetches content. Let's assume we're here on the left and we're trying to access a simple website. Now around the globe, remember, we have these different edge locations or points‑of‑presence that are meant to be caching some of our content. So when we make our first request here, let's say the pluralsight.com/pictures for static content, that request gets forwarded to the closest edge location based on DNS resolution. So it says, hey, this website needs to go to this DNS record, which is our CloudFront distribution, go to this nearest edge location, which is at the top in this example. If the content is there, it will retrieve the content immediately, very quickly, very efficiently. If the content is not there, it's then going to check the Regional Edge Cache shortly after that. So if it's a miss at the edge, it checks the Regional Edge Cache to see, hey, do you have my static content? Again, if the Regional Edge Cache has it, it sends the content back, and we're good to go. However, if this does not contain the content either, it then hits our origin servers, which are configured within our distribution. So this would be a cache miss, and it would be an origin fetch because it has to fetch the content from the origin, and then it will cache it from that point on. Overall, just be familiar with the general flow here. It checks the edge location; if it's not there, it checks the Regional Edge; if the content's not there either, it hits the origin server and fetches it. Moving on to some more important concepts here. When you're configuring a distribution, you can set up a time‑to‑live value. What this is is you set a minimum, maximum, and default time‑to‑live so you can control how long your files live in your caches before CloudFront forwards requests to the origins again. So if you have data that is constantly changing, maybe you set a low TTL, and on the other end, if it's pretty static and pretty consistent, maybe you set a large maximum TTL. You can also do what is called an invalidation. By invalidating caches and files in a cache, you essentially manually remove them from the cache itself before expiration time reaches, and you force more origin fetches for updated files. In addition to invalidating, you can also use versioning to force‑fetching of updated files as well. So maybe you just point to the different files on the back end using the same URL, and what this could do is say, hey, file_1 is old, it looks like file_2 is the new version, I'm going to fetch the new version and cache that; that way, I'm not holding on to the old version. Versioning is a little bit more complicated, and it doesn't come up too often on the exam, but for sure understand time‑to‑live and invalidation. Now moving on here, let's wrap things up really quickly with an exam scenario for using CloudFront to front an S3 origin. Let's assume in this scenario you run an application that generates reports daily online, and it creates these static HTML pages. These HTML pages are expected to reach millions of users around the world. In addition to that, you're storing the static files in an Amazon S3 bucket. So with that, you need to design an efficient, secure, and cost‑effective solution. So let's go ahead and look at an example here. On the right side, we have our Amazon S3 bucket, which is going to serve as our origin for our static content. One of the beautiful things about this is that it helps us reduce the number of S3 API calls, because what we're doing is we're going to store the cache content closer via the edge locations, which you can see here in the middle of the diagram. So once we set up our S3 bucket as the origin server, we can start assuming we're going to start getting users from around the world. So let's say North America, South America, and the Asia Pacific regions, and each of those is going to have their own edge location that is closest to them. So now, when we set this up, the first time these files get hit, they're going to get stored within our cache. So you can see the copies of the HTML content at each edge location. And because of this, when our users start making the call to our URL, they're going to get their cached content that is closest to them via our CloudFront distribution, which can serve millions of requests around the world very quickly and very easily. So now people are hitting the point‑of‑presence closest to them and not hitting our S3 bucket directly. This is both secure and cost‑optimized. Now speaking of security, let's go ahead and wrap this exam scenario up, we're going to wrap this clip up, and we're going to talk about security features that you need to be aware of for CloudFront for the exam.

Amazon CloudFront Security
All right, we just got done talking about CloudFront origins. Well, you have to be able to secure your origins to pass this exam, so let's talk about security specific to the CloudFront service. First up, origin access controls, OACs. We briefly talked about these earlier, let's dive into what they really are and why you need to know them. At a high level, there are two different ways to control access to your S3 origins. The first is an origin access identity, and then there's an origin access control. Both of these serve similar purposes, but OACs are the recommended approach because they support many more advanced features compared to OAIs. Now the big thing to take away here, though, is you might see either/or on the exam. The big thing to remember is why you use them. You likely shouldn't have to know the in depth differences between the two, really just understand their use case at a high enough level. That should be good enough for the exam. Now speaking of their use case, what these are are methods of sending authenticated requests to your back‑end AWS origins that you've configured. However, it's important to know these are not an IAM role, nor are they an IAM user. These are completely separate from those type of entities. And, when you're using them, how it works with an OAC is you use S3 bucket policies to authorize access for the distribution ID and the CloudFront service itself. So there are two different types of principles and conditions you have to look for to grant access to your CloudFront origins distribution. Now, speaking of bucket policies, here's an example bucket policy for an S3 origin. You're going to notice here at the top, we're allowing this principal, the service for CloudFront. In addition to that, because this is an OAC we can both get and put objects. So we're allowing this OAC to do both. After the action, we have the resource. So, remember, these are very specific. In this case, we're saying all objects within our static bucket based on the syntax. So with this, our CloudFront service can get and put objects for this ARN. And then the last bit here, which is super important, is a condition where we are checking for a source ARN to match our distribution ID. What this ARN is is an assigned ARN that you get from AWS when you create your new distribution. So all of this has to be put into place to grant access to your origin access control to hit your S3 origin in a secure manner. Moving on to some more security concepts. When you use CloudFront, you can leverage AWS WAF, which is Web Application Firewall, and you can create web ACLs and then associate them to your CloudFront distributions. This is useful if you have any scenarios for needing to inspect and potentially block malicious web requests that are being made to your distribution. We cover AWS WAF in its own course within this learning path, but for now, just understand it's a Web Application Firewall service that works with CloudFront. You can also use another security service that we cover in a different course called AWS Shield. Shield allows you to easily mitigate and prevent DDoS attacks against your distributions. By leveraging this, which is built in, by the way, it helps improve resiliency of your apps and you can leverage Shield Advanced if you want to use paid features which are more advanced. We're not going to cover those here. Again, we'll cover those in a different course, but just understand it works with both WAF and Shield. Then the last thing here, which is a built‑in feature, is a georestriction feature, also known as geoblocking. Now, on the exam, you might see this as geographic restriction, just keep that in mind, but it all works the same. It's meant to allow you to prevent users from accessing content from specified geographic locations. With it, you can use both allow list, so you can deny everyone except for who's allowed, or you can do the opposite where you can specify exactly who you want to block. In addition to allow and block, you can also specify country‑based restrictions using built‑in CloudFront technologies, or you can leverage third‑party geolocation services to go ahead and handle allowing and blocking as well. So in other words, you have a lot of options you can use to implement georestriction. Big thing here is just remember why you would use this. You can allow or block specific geographic areas from accessing content. Now let's actually look at an exam scenario where georestrictions are useful. Let's assume you're working for a media company, and this media company has already decided and started to use Amazon CloudFront for their CDN. Well, you just got a new company mandate that's requested that all new content that gets released is released in different phases. Now, each phase that happens needs to include releasing to specific lists of countries, so not everyone can get it right away. What that means is you must make sure that viewers outside of the listed countries for each phase are not able to view the content. Well, this is perfect for using georestrictions in CloudFront, and you can leverage allow list for each phase. So you say, hey, country A, B, and C is allowed, that means everyone else is blocked. Again, this is perfect for this type of use case. Now, that's going to do it for the security portion of CloudFront. Let's wrap this up, and we're going to move on to custom domain names and TLS, coming up next.

CloudFront Custom Domain Names and TLS
All righty, for the exam, you're going to have to know how you can set up your own custom domain name with your CloudFront distribution, as well as implementing HTTPS. So let's go ahead and dive in. First things first, when you create a distribution, you're going to be given an AWS‑provided URL to leverage. Here is an example here. It's a random set of strings .cloudfront.net. Now by default, you can use HTTPS with this. However, that's not customized, right? So if you want to do that, there's some things we have to do, and we'll discuss those here coming up shortly. One of the important configuration options for your distribution is known as the Viewer Protocol Policy. This allows you to configure the protocol policy that is getting used for your viewers to access your content. So by default, for example, you might allow both HTTP and HTTPS traffic. However, for the exam, you need to understand you can require secured HTTP. So you can set your Viewer Protocol Policy to say, hey, I want you to redirect HTTP to HTTPS, or you can return custom errors and say, hey, only am I allowing HTTPS, nothing else. Those are two valid options for that type of scenario. Now in addition to those, if you want to use custom DNS with TLS, so HTTPS with your own custom domain name, when you're using ACM certs, which we'll cover in a different course as well, those Certificate Manager certs have to be deployed to the us‑east‑1 region in order to be used. They cannot live anywhere else if you're using them directly with your distribution. Really be sure you remember this for the exam. Certs for CloudFront need to be in this region. Now, if you're using HTTPS with an ALB as your custom origin, one of the options is you can set the Origin Protocol Policy to HTTPS Only. So, like the viewer, instead this is for the origin, so you're saying, hey, from my distribution, that connection to my origin, I want to set it to HTTPS Only. Well, if you do that, well then you need to request an ACM cert in the region where your ALB is deployed. So in other words, to have full encryption for the distribution, you deploy the certain us‑east‑1, and if your ALB is in us‑east‑2, well you need to deploy the cert in us‑east‑2 to assign to your ALB. That's a very tricky scenario that comes up on the exam. But, with that out of the way, we're going to go ahead and end this clip, and we're going to move on to a demonstration where we're going to set up our very own origin in CloudFront.

Demo: Setting up an S3 Bucket Origin in CloudFront
Okay, let's go ahead and jump into a demonstration. In this demonstration, we're going to put together all of the components we looked at earlier, and we're going to go ahead and set up an S3 bucket as an origin for a CloudFront distribution. Before we jump in, let's look at the architecture high level and what we're going to accomplish. What we're going to do within the console is we're going to set up or look at an existing S3 bucket that has a file in it, and then we're going to create our distribution to use that S3 bucket as an origin, and we're going to secure it with an origin access control. In addition to that, we're going to set up custom TLS and custom DNS so that we can use it, get our image, and view how it gets cached and invalidated, etc. So let's jump into the console now and get started. All righty, I'm now in my hands‑on playground here. Before we start creating a distribution, I want to review some of the infrastructure. What I've done is I created this CloudFront demo bucket with a random string, and I uploaded this image.png file. What this file is, if we open it, is the diagram we just looked at in the previous portion of this clip. So this is what we're going to use to cache within our CloudFront distribution. So I'll close this, go back to my bucket, and the other thing here is our Route 53 public‑hosted zone that's provided within this account. This is what we're going to use in order to set up our custom DNS. With that out of the way, I'm going to jump back into CloudFront, and let's create our distribution here. The first thing we do is select our origin. So notice, there are tons of options, we have S3, ELB, API Gateway, and even VPC Origins, with some other ones. Now for the exam, you need to be familiar with the ones I just highlighted here, S3, ELB, API Gateway, and VPC. VPC is for EC2 and EIPs. So for this, I'm going to choose my CloudFront demo bucket. We'll skip over the next portion, it's not important for the exam, you see it names our origin, which is the bucket name, of course, and we get down to origin access, so this is very important. This is where you choose what kind of access controls you want to use to talk to your origin. So since we're using a bucket, if it was public, that means the bucket needs to allow public access. Now, while this would still offer performance gains, it's a security issue. So we're not going to do this. If you look at our bucket, I go to permissions, you're going to see we have block public access on, so this is a private bucket. So instead, what I'm going to do is choose Origin access control. So this is how we can use an OAC to restrict access for only allowing CloudFront to hit our bucket. So what I'm going to do here is create a new one, we'll leave the name the same, this is the default, which names it after the origin, you can give it a description, and we want to choose sign requests, we want this to have a signed authorization in it for it to be allowed to hit our bucket. So I'm going to click on Create, and now we have our new OAC to use. Now you're going to notice here what I just highlighted is we have to update that bucket policy, which we talked about in previous clips. So once we're done with this, we're going to do that. For now, we'll leave this, we'll skip down here, and you'll notice we could add custom headers to the origin request. So if we wanted to, we can make up a header name here, and then we could give it a secret value. So this will now set this header name with this value every time the origin is hit. You can use this for custom application processing, security,etc., the use cases are kind of endless. Below this we see origin shield, so we can set up an additional caching layer by enabling this. Now, I'm not going to do this because we won't be able to in the sandbox, but this is an option. Under Additional settings, this is a little out of scope, so we're not going to cover this stuff, instead, I'm going to skip down to this cache behavior section. Now, this also, to a certain extent, is a little out of scope, but there are some things I want to talk about within this. First off, you can compress objects automatically, which we want to enable because that offers performance gains. But the next thing here is the viewer portion. Remember, we talked about Viewer Protocol Policy. So do you want to allow HTTP and HTTPS? Do you want to redirect HTTP to HTTPS? Or do you want to offer the most secure solution, which is HTTPS Only? So I'm going to choose this. I want to restrict it to only allow secure HTTP. What that means is if I tried to view this distribution via a non‑secured protocol, I'll get a denial. In addition to that, since we're using an OAC, we have several options for the allowed methods. The default is to GET into HEAD, but you can select other options including PUT, POST, PATCH, and DELETE. OAIs, or Origin Access Identities, would not allow you to do this. For us, we're just going to do GET and HEAD because we want a simple setup. And the next thing we have is restrict viewer access. So if you want, similar to signed URLs or pre‑signed URLs in S3, CloudFront offers something similar. You can have signed URLs and signed cookies specific to your distribution to allow trusted access. So at a high level, without getting too in depth, you can use this to allow your distribution to be accessed to premium users. So similar to a pre‑signed URL use case, you can use CloudFront‑signed URLs as well. We're not going to do this, setting this up is out of scope, but understand that CloudFront does offer this option. Moving on, we get down to the caching settings. So, we're going to use the cache policies that are default here. This is going to be out of scope for this exam, it's more specific to security and networking, but we're going to use the caching‑optimized default policy here. After we leave this as the default, under Additional settings, there is something I want to go ahead and look at here. What we have here is field‑level encryption as an option. You can specify this if you allowed PUT, POST, PATCHES, and DELETES to essentially encrypt specific portions of the PUT and POST requests going to your origin. This can come up on the exam, it's very rare, but understand if you need specific encryption, you can use field‑level encryption for your origin requests. I'm going to minimize this, and then we get down to Function associations. Now, we're going to cover functions later on, but at a high level, you can use an edge function to perform custom logic closer to the end users. Again, we're going to talk about these in depth later on, so I'm going to leave these as default and use none. Skipping down, we get to Web Application Firewall, so this is the AWS WAF configuration we talked about. Do you want to enable it and protect it via a WAF ACL, or do you want to just say no, we don't need to worry about it? This will come down to the use case. If you need to block malicious requests specific to application layer traffic, you would enable WAF. For this demo, I'm going to say do not enable so we don't run into any permission issues, and then we're going to skip down here and go down to Price class. For the exam, this might come up, there are three different price classes: use all edge locations, which offers the best performance. You can restrict your edge locations to North America and Europe. Or you can restrict them to only be used in these particular regions and continents. Typically, if you're going to use CloudFront, you're probably going to want to use all edge locations, that's the point of using a global CDM. So for this, we'll use all, and then we get down to our custom domain stuff. So what we can do here, instead of using the default distribution URL, we can add an alternate CNAME. So let's do this here. What I'm going to do is call thisapp., and then we're going to copy and paste in our hosted zone. So let me copy this, I'll paste it in, and there we go. So now we can use our custom domain name once we get all this stuff set up. In addition to that, I want to implement TLS for this domain name. So under Custom SSL cert here, I'm going to click on Request cert and open this in a new tab, and it's going to bring me to Certificate Manager. Now we'll cover Certificate Manager in a separate course within this learning path, but at a high level, it allows you to request public certs to attach to Amazon resources. So I'm going to click on Next, I'm going to give it my fully qualified name here, which is the one we just typed in. So let me copy, go back, I'm going to paste this in, and we're going to accept the default, so DNS validation, RSA encryption, I'll go to request, and here we go. So now if I refresh, we see it's pending validation, and what we're going to have to do is validate that we own this domain. So if I go back here, sometimes the UI gets bugged, we'll see that it's pending. I'm going to create records in Route 53 because we're using that for DNS management, I'll confirm, and what this did is it just created records for us automatically in our zone, and you can see right here, which I just pointed at. So it created this to validate that we own this domain. Since we own this domain and we validated, eventually when I go back here, and I list, this is going to be issued, which means it says yes, you own this domain, you can have this TLS cert to use at your will for your AWS resources. So now since we have this, I can go back, I'll refresh, I'll choose my cert here, and there we go, we now have custom domain, and TLS set up for that custom domain. Now within this there are some settings, legacy clients. So if you wanted to, you could enable this, which allows specific clients that don't support the most up to‑date edge location customizations. This is not very typical, so just like they say, a lot of people do not need to use this, I would recommend you don't do that because look how expensive it is. Skipping down, we can then pass in a default root object. So for this, what we want to do is I want to make it image.png, so anytime I hit my distribution, I want it to automatically load this root object. Now obviously this is dependent on the scenario, but for simplicity and speed, I want to do this. I'm not going to have any other objects, so I'll just say, hey, this is the default object to pull. After that, the last thing I want to cover here is you can enable logging. So you can say, hey, who is hitting my distribution? You can turn it on, you can log cookies, and you can say, I want to deliver it to CloudWatch, Kinesis Data Firehose, Amazon S3, or S3 Legacy. The big three here are the ones I just highlighted. Understand you can turn on logging for your distributions and send them to CloudWatch, Kinesis Data Firehose, or S3. To speed this up, I'm going to turn this off, but again, for the exam, understand you can enable logging at your distribution. I'll click on Create, and there we go. Now you're going to notice we have this popup up here. Remember, we need to update our bucket policy to allow our OAC access. So lucky for us in the console, it allows us to click this Copy policy button, I can go to my bucket, Permissions, go to Bucket policy and edit it, and just paste this in. So you'll notice it looks just like when we talked about it in a previous clip. We are allowing the service of CloudFront to get an object for this resource ARN, which is our objects within the bucket, and then we have our condition saying only this distribution ID. So this is what is allowing our OAC access to our private bucket. I click on Save changes, and there we go. So now the access is in place, I can clear this, and what we'll do is we'll wait for a moment because this is still deploying and it can take some time, but while this is deploying, what I can do is set up the DNS. So I'm going to go to Route 53, and I need to create a record to reference that distribution. So I'll create my record, we called it app., it's an A record, and it's going to be an alias type. This is important for the exam as well. When you're referencing CloudFormation distribution's select alias. I choose my endpoint, so CloudFront distribution, and it's going to restrict it to us‑east‑1 because that's the only place this lives. I then select my distribution, which matches, I'll click on Create records, and there we go, we now have our DNS entry that we can use once our distribution is ready to go. So what I'll do here is I'm going to select this, I'm going to open it in a new tab, and there we go, it was that fast actually. So even though it's deploying to other edge locations, the one nearest to me is already ready. So this is perfect, we get our image, it's via our custom TLS and domain name, and if I look at the headers here, let me go ahead and load up my console view. I'll zoom in, and if we look and refresh here, I want to show you something. If I select the original call, notice the headers over here on the right side, I'll zoom in a little bit more, you'll notice, Hit from cloudfront. So, this is already stored in our edge location, and that's because after the refresh, it already fetched it from the origin. But, what if we want to update this and it's already cached and it doesn't expire? Well, let's test that out. What I'll do is I'll go to my bucket, I'm going to drag and drop another image in, and this is an updated image with the same name. I'll click on Upload, we'll close, and now we have this new image here, which we'll look at in a moment. So now if I go back, what happens when I refresh? Well, I get the same image because it's the same file name and it's already cached. So how can I force this to update? Well let's invalidate it. So I'll go back to my distribution, find invalidations up here, and I'm going to create a validation for all objects. Now typically you would use some type of specific pathing here, but we have one object, so I'll create this at the root, and what I'll do here is I'm going to fast‑forward, this takes a moment. All right, so that took 20 seconds, I cut forward, it's completed. Now if I go back and I refresh, boom, we get the updated image, so that file was invalidated. If I select this, you're going to see Miss from cloudfront. So this fetched from our origin, and now it cached it. So now if I refresh one more time, we're going to see it Hit from cloudfront. So there you go. We've now forced an invalidation, we updated that image, it's all working. Let's go ahead, we're going to end this demonstration. Hopefully you learned a lot about how to configure your bucket as an origin, and we're going to move on to the next clip whenever you're ready.

CloudFront Functions and Lambda@Edge Functions
All right, we just got done with a demonstration on creating your very own distribution and setting up an S3 bucket as an origin, let's now move into what is known as an edge function, specifically CloudFront functions and Lambda@Edge. First up, let's quickly review our existing scenario here. If you remember in one of the previous clips, we reviewed this exact architecture. With this, we reviewed, hey, your request gets forwarded to an edge location. If your content's there, it's retrieved. If it's not, it goes to the Regional Edge Cache. If the content's not there, it fetches it from your origin and then returns it from there. Now, within this flow, you need to understand the overview of requests and response. So there are four portions of a request and response that you need to know when you are using CloudFront, especially when we are talking about edge functions. These four portions are the Viewer Request, the Origin Request, the Origin Response, and the Viewer Response. With that overview out of the way, if you ever need to perform custom processing at the edge, there are a couple of options, and let's review those now. First, we have a CloudFront function, and then in addition to that, we have a Lambda@Edge function. Now let's explore both of these in depth right now. The first one is a CloudFront function. These are lightweight functions that are managed within the CloudFront service itself. In other words, they're a very lightweight compute that you can use to perform custom logic. However, these only support vanilla JavaScript language, so you are limited on what you can write these in. But, even though they're limited, they are perfect for high scale, latency sensitive CDN customization requirements. So again, if you need to perform some type of custom logic very close to the end user, these might be a great solution. With CloudFront functions, they offer sub‑millisecond startup times and they can handle millions of requests per second. So again, these are great for latency‑sensitive customizations. With that, however, they do have less resource power, so they're not as powerful as the next thing we're going to talk about, which is a Lambda@Edge function. In addition to limited resources, but extremely fast startup times and scaling, these only work at the following two parts of the request and response. They only work at the Viewer Request portion and the Viewer Response portion of the entire flow that we looked at earlier. So in addition to being limited by the language and the resource power, you can only use them at two points. The other option is a Lambda@Edge function. These are Lambda functions that are managed within the Lambda service, and they support Node.js or Python languages, so you have some more powerful options. When you use these, they have to be deployed to the us‑east‑1 region. Again, us‑east‑1 is very common for a lot of these use cases, so hopefully it's a little bit easier to remember now. With your Lambda@Edge, they work at all four portions of the request and response. So Viewer Request and Response and Origin Request and Response. Now, one of the differences, which is very key, is that these are really only meant to support thousands of requests per second, but they can run up for a lot more time. So, CloudFront functions are very quick and a lot quicker than Lambda@Edge, and they support more scaling, but these give you more options and customizations, and they offer more resource power. So, it's really going to come down to this specific use case. And speaking of use cases, let's look at some use cases for edge functions in general. First up, performing basic authentication and authorization closer to your users. For instance, maybe you're checking JSON Web Tokens or cookies to authenticate users before allowing access to your origins. You can do that at edge locations. The second one here is customizing content based on user location, user devices, or user preferences. So maybe you're displaying different personalized greetings based on language‑specific content and locations, or you're running region‑specific promotions based on the user location for your e‑commerce store. Thirdly, you can serve different versions of a website to test user engagement. So for example, A/B testing, where you can randomly route users to different versions of your web page and track behaviors to see which platform is better. Fourthly here, you can actually optimize content delivery by compressing files or even converting formats based on the users. So for an example here, maybe you're compressing images or converting them to a different format based on a user's browser capabilities. And then lastly, very common on the exam, adding security headers or enforcing HTTPS connections. You can actually add different security headers like content security policy, strict transport security, and many, many more, all using edge functions. Now with all of those use cases reviewed, quick exam pro tip to wrap things up. Consider using edge functions if you ever need to perform any custom logic closer to an end user. And with that exam pro tip out of the way, remember, using edge functions, all of your use cases, and the differences between the two options, we're going to wrap up here and move on to a demonstration where we use one of these functions.

AWS Global Accelerator
All right, let's go ahead and move on to another similar type of caching service within AWS known as Global Accelerator. AWS Global Accelerator is a networking service that's meant to help send traffic through AWS's global network infrastructure via what are called accelerators. These accelerators send traffic to your configured endpoints. The purpose of this service is to help decrease any user latency and increase performance for your networking applications. One of the primary use cases is that it helps with IP caching and whitelisting because it provides you two anycast IP addresses that you can use to send traffic to different edge locations and then to your applications on the back end, which are your endpoints. Now, this is extremely similar to CloudFront because it does use edge locations. However, there are some primary differences you need to know for when you would use this service as opposed to CloudFront. For Global Accelerator, this is going to be primarily for TCP or UDP traffic. If it is focusing on either of those protocols as opposed to Layer 7, like HTTP, you're going to want to use Global Accelerator. It's also good for any scenarios where you have to have static IP requirements for multi‑region global deployments. That is one of the primary use cases. It gives you these two static IPs that you can use for global workloads. And, with that in mind, because of all of this, you get more control over failovers. Again, you can't choose which edge location that a user will get for CloudFront, but you can control failovers using Global Accelerator. Now we talked about endpoints and accelerators. Accelerators are what help go to edge locations, but let's talk about the supported endpoint resources that can come up on the exam. You can use Global Accelerator to point to Elastic IPs, so anything that you can attach an Elastic IP to, you can use with this service. You can also use Amazon EC2 instances. So if you're using default public IPs, it doesn't matter, you can use EC2 and point your accelerators to one of these endpoints, which might be a web application or a TCP UDP application hosted on an instance. Now, in addition to this, it also supports Application Load Balancers. Now you might be wondering, well, you said TCP UDP, and ALBs are for HTTP communication; that's true, but it does support it if required. This is why you really have to take the time and read through the scenario and pick out the key indicators. For instance, maybe you need that static IP address whitelisting ability. Well, that's what Global Accelerator provides over CloudFront. And then lastly here, honestly, probably one of the most common to appear on the exam is a Network Load Balancer, primarily because these support obviously TCP and UDP traffic. It is very common on the exam to see Network Load Balancers as endpoints with your accelerators within Global Accelerator. It's also important to call out, this is tricky. Application Load Balancers can be internet‑facing or they can be private. I know it seems confusing, but they can be private. Now, moving on, let's actually look at a high‑level architecture diagram on using Global Accelerator. On the left side here, we have an EMEA User and then we'll say we have a North America User. In between them and our workloads, we have our edge locations here, and then we have Global Accelerator, and on the right side we're going to have our endpoints. So let's go ahead and walk through this. When you set up Global Accelerator, you get assigned two static anycast IP addresses that you can use to point to multiple back‑end resources. This is one of the biggest benefits, again, of using this service. Once you set it up, the users can start making requests from around the globe, and those requests are going to get sent through the nearest edge location for performance, and then they use the internal network for AWS to avoid congestion and latency that you would typically get over the internet. So that is one of the biggest performance gains. Once the traffic is going through the edge and using the backbone network here, it then gets sent to your configured endpoints. So remember the endpoints for the exam. You can use ALBs, NLBs, EC2 instances, and anything with an Elastic IP attached to it. Now, when we look at this diagram, everything in this box here that I've just highlighted is internal to the AWS network backbone. This is how it uses this service to avoid that congestion, avoid that potential latency, and even avoid that potential network jitter that would typically occur on the public internet. Again, this is one of the performance indicators to use this service. Now wrapping things up real quick with an exam pro tip: you want to think AWS Global Accelerator if you ever need to minimize latency and packet loss for TCP and UDP applications, especially for multi‑region deployments. This is a perfect scenario for this service. Now that's going to do it for our overview on Global Accelerator, let's move on to talking about security and health checks, coming up next.

AWS Global Accelerator Security and Health Checks
Okay, we now know how to accelerate our TCP and UDP workloads using Global Accelerator, let's talk about some of the security components and health check capabilities you need to know for the exam. First up, let's dive into health check capabilities. When you use standard accelerators within AWS Global Accelerator, they're going to automatically perform health checks for you of the endpoints that you've configured and associated with your static anycast IP addresses. Because of how these operate, traffic will only ever get sent to healthy endpoints. So it's kind of performing like a load balancer, it's going to avoid sending traffic to that endpoint. With that in mind, however, you can use those default settings, which we just talked about, or you can customize them to meet your needs, so you can customize the protocol, port, timeouts, etc., and make them fit your requirements. One of the key underlying ways this works is that Global Accelerator leverages Route 53 health checker data, which we talk about in a completely different course, and you do need to understand how those health checkers work. But, what is happening is Global Accelerator uses that data, and that's how it determines what is healthy. An exam scenario is you need to be sure you're allowing traffic from the health checkers within Route 53 in order to leverage this capability. What that means is you're allowing it within your security groups, network access control lists, and even your internal firewalls. The long story short for these is these allow you to perform quick failovers to other endpoints. So that, again, is one of the biggest benefits of using this service for decreasing latency for TCP and UDP workloads. Moving on to security concepts you have to be aware of for the exam. Your ALBs, NLBs, and EC2s, when you're using them as endpoints, enable internet traffic to reach them directly via those endpoint configurations. That's important to know. Essentially, you're allowing internet traffic in. The reason we call this out is it is true even for resources deployed in private subnets. I know that seems very confusing, but this is how the service operates, it allows the internet gateway, which is required to pass traffic directly to those resources. Now with that in mind, you don't have to have public IP addresses on your ELBs or your EC2s, and it still works. I know for the most part that's going to go against the common knowledge where you have to have a public IP to use an internet gateway, but the thing to keep in mind here is you're not using the internet gateway in a typical manner, it's only allowing the Global Accelerator service to use the internal backbone for the AWS network to talk to your resources. Now, in addition to the networking side of things, security groups are still going to apply rules to any traffic reaching your resources. So be sure you're allowing or denying exactly what you need. The big thing to keep in mind here is you can still have resources in private subnets, but you have to have an internet gateway in place for this service to reach those components. Now that's going to do it for security concepts and health checks regarding Global Accelerator. We're going to move on to another caching service specific to applications, coming up next.

Amazon ElastiCache
All right, we've talked enough about network caching, network latency improvements, let's move on to application‑specific caching, and in this, we're going to talk about Amazon ElastiCache. Amazon ElastiCache is a managed, distributed in‑memory caching service for the following open‑source technologies. You can use Memcached, you can use Redis, and you can use Valkey. Now it's important to understand these tools are not specific to AWS, but the service itself is, it's a managed service to leverage those technologies. By leveraging ElastiCache, it's meant to help you meet high‑performance, low‑latency application‑specific requirements, so it's not specific to networking, it's specific to applications. When using it, it's really good for heavy and repeated read requests. So if you're constantly polling large datasets and reading large datasets from an application, you can use this to speed those up. Now, an important note here, that is when you use the non‑serverless edition, you deploy what are known as clusters for your ElastiCache nodes. For those clusters, you control access to them via security groups. So network security controls are done via security groups. Now, moving on to a quick exam pro tip regarding the service. Using this service requires making changes to existing code in order to properly use it. What that means is you have to refactor and make changes to things, so it's not just a quick switch. That's a very important thing to remember for the exam. Moving on to use cases. A popular use case for this service is caching for your web applications, so you can cache database query results, you can store session data, and you can even store commonly‑retrieved API responses, all to reduce load on your back end and speed up response times. It's also very common to use this for session stores, so maybe you want to manage user sessions in a distributed web app and you can maintain consistent session data across multiple servers. A very common example could be e‑commerce applications where you want to store your user data like cookies, commonly‑polled products, etc. This is a common scenario on the exam. And then thirdly, it's very good for online gaming leaderboards, so you can maintain online leaderboards for a multiplayer game, and you can update player scores and rankings in real time for users to see, and it caches that data for quick responses. Now let's go ahead and compare the three different technologies at a high level. You're not going to need to know these different services and techs very in depth, but you need to know general high level when you might use them. So what we're going to do here is quickly run through the comparisons and differences. For this, Memcached is going to be specifically for simple data types. The other two are better for complex data. They all allow encryption‑in‑transit, but a big thing here is Memcached also does not support data tiering, so if that's a requirement, look at Redis or Valkey. Also, Memcached has no high‑availability built in. The other two do. So again, if that's in your scenario, immediately rule out Memcached. In addition to high‑availability, Memcached also doesn't support automatic failovers. So so far you can see Memcached is really good for very simple, non‑redundant situations. The other two are going to be better options for more complex, highly‑available scenarios. And then lastly, the cherry on top here is Memcached only has limited backup support, but the other two fully support all types of backups. So again, if you need something more complex on the exam, I would say lean toward Redis, but if you need something very simple with limited configuration options, you might lean toward memcached. In addition to understanding when to choose one of these, don't confuse this on using DynamoDB Accelerator, DAX. That is completely specific to using for DynamoDB, and it requires no changes to your code. That is a drastic difference compared to ElastiCache. And with that being said, let's go ahead and wrap this up, and we're going to move on to our module summary and exam tips clip before we close off this module.

Module Summary and Exam Tips
Okay, way to hang in there. Once again, we've reached the end of this module, so let's have a quick summary, and I'm going to go over some exam tips I think are good for you to take along with you into your certification exam. First things first, four questions you should ask yourself within your scenarios. Firstly, can it be cached? For instance, some things just can't be cached or they don't make sense to be cached. But, if it can be cached, and you need to, what kind of cache should you use? Do you need some type of application cache, networking caching, etc. Building off of the second one, for a third question, how does the content get updated and how does it get retrieved? Are you constantly refreshing it? Does it have to get polled? Do you need to do fetching to your origins? Or do you need to have static IPs, etc.? These are all three important questions to ask yourself. And then the last one, do you need anything besides speed? If it's just speed, that might simplify some of your options; otherwise, really take the time to read through requirements and see some of the key indicators that would lean toward one service or another. So starting things off, let's review CloudFront. Remember, this is the service for caching content at edge locations to speed up delivery of data to end users around the world. It is meant to work with HTTP and HTTPS traffic, so application‑layer traffic. For this exam, you have to be familiar with specific security configurations, including using Origin Access Controls and Origin Access Identities. You need to understand how and when to use georestrictions. You need to understand that it integrates with AWS Web Application Firewall, as well as AWS Shield. And then lastly, you need to remember ACM certs, when you want to use custom DNS names with TLS, have to be deployed in us‑east‑1. That is a common scenario to come up on the exam. And then the last really big point here, you can use this service for both resources in AWS, as well as on‑prem. It just needs to be able to reach the service that it is caching content for. That's tricky, it comes up sometimes, so be aware of that. Also remember, you can invalidate caches in your distribution. So if you want to update files and force people to start fetching newer data, you can do that using invalidations. Moving on to the next topic, edge functions. Let's compare CloudFront functions and Lambda@Edge really quickly. CloudFront functions are only going to use JavaScript, but they support millions of requests per second, so they scale very quickly and very easily. However, you can only use them at Viewer Requests and Viewer Response portions of the entire request and response. From a positive side though, they do have sub‑millisecond startup times and very quick execution. So if you need something very fast, very short, very low‑latency, this is usually a really good option. And then the last thing here is they are managed entirely in CloudFront, so you don't manage them outside of the service, they live in CloudFront. When we move on to Lambda@Edge, these allow more customization, so you can use Python and Node.js to write these. These are not as highly‑performant, they allow up to thousands of requests per second as opposed to millions, but they do allow you to use them at every portion of a request and response. So Viewer Request and Viewer Response and Origin Request and Origin Response. In addition to those, they're also going to be better for longer‑running scenarios. So if you have to do something that takes several seconds, you might lean Lambda@Edge. And then lastly here, as opposed to CloudFront, these are managed entirely within the Lambda service within us‑east‑1. Remember that for the exam, they're in Lambda, and in us‑east‑1. Either way, edge functions in general are meant to allow you to perform custom logic closer to the end users. That is a key indicator for the exam to use an edge function. Now shifting gears and talking about Global Accelerator. Remember, this is a service that uses edge locations to help latency for TCP and UDP applications. These are specific protocols to keep an eye out for with regards to latency improvements and caching capabilities. The reason I say this is that's going to be a key differentiator compared to CloudFront. The exam likes to test you on knowing when to use this or CloudFront for your service. When you're using Global Accelerator, you get provided two anycast IPv4 addresses, which are great for whitelisting and IP caching. In addition to that, it does support health checks of your back‑end applications using Route 53 health checker data. What that means is you have to allow those health checkers to reach your service, and once you do that, Global Accelerator will only send traffic to the healthy endpoints. Some key exam indicators to keep an eye out for for this service are any requirements for static IPs for globally deployed applications. That means multi‑regional deployments using the same set of static IPs. It's also really good for latency improvements specific to things like online gaming. Again, you have global users for online gaming, so this is a perfect use case for Global Accelerator to use the edge locations, and then the AWS Backbone Network to speed up network delivery. Moving on to Amazon ElastiCache for application caching. Remember, this is a managed in‑memory database and caching solution. To use it, though, you have to refactor existing applications, it's not something you can just plug in and start using, you have to take it into account. Within the service, it supports Redis, Memcached, and Valkey. So if you see any of those indicating services, I would immediately think ElastiCache. And then lastly here, just remember, Redis is going to be more feature‑heavy, so it allows more complex scenarios, as opposed to Memcached. And finally here, you need to use DAX with DynamoDB workloads instead of ElastiCache. This commonly comes up trying to trick you, so remember, ElastiCache is good for all other type of in‑memory caching, but DynamoDB should use DAX when we're specifically using that service. Now, that's going to do it for this module, feel free to review some of the content if you need to. We're going to end here, and I'll see you in an upcoming module.

Miscellaneous Services
Amazon Pinpoint
In this module, the last one of this course, we're going to cover some miscellaneous services that I've deemed as important enough to know, but we're not going to dive in into too much detail with them. First up on that list is going to be a service called Amazon Pinpoint. So let's go ahead and explore what this is. Amazon Pinpoint is specifically meant to enable you to engage with your customers through a variety of different messaging channels. How it works is you create what are called journeys to customize your engagements with your consumers. What that means is that this service is perfect for the following groups of users. It's a perfect homerun for marketing teams. In fact, it is designed for marketing engagements. You can send out communications, receive responses, etc. It's also good for business users. You might have a group of business people that don't really want to care about the technology, they just want to send out their engagements to their consumers and get responses, etc. Well, this is also perfect for them. On the exam, these are going to be the indicators. Anything talking specifically about marketing campaigns, user engagement, especially tracking user engagement, and sending emails to targeted audiences. Now let's look at one quick exam scenario for Pinpoint. Let's assume you're creating a marketing communications service that's going to target mobile application users. You need to be able to send confirmation messages with SMS to your different users, and those users are going to have to be able to reply to those SMSs. You also want to store their responses for 365 days for future analysis. Well, in this scenario, I've highlighted the key terms that I would consider key indicators. You're targeting mobile app users. They are replying to SMS messages, and you want to store their responses for up to a year. In this case, in the middle, we can leverage Amazon Pinpoint. Remember, it is specifically designed for sending marketing messages, push notifications, emails, and even SMS text messages, so it makes it ideal for this use case. When you create your journey, you can then target your users via your SMS text. Once they receive the SMS text, they can then respond to the journey and the queries, and you can actually send those responses directly to a Kinesis Data Stream. Now remember, Kinesis Data Streams offer up to 365 days of data retention, and with it you can pull off those responses and perform analytics in real time. In addition to Kinesis Data Streams, you can also use services like Lambda and Amazon S3 to go ahead and trigger responses or just send different information to. The big thing to take away here is that you can use Pinpoint with their journeys within the service to target users. Let's go ahead and wrap this up, we're going to move on to the next service whenever you're ready.

AWS Amplify
All right, next up on our miscellaneous service journey through the cloud, let's look at AWS Amplify. AWS Amplify is a service within AWS that offers people and users different tools and libraries for front‑end web and mobile developers so that they can quickly build full‑stack applications in the cloud. It offers the following types of toolsets to speed up development. It offers plug‑and‑play front‑end libraries and UI components. In addition to that, it offers back‑end building services, so it allows you to very easily and very quickly build your back end without having to worry about all of the underlying infrastructure, like you normally would using something like ECS or EC2. And, on top of back‑end building, it also allows you to host your front end. Again, that's part of the definition. It's for front‑end web and mobile developers to build full‑stack applications. Now, in addition to these different components and plug‑and‑play libraries, it is also meant to provide easy authentication and authorization for your applications. Again, it removes some of the burden of having to stand this stuff up, and you can leverage the service to handle it for you. The big thing to look out for on the exam is when you need simplified development for a less‑experienced team. For instance, if you just have a developer and they need to stand up a full‑stack application quickly and easily, you might consider this service. Now, within Amplify, there's something called Amplify Hosting, and this is a Git‑based workflow for hosting full‑stack serverless web applications, and it handles continuous deployments. This feature within the Amplify service currently supports a large majority of popular frameworks and libraries and toolsets. Now, speaking of frameworks and toolsets, a common thing to keep an eye out for is server‑side rendering with your serverless applications. This service itself supports most of the most popular frameworks for server‑side rendering when you need to use it. So Next.js, Astro, Nuxt, etc., pretty much any server‑side rendering framework with a custom adapter can be made to work with this service. And with that being said, I can tell you, if you need server‑side rendering with a serverless app, please remember, Amplify is a great choice. You can't do that with static website hosting in S3 natively. It also supports single‑page application frameworks, so React, Angular, View, and Ember, just to name a few. And then lastly, if you have to use a static site generator to build your services and build your applications, it supports some of the most common ones as well. So Gatsby, Jekyll, Hugo, etc. Again, the big thing to take away here is it allows you to leverage pre‑built frameworks and toolsets to easily host full‑stack applications. Now on the exam, consider Amplify for anything related to managed server‑side rendering in AWS, especially if there's easy mobile development requirements or you have developers running full‑stack applications. And with that out of the way, let's end this clip, and we're going to move on to another service that you need to be aware of for the exam.

AWS Device Farm
All righty, up next, let's look at AWS Device Farm. What this service is is a service specifically meant for application testing for testing and interacting with Android, iOS, and web applications. With this service, it's actually usable on real actual phones and tablets that are hosted by Amazon Web Services. Now when you use this service, there are two primary testing methods. There's automated, and then there's remote access. Now we're going to look at these here in a moment, the big thing to remember here is that you use it for mobile testing. And, speaking of remembering, here are some exam indicators to keep an eye out for: any scenario that requires app testing on mobile devices in AWS, and more specifically, if you need actual phones or tablets for automated testing. These are excellent indicators for this service because AWS hosts those devices for you and you can just use them. Now, speaking of using them, let's look at the two testing methods before we wrap up. The first was automated. This is where you can upload your own testing scripts or you can use built‑in tests for automated parallel tests on multiple mobile devices. Again, remember, this supports Android devices, iOS, and even mobile web browsers. The second type was remote access, so this allows you to literally do what it says, access remotely those devices. By using this method, you can swipe, you can gesture, and you can interact with the devices in real time via a web browser. So in other words, remote access is good for manual one‑off testing, and then automated is more useful for, of course, automating bigger test frameworks and test guidelines. Now that's going to do it for AWS Device Farm. Again, remember, this is perfect for testing on mobile devices, especially when you don't want to buy your own and you want to leverage something managed by AWS. And, with that being said, let's move on to another service within AWS called AWS Wavelength.

AWS Wavelength
Nowadays, 5G cellular networks are pretty much everywhere in common acceptance. Well, with that, AWS Wavelength is there to bridge the gap for leveraging those 5G networks. What this is is a service that allows you to build applications that require edge computing infrastructure to deliver low latency to mobile devices and end users. This is specific to 5G networking using cellular carriers. Now it's also used to increase the resiliency of existing edge apps. So if you already have something deployed in the cloud and you want to increase resiliency, you can do that with this service. How it works is it deploys standard compute and standard storage within AWS services to the edge of communications service provider networks, so things like Verizon or Vodafone. It literally allows you to extend your VPCs into what is called a Wavelength Zone, which is a separate zone specific to one of those carriers, and you can see an example here for us‑east‑1, specific to Verizon. Now I'm going to tell you, on the exam, this is typically used for specific 5G networking requirements. In addition to that, try not to confuse using this service with something known as a Local Zone. Local Zones are also more specific to local regulatory or data sovereignty laws and requirements, so do remember that as well; the big thing to remember here is that it's for 5G networking, more than likely. Let's go ahead, we're going to wrap this up, and we're going to move on to another service called Amazon AppFlow.

Amazon AppFlow
All right, you might recognize this application service name from earlier in a different module, but let's go ahead and review it much more in depth now, Amazon AppFlow. Amazon AppFlow is an AWS service that allows for fully‑managed integrations for securely exchanging your data between a SaaS application and AWS services. A more specific example would be transferring data to or from Salesforce and Amazon S3 buckets. When using this service, it supports most of the common SaaS providers, so Salesforce, Zendesk, and ServiceNow are some of the more common ones. As far as destinations go, you can configure Amazon S3 buckets and Redshift tables, or you can actually even use other external SaaS services and providers. And, once you're configuring your source and your destination, well, you have to schedule your exchanges, so, you can either schedule them on a repeated schedule, you can trigger them based on events that happen, like EventBridge, or you can go in and you can start them on‑demand for a one‑time trigger. Now moving on, let's review an exam scenario from an existing module. What I'm going to do is go through this a little quicker, and we're going to change some of the verbiage, but this is extremely similar or basically almost identical to this module here where we reviewed this with S3 events. Let's assume you're using Amazon EC2 instances to receive data from a third‑party vendor, and they need to upload that data to an S3 bucket. Currently, the same EC2 instance is used to both receive and upload the data, as well as send notifications to an admin after all of this is done. Recently, you've really noticed a lot more slower performance, and you want to improve that performance as much as possible while also maintaining the least amount of operational overhead. Well, if you remember this scenario from event‑driven architectures, again, this works really well with S3. So the key thing here that I want to call out here is the top portion. AppFlow is meant for easily transferring data between SaaS applications and AWS services like Amazon S3. With this, you can set up and automate secure data exchanges, or you can trigger them manually. So again, feel free to review this architecture in that other module, I just wanted to bring it up again because, well, it covers AppFlow. Now, an exam pro tip to wrap this all up: Amazon AppFlow is specifically designed to integrate with SaaS sources for data ingestion. If you get something like that in your scenario, I would consider this service. The reason it's so good is it offloads operational overhead and it improves application performance because it handles all of those transfers for you, you just configure sources and destinations. And with that out of the way, let's wrap up this service, and we're going to move on to our final miscellaneous service here called Amazon SES.

Amazon Simple Email Service (SES)
All righty, let's wrap up this module's last service, Amazon Simple Email Service, otherwise known as SES. This is an AWS service that's meant to serve as an email platform for easy and cost‑effective methods to both send and receive emails. What it does is it allows you to use your own email addresses and your own domains to both send and receive your business emails. It's meant to essentially solve for deploying a large‑scale email solution, so it offloads some of that burden for you and you can just focus on the actual workloads. One of the most important features that it offers is DKIM support, so D‑K‑I‑M. This is a key indicator for email services within this exam. It does allow you to basically send emails from anything that can make an API call to this service, so EC2 instances, on‑premise VMs, ECS containers, or even Lambda functions. And in addition to that, it allows you to get stats about your email, so you can see how many bounces occurred, how many complaints have come through, and you can even see successful deliveries. Now, one quick exam tip to wrap this service up. Think this SES service if you need to reduce time resolving complex email delivery issues and you want to minimize operational overhead. Those are key indicators for SES. Now, in the real world, I'm going to be honest, this is a huge pain because it's really hard to set up and get to work properly; however, for the exam, you need to keep it in mind for this particular type of issue. Now let's go ahead and wrap up, and I'm going to see you in our summary and exam tips clip.

Module Summary and Exam Tips
Okay, way to hang in there. This is the final clip of this entire very long course. So let's go over a quick module summary and some exam tips. First up, Amazon Pinpoint. Remember for the exam, this is perfect for marketing campaigns, tracking user engagements, and sending emails to targeted audiences. It's easy to confuse this with SES from an email standpoint, but as soon as you see engagements or targeted audiences, I would think Pinpoint instead of SES. We then talked about Amplify. This is perfect for hosting full‑stack serverless web applications with continuous deployment, and it's going to be specific to having to leverage server‑side rendering, single‑page applications, or maybe a static site generator. Again, this offers more toolsets and configuration options as opposed to having to set things up like static website hosting in S3 yourself. More specifically, server‑side rendering, which is not possible doing it yourself. The third one here was Device Farm. Remember, we talked about Device Farm to use for automated or manual app testing on mobile devices. The key thing here is that these devices are hosted in AWS. So what this does is it allows you to go ahead and leverage their devices for remote access testing or automated testing for your mobile apps. And then we also talked about Wavelength. Wavelength is perfect for extending VPC capabilities for your compute services that are specific to 5G workloads. Again, try not to confuse these with Local Zones, these are typically used for cellular 5G networks. And then the last service I want to cover here is AppFlow. Remember, this is perfect for securely transferring data to and from SaaS providers and AWS services like Amazon S3. Now, that's going to do it for this entire course. I want to thank you so much for your time, I know it's extremely valuable. Hopefully you've learned a lot and you feel comfortable moving forward to your next course while studying for this exam. Feel free to connect with me on LinkedIn. We're going to go ahead and end this here. Once again, thank you so much, and I'll see you soon.
