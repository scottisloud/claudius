Amazon Relational Database Service (RDS)
Reviewing Relational Databases
Hello, and welcome to this course in the AWS Certified Solutions Architect ‑ Associate learning path. In this course, we're going to talk about storage services, we'll look at databases, we're going to discuss machine learning, and talk about big‑data analytics. In this very first module, we're going to look at a service called Amazon Relational Database Service. First up, let's talk about relational database management systems, or otherwise RDBMS. These are extremely critical that you understand when you're designing architectures for anything, not even just the cloud. A relational database, in the simplest terms, is a type of database that's going to store different related data points in the form of what is called a table. Now, tables are meant to store logically‑connected data, in other words, related data. In those tables, you have rows, each row is considered basically an item. With each row, you also have columns, so each column is going to be considered a field. Now let's actually look at a relational database example for a table. In this table, we have a simple customer table, we have Customer_ID here on the left, we have their first names and their surnames, along with their addresses, country, etc. If we were going to go ahead and look at related data points here, well, our Customer_ID of three right here is the entire item, and then Address_1 would be a field or a column that we're trying to select or look at for that item. So we can reference for Customer_ID three, any of these other fields, we can look at the first name, the zip code, etc, and the nice thing is these are all related data points. Let's actually look at some example use cases where you might use a relational database. They are used every day within financial systems, so think things like banking transactions, accounting software, payment processing, etc. They're also heavily used on ecommerce platforms, so different catalogs for products, managing customer orders, and even inventory. And speaking of inventory, they're very useful for inventory management, so you can keep things like stock‑level information, supplier information, and even order tracking to track your inventory stocking. They're used in HR, or human resource management, so you can keep employee records in there, you can keep payroll systems, etc. They're used every day as well within things like reservation systems, so hotel bookings or maybe you're booking an airline ticket to go to reinvent, that's a relational database. And then finally, they are used even within things like telecommunication companies, so your internet service provider accounts that host all of your data, your different billing transactions, etc. So, really the long story short here is that relational databases are extremely common and probably by far the most common type of database that's used today. For understanding relational databases, there are two different types of processing you need to be familiar with for this exam, the first is an Online Transaction processing, or OLTP. The main goal of using OLTP is for processing database transactions, so we're talking about things like processing online orders, updating your inventory management, or even managing your customer accounts for your application. On the flip side of that or on the other side of that, we have Online Analytical Processing, or OLAP. OLAP is primarily meant to analyze aggregated data and then you can use that aggregated data to either do things like generate different reports, you can perform data analytic queries, and using those queries and analytics, you can identify trends. Now, a lot of people, including myself, early on get these confused so let's look at some exam tips related to those. The first thing here, most commercial RDBMS softwares and services are going to use structured query language, otherwise known as SQL or S, Q, L, to access the data within the database. So, if you look back at that example, that table example you looked at, you would use a SQL query to get that field from that item. Regarding OLTP, these are again great for processing lots of transactions that are small, like customer orders, banking transactions, payments, etc., this is perfect for using Amazon RDS and relational databases. Regarding OLAP, we are not going to use the services we're about to talk about in this module for these type of transactions. You're going to use other services like Amazon Redshift, which we will cover later in this course, and this is going to be useful for things like data warehousing and other specific OLAP tasks, like analyzing large amounts of data and generating reports based on that data. The big thing to call out here, OLTP workloads are what we're going to cover within this module. OLAP is covered later on, so Amazon RDS, which we're going to talk about here in the next clip, is perfect for OLTP workloads. With that understood, let's go ahead and wrap this up here and we're going to move on to talking about Amazon RDS.

Amazon Relational Database Service (RDS)
All right, let's dive into Amazon RDS, which is also known as Relational Database Service. Amazon RDS is an AWS cloud service that's used for setting up, operating, and even scaling your relational database needs within the AWS cloud. Amazon RDS supports a majority of the most popular database engine types for relational databases. You can see, it supports things like Microsoft SQL Server, MySQL, Postgres, etc. Now, down here on the bottom right, we have Aurora, and that's their own proprietary flavor that we're going to cover in its own individual module because it's very important, however, I just wanted to call out that it supports pretty much every popular engine type. Now, why would you use Amazon RDS? Well, Amazon RDS is a managed service, so it's going to handle things like managing backups, it's going to handle this software patching for your instances, and it even allows you to incorporate mechanisms for automatic failover detection, and even recovery of your failures. When you use RDS for your relational databases, you can set up things like automated backup schedules or you can even manually create your own backups via what is called a backup snapshot. In addition to those benefits, you also can easily achieve high availability and optimal read performance, and we're going to cover how you can do so in some upcoming clips. Now, when you're trying to control access to your relational databases with this service, there are several different things you can do. You can leverage IAM to actually control who can manage the resources themselves, and you can put in place VPC Security Groups and NACL's to control the network access portion of your architecture. And then, lastly, they actually even allow you to incorporate encryption at rest, and, of course, in transit by default, and the encryption at rest can be customized to use whatever type of KMS key that you see fit. Now, when you're using RDS, at the traditional level, by default, when you start up your own database instance, you actually provision a DB instance, and this is going to be similar to how you would actually provision an EC2 instance, what I mean by that is you choose things like instance type, storage volume size, and storage volume type. Now a database instance, or DB instance, is an actual instance that is an isolated database environment in your account. On this instance, you can host one or more databases, so it's just like a normal server, essentially. You can have multiple databases on a single server and it's the same for a DB instance. Now, to interact with these DB instances, you would access their databases hosted via any other normal database tools, so things like DB Workbench, etc. However, you do not have the same access to the underlying instance like you would in EC2 instance, that's a big differentiator. Now, moving on, let's talk about instance classes. The DB instance class is just like an instance class for EC2, it's going to help determine the computation and memory capacity, so how many virtual CPUs and how much RAM. Now, you need to be familiar at a high level that they offer different classes. So, just like EC2, they offer different instance classes for different needs. They offer things like general purpose, which is good for general workloads and development and testing, they offer memory optimized instances, compute optimized instances, and even if you need it, burstable performance instances. Choosing the right instance class comes down to your workload requirements. Now, in addition to the instance class, there's also different instance storage types, so again, this is just like an EC2 instance. For RDS, though, they offer three main categories, there's provisioned IOPS, which is going to be perfect for input and output intensive workloads, they offer general purpose, which is going to be recommended for development and testing purposes only, but this is going to be the most cost‑effective option for actual workloads. Now I say actual workloads because they offer magnetic instance storage types as well, but these are not recommended for any workload, but they offer them as supported for backward compatibility. Generally speaking, you're going to want to use either provision IOPS or general purpose. You want to usually avoid magnetic. Now, moving on here, let's look at an instance syntax. You're going to notice this looks almost identical to choosing, again, EC2. The only difference really here is this prefix, db., so this is how you can tell this is a DB instance because it has db.m5.large. So this is how you would distinguish that this is for RDS. Now that's going to do it for our overview on RDS. Let's go ahead and wrap this up here and then we're going to start talking about more specifics coming up next.

Amazon RDS Networking
All right, welcome to the next clip in this module where we're going to start looking at the networking requirements and configuration options for RDS. Now, when you create a DB instance, they get created within a VPC. Now, which VPC you want to deploy to is completely up to you. You can use your own custom VPCs, or if you want to, even though it's not recommended, you can use default VPCs. Now that we understand that, let's look at some networking concepts specific to RDS. It is typically going to be a best practice to isolate your database instances within their very own subnets, so their own tier within the network. Now, when you're creating your instances, you have to deploy them to subnets, of course, and you create what is called a DB subnet group, or a database subnet group. These are supposed to be VPC subnets specific to deploying your database instances only, you want to avoid deploying anything else within those subnets that's not RDS related. Now, we covered this earlier at a very high level, but you do control network access to your instances via VPC security groups. Now, because these are just like EC2 instances underneath the hood, they have network interfaces and that's where you use security groups on. Remember, security groups are essentially a firewall or a virtual firewall that get attached to your network interfaces, and in this case, we can use them with RDS. Another option you can have is public access, so you can make your RDS database publicly accessible, and it's an option that you just go ahead and turn on when you're deploying your instance, and then your instance gets, of course, a public IP. Now I'm going to tell you this right now. You really never, ever want to do this. You should never have a publicly‑accessible database instance within any architecture, that is a humongous security vulnerability and an attack vector that you can easily avoid by just not doing it. So, while it is an option, you should never be doing this. Lastly, you cannot access your DB instance via things like SSH or RDP. Again, remember, it's a managed service. And typically, with managed services, you don't get as much control or customization or access. So even though this is running on an instance underneath the RDS service, you don't actually get access to that instance by default, that's not how this service works. Now, let's actually look at a subnet architecture for the networking for RDS. In this simplified diagram, we have two AZs, so it's multi AZ, and we have a three‑tiered architecture. Now, best practice is you're going to create multiple subnets and multiple tiers to host different portions of your application. So, for instance, at the top, we have a public web tier, in the middle we have a private app tier, and then on the bottom of the diagram, we have our private database tier. Now, with this subnet design, we're going to heavily rely on VPC security groups to set up what traffic is allowed from where. Now typically, database VPC security groups should only allow extremely specific traffic from very specific sources. And, an exam tip regarding that, remember, you can reference other security groups within your inbound rules. So, an exam tip for this diagram is we could reference our App layer security group within our database security group inbound rules, this allows for easier management for the networking rules because we don't have to track IP addresses manually or specify an IP CIDR. And last thing here, your database subnets should be private access only with no public access in place. Again, they do offer this option and if you need it, you can turn it on. However, you should always avoid this whenever possible. Typically speaking, the database layers are likely going to be the most internal layer from a security approach with literally the least amount of access granted. Now, if you look at this and we extrapolate it a little bit and remove some of the noise, you can see there are three tiers, so this is a typical three‑tier architecture that you'd see on your exam and when you're designing your own. Now, if you took the core services course from earlier, then you've seen something similar to this where we break down each tier, but the big thing to note is that multitiered architectures are going to allow for better networking segmentation, better security, and better access control. Now, with that out of the way, let's go ahead and wrap up this networking clip, and we're going to move on to backups and maintenance.

RDS Backups and Maintenance
Okay, welcome to this lesson, where we're going to start looking at relational database service backups and maintenance. First thing we want to talk about here are automated backups. With RDS, you can use automated backup scheduling, how it works is you specify a backup window when you're creating your instances. Now, during that window, if this is enabled, RDS is going to create a volume snapshot of your database instance. That first snapshot is going to be an entire instance that's captured and then it's incremental from there. In addition to the backup windows, you also set what is called a retention period. Now, retention periods are there to specify how long you want to keep your backups. Right now, at the time of this recording, you can specify 0 or 35 days. So, if you specify zero, that actually disables automatic backup retention entirely so you're not running backups. Any number between 1 and 35 is going to be the number of days after the snapshot was created that you want to store the backup for, so be familiar with this limit, it goes up to 35 days. When you're using automated backups, you can recover to any point in time during that backup retention period. So any of those automated backups that are getting created, you can use them and restore. And speaking of those backups that are stored, they get stored in S3. However, you have no access to the underlying storage, this is a very tricky and common exam scenario where they ask you about cleaning them up. You cannot access the S3 storage, you can only access the backups themselves and manage those, keep that in mind. Now, I put this in here as an exam pro tip and kind of as a reminder. Remember, you can set the automated backup retention period to zero if you want to completely disable them, keep that in mind for your exam. Continuing on to automated backups, I like to touch on this again because this is important. The first snapshot of a database instance is going to contain all of the data for the entire database. Every snapshot that is taken after that initial one is going to be incremental and what that means is it's only going to capture data changes between the one before it and the one it's taking now. So, in other words, the first one will take a while and the other incremental ones are a little bit faster. Now, a pro tip here again is you can enable automated backups, including snapshots and transaction logs which get backed up, to be replicated across region. So, if you need to for disaster recovery or compliance reasons, you can replicate backups to different regions. Now, it's important to understand this is not supported for what is called a Multi‑AZ DB cluster. Now, don't worry about this right now, just understand you can't use this for Multi‑AZ clusters, but we're going to talk about these much more in depth later on, I just want you to see the term and how it relates to backups with cross‑region replication for now. But moving on, let's shift gear to manual backups. Manual backups allow you to trigger backups literally whenever you want. You also get to specify the retention period because you have to manually delete them, so you can keep them around literally as long as you want to pay for them. Now, with backup restoration, of course, backing up is good, but they don't really do that much good if you're not able to restore them, so let's talk about restoring. You can actually create a brand new database instance by restoring one of your database snapshots that's taken whether it's automatic or manual. However, you cannot restore from a database snapshot to an existing database instance, so really remember that, it's going to create a new instance for you when you restore. Now, database instances perform what is called lazy loading and it's there to load data in the background to speed things up and make it more efficient. And if you try to access data that hasn't been loaded yet, it's going to download it from S3. More than likely, you're not going to see this particular point on the exam, but this is a very good thing to know when you're actually using the service. Remember that it lazy loads, and if you try to access data that hasn't been loaded in, it downloads it from S3 and then continues on in the background. In addition to that, you can actually use a completely different storage type when you are restoring your instance. So, if you have general purpose and you backed it up and you want to restore it, well, you can change that to provisioned IOPS, for example. And then, lastly, if you use MySQL engine versions or engine types for your relational databases, you can actually restore on‑prem backups that are sent to S3. Now, another exam pro tip, you pay for RDS storage, even if you have stopped your RDS instances. So, what this means is even if you stop your instance and the computes not running, you're paying for the volume storage, and you're going to be paying for the backup storage, so really keep that in mind. Now that's going to do it for this clip on backups and maintenance, let's go ahead and wrap things up. We talked about automated and manual backups, be sure you remember things like backup windows and retention periods, and we're going to move on to a demonstration clip coming up next, where we're going to create our very own instance.

Demo: Create an RDS Instance and Enable Automated Backups
In this demonstration clip, we're going to go ahead and we're going to work on putting all of the pieces together that we've just talked about. Let's create an RDS instance, and we're going to enable automated backups for that instance. All right, let's go ahead and get started creating our very first RDS database instance. I'm in my sandbox environment here, and I've loaded up the RDS service and that's brought me to my Dashboard. Now, from here, I'm going to select Databases, and I'm going to create a new database. And I just want to call out, we're not going to cover every single configuration here because we're going to cover certain aspects of creating instances and more advanced scenarios within their own clips, so for this, just keep that in mind if we skip some of these sections. Now, the first thing we have to do here is select the creation method. For the sake of this demo, please don't choose Easy create if you're trying to follow along because that will skip some of the settings we want to talk about. Choose Standard create, and then we can go ahead and get going. The first thing we have to do after that is choose the engine option, so notice all of the different engine types that are supported, pretty much every single popular, Relational Database System engine is supported with RDS, which is awesome. You can go ahead and ignore Aurora for now because we're going to talk about Aurora, which is a specific feature in RDS, within its own module. So, for this, just realize all of the different engine types, there's Db2, Microsoft SQL, MariaDB, etc. For this, I'm going to choose PostgreSQL. And under this, you can then choose your engine version. So notice, the amount of engine versions that are actually available for using Postgres with RDS. Now, obviously, if I choose a different version or engine type here, that's going to change some of these settings. But the big thing to know is that you can customize the engine version if you need to for any compliance reasons or maybe application compatibilities. For this demo, I will choose the default, and then we can skip down to Templates, so this is just an easy, quick way to essentially eliminate and set some of the settings for this process. For this, we'll choose Dev/Test just because it allows us to still have flexibility in what we want to change. So I'll choose Dev/Test and that comes down to Availability and durability. Now we cover MultiAZ deployments in a completely different clip, so we're going to skip those because we will cover those in their own demonstration and their own lesson later on. So for this, I'm going to choose Single DB instance because I just want to create one RDS instance. The next thing we see here are our Settings. So, the first thing under Settings is the instance identifier, this is how you're going to actually reference your RDS instance that is hosting your databases. So, we'll just go ahead and I'm going to call this database‑pluralsight, and this is what we will see in the RDS Dashboard after we create this. Next up is our credentials. Now, we're going to use basic password authentication here because we'll cover other methods later on, but this is the simplest way. What we're doing is saying, hey, RDS, this is going to be our master username to connect to our RDS instance and start interacting with our databases. So I'm going to leave this as default, and then we can select the credential management. For this, we'll choose Self managed, and this is where we can either create our own password or we can say, hey, just go ahead and auto generate one for me and I will get it later on by viewing credential details. So I'm going to go and select this option, but it is important to note you can create your own password if you need to. Moving on to Instance configuration, this is where you select your database instance class that we talked about, so do you need general purpose and standard, maybe you need memory optimized, etc. What we're going to do because this is in a sandbox, we're going to have to deploy a Burstable class, so I'm going to select db.t3.micro. The other ones won't work unfortunately due to some guidelines that are in place, so we're going to choose a micro instance. The important thing to remember here is that you do have options. Now, after I set my instance class, we go down to Storage. So remember, there are different storage types, there's General Purpose, Provisioned IOPS, and Magnetic. For this, we'll choose gp3, and I'm going to set this to the minimum amount of storage that you can have, which is 20 GiB. Moving down through this, we're going to go down to Connectivity. Now they make it easy for you to say, hey, yeah, I want to connect to an EC2 compute resource here in my VPC, and you could select all of this and it will create the required resources for that to be put in place. We're not going to do that. Instead, I'm going to connect from my local laptop over the public internet, which I want to call out is a terrible thing to do, but it's just for demonstration purposes. The next thing you say is, hey, what kind of networking do you want, IPv4, or do you want dual stack with IPv4, 6, and maybe both. For this, I'll choose IPv4. And from there, you can select your VPCs that you have created or you can create a new one. However, for this demo, I'm going to choose Default VPC. Let me tell you right now, please do not do this for any real workloads, it is a terrible idea. I am only doing this to simplify connection over the public internet to demonstrate how this works. So please, by all means, do not use the Default VPC for any workloads. With that warning out of the way, let's move down to the subnet group, which I will leave as the default. And this is just saying, hey, every subnet in every AZ, but you can create your own to lock it down to specific subnets, and I will actually cover creating a DB subnet group here coming up after this. So we'll use the default, and then it gets down to Public access. So remember, please don't ever put your RDS instance to be publicly accessible, it is a terrible idea. For the demo, though, I'm going to select Yes, because I want to connect from my local laptop to show you how we can connect to this. In any other scenario, you should say no, and you should set up some type of private connection via EC2 VPC networking or some type of VPN. So, for this, and only this, I'll select Yes. And then we can create a new security group here, so I'll just say allow‑all. We'll say No preference for AZs. We're going to skip down here, and that brings us to Password authentication. So, for this, we're going to choose this first option, and we'll talk about this stuff later on, but this is what's going to allow us to connect via that username and credentials password that was created for us. Scrolling down here, I'm going to turn off Performance insights, but this just allows you to gain insights and metrics on how your database is performing. So, I'll turn this off, and then I want to go down to Additional configuration and open this up. The first thing we can do here is actually have RDS create an actual database for us on the database instance. By default, it won't do so, but we wanted to, so let's just go ahead and give this a name. I'll just call my DB pluralsight. We can specify parameter groups which is outside the scope of this, but I also want to talk about backups. So, this is how easy it is to turn on automated backups, you can just check or uncheck it. So, with this, we're saying, hey, yeah, please take a backup and then retain that backup for seven days by default. However, you need to remember the minimum and maximum, it's from 1 to 35 days. If you select zero days, which is an option, this disables retaining automated backup, so keep that in mind. So, for this, I want to turn on backups, and I want to say let's save them for one day. After we enable this and set the retention period, you can also specify a backup window. So, if you say No preference, it's going to automatically try and do it when it's low activity, or you can choose a specific window where you know that you're fine to have a backup taken. So you can set this in UTC and say, hey at two UTC, take an automated backup within that window, and it should only last for a duration amount of time that you set. So we'll leave this as is, and the next thing I want to talk about here is backup replication, so you can enable your backups to be replicated to another region like we talked about. When you do this, you set a completely separate backup retention period for those replicated backups within your target region. You can also encrypt them and do some other stuff, but the big thing to remember here is you can enable cross‑region replication for your automated backups for RDS. Now, I'm going to disable this because it will break within the sandbox, but just remember, you can turn it on. Now, I'm going to scroll past some of this other stuff just because it's out of scope for this demonstration, and I'm going to go and click on Create database. Awesome. So, now we see that we're creating our database‑pluralsight instance here, which is perfect. And while this is creating, I just want to show you the subnet groups I talked about. So, over here on the left, there's Subnet groups, and this is where you can create a new subnet group. So, in theory, you can go in here and you can create one called database‑subnets, you can give it a description, and then you can select the VPC that you have created. So, if you follow best practice and you have a Multi‑tiered VPC maybe with a web, app, and database tier, you could select that VPC, you could select the AZs that your subnets live in, and then you can select those specific subnets that are specific to your database tier. So now when you're creating your RDS database instance, if you use this subnet group, then it's only going to spin up your RDS instance resources within those subnets within those AZs. So what I can do here is create this, we now have this, and now you can reference this when you're going through creating another database. So if I go in here, Create database, we'll just skip down here. I'll skip down to networking settings, and under the VPC section, you'll see we have our other subnet group, so this is how you could lock it down to a specific set of subnets for best practice. Now, I'm going to go ahead and skip all the way down here and cancel out of this. And what I'll do is I'm going to go ahead I'm going to pause here, and then once this is actually up and ready, we'll look at what was created. So I'm going to go ahead and pause and I'll pick back up here in a minute. Okay, so our database has now been created, or our database instance I should say, and you'll notice this banner has popped up here, and under this, you can choose connection details, and this gives you your credential information if you did it the same way we did. So I have my username, the password that was created, and then the endpoint for my publicly accessible database. And what I've done here is I've already copied and pasted this off screen into an IDE so we can use it to connect here in a moment. So now, our RDS is up and running, and what we'll do here is I'm going to try and go ahead and connect to it using a local program called pgAdmin on my laptop. So, while this is backing up and going through its other processes, let's go ahead, I'm going to jump over to my pgAdmin console, and we're going to go ahead and connect to this database. So what I'll do here is we're going to quickly go through this, I'm not going to walk you through using this tool set. Feel free to download this, I'll include the URL for this application, and you can follow along if you want to. But again, I'm not going to walk through how to use this program, it's out of scope. So for now, I'm going to choose Add New Server, I'm going to give it a name, I will leave the other settings, I'm going to go to Connection, and this is where we can set up some of our details. So, for this, let me copy and paste in my hostname. The Port is correct, this is the standard PostgreSQL port, and you should be familiar with this. The username is postgres like we left, and then let me copy in my password really quickly. We'll go ahead and save this, this is going to depend on if you really want to do this or not from a security standpoint, and I'm just going to click on Save. Now, what I can do is I can go ahead and try and connect to this database. So you'll notice, we've already connected essentially, you can see our database is here, and we actually have the one that we had RDS create for us, our pluralsight database on our database instance. And that's how easy it is. So we've now connected to our RDS instance over the public internet, we've listed our databases, and one of those was pluralsight, which is what we had the RDS service create for us. And that's it, that's how easy it is to leverage RDS, spin up a DB instance, we enabled automated backups, and we connected to it using a standard tool set on our local computer. Let's go ahead, we'll end this demonstration here, and we can move on whenever you're ready.

Amazon RDS Multi-AZ and High-availability
All right, let's go ahead and talk about Multi‑AZ deployments and high availability within RDS. First up, let's have an overview on Multi‑AZ deployments using this service. When you use Multi‑AZ deployments, what happens is RDS is going to create an exact copy of your database within another availability zone, and the default terminology for this is a standby replica. In addition to this, what happens is AWS will then automatically handle any data replication, so any changes that are going on updates, deletes, etc., that's going to be handled for us between the primary and standby instances all via RDS. You use Multi‑AZ deployments to enhance your availability for both planned and unplanned system maintenances and outages, so it's very good for failures and downtimes. For Multi‑AZ deployments, there are two models you must know for both the exam and just any real‑world scenario. The first is a Multi‑AZ database instance deployment, this is where your Multi‑AZ deployment is going to have a single standby database instance. You also have Multi‑AZ database cluster deployments, this is where your deployment is going to have two standby database instances. A huge differentiator here is Multi‑AZ database instance deployments have one and clusters have two standbys, by default. Now there's some other very key differences we're going to cover here in a moment, but just keep that in mind. Overall, though, Multi‑AZ deployments for high availability are not meant to be a scaling solution, these are strictly meant to be high‑availability solutions for failovers and disaster recovery, really remember that. Now, moving on, let's look at the different architectures for an instance deployment and a cluster deployment. With instance deployments, what happens is your clients direct all read and all write traffic to the primary instance, or the master instance, within your database deployment, so this would be like that endpoint we connected to in the previous demonstration when we connected locally. What happens is you create a standby in a separate AZ and the primary instance is going to automatically replicate all data and all changes to that standby instance in a synchronous manner, so there is synchronous replication going on from the primary to the standby. Now, a huge thing to know for this deployment architecture, you cannot use the standby instance for any purpose directly, so you can't read from it, you can't write for it, that's why it is not for scalability, it's only for failover. What happens is if the primary instance fails or is undergoing some type of maintenance, well, then the standby will automatically be promoted to be the new primary instance. Now, one exam pro tip, when you use Multi‑AZ deployments and there's a failover, you still reference the RDS connection using the same address. What's cool is RDS automatically handles changing that backend resource for you, so you're still pointing to the same address, but RDS is now going to send that traffic to the standby that was promoted to become the new primary. Now, if we compare this with a cluster deployment, there's some big differences. The first thing here is that RDS actually creates two readable replica instances in two different AZs, and you can use these to serve read traffic, and they also act as automatic failover targets for planned or unplanned outages. So, right away, that's a humongous difference, you immediately can use those standby replicas as reader instances. With this, what happens is clients can leverage the reader instances to send read traffic requests to, and they can use the writer DB instance to send both write and read. So essentially, cluster deployments allow you to more easily scale by default. However, with that in mind, remember, this is meant for high availability and disaster recovery. Now, with these three instances, the writer and the two readers, what happens is RDS is actually going to go ahead and automatically replicate your data from the writer instance to both of the reader instances in a semi‑synchronous manner. So, it's a little bit different than using an instance deployment where it's all synchronous, this is going to be semi synchronous. The big difference with cluster deployments and instance deployments is the architecture like you see, these deploy reader instances that you can use for read‑only traffic, instance deployments have a standby that you cannot leverage directly and it's only there for failovers. Now, one thing to call out here, DB instance failovers are said to take roughly between 60 and 120 seconds to complete. I call this out, because, in a future module, you're going to notice there's another feature called Aurora that fails over way faster, so that's why we call this out. The big thing to remember here is that it's not immediate. Now, let's wrap things up because there's a lot that we just learned. I want to go over some exam tips before we close out. Remember, Multi‑AZ instance deployments are a failover solution for high availability and disaster recovery. You should not be using it as a scalability approach. You need to know the difference between instance and cluster deployment models. And, remember, when failovers actually do occur, they're going to occur automatically, and the DNS endpoint will automatically be updated to point to the new primary instance that was actually promoted, you don't have to do anything regarding the address. In addition to that, you need to understand the supported Multi‑AZ engine types and they're listed here, you're going to notice it's pretty much every supported engine type that RDS actually supports in the first place. Now, that's going to do it for this clip on Multi‑AZ and high availability. Please do your best to review the differences on instance deployment models and cluster deployment models, there are very key differences that you'll need to know for this exam. With that being said, let's end here, and I'll see you in the next one.

Demo: Creating a Multi-AZ RDS Instance Deployment
All right, in this demo, we're going to go ahead and we're going to create a Multi‑AZ RDS instance deployment. Real quick, before we jump into the console, let's check out this diagram overviewing the architecture. Pretty straightforward, we're going to create a new DB instance in RDS, and we're going to create a Multi‑AZ version of that, so we're going to have a primary and then a standby. Now, remember, with this, you get synchronous replication to your standby so you have an exact copy of the same database. You cannot directly write to that standby, in fact, you can't even use it directly, you have to use the primary. What we're going to do is simulate a failover by rebooting the primary, and I'm going to show you how the database is still the same by looking at some of the data, and we're going to even look at some of the configurations. Now with that out of the way, let's jump into the console now. Okay, I'm in my sandbox environment here in us‑east‑1, I've loaded up my RDS Dashboard, and I'm looking at my databases, which, of course, are zero, so let's go and get started creating our Multi‑AZ. I'm going to create my new database. We're going to run through Standard. I'm going to select PostgreSQL because that's what I'm familiar with as far as inserting data, etc. I'll scroll down here. We'll use the default engine version. And one thing to note, which is very handy, is I could actually click this button here and it's going to filter for only versions that support Multi‑AZ clusters. Now, that's a little bit different, right, because we're not doing a DB cluster deployment, we're doing an instance deployment, however, that's just a good thing to keep in mind. So, continuing on, I choose my engine version. I'm going to go down here, and I'm going to select Dev/Test for a template. And from here, I'm going to select Multi‑AZ DB instance. We're going to do cluster later on, so don't worry about that right now. For this demo, we're going to choose instance. After we do this, I'm going to go down, I'm going to give my database identifier a name, and we'll just leave this as the default. Then we're going to let RDS go ahead and create our username and our password, so we'll use postgres. And I'm actually going to do Self managed, and I'm going to copy in a simple password that I used previously. And there we go. So, our credentials are set. Let's scroll down a little bit. I'm going to choose a Burstable instance because, in the sandbox, you can't choose too many instance classes and this one is supported, so I'm going to choose db.t3.micro. I'm going to set my Allocated storage to 20 instead of 200. We'll leave it as General Purpose, or gp3, and I'm going to scroll down to Connectivity. So, again, I'm going to call this out every time I do this as long as we're publicly connecting to these DB instances. I am going to use the Default VPC for simplicity, but please, never use this for your workloads. Take the time, create a VPC, and put this in the correct subnets. Again, I'm only doing this for the sake of simplicity of this demo. So, I'll choose my default. I'll use the default subnet groups because they are all public, and I'm going to select public access. Now, final warning, please don't ever do this. So, I'll select Public access Yes, and I'll create a new security group. We're going to scroll down, and I'm going to go ahead and I'm going to click on Turn off Performance Insights. And under Additional configurations, I'm actually going to have them create an initial database name. So, for this database, I'm going to call it pluralsight. We're going to leave backups on, we'll leave all of these default settings in place, and I'm going to go down here and click on Create database. Awesome. So, while this is creating, this is going to take several minutes, I'm going to go ahead, I'm going to pause, and I will fast forward once this is done and we'll continue on. Okay, so I fast forwarded, that took several minutes, probably between 5 and 10 minutes, even for such a simple small database, so please keep that in mind. But you can see now it's starting to modify, it'll go through some other statuses here shortly. But for now, what we can do is I can start connecting to this and testing it out. So we have our database endpoint, and remember, we only have an endpoint to the primary instance. So one thing to keep in mind is this will never change, and we can see the current AZ that this lives in. So, what I'm going to do is two different things. I'm going to go to my pgAdmin here, and I've already created a connection for this database using this tool set, so I've connected over the port for the actual hostname, which is this endpoint here. Secondly, I'm going to copy this, then I'm going to jump into my terminal, and I'm going to run an nslookup on this. So, right now, keep this in mind, our endpoint here maps to a cname record, so AWS is using cname records to remap this domain name to another one. So, right now, this endpoint is pointing to this name, this DNS name at this address. Now, this is important because once we failover, I want to show you how this changes. For now though, let's jump back into my pgAdmin, I'm going to select my pluralsight database that we had them create, and I'm going to run some SQL queries here. Now, I have queries saved in a text document for you to use if you want to do this on your own time, so these will be available for you in the module assets. But, for now, what I'm going to do is I'm going to run some SQL commands here and some queries. Now, what this SQL query is doing is creating a new table called customers with some different fields with their different values and their types. So I'm going to execute this, and we'll go ahead and click Continue. And there we go, so we've now created our table with all of these fields. So now what I can do is select all from that table, so I'll run this, and you'll see we have no data, which makes sense. So what I'll do now is I'm going to run a simple command to manually insert my data into my table. So, we're inserting into our customers table all of these randomized values that I created. So I'll execute this now, and we now have some inserted data. So now if I select from that table instead, we should get some results, and there we go. Perfect, so now we have a list of customers with their unique IDs, their emails, etc. So, with this in mind, let's go ahead and test out some failovers. What I want to do first is I want to update one of these examples, so let's change some data to make sure that this is correct and that when it does failover, it's actually working as intended. So what I'm going to do is paste in this query and what we're doing is I'm looking for this email address, so Alice Johnson, which is our first user, and I'm going to update it. So I'll execute this, we get an update, and now the email is changed. So if I rerun that previous query, we see new.email right here. Perfect. So, let me jump back in to my RDS console. Now, the first thing I have to do is I have to wait for this to be done modifying. So, you'll notice under Actions, I can't do much, which is perfect, so what I'll do here is while our database is modifying, I'm going to pause again, and once this is fully available, we'll continue. All right, so I went ahead, I fast forwarded, and again, just a warning, that took a long time, I would say probably at least 5 maybe 10 minutes so just keep that in mind if you're trying to follow along or do this on your own. We can see now though the status is available, and the first thing I want to do is under Actions, I'm going to Reboot. Now, in the console, you can reboot with failover, so this is going to failover to our standby instance. So, I'm going to click on Confirm. We see it's going through a reboot phase now, and remember, it was in us‑east‑1d, So, what I'll do is I'll select this, and again, what I'll do is I'm going to pause here, I'll wait for this to be available again, and I will resume. All right, so once again, I fast forwarded that took a few minutes, and you could see we're now at Available. So now, in theory, we should be using a different instance on the back end. And, under Configuration, if I go down here, notice the Secondary Zone is us‑east‑1c so this is the zone that's now being used for our instance. Now, the GUI up here is stuck so don't worry about that, this is just buggy, this was the original AZ we were in. But now, what I want to show you here is if we run a nslookup on this endpoint again, so nslookup on the same endpoint, notice it's a different record now. So, RDS automatically remapped our record name to a different cname value on the back end, so notice 44 and originally it was 3, so this is a completely different instance now, but we don't care about that, we don't have to worry about that because it's handled for us. So now if I jump back into pgAdmin and I refresh here, we reconnect because the connection was killed when we failed over. But if I load my pluralsight table here, let's run some queries, and let's just go ahead and get all data once again, so I'll copy and paste in my simple command. There we go. We have all of the data still there because it's automatically synchronized between the primary and the standby instance. Awesome. So that's how easy it is to create a DB instance Multi‑AZ deployment within RDS. We just tested out the failover, we reconnected to the different instance, and we even verified that the cname value was changed for us automatically by RDS. Now, before we actually end this, I want to show you one thing, a little bonus point. When you're deleting your database under Actions, I'll go to Delete, you can create a final snapshot. So, if you need to, to be safe, you can create a final snapshot to restore later on based on the current data of what's in the database. Now, I'm not going to do that, but I just wanted to show you. So let's go ahead, we'll wrap this up here. I'm going to clean up on my end, and I will see you in an upcoming clip.

Offloading Read Traffic with Read Replicas
Let's go ahead and start talking about offloading your read traffic using Read Replicas. If there's an exam scenario where you need to improve read operations for your database, this is where Read Replicas will come in handy. A Read Replica is a read‑only copy of your database instance, and it's meant to help reduce any of the load on the primary or writer database instance within your clusters. Let's talk about some important concepts regarding Read Replicas. First and foremost, these are perfect for read‑heavy workloads. If you have a ton of read operations on your database, then this is the perfect solution for that. When you're setting up a Read Replica for your DB instance, they can be set up in the same availability zone, they could be set up cross AZ, and they can even be set up cross region. Now, one of the requirements for using them, however, is you have to have automatic backups in place, otherwise they will not work, so that's a very important prerequisite you need to know about. When you create a Read Replica, each of the replicas is going to get its very own DNS endpoint that you direct your read traffic to. And, in addition to that, they actually can be promoted, so if you need to in the future, when you start realizing you're going to need a separate database, well, you can promote Read Replicas to become their very own database using that same data. Now, the thing here is these Read Replicas are having traffic replicated to them from the primary or the writer instances, so as soon as you do this that replication stops. Now let's look at a Read Replica architecture because you need to know for both a cluster and an instance deployment what this looks like. In this simplified diagram, we have two servers here on the left‑hand side, and then on the right‑hand side in us‑east‑1a, we have a same region Read Replica setup for our instance deployment. When you create this, the primary DB instance is going to asynchronously replicate changes to your Read Replica, so it's not synchronous, it's asynchronous. In addition to this, you're going to have separate endpoints, so you're going to have a primary endpoint for writing and you can even read from it, and then the Read Replica has its separate endpoint which can only be used for reading. This is a very tricky scenario, remember that Read Replicas are for only scaling read operations. Once you put this into place, your applications can then point to the primary instance and do both reads and writes, but then maybe you have a business intelligence server or some type of reporting or analytics application and it just needs to generate reports based on reading traffic or reading data from your instances. Well, that's a perfect use case for a Read Replica since you're not writing to your database with this BI server, well, then you can point it to read from your Read Replica. When we promote our instances for this architecture, what happens is you're going to go ahead and stop all replication between the primary and the Read Replica. So, once we say, hey, promote my Read‑only Replica here, that's going to stop this replication, and now your clients can actually read and write to the promoted replica, which is completely separate from the original one. So, as soon as you do this promotion process, you now have two separate primary DB instances, keep that in mind. Moving on, let's look at the cluster Read Replica architectures. Remember, there's an instance deployment and there's a cluster deployment. With the cluster, we have our two reader instances in addition to the writer instance. Now the biggest difference here is that you can read immediately without any special configuration from those reader instances. The writer instances allows both read and writes, and we covered this in a previous clip so I'm not going to repeat this entire diagram you've already seen it. The thing I want you to know here is you can actually create additional Read Replicas for your cluster deployments. So you're not just stuck to the two reader instances that come by default, if you need to scale performance even further for read operations, you can do that with a cluster deployment. Now, typically, cluster deployments are going to be the best solution for high availability and scalability for read operations because, by default, they have the writer and two readers already in place so it's a lot easier to manage. With that being said, however, obviously you're going to be paying more because you have three instances instead of a two‑instance setup using a instance deployment method, so really just keep that in mind for the exam scenarios. Now, let's actually look at some exam tips regarding Read Replicas before we move on. The primary use case for these is for scaling, this is not a primary method for disaster recovery. Remember, for disaster recovery, you want to do things like a Multi‑AZ deployment. In addition to that, automatic backups have to be enabled in order to deploy a Read Replica, you have to remember that as well. And then, lastly, remember you can set up cross‑region Read Replicas, however, there is some cost associated with that for cross‑region traffic because the replication traffic is going across regions. Now that's going to do it for this clip on Read Replicas, really do your best to remember these are only used for scaling readable operations, and then also do your best to remember DB instance deployment methods and DB cluster deployment methods with regards to your reader instances. Let's go ahead, e'll wrap this up here, and we will move on.

Demo: Creating an RDS Multi-AZ Cluster Deployment
Okay, let's get started with this demonstration, where we are going to create a Multi‑AZ cluster deployment within our sandbox. Before we jump in, let's have a very fast architecture overview to see what we're going to do here. Pretty straightforward, we're going to log into our sandbox, and we're going to create a DB cluster deployment for a Multi‑AZ architecture. We're going to see that it creates a writer instance and then two reader instances that we can use. Once this is created and it's available, we're then going to test the different endpoints that are provided, so let's go ahead and jump into the console now. All right, I'm in my sandbox environment here, and I've loaded up RDS. And before I begin, one thing I want to call out, this temporary sandbox has special permissions for me to use it. So, if you're trying to follow along in our normal sandbox, this is not going to work so I wanted to call that out before you dive in. With that warning out of the way, what I'm going to do here is create a new database, I'm going to choose Standard create, and then I'm going to choose PostgreSQL. Once I choose Postgres, the next thing I want to do here is I'm just going to click this filter here to say, hey, only show me versions that support this deployment method. Lucky for us, the default here is supported so I'll select that. I'm going to come down here. I'll leave Production chosen for templates because I'm going to change some stuff. And under Availability and durability, we have Multi‑AZ DB Cluster by default. So, by default, for Multi‑AZ for Production, they actually recommend you use a cluster deployment. So I will leave that selected, and I'll scroll down to Settings. I'm going to give my database cluster identifier a name change, I'll call it cluster‑1. We'll use Master username the same. I'll do Self managed here, I'll have RDS generate it for me, and then I'll grab this once this is getting created. So, scrolling down now, I'm going to go ahead and choose m5d.large. Now, if I'm not mistaken, without doing too much digging here, this is the smallest size of an image you can get for a Standard class. So with that in mind, if you're doing this by yourself, be sure to clean up when you are done, you don't want to get charged for three RDS instances, you will be very disappointed with the cost at the end of the month. So, for this, I'm going to choose this standardized size here, and I'm going to go down and I'm going to choose gp3 for my Storage type, and I'm going to set it to 20 GiB instead of 200. The next thing I want to do here is go down to Connectivity. I'm going to select the Default VPC in the default subnet group. I'm also going to make it public access. Now, again, I know I keep saying this, but please don't do this for any real workloads, I'm only doing this for the demonstrations. Now, once I select this, I'm going to continue down. I'm going to create a new security group here, and I'll just call it allow_andru. I'm going to scroll down after this, we'll leave it as Password authentication, which is the only option. I'm going to turn off Performance Insights. And, under here, you can do Enhanced Monitoring, so if you wanted to turn on more enhanced metrics you could do that, but we're not going to worry about that. Now, the next thing I want to do here is open up additional configurations, and I want to show you a few things. The first thing is that, look, you can't have RDS create an initial database for you, it's not supported. Also, you can't choose an option group. Now, option groups are out of scope for this exam, but it's just a good thing to know. Now, for me, I'm going to set Backup retention to one. I'm going to enable encryption here. I'm going to skip down, and I'm going to choose to not enable deletion protection, and I'm going to go ahead and create my database. Perfect. So now we can see, we have three instances starting up. Now, it looks funny right now because they're all three listed as readers. But eventually, once this is done configuring, one of these will change to a Writer instance, and that will be the primary instance that was elected in this cluster. So you can notice, each instance is in a separate AZ, leveraging one of those subnets that we specified in the subnet group. So now, what I'm going to do is I'm going to give this several minutes to finish the initial steps of creating. And then, once this is done, I'm going to go ahead and resume, and we'll look at some of the configuration options. All right, so I briefly resumed really fast, this is still creating as you can see. But the one thing I wanted to point out here is, notice, instance‑1 is now our Writer instance, so this was elected as the primary instance for our DB cluster. Now, what I'll do again is I'm going to go ahead, I'm going to pause again, and then once this is ready to actually walkthrough, I will resume. Okay, so that took roughly 10 minutes, and we're still not necessarily fully available for writes. However, our Reader instances are available, and I wanted to show you how to connect using each of these. So you'll notice, we have our cluster, we have our three instances, we have two Readers and a single Writer. And underneath this cluster, when I select it, we have our two endpoints. So this first endpoint here is our Writer instance, so this is where we can read and write data to via the endpoint. So what I'll do is I'm going to set up a connection to this Writer instance, and you can see its Writer right here with the port. So, what I'll do is I'll copy this, I'm going to go to my pgAdmin console, and I'm going to add a new server. So I'm going to register a new server, and we'll call it writer. I'm going to go to Connection, and then paste in my hostname. So, remember, this is for the Writer instance, but we can also read from this address. We then have our port, which is the default PostgreSQL port, and our username, which is postgres. Now, after this was created, I went ahead and copied a password, so I'm going to go ahead and save it and paste it, and I'm going to connect. Perfect. So now, we've created a connection to the writer endpoint. So now, from here, I'm not going to because I'm just going to delete this, but we could create tables, we could write to our tables, etc. However, let's say we want to offload read requests to only the readers, well, we can do that. What we can do here is go back, I can refresh here, I'm going to go down and copy the read only or the reader endpoint, so this only works with these reader instances. I'll go back to my pgAdmin console, I'll register a brand new server, and we'll call this reader. I'll set my connection to this read‑only endpoint. Now notice, it's pretty much the same, however, they have a ‑ro, so this is meant to mean read only. So I paste this separate endpoint, everything else is the same, including the password, so I'll paste that in here, and now I can go ahead and I can connect. So now, I've connected to two different endpoints, this reader endpoint is meant to only allow me to read from my databases and then the writer allows me to do both. Now, I'm not going to test these out any further than that, I just wanted to test via making connections to each of them so you can see that they are actually completely separate endpoints, and thus they are completely separate connections. Perfect. So now, that's how easy it is to deploy a database cluster for Multi‑AZ high availability and disaster recovery. Big thing to remember here is you have two separate endpoints, one for reading and one for writing, as you can see here. And, by default, you have three instances, one writer and two readers. Now, that's going to do it for this demonstration. If you're doing this on your own, feel free to play around, you can trigger failovers, etc. But that's going to do it for us, I'm going to go ahead and cut here. I'm going to clean up all of my resources, and I will see you in the next clip.

Amazon RDS Authentication
Let's talk about authentication within our Amazon RDS databases. There are three primary authentication options you can choose from when you are using RDS, the first is Password authentication, otherwise known as Basic. This is what we've been using to connect to our existing databases that we've created in previous demonstration clips. This is where the DB instance is going to perform all administration of your user accounts, and you create the users and specify passwords after that using SQL statements. You can also use IAM authentication, this is where you actually use a token that is based on IAM credentials to access your databases on your instances. And lastly, we have Kerberos authentication, so this is where you're going to use external auth methods for your database via Kerberos and Active Directory, so this is a common way to authenticate to your databases if you have something like Active Directory already used heavily within your organization. Now, a big exam tip here, RDS integrates with something called AWS Secrets Manager to manage your admin user passwords for you in a secure way. Let's explore how this works. By using Secrets Manager, it's going to allow you to easily replace hard coding any credentials within your codes for connecting to your RDS instances in the first place, you do it by leveraging the resource in Secrets Manager, which allows you to pull out your credentials. A big thing here, this is not a free service. Now, we talk about Secrets Manager in a completely separate course, much more in depth, but in terms of using it for RDS, it is not free to use. How it does work, though, is it allows RDS to go ahead and work with the service to say, hey, I want to generate a master password for this master admin username, and I want to store it within Secrets Manager where it is then encrypted at rest and protected via a KMS key. The big benefit here is you can rotate credentials automatically, by default, they're going to rotate every seven days, but you can change it. So, really the big takeaway for this entire clip, if you need easy, secure admin credential rotation and storage, you want to think Secrets Manager for your RDS instances. Let's go ahead and wrap up here, and we're going to have a demo where we use Secrets Manager to connect to RDS.

Demo: Using AWS Secrets Manager with Amazon RDS
Let's get started with this demonstration where we're going to use a AWS Secrets Manager with Amazon RDS, we're going to generate some credentials, and then we're going to rotate them. All right, I'm in my sandbox environment here in us‑east‑1. I've loaded up RDS, let's get started. The first thing I want to do here is I need to create my new instance, so I'm going to go to Databases, and I'm going to create a new one. Now, for this demo, I'm going to make this as simple as possible, so I'm going to do Standard create, I'm going to select PostgreSQL for my engine type, I'll use the default version here, and then, under this, I'm going to say Single DB Instance. I'll leave the identifier the same, and we'll leave the username the same. But notice, I'm going to select Managed in AWS Secrets Manager, so this is exactly what we're talking about. We're going to let Secrets Manager actually manage the user credentials here all for us. Now, when we select this, you can select the encryption key that you want to use in addition to this, so what we can do is we can use the default, which is what I'll do. But if you wanted to, you could create a brand new KMS key specific to this one set of credentials for you to encrypt these. Now, it's out of scope for this, so I'm going to close this, but the big thing to understand is you select an encryption key to encrypt this data at rest. So, I'll select default, we're going to go with a Burstable class here, t3.micro, and I'll set this to General Purpose with 20 GiB. I'm going to go down, we're going to select the default‑vpc with the default subnet group and make it public access, which, again, my normal warning, please don't do this for any real workloads. But I'm going to select that, I'll scoot down here, create a new security group, and I'll call it allow_me. I'll set No preference for my AZ here, and let's go ahead and we're going to say Password authentication for our database auth. Now, this is because, even though we're using Secrets Manager, it's still basic authentication. We have a username, and we have a password. So, I'll select this, I'll scroll down here, and I'm going to click on Create database. All right, let me close this. We've got our database being created here, I'll go ahead and select it. And you're going to notice under Configuration here, we have a Master Credentials ARN, so, this is because this gives us a Secrets Manager resource which we can use. So, what I'll do here is I'm going to load this in another tab, I'll load it up, and now you can see, hey, this secret was created by RDS and it's managed by RDS, so you can't modify it, you have to modify it via that service. So, at a high level, while this is all creating over here in RDS, let's look at this. We see the encryption key that was used, it gives a secret name, so this is programmatically created for us, if you made this manually, you could give it your own name. We see this secret description, and we see the secret ARN, which we could use to reference this in some type of automation or Infrastructure as Code. So, in theory, what we could do here is lock this resource down because it has an ARN within IAM and make sure only specific users can access this data which is encrypted. Now you're going to notice down here, under the Overview, we see a Secret value. Now, this is hidden. And again, if we were wanting to, we can control access to it via this ARN. Now as cloud user, we have admin rights. So, what I could do here is click on Retrieve secret value, and then once this is created, it's actually going to show us the credentials. Now, again, what's going on here is this has not been fully put into place because our database is still being created here, so what I'm going to do is I'll pause, and then once this is available enough for us to explore, we're going to go ahead and resume. All right, so, now we've reached a status where we've successfully created our database here it's going through some status changes, which is fine. However, the big thing here is now that our credentials have been created, we can test. So, we have our endpoint, and we've seen all this stuff before. What I want to do is go back to my Secrets Manager, I'm going to refresh this page, and now when I go down here under Secret value, when I retrieve this, we get our credentials. So notice, right now it's in Key/value pair, that's because if you really look at it, it's a JSON value, so what that means is you would have to parse this correctly if you were doing it programmatically, etc. This view just gives us a nice, easy readable version, which is what we will use. Now, before we go and use these credentials, I want to look at Rotation. Notice, Rotation is enabled. So, remember, by default, the rotation schedule is every seven days these credentials will be rotated, that means a new password will be created for us. So now we can see, in addition to that, the versions of the secrets, so we have currently one version for us to use and it's current. So what I want to do is I want to go in here and let's actually try connecting. So, what I'll do is I'm going to load up apAdmin, and I'm going to make a new server. So I'm going to register a new server, we'll give it a name, and we'll just call it secrets. I'm going to go to Connection, I'm going to copy in my hostname here, let me copy my endpoint, and I'll paste that in. Postgres is the username, so what I'll do is I'll go back to Secrets Manager, I'll go ahead and load this, and I'll copy my secret value. I'm going to go back, I'll paste it, and let's go ahead and save and connect. There we go, so now we've connected to our RDS instance, we see our databases which are created by default, and we're in. So, let's actually view how easy it is to rotate these passwords. What I'm going to do here is I'm going to make sure I copy this, and I'm going to save this in a document on the side, so what I've done is I've saved this old password just to show you it doesn't work. And what I'm going to do is I'm going to go to Rotation, and I want to rotate this secret immediately. So, I'm going to click on this, I'm going to click on Rotate, and now what's going to happen here is this password is going to change here in a second, so eventually, we're going to see a different version here. And I'm going to pause this until this actually gets rotated, I'll tell you about how long it took, and then we'll test connecting with the updated credentials. While this is rotating, I wanted to show this and not skip through this. But notice, we have the current version, which was the original. And then, now, as it's creating that new rotation, we see the pending one, so this is a completely different secret version and it's now pending. And eventually, once it's updated over in RDS here, we're going to go ahead and see that this is the new version. In fact, we got really lucky, notice the status here, resetting‑master‑credentials, so now that means this will eventually be set to AWSCURRENT and we'll use that. So, what I'll do now is I'm going to go back to fast forwarding, and I will resume once this is complete. Okay, that took literally a couple more seconds so I didn't have to fast forward too far, which is nice. But notice now, we have AWSCURRENT and AWSPENDING, and that's because this is getting set up. But eventually, this will only be AWSCURRENT, and we'll notice over here that the password was now changed. So now, if I go back and I refresh here and I go back to Overview, I retrieve this secret value, we have a completely different password. So now, what I'll do is I'll go back to pgAdmin, I'm going to clear the saved password. So, what I'll do now is I'll go to Connect Server. It's going to ask for the password, I'll copy in and paste the original password, and this will fail, well, that's because the authentication is not correct. So now, instead, I go back over to my sandbox environment, retrieve my secret here, I'll copy this, I'll go back, paste in the new password, and click on OK. And there we go, we've now connected with the new credentials and everything was managed over here in Secrets Manager. And that's how easy it is, so we've offloaded the master credential management to the Secrets Manager Service where it's encrypted, and it's easily set up for secure access control via IAM. Now, in addition to this, remember, you can't edit the rotation, so let me just show you what this looks like before we wrap up. We have Automatic rotation turned on, but you could turn it off. However, when you turn it on, you can actually say, hey, I want this to be rotated every so many days, every so many months, etc., and you can actually even further customize based on that particular selection. So, with that being said, let's go ahead and wrap this up here. We saw how easy it is to rotate and then get our credentials via Secrets Manager for our RDS database. I think that's a perfect time to end here, and we're going to go ahead and move on whenever you're ready.

Amazon RDS Encryption
All right, let's talk encryption within RDS. Within RDS, there are two primary encryption methods which are, not surprising, there's in transit and there is at rest. Now, for in transit, RDS has TLS by default in place for your encryption at transit via their own AWS root certificates. Now, these root certificates are pretty much trusted everywhere within the world now so you don't have to worry about adding them manually, it's just good to know that they provide their own root certificates for TLS. Moving on, however, let's talk about encryption at rest. RDS does offer encryption at rest to increase your data protection of your resources when you're creating your instances. When you enable this, all of your logs and all of your backups and even your snapshots are also encrypted. So, if you encrypt the DB instance and you take a snapshot, well, then that's encrypted. When you do encryption at rest, RDS uses a KMS key. Now you can specify your own specific key if you want to or you can use the default key. If you decide to do this in the first place though, you have to enable encryption at creation time, you can't go back and do it later on. If you want your DB instance encrypted at rest with a customized setting, you have to do it at creation. In addition to that, once you do select your KMS key for encryption, well, unfortunately, you can't change that key once it's enabled and in place, you have to create a brand new database instance with a separate key. Continuing on the encryption at‑rest topic, the master database instance has to be encrypted in order for the Read Replicas to be encrypted, you can't mix a match. In addition to that, encrypting an unencrypted database is going to require this process. You take a database snapshot and then you restore that snapshot to a new encrypted version of a database instance. So remember that process, if you have an unencrypted database and you want to encrypt it, what you do is you take the snapshot, restore that snapshot, and make sure you encrypt it during that creation time. And lastly, this is very important for the exam, if you need to encrypt an Oracle or Microsoft SQL Server database instance, you can use what is known as Transparent Data Encryption, or TDE for short. If you see these two engine versions and you see encryption, just understand TDE is supported. Now, that's actually going to do it for this clip on encryption within RDS, let's go ahead and wrap things up. Do your best to remember that it supports in‑transit encryption using TLS certs, and you can customize your encryption at‑rest method using a custom key, however, it has to be done at creation time. Let's go ahead and wrap this up, and I'll see you in the next one.

Cost Optimization for Amazon RDS
All right, let's talk about cost optimization. Pricing and cost is very big for solutions architect work, you have to be very efficient, so let's talk about pricing and charges within this service. You need to understand, first and foremost, if you have cross‑region replication set up at any point in time, you will incur data transfer charges for that RDS data. With RDS, by default, you pay for the following, the instances or the compute that are actually running, you pay for any cross‑region replication data transfers, and you pay for any database storage regardless of the instance activity. So even if you stop your instance for a short amount of time, well, you're still paying for that allocated storage so remember that. And then, lastly, you pay for an optional feature if you turn it on called Enhanced Monitoring, which gives you greater amounts of details for monitoring your resources within RDS. Now let's actually explore some instance pricing options that you have, there are two different types. You can pay for on‑demand, which is the most common. And this is where, just like EC2, you pay by the hour for the instance hours that you actually use. So, if it's on and running, then you're paying for it. You also have a reserved instance, so this is also just like EC2 reserved instances. What you do is you say, hey, I want to reserve instance capacity for a one or three‑year term, and you're going to give me significant pricing discounts. So, just like EC2 instances when you use reserved instances over there, you get a pricing break because you're committing to a term usage. So, exam pro tips before we wrap this short clip up. Number one, be sure to leverage reserved instances for RDS to save on long‑term costs if you know the requirements. Just like EC2 reserve instances, you need to be sure on what capacity you really need to make these worth it. Second pro tip, there are no Spot instances for RDS, so don't get tricked on an exam scenario if that comes up. There is only on‑demand, and there is only reserved instance options. With that being said, let's go ahead and wrap this lesson up on cost optimization, and I will see you in the next one.

RDS Custom
Okay, let's talk about RDS Custom. There are some important concepts regarding RDS Custom you need to be familiar with for the exam. First and foremost, this is only available for Oracle or Microsoft SQL Server RDS database instances. What this is meant to do is it's meant to allow you further customization options for your operating systems and the database instances. What that means, long story short, is that you can configure more detailed settings than default RDS instances. A use case on why you might want to do this is maybe from a compliance standpoint, you need to install your own custom patches on the instances hosting your databases, or maybe you need to leverage non‑RDS native database features. So maybe there's something that's not supported by default in RDS, well, then, RDS Custom can allow you to leverage that, but there's more overhead. The big thing here, is for these two types of engines, so Oracle and Microsoft SQL Server, when you use RDS Custom, you gain access to the underlying instance, so you can connect to it via things like SSH or Session Manager. Now, this doesn't play a very big part on the exam currently, but we need to cover it because it is an option. The exam pro tip number one, you can choose this feature for RDS if you want to still use a managed service while also gaining some further customization control. Number two, this only works for Oracle databases and Microsoft SQL Server databases, it doesn't support anything like MySQL or PostgreSQL, etc. So, if there is another engine type being talked about, then immediately you know, it cannot be RDS Custom. Now with that being said, let's wrap this clip up, and I will see you in an upcoming one.

RDS Proxy
Let's go and talk about RDS Proxy. RDS Proxy is a fully managed, highly‑available proxy service for Amazon RDS. It's meant to be used to allow your applications to be more scalable and then even more resilient to particular database failures. Let's talk about some concepts about this feature within RDS. The primary function for this feature is it's meant to allow your applications to pool and then share their database connections. The whole point in reasoning of this is it's meant to drastically improve your ability to both scale and handle failures. It works by reducing the memory and the CPU overhead that would normally occur whenever you're opening brand new database connections. So instead of having hundreds of connections, you can now share connections in a single pool for many different applications, this is especially useful for handling unpredictable surges in your database traffic. So, if things are scaling up and down from a compute standpoint and they're constantly making new connections, well, then this might be a perfect solution for your needs. The big point here is that it automatically connects to your DB instances and then it's going to help preserve those existing connections. So again, it all kind of goes back to the fact that it's reusing connections instead of establishing new ones. Here are some additional concepts you need to be aware of. This has to be accessed via a VPC. In other words, there's no public accessibility allowed for this, so our typical demonstrations would not work using this feature. Some of the key points here, it's serverless, it's natively highly available, and it's even auto scaling, so all of this stuff is managed for you within RDS whenever you enable this feature. Another huge thing is that it usually does not require any application coding changes, so you don't have to worry about updating your Lambdas or EC2 Compute, whatever is connecting to your databases, well, it should connect just fine still once you enable this. And then, lastly, I know we haven't talked about Amazon Aurora yet, but this is very important. Using RDS Proxy, you can create a separate read‑only endpoint for Amazon Aurora. Now, high level, Aurora is a different proprietary version of RDS, and we're going to explore that in its own module, however, I wanted to point this out here before we move on. Now, before we actually wrap up this clip, I want to show you a very high‑level Proxy architecture diagram. How it works is we have a Proxy here in the middle, and this is a feature that you enable after you create your database. Now, what happens is it fronts your RDS database, and then you have all of your compute resources, so your Lambda connections, your EC2 connections, they're all going to get pulled into our RDS Proxy cache node instead of having to connect individually to the RDS from each application. Now, in exam pro tip here, this is very useful to optimize your connections from Lamda functions. If there's a scenario where you're having too many connection requests for your RDS databases from things like Lambda, this is a perfect use case for RDS Proxy. And with this diagram out of the way, let's end here, and then I will see you in an upcoming clip where we will discuss module summaries and exam tips.

Module Summary and Exam Tips
Let's review some quick exam tips and some module summaries before we move on. Do you have OLTP requirements? First and foremost, Amazon RDS is likely going to be a perfect choice on the exam. Relational databases, specifically running on RDS, are perfect for online transaction processing. Speaking of RDS and its different engines, it supports pretty much every most‑popular database engine out there today, so we're talking Microsoft SQL Server, MySQL, PostgreSQL, etc. Be sure you're familiar with the engine types for RDS. And continuing with RDS, let's talk about some exam tips here. Remember that RDS is perfect for a managed database service, it manages the backups for you, it handles software patching, and even allows you to implement ways to handle automatic failover and recoveries. You have to be familiar that you can enable automated backups. Now, if you're actually truly doing it, 1 to 35 days is the time range, but if you specify 0, then it disables it. Also, be aware that you can manually create your own snapshots to restore later on. When you do that, remember that snapshots stay around forever, there's no specified retention period. Also, remember that you leverage specific database subnet groups to isolate your RDS resources into, so typically, this is going to be a private subnet group specific to hosting database resources. Lastly, remember the different access controls, you can use different authentication methods, like IAM, Kerberos, and even Basic password authentication. And you control access to your resources via network controls like VPC security groups and network access control lists. Moving on, let's review Multi‑AZ deployments. Remember that Multi‑AZ database instance deployments are going to have a single standby database instance for you to use for recovery. Database cluster deployments, on the other hand, are going to have three total instances, there's a writer or a master instance and then you have two reader instances for read‑only activities. Typically speaking, cluster deployments are going to be the most efficient way to deploy a Multi‑AZ model. However, if it comes down to cost, you might want to look at database instance deployments. Let's review Multi‑AZ versus Read Replicas, as it's very important for you to understand how these work compared to one another. For Multi‑AZ, this creates an exact copy of your database in another availability zone and it's primarily meant to be used for disaster recovery, this is not meant to be a scalable solution. You need to understand the difference between instance deployments and cluster deployments like we just talked about, and remember, RDS will automatically failover to your standby instance in case of outages. We tested this in one of the demonstrations where we rebooted and then forced a failover, and we checked the backend resource after we did that. For Read Replicas, these create read‑only copies of your primary database, they can be in the same AZ, they could be cross AZ, or even cross‑region, but remember, you do incur data replication charges for that data transfer. These are primarily used to increase and scale your read performance, remember you cannot write to these directly, and because of that, these are perfect for read‑heavy workloads where you have a scenario where you want to offload those read request. And the last thing here is remember that replicas can be promoted to be their own separate database, and once you do that promotion, the replication is stopped between the existing database and this one. Moving on, let's review authentication options. Remember the three different types, there's password auth, which is Basic, you can use IAM to authenticate via a Token, and there's Kerberos authentication, so if you have Active Directory, this might be a great way to authenticate into your database. Next, we have RS Custom. The thing to remember here is that this feature allows you to have further control into specific RDS instance operating systems. Remember, this only works for Oracle and Microsoft SQL Server databases, and it allows you a little bit more fine‑grained control like customizations within the operating system. We also talked about Secrets Manager with RDS. Remember that RDS can generate the master password and then use Secrets Manager to store it and those credentials are encrypted using KMS keys, so they're securely stored. In addition to the encryption, you can leverage IAM to control who can even access those resources. Lastly, you can set up automatic rotation of your credentials. So, by default, it's every seven days, but remember, it's pretty flexible, you can do it every so many hours, every so many months, etc. The big thing to remember is that you'll use Secrets Manager to manage your credentials for RDS in a secure manner and in an automated fashion. Next up, let's review RDS Proxy before we wrap things up. Two things here, remember, pooling connections, this is meant to help reduce memory and CPU overhead for opening new database connections. Also remember, resiliency and scaling, this is perfect for handling traffic surges that you're not expecting and it will automatically connect to standby instances and standby connections. Another big thing here is to remember that Lambda and RDS Proxy are perfect for each other. Lambda functions spin up and down constantly, and instead of creating brand new database connections each time, you can leverage RDS Proxy to pull those connections. Now that's going to do it for this module, thank you for hanging in there. Let's go ahead and wrap it up. And then, when you're ready, I will see you in the next module.

Amazon Aurora
Amazon Aurora Overview
All right, let's get started with this module on Amazon Aurora. First up, let's have an overview of what Amazon Aurora is. Amazon Aurora is an AWS fully managed, relational database engine that is compatible with my SQL and PostgreSQL database workloads. It is very important that you understand Aurora is leveraged and managed via RDS, it is an engine type within the RDS service. So, because of that, you might be asking, okay, but then how does it differ from using traditional RDS? Well, let's talk about it, this is very important that you understand the distinction between the two. Amazon Aurora is built using AWS proprietary technology, it is not open source, and no one knows besides AWS how it works underneath the hood. AWS brags that it offers cloud‑optimized performance, especially compared to traditional Amazon RDS. In other words, it performs better, in fact, it's said to provide up to five times better performance than traditional MySQL on RDS and up to three times better performance than PostgreSQL workloads on RDS. However, with the performance increase, of course, unfortunately it does cost more than using traditional RDS, in fact, it's roughly about 20% more overall when you compare 1‑to‑1 usage. Now, moving on, let's talk about the compute. With Aurora, there is some compute bounds and limitations that you need to be aware of. The cool thing is Aurora can help you scale up to 96 virtual CPUs, so it can handle a ton of computation power. It also allows you to scale up to 768 GB of memory, in other words, a lot of memory. In addition to this, when you create your Aurora databases, you deploy these resources in what are known as Aurora database clusters, so it's similar to an RDS Multi‑AZ database cluster, but this is specific to Amazon Aurora. Now, for the exam, you have to be familiar with the clusters, this is a very critical service and engine type you need to know, so let's discuss it. A database cluster is going to consist of one or more database instances and a shared cluster volume, that cluster volume is basically a virtual database storage volume that is going to span the multiple AZs that you deploy in. Now, with your database clusters, you have two types, there is a Primary and a Replica. And with your clusters, you get different endpoints for your different instances within them. You have a cluster endpoint, which is going to be the current primary database instance for the cluster; you get a reader endpoint, which is going to be one of the different available Aurora Replicas within the cluster; you can specify a custom endpoint which is meant to represent a set of instances that you actually get to choose yourself from the cluster; and there's instance endpoints, so these are all going to be specific to a single individual database instance within your cluster that you can reference if you wanted to. Before we wrap this intro clip up, we just reviewed what Aurora is, let's review two big tips. Really big thing here, this only supports or works with MySQL or PostgreSQL‑compatible database workloads, none of the other engine types are supported. The big thing that it offers is much greater performance, remember, it's five times more performant than MySQL and up to three times more performant than PostgreSQL. However, on the opposite side of that, it is unfortunately more expensive than using the traditional engines within traditional RDS database instances. Now, that's going to go ahead and do it. Let's wrap this clip up, and we'll move on to talking about the different storage that goes along with Amazon Aurora.

Amazon Aurora Storage
Let's get talking about storage within Amazon Aurora. Here are some important concepts you have to know for the exam and whenever you are using Aurora for your RDS workloads. When you create a cluster within Aurora, you get a high‑performance storage system that is meant to automatically scale for you, so this is one of the big benefits of using Aurora. Now, when you deploy your cluster and you create your volume, the minimum storage is going to be 10 GB and then the maximum storage will be 128 TB, so this can grow to an extreme size if you need it to. The cool thing is that even if you grow it up to this maximum limit, this storage system is so high performing it's not going to be affected by the size, that's one of the big bragging points that AWS likes to talk about whenever they talk about Aurora. Now, with the different storage components, a big thing is data redundancy. When you create an Aurora cluster, two copies of your data are contained in each availability zone. Now, those two copies in each AZ are spread across a minimum of three availability zones, so what this means is Aurora is maintaining a minimum of six copies of your data, so it is very redundant. And a really cool thing is that it will attempt to automatically recover your database within a healthy availability zone if there is a failure in one of the other availability zones. And speaking of storage failure, let's talk about it. First and foremost, as Dr. Werner Vogel talks about, everything fails all the time, and Aurora is designed with failure in mind. It is designed to handle the following type of failures. It can handle up to two copies of data being missing without affecting write availability, and it can support up to three copies of data being missing without affecting read availability. In other words, it can handle a lot of data loss before it's actually truly affected. In addition to those numbers, Aurora storage is self healing, so data blocks and your disks hosting your data are continuously scanned for different errors, and if they find errors, they're going to be repaired automatically whenever possible. Now, that's going to do it for this clip on Amazon Aurora storage, do your best to remember the minimum and the maximum limits, and a huge thing to remember is that six copies of your data are maintained at all times when you use Amazon Aurora. Let's go ahead and we're going to wrap this up here, and I will see you in the next clip

Replicas in Amazon Aurora
All right, next up Replicas in Amazon Aurora. What are Amazon Aurora Replicas? Well, similar to traditional RDS deployments, primary instances within your Aurora clusters support both read and write operations, and they're going to actually handle all of the data modifications to the cluster volume, in other words, the shared storage underneath. When you create a cluster, there is only one primary instance and that's going to be essentially the writer instance. However, you can spin up what are known as Aurora Replicas, and you can actually have up to 15 Replicas per cluster, per region. Now you might be asking me, okay, that's awesome, but what is a Replica? Well, Replicas are instances that connect to the same storage volume as the primary instance, however, they only support read operations, so it's essentially a Read Replica specific to Amazon Aurora. The nice thing about these is once you configure them, they will serve as automatic failover targets if they're configured to be a part of your cluster. So, in other words, as soon as you create a Replica, if you have a failure on the primary instance, it will failover to one of your Replicas automatically, which is awesome. Also, Replicas do support cross‑region replication. So, on the exam, if you need to support different regions for read operations, Aurora Replicas are perfect for that use case. Moving on, let's look at a cluster architecture. Now, this is simplified and it's high level, but it does get the point across. The big thing we have here in the middle is we have six instances essentially spread across three AZs, so what's happening is here on the left, we have a single, primary, or writer instance, and then we have five reader instances and these are all spread amongst their own AZ. You can have up to 15 Read Replicas spread across your AZs for your clusters, these are going to support synchronous replication that is measured in milliseconds, so it is very high performing. In addition to that, remember, there is only one primary instance that serves as the primary writing instance for the entire cluster. And then, lastly, on the bottom of the diagram, the cluster volume, this is your shared storage that's replicated across the different availability zones in your region. This shared volume offers self healing and the replication allows for very high performance, and it can even auto scale for easy growth, so you can allow your storage to grow with you so you're not over provisioning and paying for stuff that you're not using. The big thing to remember here, again, you have your data spread out across three AZs by default with two copies in each AZ. And also remember, the number of Read Replicas that you can actually implement. Now, that's going to do it for this clip on Amazon Aurora Replicas. Let's go ahead and wrap things up here, and we're going to move on to high availability and scaling coming up next.

High-availability and Scaling
All right, let's dive into high availability and scaling with Amazon Aurora. Aurora Auto Scaling is a feature within Aurora specifically that will dynamically adjust the number of Replicas that are existing for your cluster. Remember that your primary instances will automatically be replicated across the different AZs to the Replicas that you implement. With Replicas, you can leverage automatic failover to a new primary instance all within 30 seconds or less. Or, if you want, instead of having to rely on automatic failover to your Replicas, you can actually promote a Replica manually and it will become the new primary instance, so you can choose to do this whenever you want or you can leverage Aurora to automatically do it. One thing to remember, promoting a Replica via failover is going to be faster than recreating a brand new primary instance that is separate from the current cluster, so promoting Replicas is faster than recreating. Now, let's talk about the promotion process, there is an order in which this happens, and, in fact, you can customize the order in which your Replicas get promoted by assigning each Replica a priority. Now, you need to know, priorities range from 0 to 15. Zero will be the highest priority, 15 will be the lowest priority, so the lower number has a higher priority. It can be a little confusing, but you need to know this for the exam. Now what happens is RDS is going to promote the Replica with the highest priority to be the new primary instance if automatic failover occurs. Now, speaking of automatic failover, let's talk about some confusing Replica promotion concepts. If you have more than one Aurora Replica and they share the same priority, that results in what is called a promotion tier. Already this can be confusing because it seems odd that you can share the same priority, but it is possible. Now assuming your Replicas share the same priority and they have those promotion tiers in place, RDS is going to promote the Replica within that tier with the largest size, so it's going to find the largest Replica and promote that. Now, if we have two or more Replicas in the same tier that share the same priority and the same size, well, then RDS is going to say, you know what, I'm going to randomly select a Replica within this promotion tier, and we'll go from there. So, the lower the number, the higher the priority, and then do your best to remember this step process that you see here in case there are any shared priority numbers and promotion tiers. Now, if there are no Replicas present, then the primary instance is going to automatically be recreated within the same availability zone if possible. So, if you don't have Replicas and you don't have promotion set in place, then this is the process that happens. Now with that being said, let's go ahead and wrap up here. Do your best to remember how the promotion process works and the different priority numbers. We'll end this, and I'll see you in the next clip.

Backing up Amazon Aurora
In this clip, let's talk about Amazon Aurora backups. Automated backups within Aurora are always enabled by default. In addition to automated backups, you can also take your own manual snapshots, so it's just like RDS in that nature. Now, neither of the backup types is said to impact your database performance, so feel free to take them manually whenever you want, and, of course, automatic ones are already in place for you. In addition to this, the default backup retention period is one day. So, since they are turned on by default and always enabled, AWS gives you a single day for free so you're not paying for that backup. Moving on, let's talk about three important concepts relating to the backups. The automated backups are continuous and incremental, in other words, they're just like RDS backups. You can specify between 1 and 35 days for the retention. Now, this differs slightly than traditional RDS because you can specify zero. However, with Aurora, you have to remember backups are always on and the minimum is one day. And lastly, just like traditional RDS, your backups are stored in S3 storage, however, you don't have access to that underlying storage, you can only manage, delete, etc., the snapshots and the backups from within the RDS service. Now, if you need something longer than 35 days, take a snapshot. You need to remember this for the exam, you have to understand the limitations on snapshot retention for automated snapshots. In addition to this, backtracking, this is an important feature. This is a feature for MySQL compatible databases in Aurora, where you can actually rewind your cluster to a specific time without restoring a backup, so it essentially allows you to quickly go back to a specific point in time without having to go through the full restoration process. Let's explore some of the concepts with backtracking. First off, this is not meant to be a replacement for backing up your cluster, please remember this. When you're using it, there's a target backtrack window, so this is the amount of time that you want to be able to backtrack. Now there's also an actual backtrack window, so this is the actual amount of time you can even backtrack. So, just because you specify a target does not mean the actual window will be the same. And, in fact, the limit for your backtrack window is 72 hours, this is exactly why this is not meant to be a replacement for backing up the cluster. In addition to the limits, backtracking does cause a brief instance disruption, what this means is you need to either pause or stop your applications from trying to read and write to your clusters. Moving on from backtracking, we have Aurora Cloning, this is a feature that is meant to allow for much faster and much more efficient database cloning compared to traditional methods of doing so. Let's talk about some of the use cases for this. You can use Aurora Cloning to quickly set up test environments using real data without risking data corruption, so a use case number one would be experimenting with potential changes to your data schemes, etc. You can also run workload intensive operations on a cloned database instead of the actual primary database that's actually hosting application workloads. And then, thirdly, you can create a copy of your production cluster for development and testing purposes. So maybe you need to test with actual real‑world data where you can quickly clone your Aurora database and then use that for a completely separate environment with the exact same data and you don't corrupt anything. Now, that's going to do it for this clip on backing up data within Amazon Aurora. Really do your best to remember that they are always turned on and because of that, there's a minimum of one day. Remember that if you need something longer than 35 days, you need to take a snapshot, and really do your best to understand how backtracking works and some of the use cases for it, as well as Aurora Cloning. Let's end this clip here, and I will see you in an upcoming demonstration.

Demo: Creating an Amazon Aurora Database - Part 1
Let's go ahead, and get started with this demonstration, where we are going to create our very own Amazon Aurora database within RDS. Let's have a quick architecture overview on what we can expect to do throughout this demo. First things first, this will be a two‑part demo. We're going to create our cluster in one part and then we'll use it in another part. The end goal of this demonstration is we're going to create a brand new cluster with Aurora PostgreSQL compatible databases. We're going to set up a Multi‑AZ deployment and then we're going to generate our own Read Replica separate from that original deployment, and we're going to create a custom endpoint to interact with that reader. Now, we're also going to test connecting to the database and reading and writing to the writer instance via a private connection within EC2, done all through Session Manager within our private subnets. Let's go ahead and jump into the console now. All right, welcome to this demonstration. We're now logged into my sandbox environment here in us‑east‑1, and I've loaded up RDS. But before we begin, I want to go over some existing infrastructure that's going to be used. I've deployed a CloudFormation stack and it's done a few things, it created a VPC for us, CUSTOM VPC. And within this VPC, we have three subnets split between three AZs, so we have a Public, Private, and a Database subnet. We're going to use these database subnets as our Database Subnet group, which we're going to create before we create our cluster. Now, in addition to the VPC resources, we also have created an EC2 instance here called database‑client, and this has a security group attached that has no rules and an IAM role here that I'll open up in another tab, and this IAM role will be provided to you in the text document for this module assets. So we're going to include this, you can deploy this template, and you'll have the IAM role and the VPC in there for you to use. Now, I have attached the SSM Managed policy, and then I created an additional one that allows me to Get, List, and Describe all resources within Secrets Manager. So we're going to leverage Secrets Manager to programmatically acquire the password for our database cluster. So let me go ahead and close this, and I'll refresh here. And what I'll do is go back to dashboard for RDS, and I'm going to find Subnet groups, so let's create a custom subnet group for us to use. I'm going to create new one, we're going to give it a name, we can give it a description, and then we choose the VPC. So I'm going to choose my CUSTOM‑VPC, and after that, we can see all of the availability zones that are there. Now, for this, we only have a, b, and c, so what I'm going to do is get rid of d, e, and f. And under here, we're going to see our different subnets. Now, I know because I deployed this, these are going to be my database subnets, so 20, 21, and 22 for the CIDR, so I'm going to select that, and now we have three AZs and three subnets, and I can create this. There we go. So now, we created our own private subnet group for our Aurora database cluster to live in. Now, let's go to Databases, I'm going to create a new one. I'll choose Standard create, and notice there are two options, we have MySQL Compatible or PostgreSQL Compatible. So, what I'm going to choose is PostgreSQL Compatible because that's what I'm familiar with, but you could choose MySQL. But the big thing to note here is they're not directly either/or, they're compatible with those engines, so keep that in mind. Now, I'm going to choose Postgres, I'm going to scroll down here, and you'll notice it's just like other traditional RDS deployments. You choose the Engine version, you can choose it based on compliance whatever you want, and then that's it. So I'll use the default here, I'll scroll down through Templates, and let's look at Settings. Now we can give our cluster identifier a change here, so I'm going to call this database‑cluster, and then we can change the credentials. Now, I'll use the default master username, and I'm going to leverage Secrets Manager to create our credentials for us and rotate them automatically. We're going to use this programmatically in a command‑line call here when we're interacting with our database on our EC2. So, I select Secrets Manager, we'll use the default encryption key, and we get down to cluster storage config, so this is where you can configure the cluster storage volume for specific needs. So, do you need I/O‑ Optimized, so do you need very high‑performing input/output performance or can you get away with a more cost‑effective option, which is Standard? For us, we're going to choose Standard, and I'm going to go down to instance config. So here, there are a few differences, it's not the exact same listing of menus as traditional RDS, you can see their is Serverless and then some other filtered menus. But other than that, it's pretty much the same, you still choose database instances for your underlying compute, and for us, I have to choose a Burstable class. So I'm going to choose db.td.medium, I'm going to scroll down here, and we're going to make it Multi‑AZ, so this will create a primary writer and a Replica for us in a different AZ automatically. So, I'll choose this, and then we get to Connectivity. So we're going to do it a little bit different here, I'm actually going to choose to connect to an EC2 compute resource, and I'm going to choose my database‑client. Now, I'll show you what this does after it's complete, and we will review it before we actually connect. So I'm going to choose this EC2, and you'll notice it kind of locks down some of the VPC resources, specifically the VPC we're using, which that makes sense. You're trying to connect from this VPC in a private subnet, so you can't change that. So, we'll leave this, and we get down to our subnet group. So this is going to create a new one for us automatically, but I don't want to do that, I want to create or choose an existing one. So I'm going to choose my custom‑subnets here, and you'll see it limits for no public access because it's not a public resource or a public subnet within the custom subnet subnet group. So, I'd say No public access, we'll give it a new security group here, and I'll go ahead and call this aurora. And then, you can actually see it's calling out, hey, RDS is going to add a new security group to allow connectivity, so this is what I'll show you once it's all said and done. Now, I'll scroll down here, let's open up additional config. This is just like traditional RDS engines. The database port is the default for PostgreSQL, but you can change this if you need to, we obviously won't, and we get down here to Database authentication. Now, you might notice it's a little bit different, you don't have three options, you only have two additional options, and that's because password auth is always active so you can't turn that off. However, you can also turn on Kerberos or IAM database authentication if you wanted to. We'll just use password auth for now, and let's look at Monitoring. Monitoring is going to be extremely similar as well. You can have Performance Insights, and you can encrypt those insights, and you can turn on Enhanced Monitoring and change the role or the granularity. Now, we're going to turn these off for the demo, however, you will want to turn these on for real Performance production workloads, you want to be able to monitor, you want to be able to look at insights. For the speed of spinning this cluster up, I'm going to turn them off. Next up, additional config. So again, this is just like traditional RDS, we can have it created in an initial database, which I'll do. We can change cluster parameter groups and actual just individual parameter groups, we're going to leave these default. And we can even specify our failover, so remember, you can specify failover priority for your instances and their tiers. Tier‑0 has a higher priority than tier‑15. We're not testing failover, so I'm going to say No preference, you can choose whatever you want. That brings us next to backups. So remember, you always have to have at least one day for retention, but you get one free day because of that. So, for this, we can go 1 to 35. I'm going to select one, which is the default for the free one and move on. We'll encrypt it. And I'll scroll down here, I'm going to deselect Deletion protection just so I could speed up when I clean up, and let's create our database. Perfect. So now, we're creating our database cluster, as you can see. I'll go ahead and select this, and you'll notice it's creating two instances for us. Now, right now, they're both listed as readers, this will change once our actual cluster is up and running and it elects the actual primary instance. However, for now, I just want to show you we also have Endpoints. Now, I don't want to confuse you, but I do want to tell you this because if you're practicing and you try this and it works, you'll probably be wondering why. The way this works as default how this is set up, using either of these Endpoints will allow you to write. I know it's weird because it says reader, but until we create our own actual Read Replica separate, that's how that will work. So, once this is up at an available state, what I'll do is I'll add a reader here, and we'll demo how we can use that. In the meantime, what I'll do here is I'm going to pause, and I'm going to transition over to the Security Groups to show you what was created, so let me go ahead and jump over there now. Okay, so I'm in my Security Groups under VPC here, and I just wanted to show you the resources that were created for us. So remember, it said it was going to create this RDS EC2 Security Group and it did, and what this does is it allows PostgreSQL traffic over our port range to our EC2 RDS Security Group. So, this Security Group was actually attached to our EC2 instance, so what this does is it's securely locking down any PostgreSQL traffic to only the resources that have this Security Group assigned. This is a best practice and it does come up on the exam. So, with that being said, what I'll do is I'll jump back into the Database dashboard now and let's continue on, where we're going to wrap up the first part of this demo. Okay, so I'm back in my Dashboard. Let's go ahead and wrap up this first portion of this demonstration, and I will see you in part two here in a second.

Demo: Creating an Amazon Aurora Database - Part 2
All right, welcome to part two of our demonstration here. So, our cluster is now available. Our instances are still creating, which is fine, I'll go ahead and pause and fast forward so we don't wait on that. But since our cluster is available, I can go in here under Actions, and I can add a reader, so this is adding a Replica instance here. So what I'll do here is select my Replica source, which is our primary Writer instance, and I'll give my new database instance an identifier, so I'm going to just call this reader. And then we get down to instance config, so what's cool is if you need to, you can change the instance configuration for this Reader Replica, so I can make it larger if I needed to or I could even choose an Optimized Reads class, so they actually have these listed here because they're optimized for read operations. Now, we're in the sandbox, so I have to do a Burstable instance, which is perfectly fine, and I'm going to choose the same instance size. The big thing to know is you can change this and make different instance class configuration settings for your Read Replicas. I'll scroll down here, and you can see we can also have a Destination Region. Again, I'm locked down for the sandbox so I can't change this, but you can put Read Replicas in different regions. Next up is the Connectivity, so I'm going to make this not publicly accessible just like the source. And I'm going to say I don't care about the AZ you deployed in, all I know is it's going to use the subnet group that we deployed the original cluster in. Under additional config, we see the Database port is locked, and the other settings are pretty much similar. Database authentication options, we can turn on or off Enhanced Monitoring and Performance Insights, and we can even set a failover priority tier. I'm going to leave this at No preference because we're not failing over, but you can change these even for your readers. Another thing I want to point out, notice, I can't turn off Encryption, that's because you cannot mix and match an encrypted source and an encrypted or unencrypted destination or Replica, so this has to be turned on since it's turned‑on on the database source. And then, lastly, I'm going to click on Add reader. So, what I'm going to do here is while we're creating this Replica here within our database cluster, I'm going to pause this. I'm going to install my tool sets on my EC2 while we're paused, and then I will resume and I'll talk about what we've done. Okay, so I've resumed now, I've let the writer instance become available so we can use this. Our other Replicas are still being created, which is fine, but I want to show you while these are creating the different endpoints. So we have a Writer, which we looked at in our Reader. However, you can create a custom endpoint and reference this to lock it down to a specific set of instances. So, for this, let's say I want to create one called reader., and I want to point it only to be to our custom reader that we added later. So now, what I can do is click on Create endpoint, and now we have a separate endpoint that we can reference when connecting to our database for read operations. So, this is an option to restrict essentially what instances are getting used all via your own custom endpoint, so we'll use this when we perform custom read operations here later in the demonstration. For now, however, what I'm going to do, once again, is I'll pause. And then once these are all available, I'm going to go ahead and resume. Okay, so our cluster is now fully available. We have our Writer, our Reader that was created for Multi‑AZ, and then our custom reader and our endpoint will finish creating here in a moment with this particular endpoint address. So let's go and test this stuff out now. What I'm going to do first is I need to get the password for authentication for this database. So under Configuration. I'm going to find and select Manage in Secrets Manager. I'll load this up, and we're going to use the credentials in here as the secret to connect to our database via command line. So I need to copy this, I'm going to paste this in a document off screen, and what I'll do is I'm going to go ahead and copy and paste a command here. I'll paste it into this, and I'll press Enter. Now, what this has done is it's created a local environment variable here, admin_password, and it's captured the output of a simple CLI command. So, we're looking at Secrets Manager, I'm getting the secret value of the secret‑id that we just copied, and I'm querying and parsing the output. So I'm looking for the password key, and I'm grabbing the value which is here and saving it. So now, if I echo that admin_password, we get our value, which this should match our password, and it does, so this is working perfectly. So let me clear my screen, and the next thing I want to do is I want to go ahead and I want to connect to my writers instance and start creating tables. So, I'll go back to my database, I'm going to go to connectivity, I'll copy my primary instance here, the Endpoint, I'll go back to Systems Manager, and let me go ahead and copy and paste in a command again, and I'll walk through it. So, what I'm doing here is I'm setting the PGPASSWORD, which this is a command‑line argument for the psql application that's installed, and we're saying, hey, the password is our local environment variable. Now, this is not necessarily the securest way to do this, it's just one of the easier ways so that's why I'm doing this. I'm saying the host is our cluster endpoint, the user is Postgres, and I want to connect to the Pluralsight database. So I'll hit Enter, and there we go. Now, if I list tables, we shouldn't have any, and we don't, so let's create one. Let me go ahead and copy and paste in a SQL query. We create a table called employees with these fields, and now I want to insert data into that table, so let me go ahead and copy another one, paste in this query, and there we go. We inserted three items into our relational database. I'll clear my screen really quickly, and then now what I can do is I can go ahead and create another query where I'm just going to select certain rows from our employees table where the higher date is greater than this, and we have two results. Perfect, so this is working. We now have our table in here as you can see, but now, I want to test connecting via the reader endpoint. So, what I'm going to do here is I'm going to quit. I'll clear my screen, I'll go back to my database details here, and now our reader endpoint is available. So, what I'll do is I'm going to go ahead and copy this, I'm going to paste it into a command off screen again just like I did last time, I'm going to go back to Session Manager, and I'm going to paste this in. So, similar command, same PGPASSWORD, the only difference is the host has changed to our custom endpoint, which points to our reader instance. I'll hit Enter, and there we go. I should be able to list my tables, there it is, employees. But now, let's say I want to test creating a table. So, I'm going to go ahead and copy and paste a query in, CREATE TABLE. This should fail, and there you go, cannot execute this query because you are in a read only‑transaction, and that's because we connected to our reader instance, or our Reader Replica, for our database cluster. And there you go, that's how easy it is to create your very own database cluster within Amazon Aurora. We worked on creating a Reader Replica, we created our own custom endpoint, and we connected to all of these using Secrets Manager managed credentials. Let's go ahead, we'll cut this demo herem, and I will see you in an upcoming clip.

Aurora Serverless
Let's go ahead and look at Aurora Serverless. Amazon Aurora Serverless is an on‑demand, auto‑scaling configuration option for your Amazon Aurora databases. This feature and configuration option is meant to provide you a simple, cost‑effective option for infrequent, intermittent, and even unpredictable or spiky workloads for your databases. The benefit of using Aurora Serverless is that it scales based on the actual demand, so what this means is you don't have to be perfect about planning for your capacity needs. You instead, set a scale and allow Aurora Serverless to handle the scaling needs for you. Because you set this to scale and you're not planning around static instances, it means it could be a more cost‑effective option for you to use for your databases. The fact that you can scale down to next to nothing and then scale up on demand is perfect. However, it is recommended that, most of the time, you're going to want to use Serverless for development or even initial testing, so a good use case is to use this to test your initial workloads and then plan your capacity around those results to get better pricing. However, if you do have workloads that are brand new and you have no idea how databases are going to handle it, then this is a good thing to use. Now, before we wrap up, let's look at something called an Aurora Serverless Capacity Unit, or an ACU. Now, in ACU, or Aurora Capacity Unit, is the unit of measure for Aurora Serverless, it measures the amount of capacity being consumed. Each ACU that you specify, provides your Serverless database with 2 GiB of memory. The more ACU that you actually assign your Serverless database, also means you get more compute and networking power. Now, there's no necessarily specific listing for the amount of computing power per ACU, but the big thing to understand is as you scale up ACUs, that means your Serverless database is also scaling up its compute and networking. And lastly, you can set a minimum of 0.5 ACUs, which would be 1 GiB of memory, and a maximum of 128 ACUs, so 256 GiB of memory. Now, that's going to do it for Aurora Serverless. Some key takeaways, remember, this is perfect for unknown or spiky workloads and initial testing to get a better idea on how to plan your capacity. Also remember that ACUs are how you plan your scaling for your databases. Let's go ahead, we're going to wrap this up, and we're actually going to jump into a demonstration clip, where we create, or walkthrough, creating a serverless database.

Demo: Creating an Amazon Aurora Serverless Database
In this demonstration clip, we're going to work on creating a brand new Amazon Aurora database, however, we're going to use this serverless engine type. So, let's go ahead and jump into the sandbox now. All right, I'm in my sandbox environment, let's get started. I've loaded up RDS here, and the first thing I want to do is create a new database. Now, I'm going to use Standard, and I'm going to select Aurora PostgreSQL because it doesn't really matter how I'm going to actually create this database because the sandbox doesn't allow it, and even if I did, I wouldn't connect to it. So, I'm going to leave the default here, we'll scroll down, and I'm going to find Settings here. So, notice everything is still the same. You give it a cluster identifier, you set the master username and you can set the password, or let Secrets Manager own it. And when we come down here, the big difference that we're going to see here is Serverless v2 as Instance configuration. So, when you do this, notice it's Serverless v2 is the only option right now, and just like it says, it's for instance scaling for your different workloads. Now, the big difference with Serverless and any other instance selection here, you have this static amount of resource listed, 2 vCPU and 8 GiB of RAM. But with Serverless, you specify Capacity ranges. So, what you do is you set a minimum scale and a maximum scale, so how much do you want to allow it to scale? Now, the minimum can be all the way down to 0.5, and the maximum can be up to 128, so this can scale quite a bit. And you'll notice, like this section I just highlighted, I'm not necessarily a big fan of this, but it is what it is, this is how they named it, but one Aurora Capacity Unit provides 2 GiB of memory. And with that memory, you get corresponding compute and networking. So, what this is saying is for every 1 ACU, you get 2 Gib of memory. And, every time you increase your ACU count, you're getting more corresponding compute and higher networking bandwidth and performance. So essentially, the bigger the number for your ACUs, the higher performing your actual Serverless database will be. So, right now, as the defaults, the minimum is 8 ACUs and that provides 16 GiB of memory, as well as the corresponding networking compute, and we have the maximum at 64, which provides 128 GiB of memory. Now, you need to be semi‑careful planning here because you do get billed for your consumed capacity units, so if you set this to 128 and you are constantly hitting this limit, well, you're going to be getting billed for using that amount of capacity, so keep that in mind. This is meant to be a more optimized solution for scaling and cost and it might not be the case if you're constantly maxing this out. So, we'll go ahead and I'm going to reset this to 24, and then I'll set the minimum to 1, and you can see it changes the scale for us. Now, I'll scroll down here and let's just view some other settings. Notice, you can still deploy Multi‑AZ, you can set the connectivity up just like a Standard database cluster, so you can say, hey, connect or don't connect, you can select the VPC, as well as the subnet groups for that VPC, and you can even allow public access to be in place or not. Pretty much everything else is the exact same as a normal database cluster, your VPC security group, different database ports you can set, etc. So, if we scroll down here, you're going to notice, yeah, it's pretty much the same, Performance Insights, Enhanced Monitoring if you wanted it, etc. You can have them create an initial database, you can set the failover priority, which is pretty interesting because it's Serverless, so it's hard to think sometimes that you can failover between Serverless databases, but this is an option. And then, finally, the backups and encryption, everything is almost identical, minus where you set the capacity range up here at the top where we were selecting the instance configuration. Now, that's going to do it. You're not going to be able to create this database within the sandbox unfortunately, it's not allowed, but I just wanted to walk you through what it would look like if you were creating one. Everything else is pretty much identical, minus where you set the actual resources for your scaling. So let's go ahead, we're going to end this demonstration here, and I will see you in an upcoming clip.

Amazon Aurora Global Databases
For the exam, you're going to need to be familiar with Aurora Global Databases, so let's look at these now. An Aurora Global Database is the recommended DR solution that's going to help span multiple AWS Regions. When you create a global database, it creates a new database in a different Region and it allows for low‑latency global reads and easier disaster recovery for an entire Region‑wide outage. How it works from infrastructure standpoint is one of your AWS Regions is considered the primary Region, and this is where your data is going to be written to and then even managed within. After that, it's going to replicate data to the different Regions that you've selected for your Global Database. And speaking of those other Regions, you can have up to five read‑only secondary AWS Regions. The whole purpose of using a global database is it allows for dedicated infrastructure for a better level of handling failovers. Now, this might be a little confusing, but let's go ahead and look at these a little bit more in‑depth here. How they work is Aurora is going to actually replicate the data from the primary Region that you choose to all of the other secondary Regions that you've set up. Within those secondary Regions, you can actually spin up an additional 16 Read Replicas for each of them, so you can scale to a massive size for your infrastructure if you actually need it. On top of this, replication between the Regions is typically going to be less than a one full second, it is very fast, there is very low replication time. And on top of this, disaster recovery and promotion of your secondary Regions is actually going to allow for an RTO of less than one minute, that is an impressive recovery time objective, especially considering that you're scaling to a global scale for your database workloads. And in fact, this is something to keep in mind for the exam, if you need an RTO of less than one minute, especially at a global scale, you want to think global databases within Amazon Aurora. Now let's talk about database operations with these global databases. First up, global database switchover, this is an operation where you're going to have planned operational procedures in place, so things like rotating between the different Regions that you've set up. They also offer global database failover, these will be for recovery of your Amazon Aurora Global Databases after an outage. So these are the two primary operations, I don't necessarily think you will get tested on these, however, these are good to know for real‑world stuff. Big thing here, switchovers are for planned operations, failovers are for recovery of outages. Now that's going to do it for Global Databases in Amazon Aurora. Remember, you have a primary Region and then you have up to five read‑only secondary Regions, and there's very low replication time and very fast recovery. Let's go ahead, we're going go wrap this up here, and I will see you in an upcoming clip.

Amazon Aurora Machine Learning
Let's talk about Amazon Aurora machine learning. Aurora machine learning is a service feature within Aurora that's going to allow you to go ahead and incorporate predictions via different machine learning services that exist within AWS. The nice thing about it is it doesn't require that you have any experience with machine learning. Let's look at the services that it interacts with. It works with Amazon Bedrock, which is a fully‑managed service that's offering use of the leading foundation models. It works with Amazon Comprehend, which is meant to be a managed NLP service where you can extract insights and sentiments from documents and different inputs. And lastly, it works with SageMaker, this is a fully managed, machine‑learning service that is primarily used by data scientists and developers within the machine‑learning world. Now, these services are way out of scope from an in‑depth level, we'll cover them later on in a different portion of this exam prep. But for now, just understand Aurora machine learning interacts with these three services very easily. And speaking of that, let's talk three use cases, these are all possible exempt scenarios you need to be aware of. You can use this to detect fraud within your applications for your database workloads, you can use it to build customized marketing and customized ads for customers, and you can even use it to recommend products to customers all based off the data that lives within your Aurora database clusters. Now, that's going to do it for this clip. Let's go ahead and wrap things up. And then, coming up, we're actually going to have a quick module summary and review some exam tips.

Module Summary and Exam Tips
Okay, let's get some module summaries and exam tips reviewed before we move on. First up, Amazon Aurora as a Service. This is a fully‑managed relational database engine based on proprietary AWS technology. Remember that it runs on Amazon RDS, but it uses their own engine. And speaking of engine, it is only compatible with MySQL and PostgreSQL databases. Now, remember it's not exactly either of those two engine types, however, it only works with either a MySQL or PostgreSQL‑compatible workloads. Now, it's very high performing. However, it does cost roughly 20% more compared to traditional RDS workload, so you got to keep that in mind. There are tons of benefits, but there's also more cost. And speaking of benefits, it says within the documentation at least, that it's meant to provide up to five times better performance than traditional MySQL and up to three times better performance than traditional PostgreSQL when compared to using RDS. Now, moving on, let's talk about storage and data within Aurora. Remember, you can autoscale your storage with this service, and storage is going to start at a minimum of 10 GB, and it can go all the way up to 128 TB, so it can store a lot of data. One of the bragging points is the data redundancy within Aurora, it maintains a minimum of six copies of your data spread across three availability zones in your Region. In addition to that, the service will automatically try to recover your database as long as there is a healthy AZ available. Now, you really need to be familiar with database clusters with Aurora as well. I'm not going to review this entire diagram again, but I wanted to put it here to refresh your memory. Remember, you have one primary instance that's a reader and a writer, and then you have reader instances and Read Replicas that you can use, and those can be spread across different AZs and even different Regions. You can have up to 15 Read Replicas spread across AZs for your clusters, and be sure to remember that they all use a cluster volume, which is a shared storage that's replicated between the AZs. Now let's talk about high availability and scaling with Aurora. Remember, Aurora has autoscaling, and this is meant to dynamically adjust the number of Replicas within your cluster. When you have primary and reader Replicas, the primary database instances are automatically replicated across the different AZs to your Replicas, and it's done very quickly. Regarding Replicas, remember, you can leverage Replica promotion if you want to, or you can actually let Aurora create a new primary instance for you if you ever have a failover event. When Replicas get promoted, their priorities range from 0 to 15, and 0 has the highest priority, you do need to know that for the exam. And then, lastly, promoting a Replica via failover is going to be faster than having Aurora create a brand new primary instance, so if it comes down to speed, think about promotion. Moving on to backups, remember, you can leverage automated backups with Aurora, in fact, they're enabled by default, they're continuous and incremental just like traditional RDS backups. You can specify anywhere between 1 and 35 days for backup retention. And remember, you can't set this to zero, but you get one day free forever via these backups. And then, lastly, just like the traditional RDS setups, these are stored in S3 and you have no access to the underlying storage. You can only manage the backups from RDS. Next, remember about backtracking. First off, this is not a replacement for backups, remember that. In fact, the backtrack window is only going to allow up to 72 hours of backtrack time periods, so anything more than 72 hours can't use this feature. Now, when you use this feature, it does cause a brief disruption in your database, what that means is you're going to need to pause or stop your applications otherwise you might get some errors. Moving on to Aurora Cloning, remember this feature to quickly set up test environments using real data all without risking any data corruption. Moving on, Amazon Aurora Serverless, this is a perfect solution for scenarios when you have unknown or spiky database workloads. Remember, you set a minimum and a maximum amount of Aurora capacity units and your serverless database scales appropriately on demand. And then finishing things up with Aurora Global Databases. This is the recommended DR solution and it spans multiple Regions within Aurora. You have one primary Region where your data is written and managed and then you can have up to five secondary read‑only Regions that you can actually scale as needed. Do your best to remember you can add Read Replicas even within those secondary Regions, which really allows you to scale massively. And then, lastly, this offers an RTO of less than one minute and an actual recovery point objective of about five seconds, so it's a very impressive RTO and RPO measurement. Now that's going to do it for this module. Thank you for hanging in there. Let's go ahead and wrap it up, review if you need to certain aspects, and then I will see you in the next module whenever you are ready.

Amazon DynamoDB
Amazon DynamoDB Overview
All right, let's get started with this next module, where we're going to explore one of the most common services for database workloads in AWS, Amazon DynamoDB. First things first, let's have an overview of what this service is and what it offers. Amazon DynamoDB is a fast and flexible NoSQL database service for your applications. One of the biggest benefits is it offers a consistent, single‑digit millisecond latency at any scale. Now, nonrelational, or NoSQL, databases tend to be considered unstructured and they vary with their data models. When we say unstructured, what we mean is that their schemas are essentially flexible. You can add different attributes on the fly as needed. Let's explore what DynamoDB offers more in depth. First and foremost, this is the fully‑managed serverless purpose‑built database meant to support document and key‑value data models. If you hear NoSQL, you want to immediately think DynamoDB as the leading option. It automatically handles scaling for you for massive workloads and it supports millions of requests per second. In addition to that, it supports hundreds of terabytes of storage. No matter the size of your workloads, it still offers single‑digit millisecond performance. So if you grow two terabytes of storage and you have thousands of requests per second, well, you still get single‑digit millisecond performance, which is one of the biggest reasons you might choose this solution. Some of the use cases include things like mobile apps and web applications where you're storing unstructured data. It's very good for online gaming, so storing scoreboards, user performances, and their records, etc., and it's good for financial trading applications. Moving on, let's explore what are called tables. Your data within DynamoDB is stored within a table. Each table is going to have a unique Primary Key. This Primary Key is also referred to as a partition key because it's how you partition your data within your tables. Each row within your table is considered an item. Tables can have an unlimited number of rows, in other words, they can have an unlimited number of items, which again is one of the biggest draws to using this service. Each item or each row is going to have multiple columns, and those columns are called attributes. You can add attributes to your rows at any point in time, this is why we say this is a flexible schema. Now, there are some limitations for items. So even though you can have an unlimited amount of items, each item can only be up to 400 KB in size. Now, if you think about it, that's a pretty big item because items are textual‑based essentially and they're just going contain some type of text. So 400 KB is a lot, but it is a limit. And then lastly, one of the biggest points here that we like to reinforce, your schemas can evolve over time as you need them to. This is one of the biggest benefits of using NoSQL databases as opposed to relational databases. Next up, let's talk schema data types. There are three types of data that can go within your attributes within your items. There's scalar types, which are going to be things like strings, numbers, Booleans, and nulls. You have document types, so lists and maps. And when we say maps, we essentially mean JSON documents. And then, lastly, there's a set type, so string sets, number sets, and binary sets. Now here are some examples of each of those individual types that we just talked about. I'm not going to explain each of these, I just want to show you them and have them for future reference if you want to look back at this later on. Moving on, let's look at example tables, so this first example of a table in DynamoDB has a single Primary Key. We're going to call the table Products, and we're going to pretend we're storing product information about gaming consoles. So this first portion here would be considered the Primary Key, and this is a single Primary Key. What that means is it is made up of a single unique attribute, and this would be the partition key. We use this key to reference that individual item. Each of these other portions of the table, so product_name, price, quantity, etc., are the attributes of the item. Let's move on to a composite Primary Key. So a similar table, but this time we're going to track orders for customers. So, in our Orders table, we could leverage these two attributes here on the left side to make up a composite Primary Key. What that means is it's made up of two attributes that make up a unique Primary Key. The first one here, the customer ID, would be the partition key, and the second key would be the sort key. So you use these in combination with one another to make up a composite Primary Key to better scan or query your data. Big thing here, exam pro tip. If there is a NoSQL or unstructured scheme of requirement, I would immediately think and consider using Amazon DynamoDB. Let's go ahead, we're going to end things here. We reviewed what DynamoDB is. We talked about tables, items, and attributes, and we even reviewed an example. Let's wrap up, and I'll see you in the next clip.

Amazon DynamoDB High-availability and Monitoring
On the exam, you're going to need to know how you can implement DynamoDB, leverage high availability, and even monitor some of the metrics. Let's go and start talking about Amazon DynamoDB resilience. DynamoDB will automatically replicate your data across three distinct availability zones within your region. In addition to this, the service itself offers a 99.99% service‑level agreement for availability of your data. And lastly, DynamoDB is meant to provide zero downtime maintenance that is all handled by AWS. Since it's a managed service, that means they handle all of the underlying infrastructure for you. Moving on, let's talk about monitoring. First up is Amazon CloudWatch. DynamoDB will automatically monitor your tables, and it can report metrics to Amazon CloudWatch for you to monitor and even report on. Also, you can go ahead and leverage AWS CloudTrail to capture any API calls and any related events made to your DynamoDB resources, and then you can log them and analyze them as you see fit. Now, a big thing here, exam pro tip. Your tables within DynamoDB are Regional resources, so you don't specify the AZs you only pick the Region. Now, with that being said, let's go ahead and wrap this clip up. We talked about how redundant your data is, it's spread across the three AZs. We discussed the service‑level agreement, as well as some different monitoring methods that you can use. We'll end this here, and I'll see you in the next one.

DynamoDB Capacity Modes
All right, let's talk about capacity modes within DynamoDB. Capacity modes are one of the most critical things you need to understand when planning your DynamoDB workloads. There are two different types, there's provisioned and there's on‑demand. With provisioned mode, you manually define the amount of capacity that is required for reading and writing from your tables. With on‑demand mode, this is a flexible capacity mode, allowing you to be on demand or pay per request for your capacity that's being consumed. With DynamoDB provisioned mode, you have a write capacity unit, also known as a WCU, for all of your writes. With on‑demand, you have read and writes automatically scaled for you. With provisioned, in addition to your write capacity units, you also have read capacity units, so these are RCUs, and these specify the amount of capacity required for reading from your tables. A big differentiator here is for on‑demand, you pay per request, so your pricing is based on the amount of requests that are being made for reads and writes. For provisioned mode, however, you're going to pay for your provision capacity units even if you are not using them. So, if you do a bad job of provisioning your capacity units and you don't really use them, well, then you're over paying for what you actually use. On‑demand itself is meant to be a cost‑savings model for spiky or unpredictable workloads, however, it could end up being more expensive than provisioned if you're really hammering your tables so you need to really take your time to make sure this is the best option for you. With provisioned mode, it requires that you plan correctly to leverage it properly. You could get throttling errors if you don't give it enough capacity units, or you could be overbilled for not using everything that you provided. However, with on‑demand, this is going to be perfect for unpredictable and spiky workloads like we just talked about. Now, the downside here is you need to be careful, if you're constantly talking to your table and reading and writing from it, well, then on‑demand might not be the best choice, you might want to use provisioned instead. Moving on, let's talk about consistency with your data and capacity units more in depth. You need to understand strongly and eventually consistent, so let's explore some of those now. Strongly consistent read requests are going to consume one read capacity unit, or RCU, per individual read request. This is going to return a result that reflects all writes that have occurred successfully within your table prior to this read, so it's essentially immediate. Eventually consistent read requests are going to consume one half of a read capacity unit per request. These are going to offer consistency across all copies of your data, and it's going to usually be reached within a second of them being written. These offer the best read performance based on latency measurements. With that being said, if you need data immediately returned, you're going to want to use a strongly consistent read. And lastly here, for write capacity units, it always requires one write capacity unit for one write per second for items up to 1 KB in size. So, if you have a 2 KB item and you want to write it, that's going to cost you two write capacity units for one second. Now, I've included this cheat sheet here for capacity unit reference. I'm not going to explain these to in depth, but I leave these here for you to reference later on. For one read capacity unit, you can get up to two eventually consistent reads for up to 4 KB per second per read. You get one strongly consistent read for items that are 4 KB inside per second per read. The big thing to note here is the items are 4 KB in size per second as your unit of measurement. And then lastly here, again, you have one write of up to 1 KB per second per item. Now let's have a quick exam pro tip to review these. If an item is over 4 KB, you want to round up to the nearest capacity unit requirement for your reads. What we mean by this is one strongly‑consistent read for an item that is 5 KB, it's going to take 2 RCUs because the nearest number that's divisible by 4 would be 8, so that's going to take 2 units. Moving on, let's actually look at some capacity unit action examples, so these are the API calls that consume your capacity units. For the read consumption calls, we have things like GetItem, BatchGetItem, Scan, and Query. And then, for the write consumption, we have PutItem, UpdateItem, DeleteItem, and BatchWriteItem. These are all real‑world examples of the API calls th.at consume your capacity. Moving on, we just briefly looked at scans and queries, let's talk about those, you have to know how to use these to optimize performance. For scans, when you perform a scan on your table, it reads every item in the table or every secondary index, and it's going to return all data attributes for every item in the table or that index. The thing to realize here is these are way more resource intensive, and they cost way more from a read capacity unit standpoint. A query, on the other hand, is an operation to find items based on specific primary key values and then it will only return those items that match that particular partition key value that you specify. You're going to want to use queries whenever possible, they're much more efficient and cost effective. Moving on to another topic called DynamoDB auto scaling. You're able to leverage what is called AWS Application Auto Scaling to dynamically adjust your provisioned throughput capacity for you. How it works is you create a scaling policy for your table or your global secondary index and you apply it to your tables. This scaling policy is going to specify if you want to scale read capacity, write capacity, or maybe you want to scale both. So what happens is you specify the minimum and the maximum provision capacity units for your table and your index, so this one takes advantage of the provisioned throughput mode. That's going to do it for this lesson on DynamoDB capacity modes and units. Do your best to remember the provisioned mode and the on‑demand mode, as well as the use cases. You need to understand when you would consume write capacity units and read capacity units, as well as when you would use a query and a scan. Let's go ahead, we'll wrap this clip up here. And coming up next, we're actually going to have a demonstration where we create our very first DynamoDB table.

Demo: Creating an Amazon DynamoDB Table
In this demonstration clip, we're going to work on creating our very own Amazon DynamoDB table, we're going to view some of the settings, and then we're going to work on interacting by putting, deleting, and updating different items. All right, welcome to the AWS sandbox. I've loaded up DynamoDB. I'm in my Dashboard here. Let's create our table. Now, you can do this by selecting Create table over here, or I can go to Tables and select one of the two options here. So I'll click on Create table, and the first thing we have to do is we have to give our table a name. So, I'm going to go ahead and I'm going to paste in my table name here, CustomerOrders. What we're going to do in this demo is create a demonstration very high‑level database to show you how you can track customer orders. Next, we have the Partition key and an optional Sort key. So, Partition keys are required. Remember, this is the primary key for your table and it acts as the hash value, as they say here, that's meant to be used to actually get your items from your table, and basically, it's how you partition the data. Now, if there is an instance where you might have duplicate partition keys, maybe for instance in CustomerOrders, we use the customer ID as the partition key. Well, what happens if they have multiple orders? Well, maybe we can go ahead and make it a composite primary key and make this sort key the order ID, that's just one example. Now, for this simplified demo, I'm only going to use the partition key. So what I'm going to do here is copy in my partition key, and we're going to partition based off OrderId, so each OrderId will be a unique value. For the type, notice we can make it a String, a Number, and Binary. For this, I'm going to make it a Number because I'm going to have my OrderId only contain numbers. Now, obviously, OrderIds could contain Strings and Numbers, so characters and numbers, so maybe I'd want to do a String. But I'm going to do Number because my data is only made up of integers for this key. So we have our partition key set, which is our primary key. I'm going to go ahead, I'm going to scroll down, and next we have table settings. Now, we're not going to choose Default because we want to walk through this, so I'm going to choose Customize, and the first thing we do is choose the Table class. Now, for typical workloads so anything production, etc., you're going to choose Standard. If it's some type of development workload or just something that's hosting data that's never really accessed, you can choose Standard Infrequent Access. What this does is offers you a lower cost for the storage of your items and your data. But again, it's infrequently accessed, so if you're accessing it quite a bit, well, then you're actually going to be getting charged more. So what we're going to do is choose Standard, and we're going to move down here to read and write capacity settings, so this is where we get to our capacity mode we talked about. We could choose Provisioned where we set the capacity units, or we can choose On‑demand where we can just let the DynamoDB table scale for us for all of our workloads. Now, for On‑demand, you can set a maximum throughput level, so you can set the max read and write requests. We're not going to do that, just understand you can do that. What we're going to choose is Provisioned mode. Now, after we do this, we're going to show you how you can switch between the capacity modes. So for now, we're going to choose Provisioned. The first setting we have here is Read capacity. You'll notice, you can have Auto scaling turned on or off. If you have it turned off, you set the static number of provisioned capacity units. If you turn it on, you can say, hey, I want you to scale down to 1 capacity unit, but you can go up to 10 or maybe 100 whatever it is that amount of capacity units. You also set a target utilization rate, so maybe I say 50%. In other words, I want to say, hey, you can go up to 10, down to 1, but aim for about 50%, so 5 capacity units. But maybe for writes we know, hey, I know exactly how much I'm writing, go ahead provision five capacity units and just stay there, this is going to offer you the best pricing point for your tables. If you know your capacity units, it's typically going to be better to statically define them. So we'll leave auto scaling on for read, we'll leave it off for write, and let's scroll down here. Now, you're going to notice Secondary indexes. I'm not going to cover that in this demo, it's out of scope. But this is just meant to perform queries, like they say, on attributes that are not part of your table's primary key. So we have that OrderID, but maybe in the future we realize, hey, we need another index that we need to actually query off of and we can't do that with our current schema, so I want to add a different index. Again, little out of scope for this, so I'm going to skip over this. And we get an estimated cost, which this is just a nice to have. So based on our current provisioning, you see it's going to cost us roughly $2.91 a month just to have these capacity units in place. So that doesn't count on the data, it's not taking that into account, so we'll leave this as is that looks pretty good. Next is Encryption at rest, which I'm going to leave owned by DynamoDB for this. We cover encryption at rest in a different demonstration later on, so I'll leave the defaults. I'll skip over Deletion protection, but you do need to know you can turn this on, which is really important for things that might be production. For instance, maybe you want to prevent any deletion accidentally, you can do that via this trigger. So I'll turn this off, and then we get down to Resource‑based policies. Now this is neat, remember, you can use IAM policies on users and roles or you can use resource‑based policies to control access as well. So an example of a resource‑based policy could be something like this, maybe I want to allow traffic from only a specific IP as you can see here. So, maybe you have some type of NAT address or a proxy or something else for your enterprise and you want to allow this particular user or maybe a group of users and roles all DynamoDB actions as long as it comes from this IP, so this helps serve as a secondary layer of protection in addition to IAM permissions. Now, I'm going to clear this, we don't need to do this for here. I'm an Admin as Cloud user, so just really keep in mind that you can use this to further enhance controls. I'll scroll down here, and I'll click on Create table. And there we go, we see it's creating, we see our partition key, which is a number here. And what I'll do here is I'm going to wait until this is ready and active, and it is, and let's explore this. Perfect. So our table is now up and ready. Perfect. So we have our General Information. We don't have a sort key we just have our partition key, it's provision capacity, etc. So let's actually start playing around with looking at inserting, updating, and getting our data and our items. First thing we can do here, Explore table items. Notice, we're not going to have any items here because, well, we haven't put any items into our table. So instead, I'll go back. I'm going to select my table again, and let's actually, under Actions, I'm going to create a new item. Now typically, you would do this via command line or SDK, but for visualization purposes, I'm doing it in the console so you can see what this looks like. The first thing we have is our OrderId, this is a required attribute because it is our partition key. This is how we partition our data. So, what I'll do is I'll make a number up, 123123. And then, let's say we want to create some new attributes. So this is a CustomerOrders table, I'm going to add a new attribute, and I'm going to call this CustomerName. So, what I'll do here is insert CustomerName, and I'll insert my name. Well, in addition to that, maybe I want to add another number here called OrderTotal. And for this order, I'm going to say it was $19.99. So, OrderTotal, it's a Number, and here's our number. And then, lastly, let's say I want to put in a String set, and we're going to call this Items, so what items were a part of this order? So for this, what I'll do is I'll insert two items. We'll say I bought an Xbox, I'll insert another field, and I'm going to say I bought a Controller. So, in this order in my String set here, I have two separate Strings, Xbox and Controller under our Items Attribute name. So, what I'll do is I'll go here, I'll create my new item, and there we go. So now, if I explore items, I'll go back in here, Explore table items. I refresh, and there we go. So, the first thing I want to call it also is this is a scan of our table. So to scan the table, it costs us 0.5 capacity units for a read. Notice when you Scan, you select the table, and you select the attributes, so you could say All or Specific based on your partition key. So, what we did is I just selected All, and I had basically just scanned the table. So down here, we see our order, so we have our OrderId, CustomerName, the Items list, and the OrderTotal. Now, let's demonstrate how this is a flexible schema. Let's say, you know what, I actually want to have the order date in here as well, I need to track that for my customer orders. Well, you can do that. Let's go in. I'm going to jump over into my terminal here, and I actually have a script that's going to automatically load some data. Now, I'll make this script available for you to use if you want to play with this on your own time, as well as the CSV file, so just keep that in mind. So what I'll do here is execute my script. We're going to get some output here, and there we go. So we added several more orders here, let's go and check that out. I'll jump back into my console. I'm going to Scan my table again, and we see we have many more orders. But notice, we added a different attribute, now we have OrderDate for all of those recently added ones. This is what we mean when we say this schema is flexible, you can add attributes as needed whenever you want, and the existing items don't even need to have it filled out, so that is how it is flexible. That's one of the biggest benefits of using DynamoDB and really just NoSQL in general. So now we have a bunch of items in here, let's practice querying an item. What I'm going to do here is let's just say I want to query only this OrderId. So I'm going to go up here and I'm going to Query. We select our Table, we select specific attributes, and you'll notice we can select OrderId. I'll paste in my partition key value, and I will run it. Now, we only get that data from that order, and you'll notice it just gives us the exact attribute we looked for. Now, another benefit of Query is say we want to query for that same order, I can filter even further. So I can actually look for OrderTotal, we'll use that attribute name. I know this is over 499 so I'm going to say Type Number is Greater than or equal to $450.00. I'll run it, and we get the same item, so we're able to filter based on that particular attribute name that we added. Now, last thing I want to show you, we just explored our items, we queried, and we scanned. One thing to keep in mind for the exam and in your real‑world scenarios, you can change the capacity mode one time every 24 hours. So under Actions here, I can go to Edit capacity, and I can actually switch this to On‑demand. Maybe I realize, hey, I don't need provisioned. This doesn't really get used a lot or I don't know how much it's going to get used, I'm going to set this to On‑demand instead. I'm going to go down here and click on Save changes, and there we go. So now, it's going to update our table for us, you'll see it's set to Updating. And this will eventually be set to On‑demand, where it will scale for us automatically. Now, remember, you can only do this one time per table every 24 hours, so keep that in mind. But with that being said, let's go ahead, we're going to wrap this demo up. We saw how we can explore items. You can go ahead and actually scan or query your table as needed. And because it's flexible in its schema, we can add attributes as required. Let's end this demo here, and I will see you in the next clip.

DynamoDB Security
Okay, in this clip, let's discuss DynamoDB security. DynamoDB, by default, is a publicly‑accessible service, what that means is there's a public endpoint. Another example of a public service would be Amazon S3 or AWS IAM. Because of this, you really want to make sure that you leverage the correct IAM policies to control your access and your authorization to your tables. Now, even though it is public by default, they do offer encryption at rest by default, and it's encrypted using KMS keys, so you can specify your own custom keys or you can leverage the default service key. In addition to this, if you want to restrict traffic to only be within the AWS network, you can leverage VPC endpoints for DynamoDB to keep traffic within AWS. In fact, this is one of two services that can leverage a gateway endpoint, which is free. Next up, you can also leverage resource‑based policies for your tables and your indexes. So you can attach a policy to your tables, which is a resource‑based policy, and control access that way in addition to IAM policies on your user entities. Now, with that being said, this is a very short clip. We just went over an overview of what security means within DynamoDB. Remember, it's publicly accessible, you leverage IAM and resource‑based policies, and you have encryption by default. Let's go ahead, we're going to wrap this up here in. And in an upcoming clip, we're going to have a demo where we create a custom table and use a custom key for our encryption needs.

Demo: Using Different KMS Keys for DynamoDB Encryption
All right, in this demonstration, we're going to create multiple DynamoDB tables. And then for each table, we're going to specify a different KMS key for encryption at rest. So let's jump into the console now. All right, I'm in my sandbox environment. I've loaded up my DynamoDB Dashboard. Let's create two different DynamoDB tables with two different encryption keys. First thing I'm going to do is create a new table. We're going to go ahead and give this table a name. So for this, I'm going to call it DefaultTable. And for Partition key, I'm just going to make something up. We'll call it cust_id. And then when I go down here, I'm going to click on Customize settings because this is the only way we can change the KMS settings in the console. So I'm going to skip over a majority of this stuff, we covered it in a different demonstration clip. I'm going to scroll down here, and I'm going to find Encryption at rest. So, by default, it uses a key owned by Amazon DynamoDB. So, no matter what, these are all KMS keys, the only difference is the amount of control and the cost associated with the keys. Now, also keep in mind, we cover KMS far more in depth in a different course within this learning path, this demo is only going to cover it at a high enough level to understand how it integrates with DynamoDB. With that being said, the default option is free to use, so you're getting free encryption at rest using this key. The only problem is anyone that has access to your tables and can actually get items also can decrypt the data. You have no control on who can use this key. You also have AWS managed key options, which is going to be an AWS KMS key, which is specific to your account, but managed by AWS. So this offers a bit more control, however, you can't manage the key yourself, it's just only used by your account. That's another thing to call out, if you use this option, this key is not necessarily unique to only you. This option is unique, and you can see it gives you an alias which will be created for you if it doesn't already exist. A big difference is there are charges that apply, so you do get billed whenever this key is used, so if you encrypt, when you're decrypting, etc. So, for this, we're going to choose this option. I'll show you the next option in our next table. So I'll use the AWS managed key. I'm going to scroll down here, and let's click on Create table. Perfect. So this is creating and eventually it will be active. But for now, what I'm going to do is create a different table. I'm going to Create table. I'll give this a name, we'll call it CustomTable. I'll give it a similar partition key. We're going to scroll down, customize our settings, go down to Encryption at rest, and here we can choose a specific custom KMS key in our account. Now, the thing here is we don't have a key that I've created, so I need to make one. So what I'll do is I'm going to load up KMS in a different tab here, Key Management Service, and under here, let's create a brand new key. But, before I do this, I want to show you the AWS managed key, which is an option here. You're going to notice we have DynamoDB, so this was that alias that they said would be there when we configured that previous table. So we have no control over this key, notice it's created, managed, and used on our behalf by AWS, specifically a service within AWS. All we can do is view the keys and audit their usage, that's it. So this key is what's being used by our table in order to encrypt our items that live within that first one. So, what I want to do is, under KMS, I'm going to create a Customer managed key. So what I'll do is create a key. And again, I want to reiterate, we're not going to get into all of these options in this course, this is covered much more in depth in a different course. So I'm going to select the defaults here. One thing I will call out is it has to be symmetric for DynamoDB to use it. I'll click on Next. I'll give my display name here. We can give it a description. We can go down to Tags, which I'll skip. I'm going to click on Next, and now we choose an admin, so what key admin do you want to assign. So, I'm going to select us, cloud_user, so we can delete this, update the policy, etc. I'll go to Next, and now we specify users. So, what I'm going to do here is skip this part because I'm going to paste in a custom policy. I'll click on Next, and we get our Review screen. So we're creating a new key, we have our alias here, our description, we have our key policy, so this is what I want to edit here. Right now, we are allowing cloud_user to be an admin so I can do everything I need to, I can create one, I can delete it, etc. One thing to keep in mind here is once this key is gone, if you're using it to encrypt data, well, then you can't decrypt that data. The key is no longer around, you're out of luck, so be sure when you schedule a key for deletion, you really want to delete it. With that being said, let me go ahead, I'm going to paste in and updated policy. And all I've added here to this additional policy was I'm allowing DynamoDB access to Encrypt, Decrypt, etc., to use this key. So, we're restricting it to this account for the DynamoDB service and it can use it. It can't do anything administrative with the key, though. So, I've had this in here, I'll click on Finish, and now we have our new key, CustomDynamoDB key, as you can see here. So what I'll do is I'll copy this ARN, go back to my table, and I'm going to enter an ARN in here manually. So now we're saying, hey, for all items and data within this table, use this custom KMS key to encrypt that data. So I can go down, I can click on Create table, and there we go. So now, we have two different tables with two different encryption at rest methods or keys being used. A perfect use case for this is maybe you have two tables with two different user groups and for compliance purposes, you want to make sure they can only access their respective table, well, that's possible using the method we just did. With this CustomTable, based on the encryption protocol that we used, or the encryption key that we used, only users with access to this KMS key granted via this key policy that we have here, or via IAM, are able to actually decrypt and get items within this table, so this is a perfect security measure to protect your data within DynamoDB further than the default keys that are offered. And with that being said, I think that's going to do it for this demonstration. We created two different tables using two different encryption keys with KMS. Let's go ahead and wrap this up, and I'll see you in the next clip.

DynamoDB Global Tables
In this clip, we're going to discuss global tables. In Amazon DynamoDB Global Table is a feature that deploys a multi‑Region, multi‑master database within DynamoDB. These work to provide you very low latency for replication between your different tables across the globe. When we say low latency, we mean less than one second. You use this when you want to achieve an Active‑Active implementation of your DynamoDB tables, this means that you can read and you can write to any of the tables within your selected Regions. How it works is DynamoDB is going to create identical tables within your chosen Regions, and then it will automatically propagate ongoing data changes between the different tables. One prerequisite here is you must enable a feature called a DynamoDB Stream, which we will cover in‑depth later before you can actually use a global table. Streams are actually the underlying functionality and how these work. Now let's look at an architecture before we wrap this clip up. In this diagram, we've deployed a global table within DynamoDB. The global tables are going to provide automatic, multi‑active replication to all of the selective Regions that you've deployed to. With this, any changes that are made, so PutItems, DeleteItems, etc., are replicated to offer fast and localized read and write performance at a global scale. So as soon as you put an item or delete an item, that change is reflected in less than a second across the globe. So, with that being said, remember, global tables are a perfect choice if you have global workloads with global infrastructure and you need to achieve Active‑Active implementations from all of the different locations that you live in. Let's go ahead, we'll wrap this clip up here. We just discussed global tables, and upcoming next, we're going to talk about Streams.

DynamoDB Streams
All right, let's talk about DynamoDB Streams. DynamoDB Streams are a very important concept for DynamoDB tables. What they do is they capture a time‑ordered sequence of the item‑level modifications. So if you update an item, you delete an item, you create an item, that is a time‑ordered sequence that is captured. The records within the Streams are created for any creates, updates, or deletes of any item within any table that these are enabled on. In addition to that, the information for the Streams are stored for up to 24 hours so you can reference historical data for 24 hours or less. On top of the amount of time that they're live for Streams are also written in near‑real time, so it's not immediate, it's not real time, it is near‑real time, that's a very important distinguisher to keep in mind. And then, lastly here, DynamoDB Streams are commonly combined with what is called a Lambda function for triggering workloads. Lambda functions are the serverless platform in AWS, they're very useful for event‑driven workloads, and this would be a perfect scenario to use them. Now, let's talk about some specifications for these Streams. The StreamViewType is the configuration where you specify what kind of information you want to be written to the stream. Right now, at the time of recording, there are four types available. Let's go over the four types. Now, the first is KEYS_ONLY, this is where it's only going to capture the key attributes of the modified item. So remember, you have a primary key and then all of the attributes with that item in that row, well, this would only capture the key attributes for that. The next is new image, so this is going to give you the entire item as it appears after the modification occurred. So example, maybe you added an attribute to one of the existing items, well, then that's going to be captured in this NEW_IMAGE view type. You then have OLD_IMAGE, so this will be the entire item as it appeared before it was modified. So, using that same example, if we added an attribute to our item, well, it wouldn't appear in this view type because this captures the OLD_ITEM. This would be good for historical reference and capturing or storing of data. And then, lastly, NEW_AND_OLD_IMAGES, this is going to contain both the new and the old image of the item that was modified. So, using that same example, if we added a field, you would get two images, one without the field and one with the field. Now let's look at the architecture of for our Streams before we wrap things up. We're going to assume we have an EC2 instance that has permission to put an item into our table. Well, if we have streams enabled, what's going to happen here is that that change is going to be recorded within our Stream, so we have the data changes being captured and recorded whenever they happen. Now, each individual change that occurs for your table in a Stream is called a Stream Record, they're going to contain the primary key attributes of the individual item that was modified. In addition to this, depending on the Stream view type that you chose, it could contain other items as well. Each record that is recorded is going to appear exactly once and only once within the stream, and it's going to be in the same sequence that it occurred in compared to other Stream records. When you have a group of Stream records, that actually makes up a Shard, which is on the right side of the diagram. So, Shards will occur based on the amount of data changes, different buffering, time limits, etc. A very common architecture to be aware of is leveraging a DynamoDB Stream to invoke something like a Lambda function. You can actually natively trigger Lambda functions to perform event‑driven workloads based on data Stream changes. So, for example, maybe you want to investigate old and new images and compare the differences, well, you can do that very easily with the Stream. Now that's going to do it for this lesson and clip on DynamoDB Streams. Do your best to remember some of those important concepts we reviewed. Remember the different view types that are possible and remember this common architecture we're looking at here. I will say another really big thing to keep in mind is that streams are the fundamental way that global tables work within DynamoDB. This is exactly how all of the data is replicated for changes between the different regions. Now, that's going to do it for this lesson and clip on Streams. We'll go ahead and end this here, and I will see you in the next one.

Demo: Triggering Lambda Functions via DynamoDB Streams
In this demonstration clip, we're going to go ahead and we're going to have a demo on triggering a Lambda function using DynamoDB Streams. Before we jump into the console, let's look at the architecture really quickly. What we're going to do is we're going to have two different DynamoDB tables, as you can see here on the left. We're going to have a source table where we're going to enable a Stream. Now that Stream is going to be set up as a trigger for our Lambda to function. So what's going to happen is that Stream is going to have the data changes, so inserts, updates, deletes, and once that function is invoked, the function will print the new NEW_IMAGE and then archive the OLD_IMAGE into a target table, which is a completely separate table. So, without further ado, let's go ahead and we're going to jump into the console now. All right, I'm in my sandbox environment. Let's get started. First thing I want to do, I'm going to create our two DynamoDB tables. So I'm going to create my first table here, and we're going to call this SourceTable. We're going to give it a partition key of id. I'm going to scroll down, I'm going to customize the settings, and I'm going to turn off auto scaling for both the read and writes. I'll skip down and click on Create. While this is creating, I'm going to do the same exact thing, but for our TargetTable. So create table, I'll call it TargetTable. Give it the same partition key. I'm going to customize and do the same exact settings here, so Off and Off, and then I will create. Perfect. So now we have our two DynamoDB tables that we're going to use, the next thing I want to do is I'm going to jump into my Lambda functions here. So, under Lambda, I've selected Functions, and I'm going to create a new function. I'm going to author it from scratch, and I'm going to give it a name of StreamsFunction. For runtime, I'm going to choose Python 3.12 because that is what I'm most familiar with and the most comfortable with. We're going to leave the role as the new role with basic permissions, but we're going to have to edit this, and I'll show you exactly why here in a little bit. So I'll leave this as is. I'll create my new function. And there we go, we've now created our brand new Streams function. So, under Configuration, Permissions, we actually have our role here. I'm going to load this up in a different tab for later. But you'll notice, all it does is it allows us to create log groups and log Streams with events for our Lambda functions named within CloudWatch logs. Now, what I'm going to do here is go to Code, and I'm going to copy and paste in some code here. Before I do this, notice for this sandbox, I'm using the new console editor. You're more than welcome to switch to the old one. I'm not going to walk through the differences, so feel free to do whichever editor you're most comfortable with. I'm going to leave this new one in place, and I'm going to continue on. So I'm going to paste in some code here, and we'll walk through it really quickly. I'll zoom in just a little bit. I'll accept this to get it out of the way. And unfortunately, I have to re‑paste in my code, which is awesome, thank you very much AWS. But let's go ahead and resume. Now, what this does at a very high level, we're creating a DynamoDB client for boto3, I'm setting a TARGET_TABLE_NAME variable using environment variables that I'm about to set right after this, and we're performing some logic. I'm printing the event so we can see what's going on. And then, for each record in our event, we're assuming that it's going to be old images and new images for the Stream view type. We're printing out the new image. And then, for the old image, we're actually writing it to this TARGET_TABLE_NAME to essentially archive it in a separate place. Now, that's very high level. It's a little out of scope, so I'm not going to dive into this too in‑depth. But, understand, this code will be available for you to play around with if you so choose. So, I'm going to deploy my code, it's now updated, and the next thing I want to do is go to Configuration. I'm going to go to Environment variables, and I'm going to Edit and add a new one. So I'm going to add a TARGET_TABLE_NAME. And then for the TARGET_TABLE_NAME, I'm going to go ahead and select TargetTable and paste that in as the value. Perfect. Now, the next thing I want to do here is I want to set up my trigger for this function. So under Tables, SourceTable, I'm going to find Exports and streams. Now, in here, we can export to S3, we can set up some other data Stream stuff, but I want to go to DynamoDB streams. So, in here, I'm going to turn this on. Now, this is where we select the Stream view type. So do you want only the key attributes, do you want only the new image, only the old image, or do you want both new and old? For this demo, I'm choosing new and old. So I'll select it, and I'm going to Turn on the Stream. So this is now turned on, so if I scroll down here, close the sidebar to make it easier to read, we have our Streams in place, they're on, and here's our view type. Now, we can set up a trigger from within this console, so let's go ahead and try and do that. I can tell you this is going to break because we don't have proper permissions, and I'll show you what that means. So, I'll click on Create trigger, I can choose my functions, so I'm going to choose StreamsFunction here. Now, for Batch size, you can choose the number of records you want your function to read at one time before it's actually invoked, so you can set between 1 and 10,000 records. For performance reasons, you'd likely want to have a much bigger batch size, but it's all dependent on your use case. For this demo, we're doing a very simple example, so I'm going to set one, and I'm going to turn on the trigger. So when I click Create, oh, well, cannot access the Stream because it doesn't have the right permissions. Notice that the role for the function can't perform these actions in IAM, so let's change that. I'll Cancel here. I'll go to my IAM role. I'm going to add a new inline policy here, and I'm going to copy and paste some JSON in. Now, this is just adding on to what we already have. I'm creating log groups, putting log events, and I'm actually allowing all DynamoDB actions for all tables in this account. Now, I'll say this really quickly, don't ever do this, I'm only doing this for sake of simplicity for the demo. So I'm going to go ahead, I'm going to Save this, give it a policy name, create my policy, and there we go. So now, I'll close this, go back to DynamoDB, I'm going to go down here, and reset up my trigger. So, I'll create it, select my function, create my trigger, and there we go. So now, at the bottom, we can see what triggers. We see it's Enabled, and we see the last processing result. So let's actually test this out. We have our function in place here. If I refresh this page, we're actually going to go ahead and see that our trigger is now in place on the UI. There you go, DynamoDB, so the Stream is now set up to invoke our function. Now, for this, I'm going to go ahead and go to Monitor and click View CloudWatch logs because it offers me a shortcut to go ahead and view the logs that are going to occur. Now, there won't be any yet, which is fine, so I'm going to go back to log groups and we'll wait. Now to go ahead and get this going, I'm going to start adding some items. So I'm going to go to SourceTable, create new item, we'll give it an id of 001, and I'm going to add a new attribute of a String and we'll call this description. I'll go ahead and say old for the value of our attribute of description as a String and create my item. Awesome. So now I have my item, let's go ahead and change this item. So I'm going to select this, Actions, Edit. So what I'll do is let's go ahead and change the description, so I'm going to go in and say this is new. Now, you're going to notice when I click Save and close, what it actually does is it deletes and then recreates the item. So now we have this in here, if I go back to CloudWatch and refresh, we have our log Stream. If I go in here, we'll go down to Log streams. There we go, so here is the event, so this is what the event looks like in JSON, you see the eventID, eventSource, we see the Region, and then some information about the record. So you'll notice, in here, we have NewImage and OldImage. So you can see the NewImage was what we updated and then OldImage was the original one before it was changed. You can also see the Stream view type as NEW_AND_OLD_IMAGES, which is what we configured. Now with the other log records, you can see successfully wrote old image to TargetTable with the old image attributes and information. And then the new image is logged, well, within the new table. So now, if I go in DynamoDB, look at TargetTable, we can see, hey, our old item was written here, so 001 id, old description, and we added a RecordedAt timestamp to say, hey, this was the old item, and this is when it was changed. So now, I can go to SourcTable, and I can add a new item here. We'll give it an id of two. We'll add a new attribute of we'll just say a Boolean, we'll set it to False, and I'll create the item, and this will start to trigger more events for this Lambda function. So what I can do here is go in here, reload it, and there we go. We see our other event that came in. So, for this one, we have a different event. Notice, there is no OldImage, it's only a NewImage, so this works for both existing changes and brand new items. So we didn't write this one to our TargetTable because it wasn't a change, it's just logging in CloudWatch that a new image was logged. But again, I can test this by going into Items, selecting it, editing it, and let's go ahead and change this to True. I'm going to Save and close, that deletes and recreates the item. And now, if I go in here, we get our next entry, we have our event again, which has now the OldImage and the NewImage, you see Old and New. We wrote the old image to our target table, and we have our new image logged in CloudWatch. So, if I go back to my Tables, TargetTable, go ahead and refresh here, we have 002, Boolean, no description because it's a flexible schema, and the recorded timestamp, so this is all working. We've now configured our source table to go ahead and have a Stream enabled to trigger a Lambda function, which then log that data to an existing different AWS resource, including CloudWatch Logs and a separate DynamoDB table for archiving purposes. So that's how easy it is to enable DynamoDB streams on your tables and trigger an event‑driven workload. Let's go ahead and wrap this up here, and I'll see you in the next clip.

DynamoDB Accelerator (DAX)
All right, let's talk about caching our data using DynamoDB Accelerator, otherwise known as DAX. DynamoDB Accelerator, D, A, X, or DAX for short, is a fully managed, highly available, in‑memory caching system specifically for DynamoDB. Let's explore some concepts specific to DAX that you need to know for the exam and really if you're going to implement this in your workloads. First and foremost, it is meant to help speed up read requests to your tables using caching. By leveraging this feature, it's there to offer you up to 10 times the performance when compared to directly reading and writing to your DynamoDB tables. It works by helping to reduce request times from milliseconds to microseconds, even if your database is under load. I know that doesn't seem like a significant gain, however, dropping from milliseconds to microseconds is a massive gain in performance when you talk about computing. Also, by using it, it's actually compatible with existing DynamoDB API calls. What this means is you don't have to redo your architecture to use it, you just make your read calls, your put calls, etc, and it uses the cache automatically for you, which is in front of your DynamoDB table. Now for DAX, exam pro tip here, if you have a scenario where you need to improve the performance efficiency of your table without reconfiguring the application, you want to think DynamoDB Accelerator, that is one of the biggest offerings that AWS talks about. You can put this into your architecture, and you should not have to redo anything about your applications using your tables. Let's go ahead and visualize an architecture using DAX before we wrap this clip up. On the left side here, we have our EC2 instance, which we're going to assume has permissions to read and write to our table, which is over here on the right side of our diagram. In the middle of the diagram, we have our DynamoDB Accelerator in place, so we've enabled this feature for our table. Now, by default, items and the different queries that you execute for your table are going to be stored within the DAX cluster. This DAX cluster, which is in between your applications and your table is actually made up of different cache nodes that are storing all of this information. In addition to this, you need to know how writes and reads work, so let's talk about writes first. Writes are considered write through, and what that means is that data is first written to the DynamoDB table directly and then it is written to the DAX cluster. So the big thing to remember is writing goes directly to the table and then it will go into the DAX cluster, you don't write to the cluster itself. Now, if we talk about eventually consistent reads, so in this example, we'll say BatchGetItem. With eventually consistent reads, which is the default behavior, it's going to attempt to read the item from DAX, and if it's not in the cache nodes, then it will reach directly to the table. The other type of read is a strongly consistent read. So in this case, we'll just say GetItem, and we're making it strongly consistent by making the API call ContainConsistentRead: true. When you use a strongly consistent read, DAX actually passes the request directly through to the table and the results are not cached, that's important to understand for actually implementing this. Now that's going to do it for this clip on DynamoDB Accelerator. Big thing to remember here is this is used to put in place a caching system to speed up read requests within your application architectures without having to make coding changes. Let's go ahead. We'll wrap this clip up, and I'll see you in an upcoming clip.

DynamoDB Items Time to Live (TTL)
In this lesson, let's talk about expiring your items using DynamoDB item TTL. Time to live, or TTL, is an important feature to understand how to use within your tables. First and foremost, it's meant to be a cost‑savings approach for actually deleting your items that are really no longer needed after a certain amount of time. How it works is you define a per‑item expiration timestamp within its very own attribute, so what this means is TTL has its own column that you fill in for each of your items. When you set a TTL attribute value for your items, those items that have a TTL are going to automatically be deleted within a few days of that expiration time, and the beauty is all of this is done without you having to consume any write throughput, so write capacity units are not consumed. Now, a big thing to call out here, notice, items are deleted within a few days of your TTL value, they're not immediately deleted. I'm not really sure this comes up on the exam, but I'd like to point this out for any real‑world scenarios where you're trying to use this and you're saying, hey, why are my items not being deleted right now, well, this is the reason why more than likely. Let's actually look at a TTL example using a table. In this example, we have a Web App User Session Table, and we have a very simple set of data. Now, TTL is commonly going to be written in epoch time, so this top example here with our ttl column or attribute is Thursday, October 10, 2024 at 2:28 P.M. and 14 seconds. In addition to the TTL time, so the expiration time, AWS also recommends you calculate TTL by adding time to the createdAt and updatedAt timestamps. So in this example, I have a created_at column or attribute that we reference and then we're adding time to it to get to our TTL value, so that's really how simple it is. You specify a number of seconds based on the epoch time, and then once that timeframe hits, well, within a few days, that item is removed from your table automatically. Let's go ahead. We'll wrap this clip up. Remember, you can use TTLs to automatically expire and remove items from your table at no cost. We'll end here, and we'll pick back up with a backup lesson here, coming up next.

Amazon DynamoDB Backups
All right, the almighty important backups, let's talk about backing up your DynamoDB table. You need to be familiar with the backup methods that you can use with DynamoDB data. First, you can use DynamoDB directly, but you can also use a service called AWS Backup. Now, we're going to cover AWS Backup in its very own lesson later on within this course, but at a high level just understand you can use it with DynamoDB, and it can be used to implement automatic backups for your tables. Again, we'll cover this much more in‑depth later on, but I like to call this out here. Moving on, let's talk about backing up your tables using the actual service itself. Within DynamoDB, you can leverage on‑demand backups, and with that, you also can use point‑in‑time recovery, or otherwise known as PITR. point‑in‑time recovery is going to allow you to recover to a very specific second within your backups that you've taken. With using backups within the service, there is no impact on the performance nor the availability for applications using your tables, that's a major benefit of backing up via this service. Also, all of your backups are automatically encrypted via KMS, and you can easily find them within the service as well. They operate similar to your Aurora and your RDS backups, you can just list them, you can restore them, etc. For your backups, you're going to be charged based on the size and the duration of the backups, so the amount of data and how long you retain them. And then, lastly, you can quickly restore your entire table to the exact state it was in using all of these methods, so the on‑demand and the point‑in‑time recovery. Speaking of PITR, let's talk about it. Point‑in‑time recovery is meant to be a continuous backup and it's fully managed by the DynamoDB service. By using this feature, they're going to offer up to 35 days worth of recovery points, so that's a little over a month of recovery points to restore, which is usually going to be more than enough for recovering an application. The big benefit of using point‑in‑time recovery is you get granularity down to the specific second that you want to restore to, that's why these are very important for you to implement. Now, as far as cost goes, you're charged based just like a normal backup, on the size of the table and the duration that you have this enabled, so that's a slight difference. When you're taking an on‑demand backup, you're billed on the size and the duration of retention, but for this, you're going to be billed for as long as you have it turned on in addition to the size. A big feature point here, these do not affect your performance or the latency when you're using your tables, so there really is no reason to not have this turned on for your production workloads. From an exam‑scenario standpoint, if you need a way to quickly recover from accidental deletions or accidental overwrites, think DynamoDB point‑in‑time recovery. Now, in addition to all of this, you can also import and export data into your tables, let's discuss both of these. First up, let's look at importing data. When you import data, DynamoDB imports support CSV, JSON, or Amazon Ion format. Ion is going to be a specific format within Amazon itself. In addition to that, there's no impact on the write capacity for your table, so you're not consuming write capacity units and causing throttling. When you do import, however, you need to understand it actually creates a completely new table. You're not importing to an existing table, you are actually creating a brand new one. A nice feature that it offers is it supports cross‑account imports. So, let's say you backup a table in a CTE or a user‑acceptance testing account, well, then you can actually restore that into a production account or vice versa. And then, lastly, you can track all API calls for importing data within AWS CloudTrail. CloudTrail is the primary auditing service for API calls in your account, and we're going to cover it much more in‑depth in a different course within this path. Moving on to exporting data. First and foremost, you have to enable point‑in‑time recovery for this to even work. When you export, you have no impact on read capacity, so there's no read capacity units being consumed and you're not going to get throttling errors that way either. This is going to be very useful for auditing your workloads and your tables, it's useful for extract, transform, and load workloads and data analytics, specifically using Amazon Athena to query your exported data. We'll look at Amazon Athena much more in‑depth later on within this course, but understand you can use this with DynamoDB exports for analytic queries. And then, lastly, you can do full or incremental exports, and it supports DynamoDB JSON or Ion formats. Now, with that being covered, let's go ahead and wrap this clip up here. We talked about point‑in‑time recovery, as well as on‑demand and automated backups for your DynamoDB tables. Do your best to remember when you would use point‑and‑time recovery, as well as importing and exporting. We'll end this here, and I will see you in the next clip, where we have module summaries and some exam tips

Module Summary and Exam Tips
Awesome, way to hang in there. Once again, you've reached the Module Summary and Exam Tips clip. Let's wrap things up with some review and then we will move on. First up, Amazon DynamoDB as a whole. Remember, this is going to be the go‑to choice for your NoSQL workloads, especially if you need flexible schemas for your databases. When you're using your tables in DynamoDB, each row is called an item, and each item can be up to 400 KB in size. With your items or your rows, each of them is going to have columns, and these columns are referred to as attributes and these can be adjusted at any point in time. So this is one of the benefits of having a flexible schema, you can add attributes whenever you need them and then fill them in at that point in time. As far as data redundancy goes, DynamoDB is going to automatically replicate your data within your tables across three availability zones, that's an important thing to remember. So it's highly available by default, and remember it's a regional service, you don't pick the AZs it gets deployed in, you actually just choose the Region and then deploy it to that Region, and your table is automatically replicated. And lastly, remember, this is a publicly‑accessible service endpoint, so you want to make sure you're correctly leveraging IAM to control access and authorization to your tables. Remember least‑privileged access, you want to implement that here. Next up, you need to be familiar with capacity modes. There's Provisioned, this is where you manually define the capacity required for known requirements, so you know how many read capacity units and write capacity units that you need to actually provision for your workloads. Or, you have On‑demand, this is going to be pay per request, and it's going to be best for unknown or spiky workloads, so this is really useful for when you're in development or in testing. Next up, be familiar with capacity units and their usage. Write Capacity Units, or WCUs, are going to dictate the amount of writing that can be done to your table per second and your item size is based on 1 KB. For Read Capacity Units, or RCUs, these dictate the amount of reads your table can handle per second. Be sure to remember eventually consistent and strongly consistent reads and their differences. When we're talking about RCUs, the item size is going to be 4 KB that you're referencing. Also remember, table scans. Scans read every item in a table and an index and return all attributes for all items within there. For a majority of use cases, you want to avoid using scans because they're very resource intensive. On the other hand, you have queries. Queries are a more specific way to find items, these find items based on your primary keys and return only matching items from your table. You want to use queries over scans whenever possible because they're much more efficient both in performance and in cost. Another feature to be aware of is DynamoDB auto scaling. So if you have provisioned mode, you can enable this to help scale your different settings for your capacity units, and to do this, you set a minimum and a maximum and you let DynamoDB scale if it's necessary to those different values. Moving on to Global Tables and Streams. Remember, Global Tables provide Active‑Active, Multi‑Master, Multi‑Region replication. You can read and you can write to every table that exists within your Regions with these enabled. The benefit is they provide very low latency for replication and they do require DynamoDB Streams be enabled to actually work in the first place. DynamoDB Streams are your time‑ordered sequence of item‑level changes within your tables. The data within your streams gets stored for up to 24 hours, and it's going to contain all inserts, updates, and deletes that occur for every single item. And then, lastly, remember, streams are commonly used to trigger event‑driven workloads. A common architecture is to actually trigger Lambda functions based on those different changes. Moving on to caching, remember DynamoDB DAX or Accelerator, this is meant to improve your read requests to your tables using caching mechanisms. A big thing to remember is it provides microsecond request times, so it's extremely fast. And another big thing is it requires no application changes to use it, that's a big bragging point AWS points out is that you should be able to make the same API calls without modifying anything and it should work. And then, next up, we have backups and TTL for your items. Remember, point‑in‑time recovery backups offer per‑second granularity for restoring your data. Also remember, you can import and export data, and you can do this using data stored in Amazon S3. And then, lastly, you set TTL attributes to automatically delete items within a few days of that expiration period, this is all done automatically and it doesn't consume any capacity units. And with that being said, that wraps up this module on DynamoDB and some of the features. Let's go ahead, we'll wrap the module up. Take a break if you need it. And then whenever you're ready, I will be waiting for you in the next module.

Other AWS Databases
Amazon DocumentDB
All right, in this module, we're going to explore some other AWS database services that you need to be aware of for the exam. Up first, let's go ahead and start talking about Amazon DocumentDB. First off, to start things off with a little bit of a background, we need to talk about what MongoDB is. MongoDB is a document database that's meant to allow for scalability and flexibility with your data, what that means is it offers flexible schemas for storing your data, so it's similar to NoSQL in DynamoDB. MongoDB itself is meant to offer very robust storage, easy querying, and easy and efficient indexing features for all of your data. When you store your data, it's stored via JSON formatted documents, so it is a document database, you need to remember that. And here is an example of what that document data might look like. In this example, we have two restaurants within the JSON doc and they have their own values for their different keys, so we see the name of the restaurant, the stars, and the different categories of food that they offer, so this is a perfect example of what a document might look like in one of these databases. With that review out of the way, let's talk about DocumentDB. This is a fast, reliable, and fully‑managed database service for hosting MongoDB workloads, what that means is you need to keep an eye out for anything MongoDB related. When you use DocumentDB, you can use the same code, the same drivers, and even the same tool sets that you would normally use with the official MongoDB databases. One of the other big benefits of using DocumentDB for your document database in MongoDB workloads is that the storage volumes will automatically grow for you as your database needs them to, so it essentially acts like auto scaling for your database. Your storage for them can grow up to 128 TiB in size. Now, they grow in 10 GB increments. However, you're not going to be tested on that, but that's a really good piece of knowledge just to keep in mind, it has to scale every 10 GB. And then lastly, the infrastructure for DocumentDB is actually extremely similar to using Amazon Aurora. We'll explore this here coming up shortly, but for now, let's do a high level look at the infrastructure. When you deploy DocumentDB databases, you can choose instance based and you can choose elastic clusters. When you use an elastic cluster, it supports millions of reads and millions of writes per second so it scales to a very great amount. In addition to that, you can also add up to 15 Replica instances for read operations, so I think you're probably starting to see now how similar it is to Amazon Aurora. Also, when you do create a DocumentDB database, it has to be deployed into an Amazon VPC. After you do deploy your database, a really good feature to know is that DocumentDB offers point‑in‑time recovery for your clusters. So, you've seen point‑in‑time recovery with a few other services that we've covered in previous modules and this is no different, it offers the same type of recovery. In addition to that, it also supports encryption using KMS keys for encryption at rest. Let's go ahead and explore a diagram of an architecture for DocumentDB. In this, we've deployed to our us‑east‑1 Region, and you'll notice we have three AZs. Now when you create DocumentDB and you spin up a cluster, you get a cluster endpoint for you to use for both read and write operations, so it's just like a primary instance in Aurora. You also get a reader endpoint where you can offload your read operations to your Replica instances. So again, this is just like Aurora, you use the Replicas to offload read requests. And then, on the bottom of the diagram here, just like Amazon Aurora, your cluster's data is going to be stored in what is known as a cluster volume, which is auto scaling, and it's going to spread across three different AZs to maintain different copies so data durability and redundancy is a feature that is offered with this service. Now, quick exam pro tip before we wrap things up, if you have a scenario talking about migrating MongoDB workloads from on‑prem or any other provider into AWS, you want to think Amazon DocumentDB. And with that being said, let's go ahead and wrap up here, and I will see you in the next clip.

Amazon Neptune
Let's go ahead and start talking about Amazon Neptune. Before we dive into the actual service, we need to review what is known as a graph database. A graph database is meant to be a database that's storing different nodes and then relationships instead of tables and traditional documents. Examples of when you might use a graph database in the real world include the following scenarios, they're commonly used for social networks, so Instagram, Facebook, etc.; they're useful for recommendation engines based on what your likes are and dislikes are; they're popular for knowledge graphs sometimes referred to as knowledge base and an example could be something like wikipedia.com; and they're extremely useful for fraud‑detection platforms. Now, based off graph databases, we have Amazon Neptune. Neptune is going to be a fast, reliable, fully‑managed graph database service that makes it easy to build and run your applications that use that type of database. Here are some concepts specific to Neptune that you need to know for the exam. Neptune is optimized for storing billions of different relationships in your graph database. And, even though it is that big and it stores that many relationships, you can still query the graph with only milliseconds of latency, so it is extremely efficient. Now, it supports some of the more popular query languages to actually explore your graphs, so Apache TinkerPop, Gremlin, openCypher, and then even SPARQL, these are three popular query languages for interacting with a graph database. Neptune offers high availability with use of Read Replicas and automatic replication across three AZs. Now, a majority of the database services we're going to explore in this module have the same type of data redundancy, it spreads the data and it replicates it across three different availability zones, so keep that in mind. In addition to the data redundancy and replication, it offers point‑in‑time recovery and continuous backups to Amazon S3. So, again, it's just like other database services in AWS with the recovery and the backups. Let's explore some use cases for this database service. Neptune is perfect for building connections between identities, so what we mean here is you can easily build identity graphs for different solutions like social networks and you can accelerate updates for things like ad targeting, personalizing recommendations, and even just performing analytics to see who likes what and some of the demographics that go along with it. It's also good for security graphs to help improve IT security, so you can proactively detect and investigate infrastructure issues using a layered‑security approach. You can use Neptune to actually visualize all of your infrastructure so you can plan, predict, and even mitigate risks that might be involved with the different layers, it offers approaches where you can look at different levels of your network. And lastly, it is good for detecting fraud patterns. You can build graph queries for near real‑time identity fraud pattern detection in things like financial transactions and e‑commerce purchases. Building off of this, we have something called a Neptune Stream, so this is kind of similar to a DynamoDB Stream, but it's specific to Neptune. Neptune Streams log every single change that happened within your graph. Now they're logged exactly when they happen and they're in the exact order that the changes were made. With that, you don't get any duplicates within your streams. You can retrieve the different changed records from your log streams via an HTTP RESTful API call. Some use cases for Neptune Streams include the following, you can use it to notify people automatically when certain changes are made within your graph databases, or maybe you want to stream your data and keep a current version of it into another separate data store for other future uses or maybe compliance, so you can send data to things like Amazon S3, Amazon OpenSearch, and even AWS ElastiCache. Now we cover some of these in a different portion of this exam prep like OpenSearch and ElastiCache, but just understand the big thing here is you can store the data from your graph database in a separate data store. Let's wrap things up with some exam pro tips on when to consider using Amazon Neptune. If you see any mention of SPARQL, Apache Tinker Pop Gremlin, or Neo4j's openCypher protocols and query languages, you want to think about Neptune. Or, maybe you see something about storing social media profiles and the different relationships between users and customers within a database, Neptune is once again a great option. And then, lastly, you might have a scenario where you have a large dataset and you need to efficiently analyze multiple levels of different connections. So, with this, remember that Amazon Neptune offers you those different levels for you to explore those different relationships, so again, it's perfect for this type of use case. Now that's going to do it for Amazon Neptune. Let's go ahead and wrap things up here, and we're going to move on to another database service coming up next.

Amazon Keyspaces (for Apache Cassandra)
Let's go ahead and talk about Amazon Keyspaces for Apache Cassandra. First off, what is Cassandra? Cassandra is an open‑source distributed database that runs using NoSQL. When we say distributed database, we mean that it's running on many different machines at one time and they're all communicating together. This is primarily going to be used for big data solutions, it's not great for small stuff, it's good for big data. Now, a very, very popular enterprise that has used this in the past is Netflix. I'm sure pretty much everyone knows what Netflix is or what they do, and they actually have written several blogs about how they used it on their backend for certain services within their organization. Now, I will say they're beginning to move away from it to some other databases, however, it is one of the more popular organizations that has leveraged Cassandra. Now, moving on, since we know what Cassandra is at a high enough level for this exam. Amazon Keyspaces is a scalable, highly available, and managed service for Apache Cassandra‑compatible databases. The service is going to handle the following since it's managed, it handles provisioning, patching, and managing your servers, and it handles installing, maintaining, and operating the software running your Cassandra‑compatible databases. A big thing here is that it's serverless, so you're only going to pay for what you use. And in addition to that, your tables will automatically scale up and down as needed. One of the benefits of this auto scaling ability is that it can help you serve thousands of requests per second, and you have virtually unlimited throughput and storage. With that being said, it is unlimited, but remember you pay for what you use, so you need to be sure this is the right solution for you. Now, let's go ahead and look at some exam pro tips for this specific service. First up, if you see Cassandra Query Language, or CQL, you're going to likely want to include Keyspaces within your solution. That is a query language specific to Apache Cassandra, and this is the Apache Cassandra compatible service in AWS. Next is migrations. If you have a scenario where you're getting asked about migrating big data Cassandra clusters to AWS, again, think Keyspaces. And lastly, application requirements, this is going to be good for some applications that require low latency, maybe some specific open‑source applications, and even some IoT workloads. It's really going to come down to very specific terminology that you use to help you pick out that this is the service for you. If it were me, I would pay attention mainly to these first two, but I do want to put in this third one because it is a possible solution for some other workloads. And with that being said, we just reviewed Amazon Keyspaces for Apache Cassandra. Let's wrap things up, and we're going to move on to our next service.

Amazon Quantum Ledger Database (QLDB)
All right, next up, let's talk about Amazon Quantum Ledger Database, otherwise known as QLDB. Similar to the other database services, we need to review what the underlying purpose is, so let's talk about ledger databases. Ledger databases are NoSQL databases that are meant to be immutable, transparent, and have a cryptographically verifiable transaction log owned by a centralized authority, so there's one source of truth for a ledger database. With a ledger database, you cannot update records within them, so what we mean is you don't just replace old content. Instead, what happens is that any updates or deletions actually add a new record to the database so you have a chain of transactions that is being tracked. Ledger databases are commonly used for the following, cryptocurrencies, so things like Bitcoin and Ethereum, some shipping companies use it to track their items, the different boxes and shipping containers, and some pharmaceutical companies even use them to track creation and then distribution of their drugs that they manufacture. This is helpful to prove where things have gone and get rid of counterfeit drugs that some people might sell. Moving on from what a ledger databases, let's talk about Amazon QLDB, this is the fully‑managed ledger database service meant to provide that transparent, immutable, and cryptographically verifiable transaction log in the AWS cloud. So, in other words, it meets the exact same standards and requirements as a ledger database. Let's talk about some components that are important to understand when you would use this. The big thing here, it is a centralized component for logging transactions, I point this out because it's easily confused with using something called Amazon Managed Blockchain. This is a big differentiator, these services are different in this primary aspect, blockchain has decentralized components, ledger databases have a centralized component. Moving on, it's meant to perform roughly two to three times better than other blockchain frameworks. And to actually interact with your data, you use PartiQL queries which are similar to SQL‑style queries. And lastly, you need to remember it is immutable, you don't remove or update, you actually create new records with those changes so everything can be tracked. Let's talk about some use cases for this service. It's perfect for storing financial transactions, so you can actually create a complete and accurate record of all financial transactions like credit and debit transactions. It's good for supply‑chain systems, so you can record the history of each of your transactions and you can provide details of things that are batch manufactured, shipped, stored, and even sold. Thirdly, it's good for maintaining claims histories. So, if you have claims maybe for insurance, etc, you can track a claim over its lifetime and you can cryptographically verify that data integrity is in place to protect against data‑entry errors. In other words, you want to make sure nothing was changed along the way and there's no fraud going on. And then lastly, it's good for centralizing digital records. It helps you implement a system of record to create a complete, centralized record of things like maybe employee details so you have a centralized place where you control and change payroll, bonus, and benefits. The big thing here to take away from all these use cases is that it gives you a verifiable chain of actions and the different steps that have occurred through those different transactions. And with that being said, let's go ahead and wrap up this clip on QLDB, and I will see you in the next one.

Amazon Timestream
All right, let's talk about Amazon Timestream. First up, let's review time‑series data. Time‑series data are simply data points that get logged over a series of time, it really is meant to allow you to better track your data and the changes that have occurred. Some examples of time‑series data and how it might be used, hourly temperature readings from weather stations with their IoT devices like thermometers that are spread around the world, it's also good for monitoring server metrics that might be sent to Prometheus, and it's good for things like tracking stock prices, the different trading volumes, and even market indicators that give you signs on when to sell and when to buy. Moving on from time series, let's talk about Amazon Timestream, this is their purpose built, serverless, fully‑managed database service for time‑series data and what is called LiveAnalytics data. You can use this service to store trillions of time‑series data points per day, and you can easily and quickly analyze that data using built‑in analytics functions. It offers such high efficiency because it keeps recent data in memory so that you can access it very quickly like a cache, and it moves historical data to a more cost‑optimized storage tier that you get to choose. Now, Timestream also integrates with a lot of other services and platforms, let's review six common ones that might come up on the exam. It integrates very easily with IoT Core, so an example we could pull from would be that weather station reading from before, well, it can easily use IoT core to pull some of that data. It also works closely with Amazon MSK, which is their manage Kafka service. We'll talk about that in a different course within this learning path. In addition to manage service for Kafka, we also have Amazon Kinesis, which is another real time and near real‑time solution for streaming data. You can also visualize all of your time‑series data within Amazon QuickSight, so you can build business reports and dashboards and share them to users. And in addition to that, you can visualize and report on data with things like Grafana or Prometheus. And then, lastly, Amazon SageMaker, you can send your Timestream data into your machine‑learning models and perform your different analytics, training, etc. So these are six common integrations you should be aware of for Timestream on this exam. Moving on, let's talk about use cases for Amazon Timestream. I want to review these because it can be confusing on the exam, so I really want to make sure we understand when you would use this service. First, again, IoT sensors, maybe you have a scenario where you're relaying thousands, millions, or even billions of points of information depending on the set up. A specific use case that might come up is something like tracking weather for different agriculture needs, so maybe temperature, rain, etc. It's also good for Application Performance Monitoring, or APM, so this is where you might have large applications that need to track response times, different user error rates, and even user interactions across the different services and components that make up your application. And lastly, it's good for financial trading and market data analysis, so storing and querying different historical price points, the different trade volumes that we talked about, and again, market indicators for different orders on the market. Essentially, anything in Timeseries is where this will be good for. Let's wrap things up with an exam pro tip. If you have a scenario question where you need to store a large amount of Timeseries data and then analyze it, you want to think Amazon Timestream. Now that's going to do it for this clip, be sure to review those use cases we just talked about, understand what Timeseries data really is, and then I will see you in the next clip, which is going to be our Module Summary and Exam Tips overview.

Module Summary and Exam Tips
Perfect. Thank you for hanging in there, we've reached the end of this module. Let's go over some summaries and some exam tips. This is going to be a very short module summary. We're going to go over some exam use case scenarios that you might run into for each service. First up, DocumentDB, a potential use case here is for content management systems that require storing and querying JSON data specifically for MongoDB workloads. You want to keep an eye out for MongoDB. Next, we have Neptune, so this is going to be perfect for social‑networking applications where you want to manage user connections and maybe friend recommendations. Third is Keyspaces, so look for anything where you have an application that's using Apache Cassandra and you're requiring a transition to a managed service with less operational overhead. Next up, we have Quantum Ledger Database, or QLDB, this is perfect for financial transaction logging. So, maybe there's a bank and you need to have integrity and verifiability of data changes tracked for historical reasons and security and they're critical to your workload, well, this will be perfect to use QLDB. And then the last one here, Timestream, this is for any real‑time analytic applications, like monitoring and forecasting of IoT sensor data from weather stations around the world. The big differentiator for Timestream and other streaming services is going to be time‑series data, where you need a database to host it. And with that being said, thank you for hanging in there. This was a short module. Let's go ahead and wrap things up, and I will see you in the next module.

Other AWS Storage, Transfer, and Migration Services
AWS Snow Family
Welcome to the next module. In this module, we're going to explore several different services and concepts that are critical for a solutions architect. We're going to look at some other more specific storage services, and we're going to explore different ways to efficiently transfer and migrate data to and from AWS. This first clip is going to cover the AWS Snow Family. What is the Snow Family? The Snow Family is a set of secure appliances and features that provide you up to petabytes‑scale data collection and different ways to process that collection at the edge. When we say edge, we're essentially saying outside of AWS within your own on‑premise data center or a local branch. These services are meant for migrating large‑scale datasets into and out of AWS. They work by offering you built‑in computing capabilities so that you can run your own operations within your remote locations, and you don't necessarily have to have data center access or a reliable network connection. They come in two different flavors. There's AWS Snowball Edge, which is going to be larger, and there's AWS Snowcone, this is going to be for smaller workloads. Let's explore these now. First up, AWS Snowcone. This is going to be a small, portable, and secure device that you use for edge computing and smaller data transfers. You use them to collect, process, and then even move large amounts of data to AWS in a more efficient manner. Now, there are two different ways these operate, the first is offline. Offline works by you loading your datasets onto the actual physical device and then working with a official carrier to actually ship the device back to AWS where they offload the data into Amazon S3. Or, you can migrate via DataSync. Now we're going to look at AWS DataSync in a completely separate clip, but understand it does work with Snowcone. This actually transfers data online over the internet and a network connection. These are going to be perfect for edge‑computing scenarios where space and power are constrained, so in other words, a small dataset in a small location. With Snowcone, there are two types of Snowcones you should be aware of, there's Snowcone and Snowcone SSD. Let's explore these offerings now. For Snowcone, the standard offering, this offers you two virtual CPUs, 4 GB of memory, and up to 8 TB of hard‑disk drive space for you to store your data. The other offering, SSD, this is going to be a little bit more high performance and a little bit more expensive. This offers you the same amount of vCPUs and the same amount of memory, but you have more performant, solid‑state drives installed instead of hard‑disk drives. In addition to the more high‑performing drives, you also get more space, 14 TB. Moving on, let's talk about the other portion of the Snow Family, the Snowball Edge device. The Snowball Edge is essentially considered the jack‑of‑all‑trades device for your edge computing. It actually comes with onboard storage similar to Snowcone's and compute power for select AWS capabilities. The Snowball Edge, like a Snowcone, can also process data locally, run edge computing workloads, and it can transfer data to and from AWS. With Snowball Edge, it's meant for larger datasets, so Snowcone is smaller, Snowball's are bigger. The data transfers are faster than internet speeds because you typically ship them via a regional carrier to AWS as well. So when you start getting into large amounts of data like petabytes of data and you need to get it transferred to AWS in roughly less than a week for instance, you're likely going to want to use a Snowball Edge device as opposed to transferring that data over the internet. Another scenario would be if you have network constraints, maybe you can't share bandwidth or maybe it's a slow speed, well, then a Snowball Edge device again would be a good choice. There are three options for configuration of these devices, you can use storage optimized, compute optimized, and they offer compute optimized with GPU. Let's look at these now. First up is storage optimized. You have two different options when you choose this device. Understand that this device is meant for data transfer with options up to 80 TB. The local device itself actually offers up to 210 TB of storage, so 80 TB if you want to transfer to AWS, local storage can support up to 210. Next is compute optimized, so this gives you exactly what it sounds like, optimized compute platforms on the device. You get up to 104 vCPUs, up to 416 GB of memory, and 28 TB of SSD storage, so it is very high performing. These are really good if you do need to actually perform some type of edge computing in addition to storage transfers. And then lastly, compute optimized with a GPU. This is identical in terms of numbers to compute optimized, but you also get an installed GPU for you to leverage for any of your required workloads. Now, moving on, let's look at the process of using the Snow Family. First, your Snowball device is requested and delivered. You then install the required software on your local machines and servers and move data over to that local device. After the device is loaded, you work to get the data shipped back to AWS via a regional shipping carrier partner to quickly drop that data off. Once it's shipped, AWS receives the Snowball device. Once AWS does receive the device that you've sent back, they're going to go ahead and import that data into Amazon S3. And then from there, after it's imported, your data is then usable. After it's actually officially verified as usable, your Snowball data is securely wiped from the actual device and because it's securely wiped, nobody can access it any longer. It is completely gone. There's no way to recover it, so it's a very secure and efficient manner to transfer data. Now, exam pro tip before we wrap things up. If you have an exam scenario where you have large amounts of data, we're talking petabytes of data, limited bandwidth, and requirements to complete a transfer in a short amount of time, you want to think the Snow Family. Snowcones are good for smaller datasets. Snowballs are better for larger datasets. The easiest way to think of this in my mind is a snow cone is much smaller than a snowball. Snow cones have a little bit snowballs are much larger, so just try and think of it that way if that helps. Let's go ahead and wrap this clip up on the Snow Family, and I will see you in the next one where we start diving into Storage Gateway.

AWS Storage Gateway - File Gateways
All right, let's get into AWS Storage Gateway. The first one we're going to cover here is a File Gateway. Before we dive into File Gateways specifically, let's cover what Storage Gateway is as a whole. Storage Gateway is a service within AWS that's meant to offer you a hybrid cloud storage ability to help you merge your on‑premise resources with the cloud. Hybrid architectures are getting more and more popular where you have on‑prem and cloud‑based resources, so this is a perfect option for leveraging cloud storage. It can help you with a one‑time migration or it can work for long‑term pairing of your architectures with AWS. Behind the scenes, what it does is it allows you to essentially expose S3 buckets and storage to be used natively within your on‑premise workloads with your different servers and applications. Now, there are three types, there's a Amazon S3 File Gateway, which we're about to explore, there's a Tape Gateway, and there's a Volume Gateway. We're going to look at Tape and Volume Gateways in separate clips. So let's actually look at File Gateway concepts. How these work is you set up a local S3 File Gateway to allow NFS and Samba, or SMB, connections to S3 buckets. They're efficient because your most recently accessed data is actually cached locally, so this way you don't have to constantly download data from S3, if it's recently used, it's cached for immediate access. They work by allowing you to easiiy upload your data to S3, and with that, you can actually leverage different storage classes once they're in the service itself. One of the ways it makes it so efficient is because it optimizes your transfers of your data using multipart uploads and byte‑range downloads. Now, these are covered in a different course where we talk about S3 in depth, but these are performance‑enhancing features. Make sure you understand what multipart uploads do and how byte‑range downloads work. Lastly here, you use them in conjunction with lifecycle policies in Amazon S3. And with that, you can transition older data that's not accessed to Glacier storage tiers for cost savings. Let's actually look at a diagram of how S3 File Gateway works. On the left side here, we have our on‑premise system, on the right side, we have our AWS, and in the middle, we have our network connection. Now you start things off on‑premise by deploying an S3 File Gateway as either a local virtual appliance, like a VMware VM, you can deploy a hardware appliance, or this actually even works in the cloud as an Amazon EC2 instance. Now this diagram is covering local deployments via some type of VM or hardware appliance, but just keep in mind that it does work on EC2. You then configure your on‑premise servers here to connect to that file gateway via NFS or SMB protocols. It's very important you understand it only supports those protocols. Once you get the data configured and the appliances set up, your data is then starting to get transferred via HTTPS over some type of network connection, so VPNs, Direct Connect connections, or even if you want the public internet. Once the data is uploaded, it's transferred to Amazon S3. And once it's in S3, it supports pretty much every storage class possible, so things like S3 Standard, Standard and Frequent access, and even Intelligent Tiering, those are just a few examples of all of the supported classes. When you realize that your data is not necessarily being accessed a lot in S3, you can set up a lifecycle policy to then transition that data to different storage classes after amounts of time pass to save on cost. So a typical use case would be to upload data to S3 Standard, and then maybe after 90 days of not being accessed, you transition it to Glacier for archiving. Moving on, there's another type of file gateway you need to be aware of and this is for FSx. This is going to provide low‑latency performance for frequently‑accessed data just like the S3 File Gateway, it works by caching that data locally. The primary difference with this is that it's meant to set up easy access for Amazon FSx for Windows File Server, so that is the distinguishing feature. The other one is for S3 Storage, this is for FSX for Windows File Server. You want to use this if you need Windows‑native SMB capabilities. When we say that, what we mean is maybe you need NTFS or SMB file systems, you need Active Directory integration like groups file sharing, and maybe data deduplication capabilities. Those are all Windows‑native capabilities, and they are supported by this gateway. Now, that's going to do it on our review of File Gateways with AWS Storage Gateway. Make sure you review these, you need to understand the different use cases and that architecture diagram we just covered. We're going to wrap things up, and we'll move on to the next type of gateway, which is a Volume Gateway.

AWS Storage Gateway - Volume Gateways
All right, let's get going with Volume Gateways within the Storage Gateway service. First up, let's review some concepts regarding Volume Gateways. These, just like file gateways, are S3‑backed storage, but you mount it locally via an iSCSI device. Your data is then backed up by EBS snapshots, so this is going to be perfect for backups and migration efforts to the AWS Cloud. A key differentiator is the connection type, iSCSI is completely different than NFS or Samba, so really make sure you understand this only works with iSCSI. Now there are two types of volumes that go along with these gateways, the first is a cache volume. This is where you store your data in S3 and then you retain copies of frequently‑access data subsets locally. So, in other words, your data that's accessed more commonly is going to be stored locally via a caching system, and the other stuff is stored long term in cloud storage. This works to give you great cost savings for your primary storage, while also minimizing the need to scale on‑premise storage. Remember, one of the biggest benefits of cloud storage is you have virtually unlimited amount of space, you can't quite say the same for on‑premise storage, you have to scale physically, you have to add drives, etc. The next type of volume is a stored volume, this is where your gateway is going to be configured to store all of your data locally, it is then a synchronously backed up with point‑in‑time snapshots to S3. So, the easiest way to think of this is a stored volume is stored locally, all data is local, and it gets backed up asynchronously. Cache is where only portions of your data are cached locally, that's how I like to think of it, hopefully that helps. Let's look at a diagram for using a Volume Gateway, it's going to be similar to the File Gateway diagram we looked at earlier. We have our on‑prem here on the left, and then we have a connection and AWS accounts over here on the right. Now, the first thing you do is you deploy your Volume Gateway just like you would in S3 File Gateway. You would then connect to the configured volumes via an iSCSI protocol, so your on‑premise servers connect to your virtual appliance or hardware appliance via an iSCSI protocol, which mounts it like any other type of volume on the network. Remember, you can configure cached volumes or stored volumes, it's going to come down to the use case that you're reading within your scenarios. After you have your Volume Gateway set up and you're connected to it via your on‑premise servers via your iSCSI protocol, your data is going to start being transferred via HTTPS to AWS. You'll notice this is extremely similar to a file gateway. It can go over the public internet, a VPN connection, or a Direct Connect connection itself, it just depends on your infrastructure that you require. After your data is transferred to AWS, it gets transferred to Amazon S3 via that gateway device. If you are using the cached mode style of volume, the gateway is going to check the local cache first, and if that data is missing, it's going to request the data from S3. So it'll check the cache, if it's not there, it makes a download request from S3. Cached modes are a more storage performant and storage optimized solution. After your data is in S3, the data is stored within EBS snapshots, so data is in S3 stored as an EBS snapshot. The nice thing is you can restore these snapshots either locally or you can even restore them for an EC2 instance, so there's some flexibility there. Now, it's important to note these act just like normal snapshots of EVS volumes, what that means is they are incremental after the very first snapshot is saved. That first snapshot is a full copy, everything after that is only going to capture delta changes. Okay, that's going to do it for Volume Gateways. Do your best to remember the difference between cached volumes and stored volumes, and really remember it uses the iSCSI protocol. Let's wrap things up here, and we'll move on to the next one.

AWS Storage Gateway - Tape Gateways
All right, let's talk about Tape Gateways within AWS Storage Gateway. Fun fact, and I'm kind of aging myself here, some companies used to and still do leverage physical tapes for backups. In fact, I worked for several companies that actually did this, and I was in charge of administrating those backups. That's where Tape Gateway comes in, so let's explore some concepts here. Tape Gateways are going to allow companies to replace backing up to actual physical tapes by offering instead an S3‑back cloud storage. It works by using a Virtual Tape Library, or VTL, which is an interface that's used to allow you to use existing taped‑based backup processes and existing workflows to leverage another virtual tape cartridge. So, really what this is saying in short terms is it's all virtual, you have a Virtual Tape Library that hosts virtual tape cartridges for backups. Now it offers you cost effective and durable archive backups that are going to be able to store data in S3 Glacier Flexible Retrieval, or for even more cost savings, S3 Glacier Deep Archive. To use it, you mount a virtual tape drive and a media changer on your on‑premise application servers as iSCSI devices, so it's extremely similar to a volume gateway, however, it is specific to tape drives. Let's go ahead and check out a diagram now on how this works. This is going to look extremely similar to the other gateways we've already explored, however there are a few key differences. You deploy your Tape Gateway on‑premise. Now, with your Tape Gateway, you get three primary resources, you have your Virtual Tape Library which includes a collection of virtual tapes, and then you have two devices that you actually mount. There's a media changer which is going to act like the robot arm that would typically move physical tapes around in a physical tape library. And then you have the Virtual Tape Drive, this acts just like a physical tape drive in the fact that it performs input/output operations on a virtual tape, but the application servers or backup servers don't know that it's virtual. Your backup servers just mount these devices via iSCSI protocols and they use them like they would any other physical tape drive. Now, once your data is in place on your Tape Gateway and it's in your tape drives and your media changer is moving all of your virtual tapes around, the data is transferred just like the other gateways, so it's over HTTPS, and it can be over VPN, Direct Connect, or public internet. After it's transferred to AWS, the virtual tapes are actually backed up to S3 via that gateway device, so your Tape Gateway is tracking the virtual tapes that are being copied. The data is then asynchronously uploaded to local virtual tapes and uploaded to S3 within an actual tape library which is virtualized. The tape library on the AWS side leverages S3 Glacier Flexible Retrieval or Deep Archive depending on your configuration of your Tape Gateway. Choosing the storage class comes down to the requirements, do you need more immediate access, or do you want to save the most amount of money? So, with that being said, let's wrap this clip up here. Remember, Tape Gateways are appliances meant to replace physical tape drives and instead offer you a virtual solution to leverage S3‑backed storage. Let's go ahead, we'll wrap up here, and I'll see you in the next clip.

AWS Lake Formation
All right, up next, let's look at a service for storing your data lake information known as AWS Lake Formation. AWS Lake Formation is a fully‑managed service within the AWS cloud that is there to give you centralized governance, security, and the ability to share your data for different analytics reasons and even machine‑learning workloads. It achieves all of this by allowing you to easily create a data lake that is hosted within the Amazon S3 service. At a high level, if you're not familiar with what a data lake is, it's essentially a centralized repository where you store and process large amounts of data. Now with lake formation and in data lakes in general, you can store both structured and unstructured data. On the backend of this service, it uses something called AWS Glue to perform different extract, transform, and then load actions within your workloads. Now we look at AWS Glue much more in depth in a different module within this course. When you leverage AWS Lake Formation for your data lakes, it allows you to easily accomplish the following actions. You can leverage it to easily discover all of the different data that lives within your data lake, so different structured data, unstructured data, etc. After discovery, you can use it to easily clean and even transform your data for better ingestion of that data later on and some type of machine‑learning workloads maybe. And then lastly, it also allows you to ingest that data into the data lake separately so you can have the original data, you can discover it, you can clean it, you can transform it so it's in a different format, and then you can ingest it into a different portion of the data lake so that you can use it with different analytics services and data warehouses. AWS Lake Formation is meant to be a one‑stop shop for your big data repositories. Let's look at Lake Formation high‑level steps here. First off, you would identify the existing data stores that you want to use, so maybe you're storing data in Amazon S3 or maybe it's a NoSQL database or a NoSQL database backup, etc. After you identify your data stores, you then work on moving the data into your data lake, so you start transferring the data over. Once the data is transferred, you would begin to crawl that migrated data, so you're discovering what's in the data, you're seeing the different data points, how they relate, things of that nature. After the data is crawled successfully, then you would begin to catalog that data. Now, the nice thing is Lake Formation allows you to do this very easily. You can leverage built in mechanisms and functions to catalog the data for you. Once the cataloging is done, you would then prepare the data for some type of analytics or maybe data warehouse storage. And finally, once your data is prepared, you've cataloged it, you've migrated it where necessary, you can then grant users secure, self‑service access to that cleansed data. Generally speaking, you would grant access to the services leveraging the data and not Lake Formation directly. Now, for the exam, you need to be familiar with workflows or sometimes referred to as blueprints. A workflow or blueprint is going to contain complex, multi‑job ETL activities that create AWS Glue crawlers, they create jobs, and then even triggers to orchestrate the loading and updating of your data into the data lakes. Now, with your workflows, you can run them on demand or you can schedule them. What that means is, you can run them with a click of a button or the press of an Enter key with an API, or you can schedule them on a regular basis. So maybe you have data that's coming in regularly and you want to regularly schedule some type of workflow to transform that data, well, that's possible with a workflow. An example of when you might use a blueprint or workflow, you can use one to ingest data from an Amazon Aurora MySQL database to your S3 data lake. From there, you can prep it for setting up column‑level authorization to restrict access to a single team. And speaking of column‑level authorization, let's look at access controls. Understanding how to leverage access controls in Lake Formation is very critical due to how data is actually accessed. First off, this service does allow for fine‑grained access control for different users and the different applications that are accessing it. You can set up row and column‑level controls. Next, permissions control access to Glue Data Catalog resources, the S3 locations where the data is stored, and the underlying data in those locations. In other words, if you give certain permissions in Lake Formation for certain data sources and certain data lakes, well, then sometimes depending on those controls, you might accidentally give access to the underlying resources where the data is being transformed and stored. In other words, you need to be very careful granting access to Lake Formation. And lastly here, Lake Formation does offer tag‑based access control, which allows you to go ahead and define permissions based on specific attributes found in tags. Now, real quick an exam scenario, remember, you can use tag‑based access controls to securely authorize and grant permissions for specific required data to specific engineering teams and their roles. Moving on, let's look at service integrations. The big thing with Lake Formation is you leverage other services to read that data that's been transformed and prepared. You can leverage the following services to interact with Lake Formation. You can run analytic queries ad hoc using Amazon Athena, you can build dashboards and reports for business users using QuickSight, you can go ahead and run queries within a data warehouse using Amazon Redshift and Redshift Spectrum, and you can even leverage Elastic MapReduce on the data that's stored in Lake Formation. And lastly here, AWS Glue. Now, AWS Glue is actually what essentially runs this service behind the scenes. The only thing is it's abstracted from you, so you're leveraging Lake Formation, you're running all your blueprints and your workflows, and those are all technically leveraging AWS Glue in the background, but you don't directly interact with Glue itself. Now, let's go ahead and look at a diagram workflow on how this would work. On the left‑side here, we have our different data sources that are supported. Now, the data sources, which remember can be mixed, so they can be relational and non‑relational structured or non‑structured, well, these get ingested into your data lake which is going to be stored in an S3 bucket. After the data is ingested, Lake Formation allows you to leverage its different tool sets and its different features so that you can efficiently organize, catalog, and prep your data all within that single data lake. Lake Formation offers several features, you can leverage at data Glue crawlers and data catalogs, you can use fine‑grained access controls like we talked about with custom security settings, and you can easily and quickly integrate ETL data preparation and even exporting for different service interaction. Now, using all of those built‑in features, you can then prep your data for access via other AWS Analytics and big data services. So again, Amazon Athena can be used to perform serverless, ad‑hoc analytics queries, Redshift can be used to store data in a data warehouse for OLAP workloads, and a last example here, QuickSight can be used to generate reports and then dashboards for business users that don't want to see the data, they just want to see the graphs and how it all relates. So be familiar with this workflow, you have your data source, you ingest it into your data lake, using Lake Formation you then can run ETLs, you can set up fine‑grained access control, and you can export data for use with other services. Generally speaking, you're going to grant access to the services over here on the right as opposed to directly granting access to Lake Formation. Now let's look at some exam tips to wrap things up. Number one, you'll use AWS Lake Formation if you need to centralize permissions and access controls to your data lakes, that's for both users and services. And number two, you can leverage column‑level security to enhance data security when using something like Amazon QuickSight to build dashboards and reports. Remember, you want to leverage something like this to securely lock down access to the underlying data sources where you're pulling the data from. Now, with that being said, let's go ahead and wrap this clip up here. We just reviewed Lake Formation. Remember, this is the go‑to service for any data lake storage that you might need, as well as performing easy ETL workflows and exporting of data. Let's go ahead and wrap this up, and I will see you in the next one.

AWS Transfer Family
All right, let's get into transferring data in and out of AWS using the Transfer Family. The AWS Transfer Family is a fully‑managed service that is meant to allow you to easily move your data and files into and out of Amazon S3 or your EFS file shares. Right now, it currently supports the following protocols, you can use Secure File Transfer Protocol, or SFTP, you can use File Transfer Protocol over SSL FTPS, you can use straight‑up FTP, which is File Transfer Protocol, and it even supports Applicability Statement 2, or AS2. When you leverage the Transfer Family services, AWS is going to manage the infrastructure for you, including handling anything related to scaling needs and high availability so that it's always available. That's one of the benefits of using a fully‑managed service, you get rid of that administrative overhead and you can just immediately begin running your workloads. Now, as far as cost goes, you pay for each endpoint per hour and the data transfers that occur through the service, so those are two big cost features that you need to keep in mind. Now, an exam pro tip here, AWS Transfer for FTP can only be used within a VPC, so you can't leverage it from on‑prem into S3 via this service because it's not supported. Now, let's look at some use cases regarding Transfer Family. You can use it for subscription‑based data transfers, so maybe you have customers that pay a monthly fee and then you supply them some type of document or file each month that's updated, and they can securely transfer that file from S3 into their own on‑premise systems using a typical standardized protocol. You can use it for public data distribution, so you can just have an SFTP endpoint up there ready to go and anyone can hit it at any point in time. It's also useful for internal organization data transfers, so maybe you have multiple teams running multiple applications that work together and one team relies on the other team to upload a specific document before they can run their own workflows, well, you can transfer that data to an S3 bucket using SFTP and then trigger that workload for the other team. Fourthly, it's also good for content management systems, so maybe you just need a simple file storage system where you want to transfer data in and out of it using a standard protocol. And lastly, you can leverage it for enterprise resource planning systems, so if you need to update or upload some type of benefits package to S3 and then allow people to go ahead and download it and review it, well, then you can use Transfer Family to easily set that up. Moving on from use cases, let's look at authentication with this service, you need to know how to manage users and what kind of users are even supported. First up, you can create and manage users entirely within the service itself if you want to, so if you only have one or two users you need to manage, well, then you can use service‑managed users to go ahead and implement that. However, if you start to grow and you're running some type of enterprise organizational application, you might want to look at other identity providers. Now, Transfer Family does support a majority of the more common identity providers, so you can integrate with AWS Managed Microsoft AD, you can leverage third parties like Okta or maybe even built‑in Cognito users. And if you want to, you can actually build your very own custom provider using other services like Lamba and API Gateway. The big thing to take away here is understand you have two big categories as far as options go, you can manage within the service, or you can manage within external identity providers. Now, let's look at a diagram for how this might work. In the middle of the diagram, we have our Transfer Family so you can see all of the different supported protocols within it. On the bottom here, remember, AWS FTP endpoints can only be used from within VPCs. So, for this example, we're leveraging an EC2 instance within our VPC that's going to hit that FTP endpoint and leverage it that way. The other endpoints are fair game, you can have them public or private. Now, what would typically happen is your user would go ahead and connect to one of your exposed endpoints that you've deployed via the supported protocol. To connect to that endpoint, they're either going to use a local service‑managed user credentials or you can set up authentication in management of your users via AWS Directory Services or another custom IDP. Either way, they have to have some type of credential to connect to your endpoint that you've deployed. Once they've connected via the chosen protocol, they've securely logged in, the Transfer Family can then work with Amazon S3 or Amazon EFS by using an assigned IAM service role. So the user connects to your endpoint, that endpoint is assigned an IAM role, which has access to the underlying storage, so whether it be a bucket or an EFS file system. After this is all in place and data is starting to be transferred via your endpoint to your backend resources, you can actually leverage those backend resources to trigger further workloads, so for instance, maybe you're uploading a PDF file to an Amazon S3 bucket and you want to trigger a Lambda function once the object is created. This is a common exam scenario, so be prepared to see something like this. Really take the time to walk through this diagram and understand the flow. You deploy your endpoint, and you connect to that endpoint using some type of credential. After you are connected, your endpoint itself has an IAM service role assigned to it to allow it to interact with the backend storage system. Now, let's look at some exam tips before we wrap this all up. Four big tips to take away, this tool is very useful for bringing legacy application storage systems to the cloud, so if you have a legacy application on‑prem and you don't want to interact directly with the S3 API, this is excellent to actually go ahead and start using cloud storage. You need to understand the protocols, ASP2, SFTP, FTPS, and FTP. Also remember, that FTP only works within a VPC. Thirdly, remember data is stored within two supported back‑end cloud storage systems, it can be stored within Amazon S3 buckets or you can store your data within EFS file systems. To interact with those storage systems you assign a role to your endpoint. And lastly, they support both service‑managed users for smaller enterprises or smaller use cases and they support other Identity Provider solutions, like Microsoft AD if you have organizational requirements. Now that's going to do it for this clip on AWS Transfer Family. Let's go ahead, we're going to wrap this up, and I'll see you in the next clip.

AWS DataSync
All right, let's talk AWS DataSync. AWS DataSync is going to be an AWS solution that is meant to migrate large amounts of data from external storage systems to the AWS cloud. It allows you to easily set up data transfers between NFS and Samba shares, as well as AWS storage solutions themselves. With DataSync, there are two types of migrations, you have agent‑based, which is going to be used for on‑prem, other clouds, NFS, Samba, and HDFS transfers, and then there's agent lists, so this is only going to be used if you're transferring data from an AWS Storage Service to another storage service. Now, you need to understand the compatible destinations with this service, you can send data to Amazon S3, EFS, and FSX within the cloud. Moving on, let's look at some concepts that you need to understand for this exam. An agent is going to be a VM appliance that you deploy and you use it to read and write data during a transfer. These agents execute tasks, and a task is where you set the source and the destination, as well as the details for actually copying your data. Now you can schedule replication tasks to recur hourly, daily, and even weekly, so if you want to constantly be syncing your data, this is a good option for you. One of the bigger benefits of using DataSync in addition to the scheduled replication tasks is that you can use it to copy file metadata, you can actually exclude files you don't want to take, you can go ahead and limit bandwidth so you're not hogging all of your network bandwidth, and when you copy files over you can maintain existing file permissions. Now, with agents, you can use them to actually verify data integrity after it's copied over, so you can set them to have a task to copy data from on‑prem into S3, and then verify that the data is actually still valid. In addition to data integrity, you can also schedule them or set them to use up to 10 GB per second per task, so in other words, you can transfer a lot of data very quickly using this service. Let's look at a quick diagram. We're going to go over three diagrams for three different scenarios you might run into. This first one is on‑prem. For an on‑prem solution, you install and configure your DataSync agents with the source and the location of your transfer. Remember, for the exam, DataSync can work both ways for syncing your data, in other words, they can go to and from the cloud. After your agent is set up, you then configure your on‑premise servers to connect to the agent via one of the supported protocols, so NFS, HDFS, etc. It's important to call out you can also directly use the S3 API, you don't have to use one of the other protocols. Now assuming we have the agent configured, our on‑premise server is set up, you can then begin migrating your data over a TLS connection, which can use Direct Connect connections, a VPN connection, or can go directly over the public internet. A common scenario is to have a private connection to AWS for these transfers via Direct Connect. Now, after your data is being migrated, you can leverage multiple AWS services for storage within the cloud, this includes Amazon EFS, FSx, so FSx for Lustre, OpenZFS, NetApp, ONTAP, etc, and Amazon S3 buckets, including all of their different storage classes, so you have a lot of options from a storage component standpoint within the AWS cloud. Moving on, let's look at AWS to AWS. For this, there is no agent required for the following transfers. If you have AWS storage to AWS storage being set up within the same AWS account or if you're sending data from an S3 bucket to different storage services within different accounts, those don't require agents. However, transferring data between other storage services across accounts that don't fall into those two use cases do require an agent to be configured. Moving on to the final diagram, Edge. You can leverage a Snowball Edge and Snowcone devices with DataSync as well, so these are commonly used together. When you set one of these devices up to actually work with DataSync to transfer your data, you just set up the DataSync location as one of those devices, so either the Snowball Edge device or the Snowcone device. Remember, Snowball Edge is going to be better for larger workloads and larger data transfers. Snowcone is better for smaller transfers or smaller space requirements. Either way, just remember, you can leverage these in addition to your DataSync agent. Now let's look at some use cases where you might use this service. First, you can leverage it to quickly migrate active datasets over a network into one of your storage services in your AWS account. So maybe you want to quickly migrate an application and you want to quickly use the datasets, well, you can quickly move them to something like EFS or Amazon S3 and begin using it. You can use it to move on‑prem cold data to things like S3 Glacier Flexible or S3 Glacier Deep Archive, so this is very good for syncing your archiving data. Thirdly, you can use it to copy data into any Amazon S3 storage class or FSX offering. And then lastly, you can easily leverage it to transfer data into and out of AWS to process for machine learning. So commonly, you can be using this to sync your data to S3 and then leverage some type of managed machine learning service to pull that data and start working. Now, let's look at two exam scenarios that you might run into on this exam, this is going to be perfect for transferring large amounts of data between NFS or Samba file systems. So if you have two NFS systems where you need to sync data, this is a perfect solution. Exam scenario number two, it's perfect for if you need to verify the data integrity of any large dataset being transferred over a Direct Connect connection. Remember, this is one of the biggest feature offerings, this service can verify data integrity after it is transferred. Now, one last thing here an exam pro tip, do not confuse this service with AWS Storage Gateway services. The big thing with DataSync is it's meant to sync your data between a source and a location. The Storage Gateway services are a completely different use case, you leverage them to offload storage to the cloud and you can leverage caching locally to speed up interacting with your data. Now that's going to do it for this clip, we just reviewed DataSync. Please make sure you understand the three scenarios in diagrams that we covered, on‑prem, AWS to AWS, and Edge Computing. We're going to go ahead, we'll wrap this up here, and I'll see you in the upcoming clip.

AWS Backup
Let's talk about backing up our data within the cloud. AWS Backup is a fully‑managed service that is used to centralize and then automate your data protection and your data backups. If you see something related to backing up data, you should immediately be considering this service. Now, it works by allowing you to configure backup policies and then you can monitor activity for all of the resources you're backing up in a centralized place. Because of this service, there's no more need for custom scripts, manual processing, or even service‑by‑service tracking, it's all handled for you, and you can easily view all of this stuff within one service. One of the big features with AWS Backup is it does support cross‑Region and cross‑account backups. So, if you have a multi‑region deployment, which, with best practices you should, you can leverage one service to back up all of your resources within their respective Region. Or, maybe you have an organizational multi‑tier architecture, which again is a good practice, and you want to centralize a single account to have backups being controlled in, you can do that with a centralized account backing‑up cross account. Now, you need to be familiar with what different services this backup service works with, so it does work with AWS services, which we'll look at right now, as well as other clouds, so you can back up devices and resources from other clouds, as well as on premise, so if you have virtual machines on‑prem, you can configure them to be backed up via this service. Speaking of AWS, let's look at some of the supported resources. You can leverage AWS Backup to centralize your backups and data protection policies for all of the services you see here. These all support native integration with AWS Backup, please be familiar with these services. A lot of the common ones are going to be EC2 and EBS and even RDS and Aurora. Moving on, let's look at some backup concepts here. Backup plans are going to be the policy that you define for when and how you actually want to back up the resources. With your plans, you store your backups in a backup vault, so this is going to be a container that stores and even organizes your backups. Now, it's important to call out your backups are encrypted, which is a big benefit. Thirdly, you can specify backup policies based on resource Tags within AWS. You should always be tagging every single resource that you deploy within the cloud, this helps with organization and billing structures. You can also use PITR, so Point‑in‑Time‑Recovery. Backup will support this natively for some of its supported resources, so an example of this could be a DynamoDB table that supports Point‑in‑Time recovery for that type of resource. And then lastly, remember, you can easily manage your backup plan's lifecycles based on retention settings. Now, speaking of retention settings, let's explore some of these terms. One of the first things you define is a backup schedule, so how often do you want to run your backups? Now, AWS Backup supports every hour, every 12 hours, you can do it daily, weekly, or even monthly. In addition to those scheduled timeframes, you can actually trigger a backup manually if you need to. With your schedule, you specify backup windows, this is going to be the time that the backup window begins. In addition to the time where you want it to start, you also specify a duration of the window in hours. You need to make sure you're giving your backups enough time to complete, especially if you have large amounts of resources. In addition to those, you also specify a complete within value, so you set a number where you want it to say, hey, I want you to be done backing up within this amount of time. Thirdly, retention periods or retention lifecycles, this is going to be the number of days that you want to store your backups for. This can range between 1 day and 100 years if you define a number. If you do not define a number, so it's unspecified, that means indefinite, so your backups will stay forever until you manually remove them. And then lastly transitions, backups are maintained within a storage tier. When they first start off, they're going to be in warm storage, but you can transition them to cold storage, which is going to be a lower‑cost tier. So a use case for this would be to transition backups that are maybe 30 or 45 days old to a cold‑storage tier because you're likely not going to be using them as much as you would a warm backup. Moving on, let's explore Backup Vault Locks, this is an important feature you need to know for this service. This is going to be an optional feature that gives you additional security and additional control over your backup vaults. It works by helping enforce a write‑once‑read‑many data model, so WORM. WORM is exactly what it sounds like. You can write your data at one time and you cannot do anything more than that. You can only read it many times, but you can't overwrite or delete. Now, each vault can have exactly one vault lock in place for it. With your vault locks, there are two different modes you need to be aware of. Now these also are similar to S3 Vault Locks, so if you've seen those in a previous course, then you're probably going to be familiar with these. The first is compliance mode, this is where your vault configuration cannot be altered or deleted by anybody that includes the root user, so this is why it's called compliance mode because it's great for compliance requirements. You then have governance mode, this is where your vault locks can be removed by specific users with specific IAM permissions, so governance mode is good for test running of your actual vault lock policies to see if it's exactly what you want. The big thing to remember here is compliance mode is great for actual compliance requirements. It's going to be the strictest mode you can have. Governance is one you can monitor and govern as you see fit. Now, let's look at some exam scenarios before we wrap this up. If you need nightly EBS volume backups for disaster recovery in another Region within your account, you can leverage this service. Also, it's great for efficient management of DynamoDB and Amazon Aurora backups for several years. Remember, with automated backups within these services themselves, there are specific amounts of retention period you can set, 35 days, etc. If you need something for much longer and you want it to be efficiently managed, so you don't want to take a manual snapshot, well, you can use AWS Backup to schedule your backups and then set a retention period for many years as opposed to several days. That's going to do it for this clip on AWS Backup. Be sure you remember the different services that this can interact with and the different resources that it supports. The big thing to take away is this is a great, centralized place to manage automated backups and retentions. Let's go ahead, we'll end this here, and I'll see you in the next one.

Demo: Creating Backups with AWS Backup
In this demonstration clip, let's look at creating our very own backup plans with AWS Backup. Before we jump in, let's have a high‑level architecture overview so that we can see what we are mimicking regarding our environment. What we're going to do is we're going to log into our sandbox account, and we're going to have two different architectures set up. You can see on the right, we have identical setups, the only big difference here are the Tag values, so we have EC2 instances and DynamoDB tables. One of the environments is production, the other one is considered staging. The first thing we're going to do is we're going to go ahead and create an AWS Backup plan within AWS Backup. This is going to do a few different things. We're going to configure it to back up all resources within the account. However, within that, we're going to filter resources to look for a key‑value pair of environment in prod. That way, we're not backing up everything within the account, and we're filtering using one of the built‑in features. In addition to this, we're going to look at how you can send your copied backups to a different disaster recovery region within the same account. So the process is going to go through like this, our AWS Backup plan is going to be executed on a schedule, and it's going to say, hey, I'm looking for environment of prod. It's going to find that environment Tag value for our resources and it will back those up. On the other hand, for the staging environment, AWS Backup will say, okay, I'm not configured to look for this Tag value or I'm not matching the Tag value I'm supposed to be looking for so I'm going to go ahead and skip backing all of you up that way we can save on costs with our backups. So, with that out of the way, let's jump into the console now and let's begin creating our backup plan. All right, I'm in my sandbox, let's go ahead and get started. I've loaded up the two services that I've created resources in that we're going to play around with. If I look at instances here under the EC2 service, we have two Application Servers. Now, the difference with these servers is going to be the Tags, so one is staging and then the other one will be prod. So, in theory, we're mimicking having multiple environments in one account, and we only want to back up production, we don't care about the staging server. Under DynamoDB, I've created two tables in here. Now, I made these completely separate names, and I put staging in the name just to make it easier for us. But if I select this and I go to look at the Tags here, so under Additional settings we'll scroll down, we'll see environment key prod, and then the staging one, of course, is going to have a Tag value of staging at the bottom here, as you'll see once this loads. There you go. Perfect, so that's out of the way, let's go ahead and begin. I'm going to go here, I'm going to load up Backup. I'm going to load this in another tab, and let's begin creating a backup plan. Now, really quickly before we jump in further, this is a extended sandbox for me personally, so if you have access to the Pluralsight sandboxes, you're not going to be able to actually complete this backup it's actually restricted, so just keep that in mind. So what I'm going to do here is I'm going to go click on Create backup plan because I want to create a brand new backup plan. Now, once we're in here, we could start with a template, you could choose a template from their list, so we'll just select one just to see what happens here, and it fills out all of the rules for you. You can also define a plan using JSON, which we are not going to do that because that would be extremely hard to walk through here. However, this is a good choice for infrastructure as code or maybe you're using the CDK. What we're going to do is build a brand new plan from scratch. We're going to give it a name, so we'll just call this Daily, and then you can add Tags to your plan. So, what I'll do is I'm going to go ahead and give it environment, and I'm going to select prod because this is a production backup. So let's go ahead and move on to the configuration section, this is where we define our schedule. Remember, you give a schedule and a backup window, so let's go ahead and give this rule a name, and we're just going to call it Daily. Now, in theory, you could have multiple backup rules in place, but we're just going to create one job per rule just so we can see how this works. So I give it a name of Daily and then we can select a backup vault. So, what we can do here is use the default, or I can go in here, open this up in a new tab, and create a brand new one, so let's go and create one called production. Now, within AWS Backup, you have two vault types. Now, you likely shouldn't be quizzed on this on the exam, but I'm going to cover them anyways because it's good to know when you're actually using this service. With the default backup vault, this is the typical storage type. Your backups are immutable and they're separate from your different sources. In addition to that, your data is encrypted and vault locks are optional. If you use logically air‑gapped vaults, so if you don't know what an air gap is, it typically means in an on‑premise environment that it's completely separated from one another so one network can't touch the other network. Now, this is logically, so it's not the same thing as physical, but it's them trying to essentially represent that. This is similar. Your data is still encrypted, you can support cross account and cross‑org sharing, but they do put in a mandatory requirement for vault lock, so you have to lock your vault. Now, for us, we're going to choose standard, but understand for security purposes and sharing, you can choose logically air gapped. Next, we can choose an encryption key. Now, I'm going to use the default here for simplicity, but you can create different KMS keys for different backup vaults. So, for instance, this is production, so maybe we didn't want developers or software engineers or system administrators to access these backups. In that case, we can create a completely separate encryption key in KMS, lock down that key with a specific policy to see who can access it, and then encrypt the data in that vault. That's a good practice and you should do that. For us though, we're going to choose the fault because it's going to speed things up. So I'm encrypting my vault with this encryption key here, and again, we can add Tags so we'll do that. I'm going to add environment production, so prod, and let's create my vault. Now in here, you'll notice we could also edit the access policy, and we can create a vault lock. Now, I'm not going to actually create this lock, but let's look through it. We can choose governance mode, which is good for testing things out, remember. And then there's compliance mode, remember, compliance mode is the most strict mode for locking your vaults. With this, you're going to set a minimum retention period and a maximum retention period if you want to, and you do have to set a compliance mode start date, so this is where your vault will be permanently locked. Now, I'm not going to do this, but understand that you can implement vault lock policies. So I'm going to click on Cancel, and let's continue on. So we have our vault here. I'm going to go back to AWS Backup for our rule configuration, and let's continue on. I'll refresh, I'm going to select Production for our backup vault, and then we see our frequency here, so I can choose Daily, Hourly, Every 12 hours, Monthly, etc. Or, if you want to, you can put in a Cron expression. Now, Cron is something you really should know when you're planning environments, this is a fundamental syntax for Linux operating systems. So, while you won't be necessarily tested on the exact syntax, this is a good thing for you to know how to do, so I do recommend you learn how to write in Cron expressions. Again though, since this is a demonstration, we're trying to make it simple, I'm going to make this Daily. Next up is the Backup window, so when do you want to start the time of your backups? So this is going to be based off UTC time. Now, my time zone is the Chicago time zone, so UTC‑05:00. So in this case, what I could do here is I'm going to set it to 9:00 o'clock, so 9:00 o'clock Chicago time. Now, this is A.M. because if you look, this is going to be in a 24‑hour time block here. So, 9 A.M. Chicago time. Next up, when do you want to start your backups, within what time of period do you want to start them? So I'm going to say I want you to start them within one hour. This is important, I want you to start this backup schedule within one hour of 9 A.M. Chicago time. Now notice you could set it to be 1 day, 12 hours, 5 days, etc. There's a lot of flexibility here, so it comes down to your requirements. After that, complete within, when do you want your backups to be done by? Now, you're going to want to specify enough time for these backups to complete. So for us, we have two very small resources, so I could select something like 2 hours. However, if you're backing up a ton of data and a ton of resources, you might want to specify longer hours or maybe even multiple days, it's going to come down to how many resources this plan is backing up. So I'm going to just select two hours, we're going to move down here, and next up is point‑in‑time recovery. So remember, AWS Backup supports PITR, what this means is you get per‑second granularity for backing up and recovering your data, so this is going to be good for certain resources. You can see Aurora, RDS, S3, and SAP HANA. So we're not going to select this because I'm not actually using those resources, but you can turn on point‑in‑time recovery. After we decide if we want to do that, we get down to Lifecycle. Remember, when your backups are first taken, they're stored in warm storage. That way you can get to them much quicker, but that is a little bit more expensive. What we can do is we can say, hey, after a certain amount of time, I want to move backups from warm to cold storage instead so I can save on costs. So, maybe after 8 days, I can go ahead and say, okay, transition this, or maybe I want to do 14 days, which is 2 weeks or you can say 2 weeks, etc. There's many different ways you can configure this, but do pay attention to the recommended minimum. Next is the total retention period so how long do you want to store the backups that are being taken? So what we can do here is we can set days, weeks, months, or you can select forever. So remember, if you don't specify a specific time, forever is the choice, so they're there indefinitely until you manually delete them. So, what I'm going to do is I'm going to unselected this and that allows us to be a little bit more flexible. So, you'll notice when you have warm to cold storage migration, it restricts your retention time, so what I want to do is deselect that and I'm just going to say, save them for one day. Now, obviously, in a real production environment, you would set this to something way longer than that, maybe months or even years or maybe forever for compliance. For this again, it's just a demo, so I'm going to say save these daily backups for a single day. So, now we have our configuration in place, we can now choose to copy our backups to a destination, so maybe you want to back these up into a completely different disaster recovery Region. Now, we're in us‑east‑1, so maybe I want to look for us‑east‑2 so I'm going to say, yeah, I want you to back this up or copy this to Ohio. Now, within that region's resources, we could create a new vault, but for this, I'm just going to select default. Another thing to keep in mind here is you can actually copy these cross account remember, so cross region and cross account, you just need the ARN and permission to do so. So I'll go ahead and unselect this, we'll leave the default destination vault in us‑east‑2, and then we can set a lifecycle there. So you can copy the same settings from the ones we just configured or you can make a brand new one. Now, I'll just use the same. And there we go, so that's how easy it is to set up copying to a different disaster recovery region so that you can recover in that region and restore later on. Now up next is you can add Tags, so recovery points are what they sound like, the points where you can recover to. So what we'll do is we'll add a Tag and we're going to say environment production because everything in here is production related. Moving down some more, Advanced backup settings, so this is going to be Windows specific at this point in time. Now, Windows VSS backups are Volume Shadow Copy Service. What this means is that it's going to be application consistent as you can see as they say, so it's more of a detailed backup than just taking a backup of the volume. Now you'll notice there are some prerequisites here, but I'm not going to do this, we're running Linux anyways. However, it is good to know you can enable VSS backups. This is very good for Windows applications in enterprise environments. So, our plan is all there, I'm going to create it, and we see it successful. So I'm going to go ahead and I'll clear this. We go up now, and now we assign resources to this plan. So, what we'll do is we'll just say Production. We're going to use a default role for IAM, but you could choose one if you wanted to restrict AWS Backup. Now for this, we'll just use default because it's way easier, so it's going to create one for us with the correct permissions. A big thing to take away here is that backup uses a role to perform its actions, so keep that in mind. So, we'll leave it as default, and we get down to resource selections so now we get to some of the fun stuff. You could do two things, include all resource types that are supported or you can filter and select specific resource types, and you'll notice all of the resource types that are currently supported listed. In addition to that, you can exclude resource types. But we're not going to choose this, instead what we're going to do is include all resource types within this account. Now, in typical architectures, you're likely going to have a ton of resources, especially for an enterprise, so this might not be the most optimal way to do it. However, a way to get around that in wasting backups is we can include this and then we can refine using Tags, so this is why, or at least another reason why, you should be tagging everything within your AWS account. So I'm going to add a Tag to refine by, we're going to say environment, you can see the different logical choices so all values contains does not begin with etc., but we're going to say equals, and we're going to look for prod. So, for all resources that are supported, if their key is environment and that value equals prod, we're going to back up that resource. So I'm going to assign resources, and we'll continue. And there we go, we now have our backup plan in place with our backup rules and our backup schedule. And that's exactly how easy it really is to create your very own backup plan. All right, now, that's it, that's how easy it is. I'm not going to let this run, I just wanted to walk you through the process. We created a backup plan. We created our very own production vault to use for our rule and our schedule within our plan, and we showed how you can filter on resources using Tag values. Let's go ahead, we're going to end this demo here, and I'll see you in an upcoming clip.

AWS Application and Server Migration Services
Olay, let's talk about migrating your services to the AWS cloud. Before we dive in, you have to be familiar with, as a solutions architect, The 6 R's of migration, let's review these really quickly before we continue on. The first is Rehosting. Now, Rehosting is essentially going to be a lift and shift of your architecture, so you're lifting your existing architecture and essentially just relocating it to the AWS cloud. Typically, this is going to be the most expensive way to do things. There's also Replatforming. Replatforming is similar to lift and shift except for they usually call this lift, change, and shift, so you're modifying a few things and then shifting it to the cloud. This is going to offer a little bit more optimization compared to lifting and shifting directly. Thirdly is Repurchasing. Now, Repurchasing is where you move to a completely different product, so maybe you're using one particular application and then you want to just go ahead and repurchase a completely different application to accomplish the same thing. An example of this maybe you use BambooHR for your HR system and maybe you want to shift to Workday instead, that would be Repurchasing. Fourthly, we have Refactoring and Rearchitecting, these kind of go hand‑in‑hand here. This is where you're going to essentially completely redo how the application you're running is actually architected and developed, and you're going to want to start using Cloud Native features. This is going to be the most optimized way to leverage the cloud. When you want to shift things to the cloud, this is the way to do it if you have the time and the money. Fifthly, we have Retire. This is pretty self‑explanatory, you're retiring your architecture in your system completely, in other words, you're getting rid of it. And the last one here is to retain, so this is going to mean that you basically don't do anything. You did some costing analysis and you decided, you know what, really this isn't for us to go ahead and Rehost or Replatform, etc., let's just retain what we have, and we'll check it out later on. Now, with these out of the way, let's dive into some services that have to deal with these. First up, Application Discovery Service. This is going to be a service for you to help plan your migrations to the AWS cloud. Now, this can be on‑prem, it could be another cloud provider, etc, it's all meant to help you plan migrations to the cloud. You use this to easily collect different usage data, configuration options and data, including dependencies between applications. So if one application depends on another, you can capture that dependency using this service, and the beauty is it's all captured for all of your on‑premise servers and external databases. Within Application Discovery Service, you can view all of those discover servers that you've already looked for. You can group them into applications, so if certain servers belong to one particular application you can group them together. And you can even track the migration status for each of those discovered applications, so you can track them separately. And the last thing here it integrates with something called AWS Migration Hub, this is going to allow you to have centralized tracking for any of your migration efforts into AWS. Within Application Discovery Service, there are two different methods of collecting or discovering information. The first is agentless. This is where you deploy an Application Discovery Service Agentless Collector. For shorter terms, we just call it an Agentless Collector because that's a mouthful. Now you deploy it via an OVA file, which is a virtual appliance file typically deployed on VMware. After installing and configuring the Agentless Collector, it's going to go ahead and start identifying the different virtual machines and the different hosts associated with a vCenter on premise. Now this information that it collects is going to include things like your server hostnames, the IP addresses and the MAC addresses assigned to the hosts, and it even includes the disk allocations, database engine versions, and even database schemas. So it gets very detailed, which is a really good thing to have when you're planning a migration. And then, lastly, you can also collect utilization data for each of your VMs in your databases, and this is going to include things like the average in the peak utilization for your CPU usage, your RAM, and even your disk input/output metrics. Again, it allows you to discover quite a bit of information. Now, the next type is an agent‑based discovery. This is going to be performed by you deploying an Application Discovery Agent on each of your VMs and on each of your physical servers that you want to migrate. The agent will work for both Windows and Linux operating systems, so what that means is that most enterprises can easily use this method if they need to. And finally, it's going to collect static configuration data, detailed time‑series system performance information, inbound and outbound network connection information, and even the detailed processes that are actually running on the machines. So this gets even more in depth, it's capturing all of the actual individual processes and tasks that are running on your apps for your VMs. Now, let's look over an exam scenario. If you need to collect usage and configuration data about your on‑prem servers and your workloads to plan a migration to AWS, Application Discovery Service is a perfect option. The big thing to remember here is it doesn't migrate, it just collects usage and config data, so it's discovering data. Now, let's move on to a different service that plays a similar role, the Application Migration Service, otherwise known as MGN. This service is an automated lift and shift, in other words, a Rehosting solution that's meant to simplify, speed up, and even reduce the costs of migrating your applications and servers to the AWS cloud. Now, this service is replacing what was once known as the server migration service, SMS, and it was also known as CloudEndure Migration. So if you see that term anywhere, you want to think MGN instead. How this service works is it works by replicating your source servers that you choose into an AWS account using native cloud services, so EC2, etc. One of the benefits it offers is that you can actually stage your resources that are getting migrated, so you can stage them, make sure they're verified and ready to go, and then you can easily and quickly cut over when you are ready to actually fully migrate. What this service does is it helps you avoid the following. You avoid compatibility issues between services in the cloud, it helps with performance disruption because you're not having to cut over until everything is already staged and ready to go, and it really helps with long cut over windows. Again, playing on just like the last part we just talked about, you can stage all of your resources and your data and your applications so it helps you cut over much quicker. Now, we need to look at MGNs, RTO, and RPO. Remember, recovery time objectives and recovery point objectives are critical components and information you have to be able to work around when you're planning an architecture. When you're using MGN, the recovery time objective is typically just minutes, it's really going to just depend on the amount of boot time required for your operating systems. For your recovery point objectives so where you can actually recover to, this is measured in the sub‑second range, so it is extremely accurate and you can get down to very granular detail on where you want to recover your data. Now, that's going to do it. I think that's long enough on these two services. Remember, Application Discovery Service allows you to discover and collect information about your servers. Application Migration Service actually allows you to begin your replication and your migration into the AWS cloud. Let's go ahead, we're going to wrap this up here, and then coming up next, we're going to look at some more migration services and some tool sets.

AWS Database Migration Service (DMS)
All right, so we looked at the application and server side of migrations into AWS, let's start looking at the database migration portion. In this clip, we're going to look at Database Migration Service, otherwise known as DMS. AWS DMS is a service provided to you so that you can easily discover all of your databases, convert database schemas, and fully manage migrations of different data stores to the AWS cloud. With this service, you can actually work with the following resources, you can look for and migrate relational databases, so MySQL, PostgreSQL, etc., you can look for data warehouses, so OLAP workloads and all of the different formats that go along with that type of applications, and you can even discover NoSQL or non‑relational databases, these are all compatible with DMS. To actually begin using the service, you start things off by creating what is known as a DMS server, this DMS server is running on an EC2 instance and what it does is it runs the DMS replication software. After you begin the server, you then create a source and a target connection to further instruct DMS where you want to actually extract your data from and where you want to load it to, so where do you want to migrate from and to? Now in exam pro tip here, you can use DMS to migrate databases while keeping the source database online, this is a very important factor to remember for the exam. If you're migrating databases and you need to maintain their online status, this is a perfect service to use. Moving on, let's look at some of the supported migration types, there are two types that you can perform and really there's only two types in general overall. The first is Homogeneous. Homogenous means it's the same type, so an example would be migrating from Microsoft SQL Server on‑premise to Microsoft SQL Server in Amazon RDS. Heterogeneous is where you're migrating from something different, so you're migrating, for example, from an Oracle database on premise to an Amazon Aurora PostgreSQL database so you're changing the database engine types. Just for your records, homo means same, hetero means different so that's the easy way to remember it. Now, let's look at the process at a very high level. The first thing you do is you connect to a source data store or database or whatever you're trying to migrate, you then set up DMS to read the data within that source data store. After it begins reading the data, you can have it format that source data for use by the target data store, so maybe you need to change the format of the database engine, for example. And then lastly, it's going to go ahead and work on actually loading that data into your target data store. Now, let's look at replication tasks and migration types, you need to know how these work and when you would use them. Replication tasks are the primary component within the service, these are what actually work to move your datasets from your source endpoint that you configure to your target endpoint. Typically, this is going to be from on‑prem into the cloud, but that's not always the case. In addition to this, this is going to be the very last step you take for preparing before you actually begin your migration. Now, there are three migrations you must know for the exam, the first is full load, this is when you migrate your data from your source database to your target database fully. This is the best option if you can afford any downtime. You also have full load plus Change Data Capture, or CDC. This is going to perform that same full data load like we just talked about, but it also uses CDC. So what CDC does is it's going to capture ongoing changes on the source database as they are occurring, in addition to that initial transition or migration. What happens is that initial load is transferred and migrated and then changes are then applied to that target after the initial full load is done. And then lastly, we have CDC only, this is only meant to be used to replicate data changes, so it's best used to replicate your changes when you start a bulk load to keep source and target in sync. So maybe you have a separate database in AWS that you want to just replicate changes to after you've stood it up, well, that might be a perfect scenario for CDC only. Moving on, let's look at source endpoints and target endpoints. DMS supports on‑premise or EC2 databases, so maybe you're running a local MySQL or PostgresSQL se database or maybe you're hosting it within EC2, all of these flavors you see here are supported, so Microsoft SQL, MongoDB, Db2, etc. It also supports third‑party managed database services, so an example would be something running in Azure or GCP. Thirdly, it supports every engine type within Amazon RDS. So if Amazon RDS can run it, then it's supported as a DMS source. It also supports Amazon S3. So if you have data within S3, you can go ahead and migrate that and transform that. In addition to S3, you can use a DocumentDB database, so a document database for MongoDB capabilities, these are all supported source endpoints for migrations. Now, on the other side of that, we have target endpoints, so the target of the migration. This is going to be extremely similar, but there are several more options. It supports, just like source, on‑prem or EC2 databases with a slight variation of the engine versions that are supported. In addition to those, it supports, just like sources, all Amazon RDS database engines. Now, here's where we start getting some more, it supports Redshift and Redshift Serverless, DynamoDB and S3. You can send things to OpenSearch Service and ElastiCache, and even Amazon Kinesis Data Streams or Apache Kafka. Now, it also supports DocumentDB, and in addition to that, Amazon Neptune, so graph databases. So you'll notice targets have a lot more support than sources do, and it really allows you to convert and transform data during your migrations, which is very handy to have. Now that's going to do it. Let's go ahead and wrap up this lesson and clip on DMS. We reviewed the service, we talked about the migrations and the different task types. Be sure you understand, you use this to migrate data from databases outside of AWS into AWS, and we'll end this here. And coming up next, we're going to talk about converting our schemas using this tool.

Database Schema Conversions
All right, we just got done talking about AWS DMS. In addition to DMS, you're typically going to perform some type of schema conversion for any of your heterogeneous conversions and migrations, so let's look at how that works here. In AWS DMS, you have two options for easy database schema conversions, the first is called DMS Schema Conversion. This is a web service that is used to assess the complexity of the migration and even convert your schemas for your databases and your objects before they get applied to your targets. The other option is the AWS Schema Conversion Tool, or SCT. This is a downloadable tool that is used to do something similar, you can convert your existing database schemas and objects from one engine to another before it gets migrated. Now, let's explore some of the concept of each. First up is DMS Schema Conversion. What this does is it offers a workflow to automatically convert your source database schemas and most of your database objects to a format that is compatible with your target database. So, for example, maybe you want to convert from PostgreSQL to MySQL for some reason, using this conversion method includes your different tables, the different views for your tables, any of your stored procedures, functions, data types, and many more. Now, if DMS Schema Conversion cannot fully convert an object that it's trying to migrate, that object will automatically and clearly be marked for a manual conversion later on so you can see what was not converted and you can do it yourself. Now, the big thing here is that it's offered only as a web version. So with that, it supports less database platforms compared to the SCT, it offers less functionality, and a few less supported sources compared to the tool itself. And speaking of the SCT, let's look at some of the concepts here. Remember, this is an installable application that you use to convert your existing schemas from one engine version to another. The cool thing with this is it works with relational OLTP schemas, data warehouse schemas for OLAP, and even non‑relational schemas, so it's very flexible. The big thing to remember here, this is only ever going to be used for heterogeneous database engine migrations, you don't need it if you're performing a homogeneous migration. Now let's actually look at some of the conversions that can occur, so here are some examples. For OLTP transaction processing, an example would be converting Microsoft SQL Server to Amazon Aurora MySQL compatible workloads. For OLAP, which is data warehouse intensive, you can migrate maybe from Snowflake databases to Amazon Redshift. And then finally for non‑relational, maybe you want to convert Apache Cassandra databases to Amazon DynamoDB to leverage that managed service. These are all real‑world examples and possibilities using the SCT. Now an exam pro tip, remember, you don't need any of these options for homogeneous engine migrations, you only need it for heterogeneous migrations. I know I keep saying that, but it is very easy to get the two types of migrations confused and forget why you use this tool in the first place. Lastly, understand that RDS is not an engine version, RDS is a platform for actually running your engines, that's an important distinction. Now to wrap things up here, let's have an exam scenario architecture discussion. This diagram is meant to go ahead and show you that you might run into scenarios that require heterogeneous database migrations with continuous replication of data. So at a high level, how this would work is you would have the AWS Schema Conversion Tool installed and working to convert the source schema, in this case, Microsoft SQL Server, and it would apply the updated schema to your target database, which, in this case, would be Amazon Aurora. Now to actually perform the migration itself in addition to the schema conversion, you go ahead and you create a DMS server that runs on EC2 remember, and this is running the replication software to actually execute your replication tasks and migrate the data to the targets. Using this setup, you can easily implement a full load with Change Data Capture migration. So what's happening is your data is migrated via this replication instance, and it uses that full load plus CDC migration type. This is going to allow for a full load migration in addition to continuous synchronous replication. Now, that's just one example of using this tool, but it is a common scenario. With that being said, let's go ahead, we're going to end things here. Please do your best to remember when you would use schema conversion, whether it be the tool or the DMS schema conversion service. We'll go ahead and wrap this lesson up, and we're going to move on to a Module Summary and Exam Tips clip.

Module Summary and Exam Tips
Okay, way to hang in there once again. We've reached the Module Summary and Exam Tips clip, let's go ahead and get started with some reviews. First up, AWS Snow Family. For this exam, you need to know the difference between using a Snowcone device and a Snowball Edge device, so let's look at some of the differences that are key to understand when you would use one or the other. A Snowcone is a smaller, portable device, so this is going to be really good for small space where you don't have a lot of requirements for compute or storage. It is meant to be a very small and easy way to get Edge computing and set up data transfers to the AWS cloud so you can avoid storing locally. Remember the two different ways you can use these devices to get data to AWS, there's offline, which is where you load up the Snowcone device and then you ship it to AWS to have them offload it. And then there's online, this is where you actually work on migrating with a tool like DataSync over a network connection into AWS. In addition to the migration modes, remember there are two different types, Snowcone and SnowconeSSD, and SSD is going to offer you higher performance for a little bit more of a cost. And lastly, again, this is going to be perfect for location constraints, so for instance, maybe you have a single remote branch office that doesn't have a lot of space or it can't support a lot of servers from a space, power, or networking standpoint, this might be a perfect solution. Moving on to Snowball Edge, this is going to be considered the jack of all trades for their Edge devices that you can leverage. This is going to be a much larger device, which offers many more capabilities and a lot more compute and storage potential. Snowball Edge is going to be the best way for fast data transfer for very, very large datasets, so for petabytes of data you would want to probably use this if there's a time constraint. It works by using offline shipping for the most part. Now you can do online, but these are known for being used for offline data transfer. You load up the device, you ship it to AWS via one of their regional carriers, and then they offload that data into S3 and securely wipe the device. For these, you have three different options, you have Storage optimized, you have Compute optimized, and Compute optimized with GPU. These all do exactly what they sound like, Storage optimize gives you the most amount of high‑performance storage, Compute allows you to actually run compute services local to wherever your remote branch is at, and then Compute optimized with GPU offers the same as Compute optimized with an installed graphics processing unit for you to leverage in your workloads, so this could be a perfect option for things that require video rendering or some type of machine learning that leverages GPU. Exam scenario. If you have something where you need to migrate something like 50+ terabytes or petabytes of data to AWS and you cannot consume or do not want to consume all of your bandwidth in a shorter amount of time, I would think Snowball Edge. Next up, Storage Gateways. You need to remember the different gateways, their different protocols, and why you would use them. S3 File Gateway, you set up a File Gateway appliance locally or even within EC2 if you need to, and this is going to be meant to allow NFS or SMB connections to S3 buckets so you can use them like native storage, so you can upload and download data and leverage different storage classes with S3 using this type of appliance. You also have FSx File Gateway, so this is similar to the S3 File Gateway in terms of how you set it up, but this is meant for FSx for Windows File Servers specifically. It's meant so you can leverage Windows Native Samba capabilities. For instance, maybe you need Active Directory integrations and capabilities or you need to run NTFS file systems things of that nature, you're going to want to think this gateway. Thirdly, Tape Gateway, this is meant to specifically replace backing up physical tape drives by offering S3‑backed cloud storage instead. It works by mimicking physical tape library components so you install those applications in those components and then you set those up using a backup server. And then the last one is a Volume Gateway. This is going to be S3‑backed storage, and you mount your appliances locally as iSCSI devices, that's very important to remember, iSCSI connections. You really need to know the difference between stored and cached volumes. Stored volumes is where you store all of your data locally and you asynchronously back it up to S3, cached volumes are where you only maintain the most recently accessed data locally and offload your other storage to S3 to save on storage costs. Now an exam scenario for this, maybe you need to offload object storage to the cloud, but you want to keep recently access files quickly retrievable, well, Storage Gateway cached volumes are perfect for this scenario. Moving on, really quickly, you need to know the difference between a data lake and a data warehouse. Data lakes are where you're going to have raw data in its native format, and it could be structured, a mix of semi structured and even include unstructured data. Typically, with data lakes, you use them to then process data into a data warehouse. Data warehouses are where you're going to store cleaned and processed data in a more structured format. You can start using tables, different dimensions, store procedures, etc. I believe the easiest way to remember this, a lake can be pretty messy, warehouses are typically nice and clean and organized because you need to track where things are going. Building off of that, we talk about that to talk about LakeFormation. Remember, this is the managed service to help set up and leverage your very own data lakes within Amazon Web Services. It stores all of their data that you have within your lake within an Amazon S3 bucket so it's cheaper storage. In addition to that, just like we just talked about, this is going to work with structured, semi‑structured, and unstructured data so you can mix and match your data as you need to, throw it all in one lake, and then process it later on. And speaking of processing, remember that it's going to leverage AWS Glue on the backend so that it can perform the different ETL actions so we can extract it, transform the data, and then load it to one of the other services within AWS for you to use. And speaking of other services, let's go over an exam scenario. Maybe you have to create visuals or reports within Amazon quick site and you want to join data within your data lake with operational data that's within in an Aurora database at that point in time. In addition to that, they might mention something like enforcing column‑level authorization for security, well, AWS Lake Formation is a perfect service for this use case and can likely be included within your answer. Moving on to AWS Backup. Remember, this is going to be the managed service to help centralize and then automate your data protection and your data backups. With AWS Backup, when you're backing up supported resources, you can copy those backups across region and even across accounts into different backup vaults. One of the biggest benefits of using AWS Backup is that you can go ahead and leverage fine‑grained control of your backup policies and your backup plans based on using resource Tags. So if you remember in the demonstration, we said, hey, back up all supported resources, but when you do that only look for specific Tag key‑value pairs. Fourth thing here, remember that backup vaults allow you to have enforcement of a WORM data model via the different policies that go along with the vaults, so vault lock policies. Remember that WORM is write once, read many, so you write it one time and then you can't update it, delete it,etc., you can only read the data. And then lastly here, the backup vault policies have two modes, Compliance mode, which is going to be the most restrictive or even route cannot undo it. And then there's Governance mode, this is going to be good for running test runs, seeing if you really want to implement certain lock policies, etc. With Governance mode, if you have enough IAM permissions, you can actually undo it and delete the governance mode vault policy. Next up, some exam scenarios. If you need to retain backups from things like DynamoDB or maybe Aurora, and they want you to retain the backups for several years and they want it to be done in the most operationally efficient way, then AWS Backup is perfect. You can automate all of that retention period, the backups, etc. Also, maybe you need to back up things like Amazon EC2 and their EBS volumes to another region very quickly for disaster recovery, well, AWS Backup again is a perfect choice. Moving on application discovery service review. This is meant to easily collect server information and then plan your migration to the cloud, remember, it's only doing collection of information. There's agent list where you deploy an agent list collector via an appliance file and this can go ahead and collect things like average and peak utilization for CPU, RAM, and even Disk input/output metrics. And then you have agent based, this is a little bit more work because you have to deploy the agent on each individual VM and each individual physical server and physical host. With the agent‑based deployments though, you get a little bit more information in much more detail. In addition to the other stuff, it also collects static configuration data, different inbound and outbound network connections, and it will even get details about the running processes and tasks on your operating system. Now, moving on to its partner here, Application Migration Service. Remember, this is going to be the automated lift‑and‑shift solution to help migrate applications to AWS. So this is actually migrating, it's not collecting data, it's actually doing the migration. It works by replicating your source servers and VMs to an AWS account using specific native cloud services, so your VMs would likely just go right to an EC2 instance and the replication would begin. Remember the big benefits here. This is perfect for staging resources, and then once all of your resources are officially being migrated and validated, you can easily and then quickly cut over to the new environment within the cloud. Moving on to another migration service, Database Migration Service, or DMS. Again, this is going to allow you to easily discover your different databases, you can convert database schemas, and you can fully manage migrating different data stores to the cloud. It works with the following, relational databases, so MySQL, PostgreSQL, etc. It works with data warehouses where you might need to perform OLAP workloads, and it even supports non‑relational NoSQL databases. Remember that it supports homogeneous and heterogeneous migrations, so same and different migrations. In addition to this, know the following types of migration tasks, full load, full load plus Change Data Capture, and Change Data Capture only, really go back and review when you would use each of these. Full load is great for when you can afford downtime, full load plus CDC allows you to replicate changes after a migration, and then CDC only is going to only keep replication in place for changes between databases. Next up, database schema conversions, this goes hand in hand with DMS. There are two different solutions you can use for converting schemas, the first is DMS Schema Conversion. This is a service used to assess the complexity of your migration, and you can use it to actually convert your database schemas and objects before migrating to the target database. The thing to remember here is that DMS Schema is going to be a web‑based tool, and because of this, it's offering a little bit more limited functionality. Now we say limited functionality because we have another tool called the Schema Conversion Tool, or SCT. This is an actual downloadable tool that you install and configure locally, and you use it to convert existing database schemas from one engine type to another. It's going to offer much more functionality and many more capabilities and power compared to the Schema Conversion offering, which is the web‑based tool. Now, moving on here, Schema Conversion Tool conversions, these are all real‑world examples of what you might use this tool for. First, OLTP workloads, so you might use this for migrating Microsoft SQL Server or Oracle Server to Aurora, MySQL or something else within Aurora. We then have a OLAP, O, L, A, P. This is perfect for migrating from one provider, like say Snowflake, and transitioning and converting and migrating data to Amazon Redshift data warehouses instead. And then lastly, NoSQL, or non relational. Maybe you want to convert Apache Cassandra workloads to something more cloud native like Amazon Dynamo DB, then you can do that using this tool. Big exam scenario, remember, you do not need either of these tools for homogeneous migrations, these are only used when you're converting to different engines. That's the whole point of the tool, it's a Schema Conversion Tool. You don't convert schemas when you're going from MySQL to MySQL, just keep that in mind. Now with that being said, that's going to wrap up this module. Thank you for hanging in there. Let's go ahead, we'll take a break, and I'll see you in an upcoming module so we can continue on.

Big Data and Analytics Services
Amazon Redshift
Hello, and welcome to this module, Big Data and Analytics Services. First up, let's go and talk about one of the data warehouse solutions in AWS called Amazon Redshift. Before we dive into the service, there are three fundamental concepts you have to understand regarding big data and they're commonly referred to as the three Vs of big data. The three Vs of big data are three defining properties, or you could call them dimensions, of big data and the different processes that surround big data itself. The first is volume, this is going to be the amount of data being stored within a data warehouse. Now typically, with data warehouses, volume is very high. The next is velocity, so this is going to be the speed of the data processing that's occurring, how fast you're collecting all of that data, and how fast you can analyze the data that's stored within your data warehouse. And then lastly, variety, so the number of types of the different data that are actually being stored and even used in your different workloads. These are all three important properties you have to consider when designing a data warehouse, especially when using Redshift. Now, speaking of Redshift, Amazon Redshift is a fully‑managed petabyte scale data warehouse service that runs in the AWS cloud. One of the key indicators is petabyte scale, if you need that amount of data for a data warehouse Redshift is a perfect option. Now, it is based on PostgreSQL, however, it's used in big data applications and OLAP workloads instead, so it's not used for OLTP, in other words, it's not used for transaction processing. The beautiful thing about Redshift is you can actually use your standard SQL clients and applications, as well as your business intelligence tools to actually interact with it, so you can manipulate the data, run some analytic queries, etc. Let's discuss a few of the pros of using this service. AWS likes to brag that it performs up to 10 times the performance of using other data warehouse solutions. Now, there are a lot of data warehouses out there, things like Snowflake, etc., and AWS likes to say that this is more performant. One of the reasons it is so high performant is because it bases its storage on columns, so it's not row based, it's column based, that is another huge key indicator. And then lastly, it uses all of the column storage to execute parallel queries using their very fast, parallel query engine, which is what provides such speed when you're analyzing your data. Now really quick fun fact, this actually received its name due to the initial desire to have people leave Oracle databases and start to leverage this service in AWS instead, that's why they call it Redshift. Moving on from the fun fact, Redshift cluster deployment modes that you need to know. There are two deployment modes within Amazon Redshift. The first is a provisioned cluster, this is where you actually pick and deploy a collection of different compute nodes and different instant sizes and you actually manage your cluster directly, so you're in charge of implementing the different instance size, etc. You then have serverless, so serverless is where it's going to automatically provision and manage the capacity for your clusters for you. What you're mainly in control of for a serverless cluster deployment is that you set a baseline of capacity that you want to have. So you're telling Redshift, hey, don't ever go below this line and then scale for me as the demand rises. Let's actually take a look at a high‑level architecture of a provisioned cluster in Redshift. When you're deploying Redshift clusters with a provision mode, you actually create your own Redshift cluster and you're going to start defining the number of nodes that you actually want, as well as the different node sizes, so there are compute nodes, there's data optimized nodes, or storage dense modes, etc. Now you're not going to need to know necessarily the low‑level details of each node, but understand you do go ahead and specify your different nodes as well as the size. After you create your Redshift cluster and you have your underlying instances in your different nodes, you could then start to connect to your Redshift cluster using your chosen SQL client application, so maybe some type of database workbench, etc. It also supports connections via the common JDBC and ODBC connectivity protocols. So, on the exam, if you see one of those connections for a data warehouse, understand that Redshift can't support that. Now, once you connect to your Redshift cluster via your chosen tool set, you can then actually start to run database queries, which are going to be sent and processed to the cluster endpoint. So an example, just selecting all from a sales table and then limiting the results. Now within your cluster, you have two different primary nodes, there's a leader node, and this is going to receive the queries that you send, it's going to parse the different queries, and then it's going to work to actually go ahead and start developing the different execution plans for how it should execute those queries. In addition to receiving parsing and planning, it also goes ahead and coordinates the execution by sending them to the compute nodes to actually execute those queries. So the compute nodes receive the orders from the lead node, and they're going to go ahead and start executing those different queries that you send in, they're going to communicate amongst themselves to say, hey, I'm running this one, etc., and then once the results are compiled and ready to go, they actually send the results back to the leader node, which will then send it back to you via your tool set. So understand that you have leader nodes and you have compute nodes. Now, real quick, let's talk about column storage here. This again is one of the advantages of using Redshift for OLAP processes and workloads. However, the one on top, we're going to make it columnar, and the one on the bottom, we're going to make it row oriented. Now, the difference here is highlighted in the green. So on the top, we have product_id which is highlighted, and that's data that's going to be stored on the disks of the database together. Notice the difference, we store one row together or we store one column. Now the column‑oriented databases are scanning fewer rows and essentially going to process less data than a row‑oriented database. Because of this, they're typically better for real‑time analytics so this is why we use Redshift for analytics data warehouses. Row‑based databases, on the other hand, are going to be much more efficient at single‑row processes, like updates, deletes, etc. This is why row‑based and relational databases via things like RDS are very important for online transaction processing, or like we mentioned earlier, OLTP. If you take away one thing here, just understand that column‑oriented databases are typically better for analytics queries. Now exam pro tip before we wrap things up. You can leverage reserved instances with provisioned clusters to save on costs, so it's just like EC2 instances. Now, that's going to do it for this first clip on Amazon Redshift. Let's go ahead and wrap up here, and I'll see you in the next one.

Redshift High Availability, Snapshots, and Performance
All right, let's keep this Redshift train moving. We're going to talk about high availability, snapshots, and some performance and architectures regarding Redshift. First, let's talk about HA, snapshots, and DR. Redshift now supports Multi‑AZ deployments, and these are going to span two availability zones within your chosen Region. This is a newer feature within the last couple of years, so for those of you who have not studied Redshift in a long time, this is a significant upgrade that you need to be aware of. Now, with that in mind, Single‑AZ is still the default deployment model for your provision clusters. However, if you decide at some point in time you want to convert, they do support conversion between the two now. You can use snapshots like you would any other type of snapshot for other services, they're going to be incremental and point‑in‑time captures of your data for your clusters that you can restore. Now, these snapshots can be automated or they can be manual, so you can set them up on a schedule, you can trigger them, or you can go in and you can manually say, hey, go ahead and take a snapshot for me. Now, these snapshots are stored in an Amazon S3 bucket, which you have no access over. So again, this is just like other snapshots, it's stored in S3, but you don't have access to that underlying storage itself, you can only manage the actual snapshots themselves. Now, when you restore snapshots, you restore them into a new cluster, but the cool thing is you can restore them into different cluster types than the one they were taken from. So maybe you want to restore from serverless to provisioned or from provisioned to serverless, you can do that using snapshot restoration. Another exam pro tip regarding snapshots. With Redshift, you can set up your snapshots to be configured to be copied across different Regions automatically, this is one of the best disaster‑recovery methods you can take for your Redshift data warehouses. Continuing on with snapshots as backups and restorations are very critical for solutions architects, understand that snapshots are going to include a lot of different information about your clusters, it's going to include things like the number of nodes, the different node types that you've assigned, and it even remembers and stores the admin username for your databases. Secondly, Redshift is going to use the cluster information to create new clusters, and then after that it will work on restoring all of the databases that are contained within the snapshot. And lastly here, the default configuration settings for automated snapshots are going to be either every 8 hours or after every 5 GB per node of data changes occur, so it's one or the other by default. However, you can set your own snapshot schedule if you want to, those are simply the default settings that come along if you don't change anything. Moving to performance, you're going to typically want to load your data into Redshift using large inserts of data for better performance. So, anytime you see Redshift and you need to load data, think large inserts. And speaking of performance, let's explore some exam scenario architectures you might run into. First up, Kinesis Data Firehose for data ingestion. Kinesis Data, in general, has many different stream types. So for instance, in this architecture, we might be having terabytes of data from different clickstream, mobile, or web applications that get sent in in real time to a Kinesis Data Stream. Now that Kinesis Data Stream can then go ahead and feed and ingest that data into a Kinesis Data Firehose delivery stream, and one of the benefits of Kinesis Data Firehose is that it can handle automatic scaling and potential data transformations for very large datasets. So, in other words, as the data is getting sent in from the Data Stream, the Data Firehose is going to go ahead and be able to transform that data into maybe a different format. Once that transformation that's optional might occur, it's going to go ahead and we can configure it to directly integrate with Amazon Redshift. So, Redshift is a supported destination for a Kinesis Data Firehose delivery stream, so it makes it a lot more efficient for data warehouses, especially for analytics on large streams of data. So on the exam, if there are scenarios about streaming data into a data warehouse, this is a perfectly viable solution. Next, we'll talk about Kinesis Data Firehose with S3. So everything on the left side of the diagram is pretty much the same, we have our applications feeding to a Data Stream in real time, our Data Stream feeding to Kinesis Data Firehose in near‑real time, but in between now, we have an S3 bucket, where we can store a data lake. So, we can send Kinesis Data Firehose traffic directly to an Amazon S3 data lake, this is going to allow for better long‑term storage and cost efficiency. From within S3, Amazon Redshift can actually run what is called an S3 Copy command to go ahead and run massive parallel processing, which allows it to go ahead and read and then load the different data in parallel files within your S3 bucket. Now you need to know these two things that I've highlighted, S3 Copy is how this works, and massively parallel processing, MPP, is the exact reason why Redshift is so high performant when reading data from something like S3. Now an exam pro tip here, you can actually maximize your parallel processing by splitting your data into multiple files. This is especially useful when the files are compressed, so gzip, zip, bzip2,etc., try to do your best to remember that. Now, before we look at the third and final architecture, I want to cover something called enhanced VPC routing. Enhanced VPC routing is an enabled feature, or an optional feature, where you can have Redshift force all of your COPY and your UNLOAD traffic between your cluster and your data repositories through a chosen VPC. So, in other words, it can't go over the public internet. What this does is it allows you to use things with VPCs for security access controls, so we're talking security groups, network access control lists, firewalls, etc. I cover that because we're going to mention it here in this upcoming architecture, S3 directly. Now you can use Amazon S3 directly as an ingestion method for your Redshift data warehouse. When you do so, you assign Redshift an IAM role that's going to leverage the S3 Copy command we talked about earlier, and what will happen is it specifies the following information when it makes that call, the database and the table to copy, it's going to look for the S3 bucket name in the prefix that's optional, and you specify that IAM role with the permissions for access to S3. When you're using this architecture, you have two options for leveraging S3 buckets, you can copy data into your cluster over the public internet via TLS, so this is not with enhanced routing, or you can turn on enhanced VPC routing and send it through an endpoint via private link and leverage some of the other access controls we talked about, so endpoints, security groups, NACLs, etc. These are two options you can use when leveraging S3. If you see that you want to keep it within VPCs and never leave into the public internet, think enhanced VPC routing. Now, speaking of the S3 Copy command, it's important you have seen this for real‑world scenarios, this is an example, so you're copying the orders database, or the orders table, from the S3 bucket ARN, and you specify the IAM role. It's really that simple. Now, you can do a lot more complex queries, but this is a high‑level example of running the S3 Copy command. Now before we leave here, one last exam pro tip. Remember, leverage large batch inserts whenever you can with Redshift, this is the way to make it more high performing and much more efficient, you don't want to do a bunch of small inserts if you can avoid it. And with that out of the way, let's go ahead and wrap this up, and I'll see you in the next clip.

Amazon Redshift Spectrum
All right, let's talk about Amazon Redshift Spectrum. Amazon Redshift Spectrum is a service and feature within AWS that you can use to efficiently read directly from Amazon S3 using Redshift without having to actually load the data into your cluster first. To use it though, it does require that you have an existing Redshift cluster already created, and then after that, you would run Redshift Spectrum. What happens is the queries that you execute are going to use a lot less of your clusters processing capability and capacity because it offloads those requests. By using this Spectrum feature, it allows you to quickly use up to thousands of spectrum instances that are managed by AWS to take advantage of MPP capabilities much more efficiently. Now, that might sound confusing, let's look at an architecture really quickly from a high level. We have our existing Redshift Cluster, so remember you have to have your own cluster set up and running to use this feature, and once tables are defined, you can query and join the tables just like any other Redshift table within Amazon Redshift. What we mean by that is you create your very own Redshift Spectrum tables to reference that data, and then you register them as tables in an external data catalog in something like AWS Glue, for example. From there, once your tables are created, whether they're in Redshift or Redshift Spectrum, you can start performing SQL queries. What happens when you run those queries is that the queries are going to be offloaded to a Redshift Spectrum cluster or layer, which hosts different instances that are completely separate from your own, so what this does is it allows you to scale very quickly by leveraging these managed clusters and instances to take advantage of MPP with up to thousands of instances. So this Redshift spectrum layer is completely separate, keep that in mind. Now you use this to read data directly from S3 while not having to load it. So, if that scenario comes up, you want to think Redshift Spectrum. Typically, when you're loading data or reading data from S3 with Redshift, you copy the data into your cluster. If you want to take advantage of not doing that, you need to use Redshift Spectrum. Now, one big thing to call out is when you update your files in S3 using this architecture, that data is actually immediately available for queries of any kind from any of your Redshift clusters, so that's a big advantage. Now that's going to do it on Redshift Spectrum, just remember you use this to read directly directly from S3 without having to load it into your cluster, and it allows you to leverage up to thousands of Spectrum layer instances that you don't have to manage. We'll go ahead, we'll end this here, and I'll see you in the next clip.

Amazon Elastic MapReduce (EMR)
All right, up next, let's talk about Amazon Elastic MapReduce, also known as EMR. First off, let's review ETL. You need to know ETL processes, especially when wanting to use this service. ETL stands for Extract, Transform, and Load. So with extract, this is where your data is retrieved from a source like a database, documents, or maybe your own application. You then transform it, so this is when the data is going to be cleansed, it's going to be formatted or transformed, and then it's essentially being prepared for storage in a target system. That target system is loaded, so that's where the load comes in. Your Transform data that's cleaned, etc., is moved into a target system where you can start running different analytics, etc. So remember ETL, understand how it works at a high level, and what each stands for, Extract, Transform, and Load. Now, some ETL use cases to be familiar with here. First, customer analytics, so you can extract things like point‑of‑sale systems, customer loyalty programs, social media interactions, etc. You can transform by standardizing different names and addresses or maybe calculating different values, and then you store in a data warehouse for things like maybe marketing analysis. Also, financial risk assessment, so you can extract things like credit reports, transaction histories. You can transform those to go ahead and calculate risk scores or normalize ratings, and you can load them to a risk management system to perform your actions. And then lastly, they're also useful for recommendation engines, so you can extract things like browsing history or purchase history from your users, you can transform that data by calculating similarity scores or generating product associations with the different demographics, and then you can load the data into a recommendation system that you also own, these are just high‑level real‑world examples of ETL use cases. Now moving on, the star of the show Elastic MapReduce, EMR. This is an AWS‑managed service to help you with your ETL processing workloads. It's essentially a managed big data platform meant for primarily Hadoop clusters, and it allows you to process vast amounts of data using different open‑source tools. Speaking of the tools, you should know these tools or be familiar with them and understand if you see them, you should think EMR. First up, Apache Spark, we have Hive, HBase, Flink, Hudi, and Presto, these are all very popular tools within big data analytics and queries and different platforms. If you see any support needed for these tools, I would consider EMR in your answer. Moving on to some concepts, what happens is you actually create your very own clusters, and these clusters are going to contain up to hundreds of EC2 instances, so again, big data processing. You use these clusters to transform and then load large amounts of data into and out of AWS, so you can send it to or take it from AWS. Thirdly here, it's commonly used to load data into S3 as a target data store or even DynamoDB, so you extract the data from your data source, transform it using EMR, and then store in things like S3. What's cool is you can actually leverage auto scaling for your clusters if you need it. So if you realize, hey, I don't have enough compute capacity to perform this in the right amount of time, well, you can leverage auto scaling to help out and increase that. And then lastly here, exam pro tip, any mention of large data in Hadoop clusters specifically, I would think EMR within the solution in some sort of fashion. Now let's talk about storage for EMR, you have to be familiar with the different storage types in your EMR clusters and what they're used for. First up, Hadoop Distributed File System, or HDFS. This is a distributed, scalable file system specifically for Hadoop, and it works by distributing stored data across through different instances, which is what makes it so high performing. This is going to be very useful for caching results during the processing or the transformation of your data. We then have EMR File System, or EMRFS, this is going to extend Hadoop to have the ability to directly access data in S3 as if it were a part of an HDFS file system. S3 is going to typically be used in these architectures to store input and output data, you're going to want to avoid storing intermediate data there that you're actually working on processing. So a big thing to take away, EMRFS works with S3 directly for input and output data. And then lastly, Local File System, this is the actual locally‑connected disk with each EC2 instance within your cluster. It's essentially an instance store volume so it's only going to remain available during the lifecycle of that instance. So in other words, you do not want to store important information on these file systems, they're very good for very fast caching results, etc., but don't store anything long term on this. Moving on to clusters and node types. Again, clusters are a group of EC2 instances within EMR. Each instance is called a node, and there are three nodes you have to know. The first node type is a primary node type. This is actually what's going to manage your cluster, coordinate distribution of your different data and your different tasks, and it even tracks the health of other nodes. You then have a core node, these are what you're going to use to run your tasks, store your data in HDFS, and they're typically going to be long running, so you're going to keep them around for a long time. And then thirdly, you have task nodes. Task nodes are going to be optional nodes that are only meant to run specific tasks. These will not store data within HDFS at all, and you want to leverage these for only very specific tasks, these are short‑living compared to long running. Now, moving on to purchase options and cluster types, you have to understand how to make them the most cost efficient for your architecture. First up, on‑demand, this is the most popular and most reliable purchase option. These are not going to be terminated, but they are the most expensive. You then have reserved, so this is just like other reserved instances in EC2, you commit to a minimum of one year, but they offer great cost savings, and you're going to typically only use these for primary and core nodes, you would never use these for task notes. Third one here, spot instances. I'm sure you're seeing a pattern, this is exactly like EC2. This is going to be the cheapest option available for your cluster nodes. These can be terminated with little to no warning, and you're probably only going to want to use these for task nodes. With task nodes, if they go away, typically you're okay with that task being restarted or retried on a different node, which would be directed by your primary node. And then the last thing here, is your cluster going to be long running or temporary? Your cluster can be both, so it depends on your workload. You can leave it long running and then have a reserved instance, on demand, etc., or you can have a transient cluster. Transient clusters essentially are temporary clusters, they're going to go away after a short amount of time so maybe you want to spin up a cluster, process some ETL workloads, and then shut down immediately once you're done, well, then a transient cluster is perfect for that. Let's look at some use cases for EMR. They're going to be very good for large dataset processing, so again, ETL workloads, that's why we reviewed ETL in the beginning. They're very good for web indexing, so when you have data you want to index via web clicks, web streams, etc. And it's also good for large scale genomics projects, so if you're having to extract tons of genomics data or some other massive dataset, you want to transform it to make it more readable and usable and then load that, well, then this is perfect for EMR. Now that's going to do it for this clip on Amazon EMR, please be familiar with the supported tool sets, the different nodes and storage types, and the purchasing options. Let's go ahead, we're going to end here, and I'll see you in the next one.

AWS Glue
Okay, let's talk about AWS Glue. AWS Glue is their serverless data integration service that's meant to make it easy to discover, prepare, and even combine or transform your data. It essentially allows you to perform ETL workloads without managing underlying servers, so it's a serverless offering. Now, the cool thing about Glue is you can connect to up to 70 different data sources and manage data within one centralized data catalog. Moving on, let's look at some concepts and some components you should be aware of. First up is a Data Catalog. This is going to be a metadata store within Glue, and it's going to store your different table definitions, your different job definitions, and even other control information for portions of your ETL workflows. Crawlers are going to be used to connect to your data sources, they connect, they start to infer or crawl over your data schemas, and then they create metadata table definitions within your Data Catalog, so those two work together. Third is the ETL job, or just sometimes referred to as a job within Glue, these are going to actually extract data from your data source, they transform your data using things like Apache Spark scripts, and then they load the data into your targets. And then lastly, we have a trigger, so this is exactly what it sounds like, it's a resource or a method to start or trigger your jobs, these can be scheduled or they can be started by events like Lambda or EventBridge. Now, let's actually look at an example architecture. Remember, AWS Glue supports many, many different data sources that you define within your data catalog, including JDBC connections, so it can connect to a ton of different resources. After you set up your data source within AWS Glue, this is where crawlers come into handy. Your crawlers are going to crawl through your data sources you connect to and then write the different metadata, the different data types, etc., for your tables into your Glue Catalog. So, once your crawlers actually crawl, they collect all that data, they begin to write it to your data catalog, well, that's where your data catalog is going to come in handy. It contains those tables and their metadata within a Glue database, so your databases can contain multiple tables depending on your source. From there, you can execute Glue jobs within AWS Glue, so the Glue jobs can be triggered or scheduled, and they're going to go ahead and perform scripts to execute your ETL workflows based on your Data Catalog, so you can process, transform, move, etc., all of your data based on metadata information in your Data Catalog. Now, speaking of ETL and some of the supported targets, AWS services, a lot of them within the big data platforms and categories, actually leverage Glue Data Catalogs in the background to perform data discovery. So we looked at Redshift Spectrum, we looked at EMR, we'll look at Athena coming up here shortly, and even some open‑source projects, like the one you see on the bottom right, Glue Data Catalog client for Apache Hive metastore, all use Glue on the backend to discover the data they use. So this is a pretty important service, it's the go‑to service for discovering schemas and data within data sources for a majority of your architectures. Now let's look at an exam scenario. Let's say there is a scenario where you have hundreds or thousands of CSV files that get sent to an S3 bucket daily. With these files, you need to convert them to Apache Parquet format and store them in a different bucket, that's a perfect use case for AWS Glue. And speaking of conversion, let's look at an architecture for converting. You have two different architecture approaches you can take with this workflow efficiently. For the beginning of all of them, we'll just assume that a user or an application is putting an object into our source bucket, so the object gets uploaded and it could be something like a CSV or JSON file. Now, option one is you're going to schedule a Glue crawler to go ahead and crawl that bucket and discover the new data that was uploaded since the last time it was crawled. Once that crawl is complete, you can schedule a Glue job and have that job perform an ETL process to go ahead and run on a regular basis and process the new data that was just crawled in the source bucket. Once your job completes, it can output that data to your target bucket, so the transformed data, let's say Parquet format, is then loaded into a brand new bucket, which is a new target. From there, you can actually query using SQL queries within something called Amazon Athena. Athena is good for ad‑hoc, one time, serverless SQL queries on loaded data in S3, and we'll discuss it much more in‑depth coming up shortly. Now that's option one, option two only differs in one particular section, you're essentially triggering your workloads. So we can say that an object created event is going to trigger an S3 event notification, which invokes a Lambda function, which can then schedule or trigger your Glue job to perform the exact same workflow. It's really going to come down to how fast you need it to be done, do you just need it to work every day, do you need it to work on‑demand, Well, then you have some different options to choose from. Now an exam pro tip, you can also leverage EventBridge in place of Lambda for that architecture. So I use Lambda, but you can substitute it with EventBridge if you need to. The big thing to remember is it can be event driven. Moving on to some more important concepts and features, these are sometimes popping up on the exam so we want to cover them. First up, Glue Studio. This is a simplified visual interface, or a U I, where you can create and then run your ETL jobs for Glue. We then have Glue Streaming, so this allows you to handle streaming data in near‑real time, what that means is you can perform ETL jobs and workflows on streamed data. You need to know for sure what a job bookmark is, this is going to be persisted state information of the different data that's already been processed by previous jobs, and these bookmarks are put into place so you don't lose your progress and reprocess data. Think of a bookmark just like a book bookmark, you put it there so you don't reread pages, well, you use these so you don't reread data and reprocess data. And lastly, we have DataBrew, this is their visual data prep tool for cleaning and normalizing your data so that you can prepare it for your analytics and even some machine‑learning processes. I like to think of DataBrew as brewing up your coffee, you're taking it, you're transforming your beans into something drinkable, well, that's kind of what DataBrew does it cleans, normalizes it or transforms it, and prepares it for something else. Now, last thing here, one more important concept or feature, Glue Elastic Views. Elastic Views are meant to make it easy to build your views, which are called materialized, which means that they are logical, they're not physical tables and physical views. You can combine and replicate data across multiple data stores without needing custom code via these materialized views. Three big things to know here. SQL, you can use SQL to quickly create your virtual tables, your materialized tables from your different sources. Automated, it copies data from your source data store and automatically replicates it to your target data store. And lastly, it's continuous, so it's always looking for changes in your source data stores and including updates to your views that you've created. Now that's going to do it for AWS Glue, really take the time to understand how this service works and what you would use it for. It is a very critical component in a majority of ETL workflows within AWS. Let's go ahead, we'll wrap this up here, and we'll move on to another clip.

Amazon Athena
Okay, up next in our journey with these data and analytics services, let's look at Amazon Athena. Amazon Athena is the serverless, interactive query service that's meant to make it very easy to analyze your data directly within your S3 buckets, and you can do that using SQL queries, so that's one of the big benefits is you can use the common structured query language in order to access and analyze data directly. Now, Amazon Athena is based on the open‑source engine called Presto and that's essentially what powers it underneath the hood, however, AWS has made their own small touches to it to make it more efficient for AWS and interacting with their resources. Within Amazon Athena, there are several supported data formats that you can query directly within your buckets, those data formats include the following, CSV files, JSON documents, ORC files, Avro files, and Parquet. Now, I will say, Parquet is probably the most common format that you're going to see on the exam and it's actually recommended by Amazon Web Services due to its performance gains compared to other formats. Long story short, you're typically going to want to get your data into Parquet format when you use it with Athena. Let's talk about some use cases for Athena. It's very good for log analysis. You can use this to very easily and very quickly query different logs that are stored within your buckets, so this includes logs like CloudTrail logs, VPC Flow Logs, load‑balancing logs, and even CloudFront logs, and those are just a few. It's also good for business intelligence, or BI, and analytics so you can create ad‑hoc reports or single, one‑time reports for your business metrics, you can analyze different custom behavior patterns based on your data, and you can even do things like processing sales data. And then lastly, it's also very good for directly querying data lakes and data warehouses, so you can query raw data or structured data in the supported format, so CSV, JSON, Parquet, etc. It allows you to perform exploratory data analysis, and you can actually create views for specific business units to report to them. Now, an exam scenario for Athena, maybe you have some process data that's getting stored in an S3 bucket and you need to run a one‑time query and import the new data that you queried into a business intelligence tool for visualizations, well, Athena plays a perfect role within this scenario. Now, this might sound familiar because it goes back to the architecture we looked at earlier with AWS Glue. Remember, after you do all your transformation, your extraction, your loading, etc., you can use Athena to perform your ad‑hoc one‑time serverless queries using SQL on your data that was loaded into a bucket. Now, in addition to Athena, it is commonly used in conjunction with a service called Amazon QuickSight. We're going to look at QuickSight much more in depth later on in this module, but at a high level, just understand that Athena easily integrates with it, and QuickSight allows you to create readily available dashboards, you can create custom reports and custom visualizations for different business intelligence users, so you can create the nice looking graphs that show what the data means and not have to expose the underlying data that really would make no sense unless you correlate it. The big thing to take away from this architecture is that Athena works directly with your data within your buckets as long as that format is supported. Let's go ahead and look at some best practices. For optimal performance for your queries within Athena, you're going to want to do your best to store your data with a column‑based design, so you want to focus on column‑based designs. Remember that differs greatly from row based, and we looked at something like that within our Redshift clip. AWS also recommends that you use ORC or Apache Parquet formatting for your data, again because these are going to offer you the highest performance and the lowest latency. Another tip, convert your formats with AWS Glue. Remember, you can use Glue to go ahead and crawl and infer your different data schemes for your source data, and then maybe you perform some type of ETL workflow either via EMR or a Glue job and you transfer that or transform that into one of the supported formats like Parquet. Next up, querying your data in S3 should use partitioning for faster queries. We're going to look at an example here coming up shortly, but the big thing to remember is that the more specific the prefix, the better the performance. An example of partitioning your data could be something like time‑based partitions or category‑based partitions, let's look at two examples now of each. Here is a dataset partition example. First up, time‑based. In this, we have two examples of time‑based prefixes, as you can see here. We also have category‑based prefixes or partitions, you'll notice a slight difference. In the top one, we do the year, the month, and the day, and then we have the logs. In the category based, we have region, category, and then the logs. Now, we could search very easily based on differences in the partition, like you see here. On the bottom, region might be us‑east or us‑west, and on the top, maybe you can go down to the exact day and then even different hours within that prefix. Again, the thing to take away here is that you want the most specific prefix possible, especially when you're querying your data. Now, the next thing I want to talk about is an Amazon Athena Federated Query, this is something you need to know for the exam, and honestly, anytime you're going to use Athena within your workflows and your architectures. You're going to use a Federated Query when you have data stored in sources other than Amazon S3, so it does work with other things outside of S3, but it's not native. It works by actually allowing you to query the data in place or you can build pipelines to extract the data from that data source and then move them to Amazon S3. By using a Federated Query, it's going to allow you to run your queries across the data stored in either relational, non‑relational, object, and even custom data sources. It allows you to do this by using what is called a data source connector, and a data source connector is essentially going to be a Lambda function that allows you to connect to your custom data source and then execute a Federated Query all through the Athena service, so it's using Lambda functions on the backend to connect and run your queries. Now, it's good to know some of the prebuilt and custom data source connectors that are supported, an example would be Amazon CloudWatch Logs. You can use one for this, you can connect to tables within DynamoDB, document databases in DocumentDB, you can actually connect to your traditional RDS databases in Amazon RDS, or even any JBDC‑compliant relational database, so maybe something on‑premise, for example. And then lastly, if you need to build something completely different, you can use the Athena Query Federation SDK to create your very own custom connector if it's not going to be prebuilt. Now, let's wrap things up with a quick exam tip. Remember, you're going to use Athena whenever you need to perform one‑time serverless queries on data stored in S3. Anytime you see CSV, JSON, or Parquet, in addition to queries, I would immediately think that I want to use Athena somewhere within the solution. That's going to do it for our clip on Athena. Feel free to go back, review what you need to. Let's end this here, and I will see you in an upcoming clip.

Demo: Query Logs Using Amazon Athena
Okay, in this demonstration, we're going to work on using Amazon Athena to query some logs that we're storing in Amazon S3. Let's look at the architecture from a high level before we jump into the console. We're going to have an EC2 already deployed for us that's hosting a simple web server. We'll also have a log bucket created for our future logs. Once we have this web server up and running and it's validated to be running, we're going to set up a custom VPC Flow Log on the instances ENI and have that log be sent to Amazon S3. When we're customizing this Flow Log, we're going to have the destination be S3, and we're going to set it up to have the default fields and a Hive‑compatible partition and prefix to store in our bucket. Using that data living within S3, we're then going to query it using Amazon Athena to show how this exactly works. Let's go ahead, we're going to jump into the console now, and begin. All right, let's get started.. I'm in my sandbox here, I'm logged in in us‑east‑1 as cloud_user, and I've loaded up my buckets. Now this is the bucket we're going to use for storing our logs. I've also got an EC2 instance here. Now, this instance is publicly accessible, and if I go ahead and actually go to this address, it should work for HTTP where we have a very simple website. So let me go ahead and continue, there we go. So, let's go ahead and let's set this infrastructure up. I'm going to go to my instances, I'm going to go to Networking, I'm going to go down and select my ENI. Now, you should know how to set up a VPC flow log already, but I'm going to walk through the process very quickly. So I say that because I'm going to skip over some of the details when I'm doing this because this is available in a completely different course that actually prerequisites this one. So, what I'll do here is I'm going to scroll to the bottom, I'm going to find Create flow log, and click it. So, we're setting up a flow log for only our ENI. I'll give my flow log a name, I'm going to set it up for all traffic, one minute aggregation, and I'm going to send it to an S3 bucket, so I need to actually copy my S3 bucket ARN here. So I'm going to go to S3, select my bucket, copy my ARN, go back, and then paste it in. Now, if we wanted to, we could give a custom prefix, however, I'm going to leave the default and then we'll walk through what it looks like. So I have my bucket, we're going to use the default format here. And then under Log file format, this is where I want to make a change. For the format, instead of Text, I'm going to choose Parquet because Amazon Athena works very well with Parquet‑formatted data. Whenever possible, you should use Parquet or ORC like they recommend, it's going to be a lot faster and storage is a little bit cheaper. So we'll use Parquet, and then I'm going to choose Hive‑compatible S3 prefix. Hive‑compatible prefixes will look at them, but they're going to show how it's a much more specific partition in prefix value, and I'll show you what I mean once we get into the actual log data. So I'm going to enable Hive‑compatible prefixes, partition every one hour, and let me create my flow log. Perfect. So now if I go down, we have our flow log in place, I'm going to go to my bucket, and what I'm going to do here is I'm going to load my bucket, and you'll notice we already have some prefixes, so this is the prefix for a VPC flow log if you're not familiar, AWSLogs, and we have the account ID, however, this is a little bit different than a previous lesson in a different course. Notice, we have aws‑account‑id=Account ID. So this is that partitioning method that we talked about, so eventually we're going to have more data in here. Now, what I'll do is I'm going to pause this clip, I'll fast forward, and once we actually start getting data in here, I will resume. Okay, so that took several minutes. Remember, VPC flow logs are not real time, they take time to aggregate and then actually get pushed to our bucket. Now, with that in mind, let's continue on. You'll notice now we have another prefix and a partition, aws‑service=vpcflowlogs. In here, we get region=, then we get the year, month, we get the day, and then we get the hour. Now, in here, we have the log files. So up here, notice the different prefixes and partitions, each one of these portions here, account, service, region, etc., these are all partitions that make it more efficient to query with Athena. So this is what that Hive‑compatible prefix setting did when I enabled it. Now, with that being said, let's actually go ahead and start querying our data. First thing I'm going to do here is I'm going to go back to my bucket, I want to load it so I have the name, and I'm going to go ahead and close this tab, and I'm going to open up Amazon Athena in a separate tab. Perfect. So this brings me to Athena, and it's loaded up the Query editor. Now, there is one thing we have to do here, we have to set up a query location for our results to store in S3. Now, if you're doing this and you're following along and you don't see the screen, under the tab here on the left, the menu, just click Query editor. From here, we're going to use the Data Catalog, AWSDataCatalog. Now you could create your own custom data source if you wanted to by creating data source and notice all of the different supported data sources, there are actually quite a bit. We can connect to Google Cloud Storage, Snowflake, data warehouses, etc. Two of the more popular ones are going to be S3 and CloudWatch Logs. Now, this is going to work with the default data source here, AWSDataCatalog, so I don't need to do anything for this. What I do need to do though, like I said, is edit my settings. So I'm going to go to Settings and edit this query location. So what I'm going to do is I'm going to browse S3, I'm going to select my same bucket and choose it, but I'm going to add a prefix here called queries so now this is going to save my queries in this location in S3 for later reference. Now, if we wanted to, we could encrypt it using different KMS keys, so I'll just use server‑side encryption, and you can even set it to a different bucket owner, so maybe you want to store your query results somewhere else within your organization. I'm going to leave it here, I'm going to click on Save, and there we go. So now we're storing our query results here, I'm going to go back to Editor, and let's begin. You'll notice first things first, we have a default database and that's it. Well, remember, we have to create our own database to host our tables with our metadata, so I'm going to do that first. I'm going to go ahead and I'm going to copy and paste some commands in here. Now, I will make all of the commands I am about to run available within the module assets, so feel free to use them and configure them as you want to. But for this, what I'll do is I'm creating a new database called vpc_flow_logs_db. I'm going to run this and it's completed, as you can see on the bottom, Query successful. We see how long it took to be in the queue and how long the runtime took. So now on the left, it automatically selected this updated database for us, vpc_flow_logs. However, we don't have tables, we don't have views, so let's create a table. Again, I'm going to copy and paste, I'm going to open up a new query, and we'll walk through this very quickly. This is a little out of scope for this particular course and exam, but I still want to walk you through how this actually works. So, we're creating an EXTERNAL TABLE IF NOT EXISTS called vpc_flow_logs. We're setting the different fields here, so version, account_id, etc. These are all the default fields for version two in our VPC flow logs. We're then partitioning that data by the following, account, service, region, and then year, month, day, and hour. Hopefully, this looks familiar. If I look at my bucket here, remember we have logs, and then we get into those partitions, account, service, region, year, month, day, hour. So this is what this means, we're partitioning our data using those prefixes that we defined. And then lastly, there's some specific stuff that's way out of scope for this course, but we're essentially setting the format and setting the storage for the input and output, but we do have to change location here. So I need to fit in my bucket name here because this is where it's going to pull the data from or scan the data, so I'm going to go here, copy my bucket name from the URL here, paste it into BUCKET_NAME, and there we go. So, I'll click on Run, and we get Query successful. So now notice, we have a table over here on the left. So now, if I expand this, we see it's partitioned, and we can search via partitions for these different values whenever we need to, which makes it very efficient. You'll notice it even calls out the different partitions themselves within the table, so this is all metadata regarding our actual data living in S3. So now what I'm going to do here is I'm going to copy and paste a new query, and if you're following along, this has to be done, you have to run MS check and repair your table. What this is going to do is, once we run it, it's going to force Amazon Athena to crawl our data and say, okay, well, I see partitions are not in there, let me add them to the data store. So, each time we'd have a new partition, I would want to rerun this command, or you could automate it via Lambda or other methods. But what it's doing is adding these partitions into our metastore, which we're going to use to query. So right now, this is the only partition that we have right now because of the hours. So that's it, now we can actually go ahead and query our data directly from our bucket, so let me go ahead and copy and paste in some queries here. From here, we're selecting all from our table limiting 10, so if I click on Run, there we go. It took 2 and a half seconds, just under, and you'll notice we have all of our results, 10 results. We see the account_id, interface_id, source address, etc, these are all the fields from our VPC flow log. So now, let's say I want to go ahead and I want to filter on, let's say, destination port, I want to look for any destination ports of 123. So what I'll do here is I'll copy and paste in again, I'm going to change this to 123, and I'm going to click on Run. There we go, so it quickly queried all of our results and you'll see we have at least 10 results, destination port 123 all the way down, and then all of the other field information relevant to those, so that's how easy it is. We've now begun a serverless ad‑hoc one‑time query on data living within S3. Now, before we wrap up, I want to show you something. Let's assume I either accidentally or purposely delete my tables in my databases, so I'm going to drop my table here. And then, after the table is dropped, we no longer have it, I'm going to drop my database. Now, first off, this is extremely destructive, don't do this if you want something in production, but I'm just want to show you something. We deleted the database and the table, but what happens to our data over here in S3. Well, nothing, that's the beauty of Athena, you're only querying it directly, you cannot affect the actual files and the data within the files. You can only query and search the data. So if there's an exam scenario where you have to look into data within S3 and you don't want to actually alter that data, this is a perfect solution for that scenario. Now, with that being said, I think that's a good point to end on. We just demonstrated how you can use Athena to directly query files over in S3 without actually altering the data. We'll end this here, and I'll see you in an upcoming clip.

Amazon QuickSight
All right, let's talk about Amazon QuickSight. We just got done looking at all of the big data tools, the different analytics tools, and I think it's time to talk about visualizing that data. Amazon QuickSight is a fully managed, serverless, business intelligence data visualization service, which is actually powered by machine learning for certain additions of it. It allows you to actually very easily create interactive dashboards and then you can share them with specific users. This is commonly used to share dashboards with business users, they don't really care about the technical aspect of things. It actually integrates very easily with things like RDS, Aurora, Athena, and S3. Now that's just a few, there are many, many more, but those are the more common ones. Moving on, let's look at some important concepts regarding QuickSight. First and foremost, SPICE, this stands for Super‑fast, Parallel, In‑memory Calculation Engine. You need to know this term and what it is is essentially an i‑ memory engine that allows you to quickly performed advanced calculations for data that is loaded into QuickSight. When you're using SPICE, again, you have to load the data into QuickSight for it to work. What that allows you to do is that you don't need to retrieve the data every time that it changes because SPICE is super efficient. Also, QuickSight does offer column‑level security, often abbreviated as CLS. Now, this is worth calling out that it only is coming with it when you pay for the Enterprise offering, so if you're using Standard, you can't use CLS. What this does is it allows you to go ahead and manage access to specific columns in your datasets to further control who can see what data. And then lastly here, there's actually no upfront costs for licenses, you actually have a pay‑per‑session pricing in place and it's for any users that you place in a reader security role. So if they're a reader and they have a session going, then you're paying for that user. Moving on, let's look at dashboards, users, and groups, these are very critical for how QuickSight actually works. What happens is you actually create separate users for using QuickSight. The Enterprise version actually allows you to also create groups of users. So again, if you pay for Enterprise, you can create groups, and you can use column‑level security. Now, it's very important to know, these users and these different groups only exist in QuickSight, they are not existing anywhere else, what I mean by that is they are not an IAM user or role or entity, they only belong in QuickSight. With that out of the way, understand that dashboards are going to allow you to store configurations and different filtering that you might have in place for quick references. The neat thing about a dashboard is you can actually share it and you can use it to analyze results and share all of that with different users and different groups. Now, you need to keep in mind here if you share a dashboard, that's going to let the users see the underlying data, so you need to be careful when you're sharing these. Now let's actually look at an architecture, again, this is going to look extremely familiar because we've seen this before. This is one of our Glue‑based infrastructures. The thing I want to call out here is on the bottom. Remember, QuickSight commonly integrates with things like Amazon Athena to allow you to readily create dashboards, reports, and visualizations for different users. Now this is primarily used for Business Intelligence users because most of the time they're not going to necessarily care about the technical aspect and the actual data, they just want to see it visualized and how it correlates with one another. Now, Amazon QuickSight Enterprise Edition leverages machine learning to help uncover different data insights, different data trends, and it actually allows you to forecast on different business metrics. So this is one of the big pluses for using Enterprise, in addition to user groups and column‑level security, you get this machine learning help. Now also get some exam tips for Amazon QuickSight, this is going to be a great option if the phrase Business Intelligence comes up. Typically, with BI, you're wanting to look at reports, you're wanting to look at graphs, and different correlations of data, well, QuickSight does all of that in an easy‑to‑use dashboard. It is commonly used in conjunction with Athena for one‑time queries so you can use it to visualize a single, serverless query on your data in S3. And then lastly, if you need anywhere within your solution to visualize data and show key performance indicators, otherwise called KPIs, this again is a perfect solution for you. The big thing to look out for in a lot of the scenarios is visualization of your data. Now that's going to do it for this clip on Amazon QuickSight. Remember, it is a data visualization tool where you create dashboards, and you can share dashboards to show different reports and graphs. We'll end this here, and I will see you in an upcoming clip.

Amazon OpenSearch Service
Okay, let's talk about Amazon OpenSearch Service. OpenSearch Service is a managed service offering that is going to allow you to run search and analytics engine clusters for many different use cases. It's actually the successor to Amazon ElasticSearch Service, so if you know how ElasticSearch is used then you know how OpenSearch is used for the most part. With OpenSearch Service, you get the ability to search all fields within your data. In other words, you're not locked down to a primary key or secondary indexes like you would be in a traditional database or a DynamoDB table, you can actually search all textual fields in your data sources. A neat feature is that it provides you a dashboard like Kanbana for visualization of the data within your different OpenSearch clusters. With OpenSearch, you can easily ingest data directly from the following sources, Amazon CloudWatch, CloudTrail logs, Amazon S3, and even Kinesis Data Firehose. Now, when you're using OpenSearch Service, you need to understand how security plays a role. You can restrict access using IAM, you can do the same with Amazon Cognito, you encrypt at rest using KMS keys, and encryption in transit is enforced via TLS. Now also, if you need to, you can actually set up PrivateLink connections and endpoints to require that VPC traffic is the only way that you can reach your cluster, so you can have it open to the public, which is not recommended, or you can have it locked down to a VPC with private traffic. Moving on, let's discuss a domain. A domain is AWS‑provision equivalents of an open‑source cluster and it has specific settings, specific instance types and counts, and specific storage allocation, so a domain is made up of all of that information. Moving on from that, you need to know service access policies. There are three types of access policies that you can use to control access to your domains. Now, I'm not sure you're going to get tested on these, but I like to call them out because these are very important from a security aspect. First is a resource‑based policy, these are called domain‑access policies and you assign them to your domains, in other words, your resource. You can also use an identity‑based policy, so this is a typical IAM policy that you attach to users and roles. And then lastly, you can actually do an IP‑based policy. So this is similar to your domain access policy, but you restrict access to specific IP addresses or entire CIDR blocks. So these are the three policy types you can use to control access. Now, let's actually explore some architectures with OpenSearch Service, it's very common on the exam. In this example, we have Kinesis Data Firehose ingesting data, and then you can potentially use Lambda functions to perform data transformations to transform your data before it gets passed through to OpenSearch, an example of this could be enriching VPC Flow Logs to contain more data or have it look like a different format or version. Now, once this data transformation is complete or not complete and you start passing data to OpenSearch, the OpenSearch Service is going to be able to directly ingest that data into its clusters and you can begin searching it. Next up, another architecture, this time we use Kinesis Data Streams. Now, it's important to call out, OpenSearch Service does not support direct ingestion from Kinesis Data Streams, instead you want to use a Lambda function in between the two resources to set up a method for a real‑time style of ingestion. So your data stream is containing real‑time data, it's triggering a Lambda function, which is putting data into your clusters. Moving on to the next architecture, DynamoDB Ingestion. In this case, you have your DynamoDB Stream set up for your DynamoDB table. That stream is invoking a Lambda to function, which is then putting those images into your OpenSearch Service cluster. Now, what you can do is use Amazon API Gateway to do the following, you can use it to search for items and different documents within your clusters in OpenSearch Service, and you can actually use it to get, update, or delete items directly within your table so you can set up different APIs to interact with the different services. Now, it is good to know you can use other services to front your API calls for both of these. So, for example, maybe you're hosting an API on an EC2 instance or perhaps you're using an application load balancer in front of ECs containers or application load balancers in front of auto‑scaling groups of instances, these are all real‑world examples on how you could set this up. Now let's check out the next architecture, this is the final one that we're going to review. In this case, we have a subscription filter set up on a CloudWatch log log group. When you set up a subscription filter to go ahead and ingest data into OpenSearch directly, really what's going on is it's leveraging a AWS‑managed Lambda function to perform that action. So, you're not going to see the function, but understand that is how it works. You set up the subscription filter, it uses a Lambda function behind the scenes, and ingests that data in near‑real time to your OpenSearch cluster. Or, you can also pass your subscription filter into a Kinesis Data Firehose delivery stream. This way, you can send that data directly into your cluster in near‑real time. Either way these are both valid methods, it just comes down to the amount of scalability you need and the amount of data retention you need. Now the last thing I want to cover here is OpenSearch Serverless. This is a feature within OpenSearch that gives you on‑demand, auto‑scaling abilities, and it's really meant to offer you a simple, cost‑effective option for infrequent, intermittent, or unpredictable and spiky workloads. So with this, you specify capacity units specific to OpenSearch and then you consume those and that's what you pay for in addition to storage. Now, with that being said, that's going to do it for Amazon OpenSearch Service. Understand you can use this for many different applications, it's really good for ingesting document data and searching with all fields in the data, and be sure to review those architectures we just looked over. We'll go ahead and we'll wrap this up, and I'll see you in a Module Summary and Exam Tips lesson next.

Module Summary and Exam Tips
Awesome, way to hang in there. Once again, we've made it through another module, let's go over some module summaries and some exam tips before we move on. First up, Amazon Redshift. Remember, Redshift is AWS's fully managed, petabytes‑scale data warehouse solution in the cloud and it's meant to be used for OLAP workloads. Remember, OLAP is better for analytic processing workloads as opposed to OLTP, which is transactional processing. Remember, you can use standard SQL and business intelligence tools in order to interact with this service, so you can connect to it using your SQL client and then run SQL queries. As far as infrastructure goes, remember, you can go Single‑AZ or you can go highly available using Multi‑AZ deployments. And it offers the following pros to using it, it is said to offer up to 10 times the performance of other data warehouse solutions. It accomplishes this by using column‑based or column‑oriented storage, so columns are stored together as opposed to rows. And then lastly, with this column‑oriented storage, it offers a massive parallel query engine, which is how it gains such high performance. Continuing on with Redshift, remember the cost‑savings approaches. You can leverage Reserved Instances to actually save on costs for your provisioned clusters. In addition to that, understand snapshots. Snapshots are like any other snapshot for other services that we've already covered, they're incremental and their point‑in‑time captures for your data, and when you restore them, you restore them to a brand new cluster. Make sure that you load your data into Redshift using large inserts or bulk inserts whenever possible to achieve better performance. Redshift works better when you're inserting large datasets, as opposed to small inserts. And then lastly, integrations and controls. Remember, this is commonly used with Amazon S3, Kinesis Data Firehose, Lake Formation, and you can use enhanced VPC Routing in order to control traffic through a VPC. Moving on to another feature within Redshift, Redshift Spectrum. Just remember that you use this to efficiently read data within S3 using Redshift without having to actually load the data first. What happens is queries are going to use a lot less of your own clusters processing capacity because it offloads all of those to a Redshift Spectrum layer which is managed by AWS. However, to take advantage of up to those thousands of instances, you do have to have your own cluster already set up in Redshift because you have to send those queries to your cluster, which then get offloaded to Spectrum. Moving on to Elastic MapReduce, EMR, this is going to be the managed big data platform allowing you to process vast amounts of data with open‑source tools and ETL workflows. If you see any of the following, you should at least consider Amazon EMR in your solution, Apache Spark for big data, Apache Hive, especially Apache Hadoop, so if you see a Hadoop cluster required, you're likely going to want to use EMR. Now speaking of those, we also have some use cases to keep an eye out for. This is very good for large dataset processing, in other words ETL workloads, it's good for web indexing, and it's great for large‑scale projects, an example we covered was a genomics project. Now here are some more things to remember for EMR, you need to remember the different node types and their different use cases. Primary nodes are meant to control the cluster, as well as offload the different tasks and track the different health statuses, Core nodes actually perform tasks, and then Task nodes are optional nodes to only perform specific tasks. Typically speaking, Task nodes are transient,a.k.a., temporary, and a majority of the time you'll have long‑running Primary and Core nodes. Next, know the different storage types. You need to know HDFS and how that works. Remember that EMR file system, or EMRFS, allows you to leverage S3 as if it were a local HDFS system. And finally, local file systems for temporary storage, remember, you don't want to store anything permanent or long term on your local file system, as they go away once the EC2 instance goes away. And then lastly, understand pricing and deployment options. You can mix On‑Demand, you can mix Reserved Instances, and you can even leverage Spot Instances. Typically speaking, you're going to leverage Reserved and On‑Demand for your Primary and Core nodes, and then a good price point is to go ahead and have spot instances for your Task nodes because if those get interrupted, you can just retry those tasks later on. And then lastly here, long‑running or transient clusters. Remember this as well, you can have your clusters there for a long time using Primary and Core nodes or you can spin up a transient cluster, which is a temporary cluster, to perform one particular workflow and workload and then shut down and go away to save on cost. Moving on to Glue, this is a very critical service, this is the serverless data integration service that makes it very easy to discover, prepare, and then combine or transform data. You really need to be familiar with the following terms and what they do. Crawlers, crawl your data and infer different data schemes, Data Catalogs are catalogs for those different informations that were captured, and then jobs and job bookmarks. Jobs are actually performing the actual work loads and workflows, and then job bookmarks essentially save your spot so you don't reprocess already processed data. Remember that Glue is used behind the scenes for the following services for data collection, Redshift Spectrum, EMR, Amazon Athena, and even some open‑source projects which you don't really need to be familiar with for the exam, but I like to call this out. Now, let's look at some use cases for Glue. Remember, this is perfect for daily ETL jobs where you need to process XML or CSV data that's stored in an S3 bucket. You can then use it to perform this job to convert that data to Parquet, catalog it, and then work with Amazon Athena to go ahead and query it. It's also good for using on raw data stored in your bucket that needs to be processed regularly with minimal operational overhead. That is a key indicator for this service, you can set up your data source, have your crawlers crawl that data source on a regular schedule, and then catalog that information for use later on without having to do anything but set it up initially. And then thirdly, you can use it to automatically transform your data that is stored in your buckets, so again, it can be easily queried using things like Athena or stored in your data warehouse via Redshift. Next up, Amazon Athena. This is the serverless, interactive query service making it easy to analyze data directly within S3 and some other sources using SQL queries. This is going to be perfect for one‑time queries of S3 data. In addition to this, it is very often used with Glue, so you catalog and crawl your data with Glue and then you actually query it with Amazon Athena. Now on the exam, you need to be familiar with the formats that are supported with Athena, these include some of the more common ones like CSV, JSON, and especially Parquet. If you see Parquet format, this should be an immediate indicator about Athena. Moving on to dataset partitioning, remember, you need to be as specific as possible when partitioning your data to really speed up your queries when using it. In these examples, we have time‑based and we have category‑based. I'm not going to review these, I just put these here for you to see again, and you can review on your own time. Moving on to Federated queries, you need to remember these. This is something that you use to run queries using Lambda functions, which are called data source connectors, and you use them when you have data stored in sources other than Amazon S3. So, data in S3 is natively usable in Athena, but if you have other sources, you can still use them, you just have to set up these Federated query connectors. Moving on to QuickSight, this is the fully managed, serverless, business intelligence data visualization service, and it actually uses machine learning with the Enterprise edition. This is going to be commonly used with Athena and S3 to create graphs, create dashboards, and create reports for business users. In addition to that, you need to remember the term SPICE, this is the in‑memory engine that allows it to perform such quick calculations. And in addition to SPICE, remember the Enterprise offering is going to give you column‑level security which allows for better access control to certain datasets. From a pricing standpoint, you pay per session, per user, so you're not paying for everything all the time, it's locked down to just this type of pricing. And lastly, if you need a visualization of your data, this is probably the best choice. Now, there are other options, but the big thing to keep in mind here is that you want to look for business intelligence requirements and business user reporting. Moving on to OpenSearch Service, this is the managed service for you to run search and analytics engine clusters for many different use cases. It's actually this successor to ElasticSearch, which was in AWS quite a few years ago. It gives you the ability to search all fields within your data, so you're not just locked down to indexes or primary keys, you can search every portion of your data in your cluster. A common exam scenario for OpenSearch Service, you need to create a logging solution for storing VPC Flow Logs or application logs, and then analyze them in near‑real time with some type of visualization. This is a pretty good service for this because you can build dashboards and different graphics using the built‑in UI that they give you via a Kabana‑style dashboard. And lastly, they do have a serverless offering which is meant for on‑demand, auto‑scaling requirements when you have simple, cost‑effective needs, or you have infrequent, intermittent, or unpredictable and spiky workloads. Serverless is good because you just set a capacity unit and consume that when you need it. Now, moving on to the final part here, OpenSearch Service Ingestion sources. OpenSearch Service is commonly used to ingest data from the following services, Kinesis Data Firehose for near real‑time ingestion, Kinesis Data Streams with Lambda functions in between for real time, and then you can even set up CloudWatch Log subscription filters to pass data to OpenSearch Service, and remember that it uses a Lambda function in between that AWS manages. Now with that being said, let's go ahead and wrap this module up. Please spend the time and review the services, especially where you don't feel comfortable in. But for now, we're going to end this module, we can take a break, and I will see you in the next one.

Machine Learning Services
Amazon SageMaker
All right, welcome to the final module within this course. In this module, we're going to look at some AWS‑managed machine‑learning services that you need to be familiar with. Within this entire module, we're going to cover all of these services at a medium level I would say. You don't have to know things super in depth for this exam, but I do want to cover stuff that is real‑world applicable while also touching on the points that you need to know to pass your certification, so just keep that in mind as we go through these services. Now, the first service we're going to talk about is Amazon SageMaker. Amazon SageMaker is the fully managed, machine‑learning service for data scientists, data engineers, and even developers that need to quickly build, clean, train, and then deploy machine‑learning models. This is essentially supposed to be the go‑to service for machine learning from a broad spectrum. Now, it's important to understand you can store and share your data without having to provision your own servers. This is one of the big benefits of using SageMaker, they offer you this infrastructure, you have to spin up instances that you pay for, but you don't have to necessarily manage all of the underlying infrastructure itself, you just leverage the infrastructure to run your machine learning. Overall, in summary, this is meant to simplify machine‑learning workflows, building, training, deploying different models, and even using and building your own frameworks for machine learning. Let's review some concepts that might come up on the exam that you need to be familiar with, the first is SageMaker Studio. This is essentially a web‑based experience, or an IDE within the console, and this is meant to run all of your machine‑learning workflows, it's meant for everything. It's going to offer you a suite of tools and developer environments to accelerate your development needs. So, if you need something simple that's an all‑in‑one approach, this might be a good option. Containers is the next one. Now, SageMaker heavily relies on containers to run different models, for different inference workflows, etc. It works by leveraging Docker containers and those are what are used for build and runtime tasks. Now a big thing to know is you can use prebuilt Docker images that have their own models, their own frameworks, etc. so that you can easily use built‑in algorithms and built‑in tooling. Now, you don't have to do that, you can actually build your own and deploy your own, but one of the big things about SageMaker is it offers you these prebuilt options. Next up is a domain. Now, a domain is the high‑level resource that you use to group and organize all of your specific relevant resources, so this is going to include things like shared associated EFS volumes for storage, different authorized users that can access the domain, you can spin up specific applications within there, and you can set up even different VPC configuration options per domain. The easiest way to think of this is one domain houses all of your relevant information and resources for that specific use case. And then lastly, we have an endpoint, so an endpoint is where you actually deploy your model that was trained and built, this is very useful for real‑time inference. What happens is you deploy your model to the endpoint and then you can specify different model variants and versions, you can specify inference instant types and different counts, etc. After the configuration is set, you can then start hitting or using your endpoint for your different requirements within your workflows. So, just remember, you deploy a model to an endpoint for use. Now exam pro tip here, SageMaker does allow you to host your own ML models to use. You don't have to use a prebuilt one, you can build, train your own, and then deploy that to your very own endpoint. Moving on, let's talk about automation within machine‑learning workflows for SageMaker. First up, SageMaker Canvas, now this is formerly known as SageMaker Autopilot. What Canvas does is it allows you the ability to use machine learning so that you can generate different predictions and you don't need to write any code. Within Canvas, you can use it to chat with popular large language models, they offer ready‑to‑use models, or you can build your own. Some examples of using it for no code or low‑code requirements are predicting customer churn, and you can even use it to identify objects or text. Now, there are better services depending on the use case or requirements for identifying objects and text, but it's important to understand you can use this for that. The next feature or service within SageMaker is JumpStart. Now, this is going to offer you pretrained, open‑source models to help you get started, in other words, JumpStart you into your machine‑learning processes. The benefit here is you can take these and then you can tune, finetune, and train them incrementally over time, so you can take one that's already built and then build it better for your own use. Now, exam pro tip here, your SageMaker instances run on actual EC2 instances that you don't necessarily control at the deepest level, but you do control the infrastructure at a high level. Because it runs on instances, that means you're paying instance pricing. To save on pricing and costs for that, you can use Amazon SageMaker's Savings Plans, be sure you remember that as a cost‑savings approach. Moving on, let's go and break down an exam scenario that could leverage SageMaker as part of the solution. Let's say you have your requirement to use machine‑learning algorithms and you want to build and then train your very own models for your custom online shopping platform. One of the additional requirements is that you need to use the machine learning models that you're building and training to then visualize many different complex, advanced scenarios. In addition to that, it needs to be able to detect trends within data. After you have all of this in place, your business‑analyst team wants you to actually integrate the models with a reporting platform to analyze the data and then visualize it using business intelligence dashboards. Well, for this, it's a perfect use case for SageMaker for the machine learning aspect, combined with Amazon QuickSight for the business intelligence reporting, so you could use these hand‑in‑hand or together to go ahead and accomplish that exact scenario. Now, you also need to remember, for the exam, you're typically going to need some type of machine‑learning experience to leverage SageMaker efficiently. Now, it does offer Canvas for easy, low code or no code options. But a majority of the time, SageMaker is going to require that you're familiar with the machine‑learning processes, so we're talking about collecting data, cleaning data, training on data, etc. Now that's going to do it for this clip on Amazon SageMaker, remember this is the go‑to service for a majority of machine‑learning requirements. We'll end this here, and we'll move on to another service.

Amazon Rekognition
Okay, let's talk about recognizing objects, images, and videos using Amazon Rekognition. Rekognition is the computer‑vision product that AWS offers, which is meant to automate the recognition of pictures and videos using different deep‑learning frameworks and neural networks. You can actually use the processes within the service to go ahead and help understand and then even label what is actually taking part or happening in your pictures and your videos. So for instance, you can look for different objects and pictures, you can see how people's faces look, etc. Now you can also create your very own collection within Rekognition, which is going to contain different faces that are stored for later detection. So, for instance, maybe you're creating a security application for opening and closing locked doors and you want to implement automatic face detection, well, that's a perfect use case for Rekognition. Now, what happens is machine learning is used and it's used to find the following within the different images or videos that are actually analyzed. It can look for different objects, so it can look for cars, it can look for cell phones, hats, etc. You can use it to actually try and match or look for people, so it can do face Rekognition like we just talked about, or you can even actually compare pictures and look for celebrity lookalikes or celebrity matches. So, if you took a picture at a red‑carpet event in the United States and you upload it to Rekognition, well, then you can use that to match the different celebrity names for each of those pictures, assuming they are actually famous and in the database. And then lastly, you can actually use it for text as well. Now there are many other options for textual matches, but you can use Rekognition to look for text. Really, in summary, what this does is it allows you to easily add very powerful computer‑vision capability in your applications. Let's actually look at some use cases for it. You can use Amazon Rekognition for content moderation, so you can leverage it within your application workflow to automatically moderate content so that your applications and your websites would be considered family friendly. This is a very common exam scenario, and we'll look at an architecture for this later on. You can also use it for celebrity recognition, maybe you want to implement some simple web application or some game on the internet and you just want to use it to automatically recognize famous people and then label them. For instance, maybe you have a fashion app where you want to look for clothing items that people are wearing that are famous and celebrities, well, that's an example of using this. You can say, okay, celebrity A was matched here, and here's the clothing items they were wearing. Thirdly, face detection and analysis, so you can use this to automatically recognize different faces and it can detect emotions, so are they happy, are they sad, are they concerned. It can look for different clothing accessories, like are they wearing sunglasses, hats, etc. And it can look for different eye positions, are they looking up, are they looking down, are they looking left, right, etc. The capabilities are pretty amazing. And then lastly, streaming video events detection. So if you have an application, like Ring for example, the famous doorbell camera security company, well, they use this to automatically recognize people, animals, specific faces, and even to create alerts once items or people or objects are actually recognized. This is what's being used on the backend, when on your Ring application on your phone you get an alert that says, hey, someone is at your front door. Moving on, we need to talk about confidence scores. Confidence scores are going to indicate the likelihood of predictions being correct, what we mean by this is essentially the presence of a face maybe or a match between faces in a celebrity database, etc. With these, the higher the confidence score, that means the greater chance there is a correct detection and a close match, so the higher the score, the better the chances. And speaking of scores, let's look at two different scenarios from a high level for architecture. This scenario will be used for both of these architectures, so you're going to say you want to implement some type of AI or ML solution for preventing or even moderating unwanted content on an application. The customer that hired you wants to avoid having to train their very own machine‑learning models to use, so they have limited knowledge, they want to be able to use something a little bit easier and not train their own model, well, Rekognition is perfect for that. So, in this first scenario, let's assume you have a web application, like maybe a social media website, and it allows different users from around the world to upload images and videos to an S3 bucket. Well, Rekognition works with batch operations with S3 stored documents so the images in the videos can then be analyzed via an image API call which then provides a confidence score, which you get to define as a minimum level for it being flagged. In other words, if it's a low confidence score, you can actually flag the content because you might think that it's something unwanted or malicious or just something not family friendly. Once the score is generated and you have the images analyzed, well, you can use other services to go ahead and implement human review for explicit content, for example, Amazon Augmented AI, A2I. You don't need to know this too in depth for the exam, but understand you can use this for human intervention within your ML AI workflows. You can use this by sending low‑confidence score results to the service for human interaction to say, hey, is this truly unwanted explicit content or not? What this does is it helps better train the models that are recognizing this unwanted content. You could also randomly use human reviews just to improve future recognition as well, so it could be good stuff, it could be bad stuff, etc. Now moving on to scenario two or architecture two, the only difference here is that we're live streaming video, so using Amazon Kinesis Video Streams, you can have your video application streaming the live videos, which then gets sent directly to Rekognition via an API call. You can use Rekognition to synchronously analyze the video streams coming in for unwanted, inappropriate, or offensive content. We can generate confidence scores once again, and in this case, what we can do is we can start storing our metadata in different database services like DynamoDB or Amazon RDS. Really, any data store is possible, I just use DynamoDB as an example. So that's another scenario or architecture to keep in mind for Rekognition. Now, an exam scenario where this would be perfect, let's say you need a solution to prevent photos or videos with undesired content from being uploaded to your application. This is perfect. In addition to that, another key indicator is something where you do not want to have to involve training your own custom machine‑learning model, you want to leverage something prebuilt for this remediation, well, then Rekognition is a great choice. Now, that's going to do it for this clip on Amazon Rekognition. Let's go ahead and wrap up here, and I'll see you in an upcoming clip.

Demo: Recognizing Images Using Amazon Rekognition
All right, let's get started with this demonstration clip. In this demo, we're going to log into the sandbox environment, and we're going to play around with Amazon Rekognition. We're going to look at some of the built‑in demonstrations, as well as upload our own image, and then see what gets recognized and the different confidence scores that go along with it. Let's go ahead and jump into the console now. All right, I'm in my sandbox environment. First thing I want to call out here, this is a privileged environment, and it's not a typical sandbox environment that we offer on our platform, so if you're trying to follow along, understand you probably won't have access to the Rekognition service or API, just please keep that in mind. But with that out of the way, let's go ahead and check out some of the features of this service. I've already loaded up Rekognition as you can see, and let's play around with some of these demos. So I'm going to click on Label detection, so you'll notice it has a sample image for us. Now, in this image, there's a guy doing a skateboard kick flip it looks like, there's a bunch of cars and buildings. And if you notice, it's already analyzed and labeled what it thinks it's recognized within this image, so it's classified a neighborhood with a 99.9% confidence score. It sees a city, a road, a street, and a person. Now there's many, many more, so if I click on Show more, you can see there are a ton of objects, people, etc., that have been recognized, or so it thinks. Now this percentage on the right, again, is the confidence score, so it's pretty certain that it sees a building. It's 92.2% sure there's a building and, in fact, it did highlight the building, as you can see here. It even gets as detailed as footwear, it can show you the shoe or the footwear that the guy is wearing as he's doing the skateboard trick. So, these are some of the results and labels that were built in and picked up. It gets as specific as a sedan even, which is pretty crazy. Moving on, let's look at facial analysi, so this is another option, remember. In this, we have a sample image where there's a lady smiling with some sunglasses on, and you'll notice it recognized the face immediately. So we have the face here and it says, hey, this looks like a face, we're 99.9% sure. And it appears to be a female, that's 98.7%. Now, this is where it gets kind of cool though, see, it's guessing age range. Well, we would think based on facial analysis that she's probably 24 to 30 years old, which that could be good or it could be an insult. And it also says, okay, they appear to be smiling and because of this, they appear to be happy. It even recognizes she's wearing glasses. So, this is how detailed facial analysis can get with this service, it's pretty impressive. Now, the last thing I just want to show you from a demo perspective is celebrity recognition. This is cool as well, you can upload a celebrity. It can say, okay, this is Jeff Bezos, we have a 99.9% confidence score that that's exactly who this is based on other data, so this actually uses a facial database in the backend to compare this and recognize this as Bezos. Now, one thing I want to show you, this is going to be similar in all of these other demos, but I want to show you the response here, this is what they look like. We see the URLs, we see the name that it thinks it's recognized, there's a bunch of other information, and the other big thing I want to show you is the confidence score, so 99.99684 etc.. This is very confident that it is Jeff Bezos based on a bunch of different landmarks, etc. So this is a very, very cool service, you can do a lot with it. Let's actually go ahead and use our own image to test what this does. I'm going to go to Label detection. I'm going to upload my own image by dragging and dropping it. And I got this image off unsplash.com, and it's simply a hot dog with some condiments in the background, and let's see what it saw. Well, it picked up the primary portion of the image and it says it's a hot dog with 99.9% confidence. But if you notice it also picks up, hey, I think I see ketchup, that's true. But check this out, this is a perfect example of why you can't just necessarily go along with these results every single time, medication and pill. So, because of how these condiments are overlaid on top of each other, this is thinking it's a pill. Now, what you could do here is with an image like this, if there's a low classification of a confidence score, so maybe below 86% or something similar, you can manually review these and edit the different label detection to make it better. Now that's out of scope for this test, I just like to let you know you can do that. But anyways, that's going to do it. I think that's a pretty good walkthrough on the different capabilities you should be familiar with with Rekognition. Remember, it can analyze your pictures very quickly, it can do a lot of different things like facial analysis, it can compare celebrities etc. Let's wrap this up here, and we'll move on.

Amazon Polly
Okay, Polly want a cracker, Let's look at Amazon Polly within AWS. Amazon Poliy is the AWS service that is used to turn your text into actual life‑like speech, so it's text to speech. This service allows you to create applications that are going to talk to and interact with you or others using a variety of languages and even different accents. With Polly, you only pay for the text that you actually synthesize, in other words, you're paying for the textual data that actually gets converted to speech. Now with Polly, there are many different voice options. The first option is a Generative engine option, this is going to give you the most humanlike and emotionally engaged voice that is available within Polly. AWS says this is the largest model that they have to date for text‑to‑speech. You also have Neural engine, or a Neural Text‑to‑Speech. This produces very high quality voices, and it's a step above standard voices which are typically offered by default for the service. We then have Long‑form engine types. So, a Long‑form engine, again, is going to be humanlike, but it's going to be better at expressing emotion compared to the Neural side of things, so you might use this for marketing videos or some type of blog post where you want to actually capture audiences attentions. And then lastly, we have Standard Text‑to‑Speech. Now, Standard Text‑to‑Speech is going to be the default option, it's going to give you a pretty decent natural sounding synthesized voice. But again, the other options are going to typically be better, however, you do pay a little bit more for them. Now I'll say this, you don't need to memorize these for the exam, but these are very good things to know. You have different options to make your voices sound more realistic. Moving on to some use cases for Amazon Polly. It could be very useful for mobile applications that read news sites to you, so maybe you have a Chrome extension or a Firefox extension, you load up your favorite news site, and then you click this button that integrates with Amazon Polly in your extension and it reads out your website audibly to you using some type of configured voice. There are several examples of something like this already in the world, and that's a perfect example on what might actually use Amazon Polly on the backend of things. They're also good for eLearning platforms where you want to read lessons out loud, so maybe you don't have any video like we're watching here, but you can have a transcript that can be read out loud to the learners, well, that's a perfect use case as well. And then the last one here is accessibility, maybe you need to add options for visually‑impaired people to be able to use your application, well, you can leverage Polly on the backend to go ahead and walk them through filling out different forms, knowing what's on different pages on your website, etc. This is a perfect accessibility option for visually impaired. Now, let's move on to documents and lexicons, these are good things to know when you use Polly. When you're leveraging Polly, you can use either plain text, so literal plain text that you might type in to a Notepad, or you can use Speech Synthesis Markup Language, SSML. SSML documents are going to look similar to HTML in a way, and we'll look at an example here coming up after this. SSML is going to give you the most amount of control over your generated speech using the text in the document. How it works is you can use different tags within the text to control things like the pronunciation of words, you can have the volume go up or down on certain words, and you can even control the speech rate for different sections of your paragraphs, it gives you a lot of different control. Now, a pronunciation lexicon is another resource, this resource allows you to customize the pronunciation of words. So maybe you have a domain specific word you want to use and it's pronounced a very specific way, well, you can customize that using a lexicon. A perfect example of this could be my name, I spell it A, n, d, r, u, and that's not a very common spelling for Andrew within the United States, so maybe I want to upload my own pronunciation lexicon to say how to phonetically pronounce my name when it's used within Polly. And then lastly, when you're working with pronunciation lexicons, what happens is you create a document, it looks like XML basically, or it is XML I should say, and then you upload it to Polly in a single Region so it's a regional resource. Now let's actually look at some examples of each. On the top here, we have our SSML document format, and on the bottom, we have a lexicon for pronunciation. What happens here on SSML, you can notice we have a tag, and this looks essentially like an HTML tag. We start the tag, we end the tag. So, for this, we're saying, hey, we want to emphasize very strongly the word best within the sentence when you speak it. For the lexicon, we're saying, hey, we're going to graph AWS for any time you see it within the text, and we want you to actually say this alias, we want you to say Amazon Web Services, not A, W, S, or AWS. Now that's going to do it for Amazon Polly. Remember, this is your text‑to‑speech option in the AWS cloud. Let's wrap things up here, and we're actually going to check out a demo where we generate text‑to‑speech using this service, coming up next.

Demo: Generating Text-to-speech with Polly
In this demonstration, we're going to work on generating text‑to‑speech using Amazon Polly. We're going to test basic text, we're going to test some SSML documents, and we're even going to use our very own lexicon. Let's go and jump in the console now. All right, let's get started. I've loaded up Amazon Polly, and again, similar to other demonstrations in this module, I'm in an elevated sandbox environment. What that means is you won't be able to use the built‑in sandboxes to probably follow along, so I just like to call that out. Now, let's go and get started. First thing I want to do here is I'm going to open up Text‑to‑Speech. Once it loads, we get our different options. So we can choose the Engine, remember we covered the four different engine types that you can use for whatever your use case might be. You then choose the language that you want it to be spoken in, as well as the voice. Now, different voices are supported for different engine types, you'll notice some of them are not supported with others, etc. So, what I'll do is let's use Generative for this first example, I'm going to paste instant text here, and let's go ahead and play this in the Ruth, Female voice in English. ‑Hey there! Thank you so much for taking this course, I really hope it is useful in your certification journey. ‑Perfect. Now, that's not too bad. It's not super realistic either, but it does do the job. Now, this is just plain text, but maybe we want to use SSML. Notice the tag value, so it's a little bit different than using input text by the default version. So what I've done here is I've created my own that I want to copy, I'll paste it in, and let's go ahead and listen to this in the Generative voice. ‑Hey there! Thank you so much for taking this course, I really hope it is useful in your journey. ‑You'll notice that I've added a break time, and it broke for about a second before continuing on. So this is the benefit of using this type of format, you can add different tags, etc. Now, I will say, using the generative voice is not going to support all of the tags, and you don't need to know that for the exam. But I just like to save people the headache, sometimes it's only supported on Standard, and I'll show you exactly what I mean. Let me replace this, and what I've done is I've added an emphasis, so this emphasis tag should really slow down the speech around the word really and make it a strong emphasis. However, with Generative, this is not supported, and you'll see the error. But if I switch to Standard and I choose a different voice, so let's just choose Matthew, and I listen to this. ‑Hey, there! Thank you so much for taking this course, I really hope it is useful in your certification journey. ‑You'll notice that it works now. And I'm sure you noticed, Standard was not as realistic sounding as Generative, so there are some pros and cons to using each different engine type. But the big thing to note here is SSML gives you more options and control over how your speech is actually said or repeated to the output. Now, the next thing I want to show you is a lexicon. So, what we can do here under Additional settings is I can add custom pronunciations. Now, we don't have lexicons yet, but let's add one. I'm going to go to the menu, go to Lexicons. I'm going to upload a lexicon, and I'll go ahead and show you this here in a moment, and we'll walk through what it looks like. But for now, I'm going to give it a name, I'm going to click on Upload, and now we have our own custom lexicon with two entities in there. Now, if I click on this, we can view what this XML actually looks like, and I say that because it's an XML document. Notice we have XML on the top, and then we get into some specifics for the AWS side. So, here is where we're adding our two entities. What this is saying is, hey, anytime there's GCP, I want you to actually say Google Cloud Platform, and AWS, I want you to say Amazon Web Services. Now, I'll make this document available in the module asset, so feel free to play along with it however you want, but this is what ours is going to do. So now if I go back to Text‑to‑Speech, we'll just do Generative, I'll get rid of the SSML just for this. I'll clear this out, and I'll go ahead and paste in my input text. Wow, I really like AWS, but GCP is cool, too. I'll turn off Custom pronunciation just to show what this looks like for now. We'll go ahead and click on Listen. ‑Wow, I really like AWS, but GCP is cool, too. Perfect, so it read off the actual acronyms. Let's say, well, I want them to say out the words, so I'm going to enable Customized pronunciation, I'm going to choose my lexicon that I went ahead and uploaded, and now when I go ahead and I replay this. ‑Wow, I really like Amazon Web Services, but Google Cloud Platform is cool, too. And just like that, we use our very own lexicon to pronounce words. Now you can do this many different ways, I just used it as an example to show how you can essentially expand acronyms, but these are very powerful. Now, that's going to do it for this demo. We went through, we played around with the different text‑to‑speech formats, and we just uploaded and use our very own lexicon. Let's end this here, and I'll see you in the next clip.

Amazon Translate
Up next, let's look at Amazon Translate. Hallo alle, ich heiBe Andru! You might be wondering ,well, what does that mean? Well, hello everyone. My name is Andru! Now, I know a little bit of German, which helps, but maybe you don't know any German, and you're really only comfortable with your own home language. Like for me, primarily, I would speak English. Well, that's where Amazon Translate comes in handy. Translate is going to be an advanced, machine‑learning service that's going to be used to allow you to automate language translations within your applications. It works by allowing you to translate your unstructured text documents, or you can even build applications that work in real time with multiple languages. So an example or use case would be maybe you need to enable multilingual user experiences within your application because you have global audiences. For instance, maybe you have users in United States, India, China, etc., well, you can use Translate to go ahead and enable the different languages that would need to be supported there to give your users the greatest experience. Some examples of that use case, maybe you have a company blog that you need to translate for your different customers, or maybe you need to actually translate emails and customer service chats before they get sent to your customers. For instance, maybe your application is hosted in China or Japan or somewhere else in the APAC region, and I'm trying to interact with it, and I only speak English, well, you're going to want to translate that because I might not understand or speak the home language where the application is actually hosted. And lastly, you can even use it to translate in‑game, live chats. So maybe you have a game user base around the world, once again, and they all interact with each other and you want to translate live based on local settings, these are all viable options to use in Amazon Translate for. Now, let's look at some examples using translate for real‑time translation. There are two primary ways that you can use translation with the service. The first options are to pass the text in directly via console like we see here, and then have it translated immediately, or you can even pass it in via an API call. So this is an example of what the JSON request would look like when you're integrating your applications for this translation with Translate. The second option is that you can upload a document via one of the supported format, so it supports .txt., .html, and .doc, which is a Microsoft Word format for right now. How it works is you set up your source language, you set up your target language, and you can upload the file and select the document type. From there, it gives you the updated file to download and use whenever you need it, so this would be very useful for converting blogs, different documents for legalities, like financial documents, etc. Now, moving on to an exam scenario. This is going to be perfect when you need to transcribe customer‑service calls in multiple languages and translate them to English to perform sentiment analysis. Now this can go many different ways, maybe you need to translate it to French, German, Hindi, etc., the big thing to take away here is that it can play a role in translating languages that are captured in textual documents. Remember that, you use this service to translate between languages, don't confuse it with capturing languages or capturing text, you have to pass in the text for it to work. Now that's going to do it for the Amazon Translate clip. Let's wrap up, and I'll move on to the next service whenever you're ready.

Amazon Lex
All right, let's talk about chatbots in Amazon Lex. Amazon Lex is the service in AWS that you want to use for building conversational interfaces for your applications whether it be via text or via voice. In fact, Amazon Lex is what actually powers your Amazon Alexa devices. So when you say things like, hey Alexa, and it answers back to you and it gives you all of your feedback, well, it's using Lex on the backend. You can use it to easily build natural‑language chatbots on‑demand. So, if you need a chatbot for customer service, let's say, for your eCommerce website, well, then Amazon Lex is supposed to be an easy, go‑to solution to get started. It actually works by using natural language understanding, NLU, and automatic speech recognition, ASR. Let's look at those concepts really quickly at a high level. NLU, this is used to basically comprehend and then even interpret the language that's being used so that your applications, or Lex specifically, can know, hey, what is the intent of this text or this speech? In other words, are they happy, are they sad, are they angry, do they have a question, etc. ASR is actually used to convert spoken languages into text, so this is typically going to be used for a transcription and analysis. So maybe I have a voice call with a chatbot and you want to transcribe or record my calls, and then you can send it to another service for analysis later on, well, Amazon Lex can do that. Now, with that being said, there's a better service for transcribing called Transcribe, which we'll look at later. The big thing to know is that you can use Lex to capture spoken languages if necessary. However, it's good to know, it's typically only going to be used for chatbot purposes. Now, an exam scenario for Amazon Lex, you can use this to build a conversational chatbot, and you don't need any machine learning experience. So, if you have an eEommerce platform or some type of website with a bunch of users and they need customer service, you can spin up this chatbot and leverage it that way. Now that's going to do it for Amazon Lex. Big thing to remember here, like we just talked about, this is perfect for building chatbots within an enterprise or an organization. Let's wrap up here, and I'll see you in the next clip.

Amazon Connect
Okay, let's talk Amazon Connect. Amazon Connect is a service in the cloud that is going to allow you to quickly and then easily scale your customer experience. In other words, this is an AI‑powered contact center, so a customer service center within the cloud. It's meant to be used by enterprises that need a simplified solution, and the great thing is it integrates with many other popular CMR softwares. So, if you use things like Zendesk or ServiceNow, it can integrate with those different solutions, and you can have a centralized place for your different customer center experiences. Now, some examples of what this service offers, you can use it to receive customer service calls and then patch them out or route them as needed, and you can even use it to proactively engage customers. So maybe you want to send notices or email campaigns or you want to reach out with a hey, can you please rate your customer service experience, well, this is possible using Amazon Connect. An exam pro tip for Connect, it's commonly used with Amazon Lex for customer service flows. Remember, Lex is perfect for conversational chatbots, and you can use that to actually start different ticket processes within Amazon Connect, or maybe you want to reach a live agent to communicate what's going on, these work very well hand‑in‑hand to complete a workflow. And then moving on to an exam scenario, you can use Connect to take or field calls from customers and then have them interact with a voice‑based Lex bot. Based on their different words that they use or their different sentiments and intents, you can then perform automated actions based on that incoming data. So, a big thing to remember, Connect is a virtual contact center in the cloud and it's commonly used with Amazon Lex for conversational workflows. Let's go ahead, we'll end this here, and I'll see you in the next one.

Amazon Comprehend
Okay, let's talk about comprehending data with Amazon Comprehend. Comprehend is the managed, serverless service that's going to use natural language processing, or NLP, and it's going to use it to help you understand the meaning and the sentiment within your textual data. NLP and sentiment are very key phrases to keep an eye out for. Now, you can use this to pick up on key phrases as well, so maybe your customers use certain slang or short phrases for certain things in their Region, well, you can pick up on key phrases that might mean something specific to their locality. An example of using comprehend is you can automate understanding whether the people or your customers are saying positive or maybe they're saying negative things about your services or your products online. It works by being able to put all of the different texts together and then using some machine‑learning algorithms to say, okay, well, when these two words are used together, it's usually negative intent, and when these three words are used together, it's positive intent. The big thing to understand is that it allows for automating comprehension at scale using machine learning. This is the service to use for comprehending and understanding sentiment in your data. Let's explore some use cases for Comprehend. You can easily use this to detect if your customers are having good or bad experiences with your call center staff. So maybe after you transcribe their data, you capture it, you can then analyze it using Comprehend, and see, okay, well, a majority of our customers are having a negative experience, we need to do something about that and fix it. Another use case is you can create indexes and then automate product reviews to detect whether the people are happy or maybe they're not happy with those products or those services. So maybe you have 100 products and each product has 10 reviews, well, you can go through all of that review data, index it, use some machine‑learning algorithms and models, and then start putting sentiment analysis together to get an idea of what's good and what's bad. Now, an exam scenario for Amazon Comprehend, it's perfect whenever you have to use machine learning to determine the quality of customer service calls, especially when you need to write sentiment analysis reports. So again, this kind of works with that chatbot scenario, maybe you use Lex for your conversational chatbot and Amazon Connect to connect your customers, well, then maybe you capture all that textual transcribed data and you'll use comprehend to say, okay, what's going on with the quality of our calls? Now moving on to a similar, but also separate feature within Comprehend, Comprehend Medical. This is used specifically to detect and return useful information from unstructured clinical text, so doctor notes, discharge summaries, test results, etc. Some very important concepts to understand for Comprehend Medical. This also uses NLP to detect references to specific medical information, so it's looking specifically for things like medical conditions, maybe prescribed medications, and more specifically and more commonly, Protected Health Information, or PHI. It works by allowing you to run asynchronous operations on documents that are stored in S3 or you can actually run synchronous operations via another API call. Both types of operations are supported, it just depends on how you store your data. Now, an exam scenario for Comprehend Medical, maybe there's a hospital application that uploads its reports in a PDF format to Amazon S3, they want to modify the code to identify Protected Health Information before actually sending the reports out to different consumers and patients. This is a perfect use case for Medical, you can store the PDFs in S3, start an asynchronous operation, that's going to go ahead and detect any PHI data, and then alert you and then you can say, okay, yeah, we want to send this or we don't want to send this data or this form once that process is complete. Two of the big things to take away, Comprehend Medical is perfect for identifying protected health information within your textual data, and Comprehend, the overall service, is good for understanding sentiment analysis within text. Let's go ahead. We'll end this here, and I'll see you in the next clip.

Amazon Forecast
Moving on, let's look at Amazon Forecast. Before we talk about what Forecast is, we need to discuss what time‑series data is. Time‑series data is going to be different data points that get logged over a series of time, in other words, time‑series data. What this type of data does is it allows you to track your data and potentially use that data to forecast future results. Now we review that because that is exactly what Amazon Forecast uses, it's a Amazon Web Service service that uses machine‑learning algorithms to give you very accurate time‑series forecasts. By using Forecast, it allows you to leverage it to automatically learn your data, it's going to do the work to select the correct machine‑learning algorithm for you, and then putting that all together, it can then help forecast your data so you have an idea of what to expect in the future. Now, this can be used for many different occupations and different scenarios and use cases. It can be used in retail stores, it can be used with financial applications, and it can even be used with weather data and healthcare data. In fact, this is actually what's used by amazon.com for forecasting sales data, so they use this for Prime day, etc. Now, it's very important to note here, this is no longer available to new customers, but it can still appear on the exam, so that's why I'm covering it within this course. With that note out of the way, let's talk about some concepts regarding the service. You collect your input data and store it together to train your different predictors within what are called datasets. These datasets are used with your predictors, which we just talked about, and this is essentially just going to be the model that gets trained using your time‑series data and those different datasets that you set up. Now, you can use these two different resources within this service together to drastically reduce forecasting time. What might typically take days, weeks, or months can now go down to just hours. And what's nice about it is it's a much more accurate way to predict that future data than many other methods that you could use. Now, if we talk about some use cases for Forecast, it's perfect for if you need to predict resources that are needed for building products in a month, using historical values, which are stored in a dataset with S3. It's even more useful if you have little or no machine‑learning experience and you want to leverage a managed service. Remember, it does a lot of the ML algorithm selection for you us0ing your dataset. It's also good for if you have thousands of Internet of Things sensors around the world that relay thousands, millions, or even billions of points of information about let's say location's weather, so you can see if it's raining, if it's dry, etc., and you need to use that information to decide agricultural plans in the future, well, this is a perfect use case again. And then the last use case here is retail stores, maybe you need to predict specific product demand based on different information points within your time‑series data, so you want to know what is good during sales, what kind of consumer demographics are purchasing what type of products, the different store traffic that's going on in addition to how those drives sales volumes, and how all of this data correlates to different store locations, this is all achievable using Forecast to predict or forecast that future data. Now that's going to do it for Amazon Forecast. Big takeaway here, this is for time‑series data for forecasting future results and future data points. Let's wrap this up here, and we'll move on to the next service.

Amazon Kendra
Next up in this module, Amazon Kendra. Amazon Kendra is a service in AWS that's going to allow you to create an intelligent search service that is powered by machine learning and natural language processing. The big thing here is it allows you to create enterprise‑search applications that can bridge information between different silos, so you might store data in S3 buckets, maybe you have files in file servers, or maybe you want to keep data on specific websites. The nice thing about this service is that it can allow your enterprise to have all of the required data from all of those different silos intelligently in one single place. Once your data is in that single place, you then get semantic and contextual understanding capabilities. What this means is the service can put together different meanings, see what's actually going to be the intent of different words used together, etc. It offers a lot more than just basic keyword‑based searches. Moving on, let's talk about some concepts specific to Kendra. Kendra allows you to perform incremental learning, which is going to improve the ranking algorithm, and then it optimizes your search results for much better accuracy using feedback from previous results. So, for instance, maybe you're searching for some data within one of your silos and your user says no, this is not good, try again. Well, it can use these constant small queries from your users and the different feedback to build a better algorithm to return proper results. It also allows you to use relevance tuning. Relevance tuning allows you to modify the effects fields or attributes have on different search relevance. In other words, what kind of terms or what kind of attributes in your searches should have more weight on the returned results, some things are more important than others, and you can set that with relevance tuning. Now, it's important to note you can do this manually or you can use tuning configurations to automatically achieve this. Let's look at some use cases for Kendra. Now you can use it for accelerating research and development. So, maybe you have previous research papers with different scientists and doctors and they're scattered all over the place, we're talking the data is in S3 buckets, it's in databases, and it's just within text files on a file server. Well, with Kendra, you can consolidate all of this information into a single place and then search it. You can also use it to minimize regulatory and compliance risks, so you can use machine learning to automate the research of different new regulations that might impact your business. For example, maybe there's new ISO requirements, there's updates to HIPAA, or maybe updates to PCI‑compliant policies, this can keep you in the know so you can stay ahead of that. Thirdly, maybe you want to improve customer interactions. You can use Kendra, in addition to other services, to really better understand what your customers are asking. Once that information is collected and analyzed, you can then start to return more relevant answers and relevant experiences to your users. And fourthly, increasing productivity for employees. By having your data searchable in a single place, you can actually increase productivity because you're ensuring that your staff is not wasting time trying to find the right data in the right spot. Instead, they can easily search using this enterprise search option here and quickly get to their answer, it's just a much more productive solution. Now, that's going to do it for this clip on Amazon Kendra. Let's go ahead and wrap this up, and we're going to move on to another service coming up next.

Amazon Textract
All right, let's talk about extracting text information using Amazon Textract. Amazon Textract is a managed service that's going to leverage machine learning to automatically extract typed text, handwritten text, and even data from actual scanned documents. This service is commonly used to extract data from different uploaded forms. And speaking of uploaded forms, it can extract information from PDFs, JPEGs, etc. There are tons of different supportive formats, and the important thing to understand is that it extracts textual information from them. Let's actually look at some use cases for Textract. It's very useful for financial services. You can use this to quickly and easily process invoices and receipts for your customers, so you can scan for important text like number of products, the type of product, the product ID, etc., or other type of financial information like applications for loans, things of that nature. It's also very useful in the public sector, so you can use it to process state IDs or driver's license or maybe passports for travel requirements. So when you're entering another country, you might have to enter a declaration with different paperwork and you have to upload your ID, etc., well, you can use Textract to take the image that was scanned of a passport, extract the important information like the passport ID, and then autofill documents for your customers, that's a real‑world example of using Textract for something important. Now let's look at an exam scenario before we wrap this up. Amazon Textract is perfect for being part of the solution for the following, maybe there's a hospital application that's uploading different health documents in a JPEG or maybe a PDF format to an S3 bucket. After it is uploaded to the bucket, maybe they need to extract text from the reports to send to Amazon Comprehend Medical to redact any PHI data, so this can work in conjunction with other services to accomplish this use case. It's extracting the data and you send that extracted data to another service to perform redaction. Now that's going to do it for Amazon Textract. Big thing to remember, you use this to extract text from different documents and pictures, and then you use it commonly with other services to perform workloads or processes with that data. Let's go ahead and wrap up here, and I'll see you in the next one.

Amazon Personalize
Okay, let's talk about personalizing data with Amazon Personalize. Amazon Personalize is a fully‑managed, machine‑learning service that uses your very own data to generate real‑time recommendations for your users. This is extremely useful for creating personalized email campaigns, targeted marketing campaigns, and even personalizing search results on eCommerce platforms. Let's talk about some examples for Personalize. Number one, maybe you want to add product recommendations to your eEommerce site. So maybe on the home page, like amazon.com for example, you can add product recommendations based on previous data. For instance, I like to play golf and maybe I buy a lot of golf stuff on amazon.com, well, eventually using Personalize, it's going to start to say, okay, Andru likes to golf, let's recommend some stuff that he might like, here are some golf shoes, here is a golf hat, etc. You can also use it for personalizing video recommendations for video streaming services, a great example of something like this would be YouTube. Typically, when you have a YouTube account and you start to watch videos, on your home page, there's going to be a for you section where it has recommended videos based on previous watched videos. This is a perfect example of how Personalize can work. Now, obviously, YouTube uses a different service, but the concept is the same. Moving on, let's look at concepts you should know for Personalize. The big thing is that recommendations or personalization are going to primarily be made based on interaction data, so clicking on things, checking out within an eCommerce site, watching videos, etc. It supports bulk data stored in S3, or you can perform real‑time data analytics or recommendations via an API call, so it supports both, asynchronous and synchronous. And then lastly, this is actually used by amazon.com on the backend for recommending items to you. Remember that example where I said I like to golf and I buy golf stuff on Amazon sometimes? Well, they use this to say, okay, well, let's recommend more stuff that other people also like that are into the same golf, sport, or category. Now, that's going to do it for Amazon Personalize. Big thing to take away here, it's meant to personalize or recommend options for your users based on interaction data. Let's wrap things up here, and we'll move on to the next service.

Amazon Transcribe
All right, let's get to talking about Amazon Transcribe. Transcribe is a managed service in AWS that helps you convert audio to text, so it's speech‑to‑text as opposed to text‑to‑speech. Amazon Transcribe is useful as a standalone transcription service, or you can use it to add speech‑to‑text capabilities for any of your existing applications. The nice thing about the service is that it supports batch transcriptions so you can upload documents to S3 and then perform a batch transcription job, or it can do real‑time streaming transcriptions. So maybe you're streaming live video and you need it to create subtitles on demand, well, it can do that. Moving on, let's talk about concepts for this service. A really neat thing is that transcribe can be used to automatically identify different languages that are being spoken in media, and the nice thing is you don't even have to specify the language code that's being used, they can do it for you. It gives you custom vocabulary filters, so this allows you to essentially modify different words within transcription output, so maybe you want to change how things look, how they're spelled, etc. Thirdly, you can use what is called toxic speech detection to help moderate social media platforms or peer‑to‑peer platforms, like online gaming or social media. In addition to toxic speech detection, you can use redaction. Redaction is going to be used when you need to mask or maybe you need to remove personally identifiable information, or PII. Now, the different types of PII that it can work with kind of vary, but you don't need to know that in depth, just understand you can use redaction with Transcribe when necessary. And then lastly, again, we kind of talked about this previously, but you can very easily use this to generate and use subtitles within your videos. So, if you upload a video, you can say, hey, please create subtitles for this video so that I can upload them to my own platform, Transcribe is perfect for that. Now, before we wrap things up, an exam scenario. This is perfect for a scenario where you might have something like a financial company that's storing voice call audio in S3. Once that data is in S3, they want to capture the text or transcribe the text from the audio and then remove PII that might exist. So, for instance, maybe a customer says their name and their address, well, you can remove that or redact that using transcribe and it's going to do it automatically for you based on your batched data. Now, with that being said, let's go ahead and wrap this up, and we're gonna move on to the last service to cover before we wrap this module up.

Amazon Fraud Detector
All right, let's talk about detecting fraud using Amazon Fraud detector. This is a very short lesson. I just want to cover some brief stuff regarding the service, so let's get started. First up, this is a fully‑managed fraud detection service that you're supposed to be able to use to automate detection of potentially fraudulent activities online and within your applications. When we say fraudulent activities, we're talking unauthorized financial transactions, maybe people creating bot or fake user accounts, and people abusing free trial accounts. Now, unfortunately, I will say I'm guilty of the free trial account thing, but if they use this service, they could potentially catch that. Now, an exam scenario for Fraud Detector, maybe they're saying, hey, you're asked to build a fraud detection machine‑learning model that is highly customizable based on your own data, this is a perfect scenario to tie in Fraud Detector. Big thing to take away here is it detects fraudulent activities, and it's supposed to be a very easy way to do so. Let's go ahead, we're going to wrap this up, and we're going to move on to the final clip, Module Summary and Exam Tips.

Module Summary and Exam Tips
Okay, once again, thank you for hanging in there, we're now reaching the Module Summary and Exam Tips portion of this module. Let's review some important things to take away before you end this course. First up, Amazon SageMaker, this is a very important machine learning service you need to understand. SageMaker was created and available for you to simplify using your different machine‑learning workflows, including training your models, cleaning your datasets, and even building and deploying machine‑learning models. SageMaker is essentially supposed to be the one‑stop shop for all things machine learning, however, you do need to have some machine‑learning experience to properly use it. They have options to make it easier to use and to make it quicker to use, but it's only really going to be helpful if you have some type of machine‑learning knowledge. Also remember, you can deploy your very own model that you trained and you created via an endpoint, you don't have to use the prebuilt ones. Moving on to Rekognition, Textract, and Transcribe. Amazon Rekognition, you need to remember that you can automatically moderate video and image content in applications, so unwanted or explicit content, you can use it to recognize faces and match celebrities, and you can even use it to send alerts based on detection. So maybe you see a person at your door, well, that's exactly what Ring devices use. It says, hey, I recognized there as a person here, they left the package, here's the timestamp, check this out. Next up, Amazon Textract, so this uses machine learning and optical character recognition, or OCR, to actually process text, it can process handwritingi and it can do even more than that in an automatic fashion. Some examples of using Textract are to extract insurance form data, so when they type in or handwrite data on the forms, you can extract that and make it computer consumable. And lastly Transcribe, this is meant to convert speech‑to‑text, your transcribing audio. You can use this service to generate subtitles on the fly or you can even use it to do things like capturing meeting notes. Remember, you can also use Redaction, which is a feature to mask or remove PII from your captured text. The big thing to remember here, Transcribe is speech‑to‑text, this is what gets really confusing on the exam. Next up, we have Polly, Lex, and Connect, these all play together really well. First up, Amazon Polly. This turns text into speech, so it does the opposite of Transcribe. You can use this to create applications that talk to and interact with you or customers using a variety of languages and even different accents. Now, an example of this would be the AWS Blog, they actually use Polly where you can convert your blog posts into lifelike speech. So at the top of the blog usually, they have a little media player where you can go ahead and click Play and it summarizes the article and reads it out loud to you, that's fueled and powered by Polly. Then we have Amazon Lex, so this is going to be used to build conversational interfaces in your applications with natural language. You want to think things like chatbots, like virtual agents, and voice assistants. The easiest way to remember this is that Amazon Alexa devices are actually powered by this on the backend. And then thirdly Connect, this is simply put, an AI‑powered contact center. It allows you to scale your customer experience and your customer service needs and it integrates with a ton of different CRM softwares. Connect is commonly used with Polly and Lex for customer service flows. Next up, Translate and Comprehend. Amazon Translate. You need to remember this will automate language translation via deep learning and neural networks. This one should be pretty self‑explanatory, but it can be confusing when they're trying to trick you with transcribe, etc. Remember, this is for language translation. Some use cases are to translate company blogs, maybe emails, or even live chats with agents. The big thing to remember is that it enables multilingual‑user experiences, Sso if you have an application that's worldwide or a global, this is a perfect thing to use to translate your application's output and input. Next up, Comprehend. Comprehend uses NLP to actually help you understand the meaning and the sentiment within your text data. That is going to be the biggest key indicator to look for, do you need to perform sentiment analysis, and if so, Comprehend might be the best option. You can use it to do things like automating the understanding whether people and customers are saying positive or maybe they're saying negative things about your services and your products, that's a perfect use case for this service. And then the final group of services here Forecast, Kendra, Personalize, and Fraud Detector. Amazon Forecast, this is for time‑series data. It's a time‑series forecasting service to give you important business insights. An example of using Forecast would be having IoT sensors deployed around the world that send millions of points of information and you need to use that information for agricultural planning. I like to think of Forecast in this way. I look at the weather forecast to see what that time series is going to be later on that day. Do I need to plan for rain, is it going to be sunny, hot, windy, etc.? We then have Kendra, so this is going to be the intelligent search service that's meant to allow your enterprises and your organizations to essentially centralize all of the data that they need intelligently in one place. This is a very powerful search service. You can do descriptive searches where you can ask, how do I get rid of this cold? You can ask keyword indicators, or you can ask for straight up facts. The big thing to remember here is that this is a very powerful search service. Third, Personalize, this is going to use your different data to generate real‑time recommendations for users, so you're personalizing results. A perfect example of that would be amazon.com. When you start buying stuff, they start recommending items and products based on those purchases, as well as other people that bought similar things. And then the last one here, and then we're going to wrap things up, Fraud Detector. Remember, this is a fraud‑detection service for detecting potentially fraudulent activities. An example of using this service would be to catch unauthorized financial transactions, or maybe you want to catch different bot or spam accounts being created on your platform, those are all doable with Fraud Detector. Now that's going to do it for this module. Thank you so much for hanging in there. This was a very long course. I hope you learned a lot. Please, by all means, go back and review some points and some services that you might need to refresh on. But for now, that's going to do it. Thanks for taking this course, and I will see you, hopefully, very soon.
