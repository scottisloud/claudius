AWS Identity and Access Management (IAM) Overview
What Is IAM?
Hello and welcome to the first module in this AWS Certified Solutions Architect ‑ Associate Core Services course. In this module, we're going to have a quick overview of AWS IAM. What is IAM? AWS IAM stands for AWS Identity and Access Management. What this is is a service that offers a unique authentication database that is logically isolated to a single AWS account. So with that in mind, what does it do? IAM allows you to create users and then grant permissions to those users within your account itself. In addition to regular users, you can also create IAM groups and IAM roles. Now, don't worry if you're not familiar with these terms just yet; we're going to cover these in an upcoming module. I just wanted to put them out there so you're at least familiar with the terms when they come up. The big thing here is that IAM allows you to implement access controls for your users to allow what they can and cannot do with your resources and the different services in AWS IA. The too long didn't read version is that Aws IAM allows you to manage your users and control their level of access within your account. That's the big takeaway. Now, really important here, this service is extremely crucial for you to understand, so you really need to make sure you really understand what it does and how it's used for this exam and any real‑world application designs. This is arguably the most important service within AWS. Now that we have an understanding of what the service is and what it offers at a high level, we want to cover the root user account. Here's some dos and some don'ts, as well as some general information. If you're wondering what the root user account is, the root account is simply a user which uses your email address that you used to sign up for the AWS account. This user has full administrative access to your accounts. It is very powerful, which also means it can be very dangerous. For this reason alone, it is extremely important that you secure the root user account immediately. If you fail to secure it and someone compromises it, they could rack up a humongous bill, which unfortunately, you would be responsible for. So it's very important that you secure this account immediately. And let's talk about some of those steps. The very first thing that you should do with this account is turn on multi‑factor authentication. Now, the MFA can be software, like Google Authenticator, or it can be a hardware token like a UB key. It doesn't matter what method you choose, just choose one and turn it on. Also, avoid using this account for normal tasks at all costs. Don't log in and start using it for small, easy things. Instead, you should be creating an IAM user, logging in with that user, and then performing the tasks. There are very few use cases where you must use the root user account to perform an action. So unless those use cases come up, like altering billing information or closing the account, then you really shouldn't use it at all. Last thing here is, please do not ever create access keys for this account. Now, again, don't worry if you're not familiar with the term access keys because we'll cover them in an upcoming module; just remember, you should never create them for the root user account. If these keys got leaked or stolen somehow, what that means is that whoever compromised them would now have programmatic access to your account. So they could do just as much damage as they could if they got the username and password. So never create keys for the root user. Now, with all of that out of the way, let's go ahead and end the clip here, and in an upcoming clip, we're actually going to have a demonstration where I'll secure a brand‑new AWS account, and then we'll enable MFA for the root user so you can see how it's done.

Demo: Securing the AWS Root Account
All right, welcome to this demonstration lesson where we are going to sign up for a brand‑new AWS account, and then I'm going to secure the root user account that we signed up with. I am at the signup page for a brand‑new AWS account, and let's go ahead and begin. The first thing we need to do is give a root user email address. So I'm going to paste in a temporary email, and then we have to give our account a name. So let me go ahead and give my AWS account a name that I will remember. So let me go ahead and give this a name. And the next thing I have to do here is verify my email address. They want to verify that we are indeed a human and not a bot that's just spinning up accounts. So they sent me an email verification code, and let me go ahead and paste that in. I'll click on Verify, and there we go. Now, we can set our root user password. Now, of course, this is very important. You need to make sure that you go ahead and you give a very complex password. Now, I recommend using some type of password manager, of course. But if you have your own process, feel free to follow it. So let me go ahead and copy and paste my complex password. And there we go. Now for the following steps here, what I'm going to do is actually cut forward because it's going to be stuff that I don't want to show. So I'm going to fill in my contact information. There will be a few questions regarding billing, etc., but I'm going to save you from being bored watching me fill that out. I'll fill it out, and then what you'll see next here in a couple of seconds is we'll be actually logged into the console. All right, we've now jumped forward in time and space. Congratulations! And we are logged into our brand‑new AWS account as the root user. You can see up on the top right here we have our account name that we gave it, and our account ID. Now, within this drop‑down, we're going to select Security credentials. Another way to get here instead of that shortcut in case it ever changes is you can just search for IAM and then locate that service and select that as well, and it should bring you to this dashboard. So just keep that in mind in case this process ever changes. Now, the first thing we see here is a big warning saying, hey, you don't have MFA assigned, and that's a security best practice, so you should do it. So you could click on this, but I want to show you how to do it the more manual way. Down here, you can see we have a multi‑factor authentication portion of our menu. I'm going to click on Assign MFA device, and the first thing we do here is we give it a name. So this allows us to easily reference what device we're using. So I'm going to go ahead and give this the name of google‑auth because I'm going to be using the Google Authenticator. Now, let's look at the device choices. You have three choices you can use as a device. You can use a pass key or security key, you can use an authenticator app, which is what we're going to do, or you can use an actual hardware token itself. So it really is going to come down to your compliance requirements and your security protocols. For this demonstration, I'm going to choose Authenticator app. I'll click on Next, and now we set up our device. So they give you the steps that you would need to set this up, and let's go ahead and follow those. Now, I'm going to blur out this QR code just for safety protocol. And I'm also going to blur out the MFA codes that I enter in. In addition to that, I am going to fast forward because otherwise you have to wait and sit here watching me wait for about a minute. So let's go ahead and skip forward now. Okay, so I've scanned my code, I've entered the codes that have cycled through, and I can click Add MFA. Now before I do this, if for some reason you're not able to scan this code, you can click this Show secret key button here, and I will blur this out, of course, so you're not going to see it, but it pops up a secret key so you can manually add this account to your authenticator app. So this is very helpful if you're having problems scanning the code. This will work the exact same way. So, lucky for me, the QR code worked. I entered my consecutive MFA codes, and I click Add MFA. And there we go. It's that simple. Now, whenever I log into this account, I'll have to enter MFA. So let's test it out. I'm going to select this. I'm going to go ahead and sign out. And now that we're signed out, I'm going to go back to sign into the console. I'm going to enter my root user email address because I selected root user. I'll click on Next. I can enter my password. I'll click Sign in, and there we go. Now I have to have that multi‑factor code in order for this to actually sign me in. So let me go ahead and enter that now. I submit it, and Voila, we are signed into our new account with our secured root user. Okay, that's going to do it for this demonstration. I hope you've learned how to secure your user account. Let's go ahead and wrap up here and then I'll see you in an upcoming clip.

Module Summary and Exam Tips
All right, welcome to this Module Summary and Exam Tips clip for this module. Nice, short and sweet module, but there are a few things we need to make sure that we really understand before moving on. First, remember IAM is a global service and it offers your account a unique authentication database. So it's specific and locked down to your only AWS account and no one else has access to it. Equally important, the root user, remember, has full, unfederated access to everything within the account, and I do mean everything. It can erase anything that's in there, it can change billing properties, and it can even shut your account down. What that means is we need to protect it. Immediately enable multi‑factor authentication for the root user. You should also implement a strong password policy for IAM in general. Of course, this is kind of subjective to your own company policies, but regardless, you really need to set a strong password, at least for this user. A combination of MFA and a strong password policy are great first steps to protect your account. Last thing here regarding root user, for this particular portion, remember, avoid using root at all costs. You should never be using this unless it is absolutely necessary. The risk involved with using this account for daily tasks just greatly outweighs the benefits. Don't be lazy; create a separate user for you to use, and then log in via that user. Now we'll cover users coming up in upcoming modules, of course, but the big thing here is just don't use root unless you have to. It's that simple. The last thing here to remember, the Root User email address needs to be a unique email. You cannot reuse email addresses between AWS accounts. A big pro tip here is to use alias addresses for your new accounts. That's a very common practice, and then that way you can leverage the same email inbox for root user administrators. Now that's going to do it for this module. Thanks for hanging in there. Hopefully, you learned a little bit about the root user and some general high‑level information about IAM. And in upcoming modules, we're going to start diving a lot deeper into the IAM service. So let's end this here, and then I will see you whenever you are ready in the next module.

IAM Users and Groups
AWS IAM Users and Groups
All right, welcome to the next module in this AWS Certified Solutions Architect ‑ Associate course. In this module, we're going to start exploring IAM users and IAM groups. First up, IAM users and groups. An IAM user is simply an entity within the IAM service, and it's meant to represent either a human or even a dedicated service account that needs long‑term credentials. You use these users within your account to perform different actions. In addition to a user, there's also an IAM group. A group is simply a collection of different users, and it's meant to simplify permission management. You place your users within a group, and then you assign permissions to that group, which then has the users inherit those applied permissions. This way you don't have to do a one‑to‑one assignment for permission policies. Now, it's important to call out that users can belong to multiple groups at one time. So keep that in mind. If a user belongs to two different groups that have two different sets of permissions, well, then that user gets both assigned permissions. Another key thing to remember for the real world and the exam, you cannot nest groups within other groups; it's not allowed. Moving on, let's talk about long‑term credentials. IAM users will authenticate via two different types of methods. The first will be basic auth, via a username and a password, which you see when you log into the console, and the next is a static set of IAM access keys. Now we're going to explore access keys more in depth later on within this module, but for now, just understand those are the two methods to authenticate for an IAM user. Now to give IAM users permissions, you assign them what are called permission policies, these permission policies contain what a user can and cannot do. Now, we're going to explore these in much more depth in an upcoming clip, so we'll save that for later. For now, what we'll do is we'll end this clip here, that's a good introduction to users and groups, and we're going to go into a demonstration lesson where we go ahead and create an admin IAM user and assign it to an admin IAM group.

Demo: Creating an Admin IAM User and Group
Okay, welcome to this demonstration lesson where we're going to work on creating an IAM user and an IAM group and assign that user to the group. Now, in this demo, I'm using our hands‑on playground, so if you have access to this, feel free to go ahead and follow along. If you don't have access, you can still follow similar steps on your own, just make sure that you clean up after yourself. So let's go ahead and dive in. I've already created a new Sandbox account. I have my username and password. So let me go to that tab here and let me sign in. Now before I hit Enter, this is important to remember, this is one of the methods for long‑term credentials. This is basic authentication. We have our username and we have our password. So I'm going to click on Sign in, and this should bring us to our region within our AWS account. There we go, North Virginia region, US East 1, and cloud user. So let's go ahead and get started. What I'll do here is I'll search for IAM in the search bar. I'm going to navigate to it. And once we're here, I'm going to go down to Users under Access management, and I'm going to create a new user. We're going to call this fake_admin, and we're going to give it access to the console. You'll notice when we do this, it says, okay, well, you need to come up with a password for this user, so either autogenerate one or assign a custom one that you want. Now, what I'll do is I'll use autogenerated, and you'll notice we can view it after it's created initially. So I'll select this, and then a best practice when you're creating users is to have them create a new password at the next sign in. Now, this is because then no one else knows what the password is after initial creation. So this is perfect. For this demo, however, I don't care, it's just me in this account, so I'm going to skip this step. However, it is highly recommended you do use this. I'm going to go down, click on Next, and now we have Permissions options. So for this, what we're actually going to do here is skip this part because I want to show you how to create a group and assign permissions outside of this. So what I'm going to do is skip it and click on Next. You see we have our fake_admin username. We're autogenerating a password, and then we're not requiring it be reset. There's no permissions because we assign none, and we don't have any tags. So I'm going to create user. There we go. Let me copy this password into a Notepad on the side, and I'll also copy the console URL for later. Awesome! So I'm going to click on Return to users list. I'll continue because I have the password. There we go. So we have our account, but there's nothing this account can do. Well, let's fix that. What we're going to do is we're going to go over to User groups. We're going to create a new group. I'm going to call it fake_admin_group. I'm going to go down, we're going to say let's add our fake_admin user to this group, and we're going to attach a default policy called AdministratorAccess. Now, don't worry about this now. We're going to explore policies much more in depth later on. But for this walk‑through, we're going to select admin access. After we do that, I'm going to click on Create user group. And it's that simple. We now have a user group, fake_admin_group, and we have users within the group. We assigned this group administrator access permissions, and since it's assigned to the group, that means our user that we just created gets those exact same permissions because they're inherited via the group, as you can see here. So hopefully you can see how easy it is to assign single sets of permissions to multiple users all at one time via a group. It's far better practice to use groups instead of directly managing permissions for an individual user. Now, with this, let's actually go ahead and sign in with this user. So I'm going to go ahead and sign out. We're going to paste in the old URL for the sign in. We'll do fake_admin. I'll paste in that password I copied from earlier, and there we go, we're now logged in as that brand‑new user, fake_admin, in the account, and we should have permissions to do pretty much anything we want since it's administrative. So if I go into IAM now, we're going to be able to see everything we were able to see previous to switching to this user because they have admin permissions as well. We can see policies, users, etc. Okay, that's going to do it. Hopefully you saw how easy it really is to create an IAM group, create an IAM user, and then assign permissions to users via those groups. Let's go in here and then when you're ready, I will see you in the next clip.

IAM Policies
All righty, welcome to this next clip where we are going to now start looking at IAM policies. This is a very important thing you need to know for this exam, and honestly, any real‑world scenarios revolving around security and architecture. An IAM policy is simply an object within AWS that when associated with an identity or an AWS resource actually defines the permissions that they are allowed to have. Now let's actually look at policy types. There are six key policy types that you'll need to know, but we're only going to focus on two of them for this particular section. First, we have identity‑based, there's resource‑based, permissions boundaries, organization service control policies, access control lists, and session policies. For this particular clip and section, we're only going to focus on these two here, identity and resource‑based. First up is identity‑based. What these are are policies that actually get attached directly to IAM identities to grant permissions. So this is the type of policy you would attach to a user, a group, or a role. If you saw on the previous demo, that admin access policy would be classified as an identity policy. These are written and stored as simple JSON documents. The format will always be the same, as well as the syntax. We actually break down an example policy in an upcoming clip. Now, by default, permissions are implicitly denied for a user. If you do not explicitly allow an action in AWS IAM, then it is implicitly denied, meaning they cannot do it. This is a key concept for the exam. If it is not explicitly allowed, then it is denied. There are two forms of identity‑based policies. There's a managed policy and an inline policy. Let's look at those now. First up, we have managed. Managed policies are attachable, standalone policies that are reusable. When you create these, you actually get a resource ARN after creation that you can reference. Now within a managed policy, there are two main types. The first is an AWS managed policy. These are policies that are created and managed by AWS, and they're usable by everyone. A perfect example of this would be the administrator access policy that we used in a previous demo. That is an AWS managed policy. You'll also notice with AWS managed policies, there are no account ID numbers within the ARN. That's because they're owned by AWS and usable by everyone, so there's no account number in there. It's a pretty interesting fact. Next, we have customer managed policies. This is exactly what it sounds like. This is where you, the customer, create, you manage, and you reuse them however you want. You have full control over what these do and how they are managed. A general best practice when you're creating a customer managed policy is to use the baseline of an AWS managed policy and then add and remove permissions as needed. It speeds up the process quite a bit. Moving on to the next form. we have an inline policy. These are policies that get added directly to a single IAM identity for very specific use cases. Now, I say specific because of how they are treated. These are a one‑to‑one relationship for policy to user. What that means is they're not reusable. You can't link them to more than one identity in AWS IAM. Because of this and how they work, when you delete the identity that they are attached to, that means the inline policy gets deleted as well. So it's gone, they're not reusable. It only exists as being attached to that user group or role. These are typically going to be best used for very strict or specific one‑off permission needs. Now let's move on to resource‑based policies. We just talked about identity‑based. The other main type we wanted to talk about was resource‑based. Resource‑based policies are also JSON policy documents, but instead of an identity, they're getting attached to actual AWS resources. Now, examples of this type of policy are a bucket policy and a KMS key policy. Now don't fret if you don't know what these are, that's not a big deal. Again, I just like to throw these terms out there so when you see them in the future, you can think, oh yeah, we learned this before, now I'm putting the pieces together and this makes a lot more sense. We'll explore one of these policies in‑depth later on in this course. The third thing here is that resource‑based policies are meant to grant permissions to a specific principle for that resource. Now, you can allow all principles if you really wanted to, but it's not a best practice. But the benefit of these is you can lock it down to a single IAM principle or identity that you want to grant access to. Fourthly, all resource‑based policies are considered inline policies. For instance, with a bucket policy that gets assigned to what is called an S3 bucket, and if you delete that bucket, well, then that policy, that resource‑based policy, goes away with it. So it's the same type of a one‑to‑one relationship. There's no reason for the policy to exist if the resource it's attached to is not around. Last thing here, these are useful for granting cross‑account access because you can specify other AWS accounts as that IAM principle. So you can allow another account to interact with services and resources in your account by leveraging a resource‑based policy. Okay, that's going to do it. I think that's enough information for now. We talked about identity and resource‑based policies. In the next upcoming clip, we're actually going to explore a sample policy that's a real world example of what you might see.

Exploring an IAM Policy
Okay, let's dive into exploring an IAM policy. On the left side here, we have an example policy. Notice it's written in JSON, and this is a real‑world example of a resource‑based policy here. So let's break down the different fields that required an optional because you really have to understand how to read and interpret policies for the exam. The first key field here is version. The version is a required section, and it's the version of the policy language. Right now at the point of time of recording, this will always be the value you see listed here. If that were to change in the future, we would update this, but this has been around as long as AWS has been around. Next, we have the statement field. This is also required because this is how you define the permission statements, right? This is a list of permission statements as an object that you're trying to assign allow or deny. So moving on within the statement list, we have the SID, or the statement ID. This is an optional field. This is meant to be an identifier for the statements within your policy. So typically, a lot of people like to put human readable names in there, like you see in this example, Allow, List, and GetObjects, to make it easier to see what permissions are actually being applied. Moving down, we have Effect. So this is a required field as well. Are you allowing or are you denying access based on the other fields? Now, there are far more complicated values you can throw in here, and we'll discuss that way later on within the course, but for now, just understand that the overall use of this field is are you allowing or are you denying access? In this case, we have an allow statement. We then have the principal field. So this is going to be dependent on the policy type that you're creating. Now, this is a resource‑based policy, so we're going to have to put that in here. And in this case, what we're saying is, hey, this is the IAM identity that you are applying this policy to. In other words, who are you granting or denying permissions for? In this case, we have the ARN of an IAM user called cool_user. Next is the action. This, again, is dependent on the policy type. So what API calls or actions are you allowing or denying? Now, these API calls will also be dependent on the resources. In this example, we're using a service called S3. So S3 has their very own API calls that matter. You can notice the syntax, s3:ListBucket, s3:GetObject. Those are specific to the S3 API namespace. So, so far in this policy, we're allowing the cool_user IAM user these two actions. Moving on, what are we allowing those actions for? Well, the resources. So this is another dependent on the policy type field. And all it's doing in this is listing the AWS resources that you are applying the respective actions to. This field will directly impact the actions list that we just looked at. So these are S3 ARNs. So for permissions, we have to grant S3 actions. So that's why we have those two S3‑specific actions in the action field. And then lastly, there is a condition field. Now this is optional as well. And what this does is it allows you to put in special conditions that you can set in order to further customize your policies. Common examples for using the condition field are requiring MFA. You can pass in an external identifier, and you can even restrict calls from specific source IP ranges. Now, in this example, you'll notice the condition is that it's checking for a Bool. What Bool means is a Boolean value, so true or false. Now, one of the built‑in strings that we can check for is a MultiFactorAuthPresent. So this is saying, hey, as long as MFA is present in the call, in other words, it's true, then cool_user can list the bucket and get object for our two resource ARNs that we've specified. This is how all of the fields fit together like a big puzzle. Again, if you need to, please review this clip because this is extremely important that you understand what each of these fields do. You do need to know it for the exam, and you obviously need to know it if you're architecting solutions. For now, though, we're going to go ahead and end things here. Take a quick break, and whenever you're ready, we're going to dive into a demonstration lesson where I'm going to create our very own custom IAM policy.

Demo: Creating an IAM Policy
Okay, welcome to this demonstration lesson where we're going to work on creating a custom IAM permission policy for an IAM identity, so an identity‑based policy. Now I'm signed into my AWS Sandbox account, which if you have access to via the playground, feel free to follow along. If not, you can also follow along in your own personal account. Just make sure you clean up. So I'm logged in as cloud user. I'm in the IAM dashboard. And real quickly let's just view what we have. I've created a fake_admin_group, which was part of a previous demonstration. And in it, we have one user called fake admin. Now in this group, I've removed any existing policies, so we have no permissions right now. What we want to do is create our own policy and add it to this group and then test. So what I'll do is I'll navigate the policies. I'm going to click on Create policy. And we're going to do it via JSON. Now, you can do it via visual if you want, but I do recommend you learn how to do it via JSON because this transfers more to real‑world scenarios, especially when you're using infrastructure as code. So, what I'll do is I'm going to get rid of everything in here, and I already have a policy copied in a Notepad that I'm going to paste in here. Now, we're not going to look through all of this. You'll just have to take my word for it. But what this is is an IAM read‑only policy. So whoever this is assigned to can't perform any write or deletes, but they can read, get, enlist pretty much everything to do with IAM. So notice the format is the same we just reviewed. We have our version, we have our statement list of all of our actions, we have a generic statement identifier. We're allowing all of these actions within this list, and we're saying, hey, the resource is for every resource. Now, in this case, it won't matter because the only thing we have permissions for is IAM, but typically doing this is not a best practice. You really want to restrict it to specific resources. Again, however, for the sake of simplicity in a demonstration, we're just saying, yeah, every resource within this account, as long as these actions apply to that resource, go ahead and allow those actions. So what I'll do now is i'll skip down to the right. I'm going to click on Next. We'll give our policy a name. We can give it an optional description. We see what permissions are getting defined within this policy, and we can add optional tags. So let me go ahead and create this policy, and there we go. Now, what I'll do here is I'm going to open this in a new tab, and let's review this really quickly. We have our policy details. So it's a customer managed policy. Remember, there's AWS managed or customer managed. It gives us the creation time, when it was last edited, and we get an ARN, so we can reuse this and assign us to multiple identities. Now, right now, there's nothing that this is attached to. You can notice here under Entities attached there's nothing. But we do want to attach this. So I'm going to go to user groups, select my fake_admin_group, go to Permissions. I'm going to add a policy, and what I'm going to do is filter by customer managed to make this easier. Here we go, policy_read_only_iam, I'll attach this, and there we go. So now we want to make sure this works. So what I'll do is I'm going to sign out, and we're going to sign back in as that fake admin. I'll navigate to IAM, type in my username, paste in my password, and there we go. Let's test this out. So in theory, I should start seeing access denied for quite a few services, and I do here immediately. But I should be able to read IAM resources, which means I should be able to see things. So for instance, if I look at user groups I can see the fake_admin_group, but let's test this out. Let's go ahead and say I want to remove this permission. No, policy not removed because you do not have permission to do so. You are not authorized to perform because we're only allowing read‑only. Okay, that's going to do it. Super simple demonstration. However, hopefully you have seen how easy it is to create an identity‑based policy that's custom to your needs and attach it to an IAM identity and then test. Let's go ahead and in the demonstration here, and when you're ready, Ii'll see you in the upcoming clips.

Demo: Creating an IAM Inline Policy
All right, welcome back! I'm in the AWS Playground environment. And in this demonstration, we're going to create an inline policy. In a previous demo, we created a managed policy, which was customer managed. And this time we're going to create an inline policy so we can show you how they behave. So I'm going to navigate to IAM. I'm going to go to Users, and I'm going to create a brand‑new user. I'll call it remove_user just because I need to remember to remove this. And what we're going to do is we're going to skip giving this user access because I'm going to delete the user right after we create a policy. So I'm going to click on Next. We'll skip through the set permissions part. I'll review it and create it. Perfect. So we have no groups assigned. We have no permissions assigned. I'm going to go into this user. And what I'm going to do is under Add permissions, we're going to create an inline policy. Now, for this, I'm going to click on JSON, and I'm going to paste in the same permission policy that I used for a previous demo. It's just allowing read=only actions within IAM. I'll click on Next. We'll give it a name, and I will create the policy. There we go. We have our customer inline policy with our permissions attached to this user. Now you'll notice that I can click on this, and go ahead and review this and edit it as is, but it's not a separate resource. Notice we're still within the user itself that it's attached to. So what that means is if I cancel out of this and I go to policies, there's no inline policies listed because, remember, these only exist as long as the identity that they're attached to exists and they're not reusable. So they're not referenceable via this typical policy menu. With that being said, that means once I delete this user, that means the policy goes away with it. So now that inline policy is completely gone, we can't reference it, we can't look at it. Okay, that's going to be it. Again, a very simple demonstration. I just wanted to show you how to create an inline policy, how you can reference that policy, and then what happens when you delete the identity that the policy is attached to. Let's end here and then I'll see you in the upcoming clips.

Understanding AWS IAM Access Keys
All righty, let's dive into understanding AWS IAM access keys. What are access keys? What these are are a long term set of credentials for IAM users and the root user account. They're meant to be used to sign programmatic requests via a command‑line interface or an AWS SDK for programming requirements. These are primarily composed of two resources. There's an Access Key ID and a Secret Access Key, and we'll look at those in a moment. The big thing here is when you're creating these, that's the only time that the Secret Access Key is viewable. So you have to make sure you either download it or save it somewhere. Once you create the key pair, if you don't have that Secret Access Key, well, then it's not going to work. Now, within the key pairs, we mentioned there were two resources that make them up. The Access Key ID is the first one, and this is similar to a username. The next was the Secret Access Key. And this is essentially like a password for the access key ID that you were given before. You have to use both Access Key ID and the Secret Access Key together to successfully make any authenticated calls to AWS. With that being said, it means you need to protect these like you would protect any other set of credentials. These are crucial to keep protected. Now, let's move on and look at an example. This is a example of an access key pair. We have our key ID at the top and we have our Secret Access Key at the bottom. Notice how much longer and more complex the Secret Access Key is compared to the Access Key ID. Now these won't work, so don't go ahead and try and copy these and use them. These are fake examples. But the format and the syntax is correct. This is what they would look like when you generate them. Now, with that being said, let's actually in the clip here, we've covered what access keys are and what they look like, and in an upcoming clip, we're going to have a demonstration lesson where I actually create a pair of access keys for us to use.

Demo: Creating Access Keys
All right, welcome to this demonstration clip where we are going to create a brand new set of access keys, we're going to configure our AWS CLI to use them, and then we'll test it out. So I'm in the IAM dashboard here within my Playground environment. And I'm going to navigate to Users, select my cloud_user IAM user. I'm going to navigate to Security credentials, and we're going to navigate down to Access keys. Now, we already have one because these are generated for us via the cloud Sandbox in the Playground environment, but I don't want to use this. Instead, I'm going to create a new one by selecting Create access key. When you get in here in the console, they ask you to select your use case. Now, a majority of the time when you select one of these, it's just going to tell you, hey, there might be a better way to do this. We don't care about that because we want to get this demonstration done. So I'm going to select on CLI, I'll click on Confirmation, I understand there's a better way to do this, and I'll click on Next. We give the description tag a value. So this is a good idea to say what the keys are used for, etc. So I'll go ahead and put a sample value in here. I'm going to click on Create access key, and there we go. Notice again, this is the only time you can view the Secret Access Key. There's a big banner at the top there. If you lose it, well, then you have to delete these keys and create a new pair. So what I'll do is I'm going to copy these to a text file. And before I click on Done, notice, you can also download a CSV file with these keys within it. Now, this is nice if you're emailing the access key information somewhere or you need to store it somewhere secure. However, for our use case, I'm just going to click on Done. You get one more warning, and I click on Continue. Okay, it's that easy. So let's actually test these out. I'm going to transition to my terminal session, and in here, I'm going to configure our AWS CLI client to leverage these new access keys. So what I'll do is I'll run aws configure. You'll notice that it says, hey, what's your Access Key ID? I'll paste it in, and then it says, okay, well, what's your Secret Access Key? Well, luckily, like we said, we save these for use later, so I'll paste that in. And then we have some other information. Now, I'm just going to accept the defaults for all of these other ones, I'll clear my screen, and then I'm going to run a very simple command here, aws iam list‑users. Okay, it must be working. We see our two IAM users here that we have in our IAM console, and we can verify that if I go back, I go to Users, there we go, we have our two users that are also showing up within our CLI call. So our access keys are working. We can now programmatically access AWS via some type of application or command‑line interface. Now really quickly, let's see what happens when you want to get rid of or deactivate those keys. So I'm going to go back into cloud_user, Security credentials, and I'm going to find the newer access key pair that we use here, and I'm going to select on Deactivate. So now in theory, these are inactive, and if I go back to my terminal and I run the same command here, boom, an error occurred, the security token included is invalid because those credentials are now inactive or disabled, so we can't use them to sign any requests. So this is a exam pro tip, if you have a compromised set or suspected compromised set of credentials like this, it's quickest to go ahead and deactivate them quickly and make sure they're not being used somewhere else. Now, from here, we can then delete if we needed to, or if you say, hey, these actually aren't compromised, it was just a mistake, well, I can reactivate them, go back to my command line, run the same command, and there we go, we're back in the game. So that's how easy it is to activate, deactivate, etc. Now, with that being said, let's wrap up this quick demonstration lesson, and then I'll see you in the upcoming clips.

AWS IAM Credential Reports
Hello, and welcome to this clip where we're going to talk about AWS IAM credential reports. An AWS IAM credential report is simply a report that you generate and it allows you to download information within a list that contains all users within the account statuses of passwords for IAM users. You can see status of access key pairs. You can see who and who doesn't have MFA enabled. And those are just a few of the bits of information that these reports provide. The biggest thing about these is that they're very useful for auditing and compliance regarding IAM users. So if there's ever an exam question where it's saying, hey, we need an easy‑to‑follow list of the status of IAM users within your account, you'll likely want to use a credential report. A key piece of information regarding these reports, you can generate a new one every 4 hours. Now, when I say this, what this really means is you can generate a new one every 5 minutes if you wanted to, but the data itself will only be new every 4 hours. So if you generate a report at 12 p.m. and you generate another one at 2 p.m., the information will be the same. However, if you generate that first one at 12 p.m. and you wait until 5 p.m. to generate another one, then the information will now be new and current. So just keep that in mind. That's going to do it for this short clip regarding credential reports. Let's go ahead and end this here, and then in an upcoming clip, we're actually going t demonstrate generating one of these reports.

Demo: Creating an AWS IAM Credential Report
Okay, welcome to this demonstration clip. In this clip, we're going to go ahead and generate an IAM credential report and then view what it looks like. So just for sake of clarity, within this account, we have two users and a user group. None of the users have MFA installed, and cloud_user has one set of access keys that is currently active. Now, let me go back to the dashboard. I'm going to scroll down on the left‑hand menu here, and I'm going to find Credential report. I'm going to select this, and it's as simple as clicking Download credentials report. Now, I'll go ahead and download this, and then let me go ahead and actually open it up. All right, so it opened up in a new tab. I'm going to go ahead and navigate there. And you'll notice it's a simple CSV file containing information about our IAM users. We have the root account, we have cloud_user, and we have fake_admin. Now let me go ahead and zoom in to make this a little bit easier to read. Well, you'll notice each user has its own entry. So we have the ARN of the user, when they were created, do they have a password, when was it last used, do they have access keys, etc. So you can see this is a pretty comprehensive list of information specific to IAM users. So again, if you're ever getting an exam question where you need some type of audit for existing IAM users to see status of passwords or MFA, this is likely going to be included within that answer. Now, to test out that theory of the every 4 hours rule for new data, let me go back to my console. I'm going to go to my user. Let's go to fake_admin. And I'm going to create a pair of access keys and see what happens when we generate a new report. So I'm going to fly through this really quickly. Go ahead and click Done because I don't care to see that since I'm not using them. And now we have an access key pair for this user. So let me go back down to Credential report. I'm gonna download a new one. And I opened it up in a new tab again. Let me go ahead and pull that over. If I scroll through it, I'll zoom in again. Notice we have the same information. So if I go down to our fake_admin here, so this line, you'll notice we don't have any access keys as being listed as active because remember, you only get new data every 4 hours. So since we just generated a report, that data is only a few minutes old; it's not 4 hours old. So in theory, we would have to wait for 4 hours and 1 minute, generate a new report, and then now our access_key_1 would show up in this list. So that's a perfect example of what we mean by generating a new report every 4 hours. Now, that's going to do it for this demonstration clip. Let's go ahead and end things here, and I'll see you in an upcoming clip.

Module Summary and Exam Tips
All righty, welcome to this Module Summary and Exam Tips clip. Way to hang in there! Let's go ahead and have a few things that I think are pretty critical that you should remember before you go into your exam. First up, it is generally going to be the best practice to leverage IAM groups to pass permissions to multiple users at one time. These groups are meant to be used to simplify permission management. You can assign the same set of permissions to hundreds of users all with one simple action. Moving on from that, let's talk about some of the policy types that you should remember. First off, remember that policies themselves in IAM are JSON documents, and they are meant to dictate what permissions are allowed. You will need to know the difference between an identity‑based policy and a resource‑based policy. Remember, identity‑based are attached to identities within IAM. Resource‑based are attached to AWS resources. Building off of that, you also need to know the difference between managed policies and inline policies. Remember, managed policies are meant to be reused and attached to multiple entities. You even get your own ARN to reference them. In addition to that, you need to know the difference between an AWS‑managed policy, like that administrator access one we used, and a customer‑managed one, which is when you create your own. Understand the benefits and the cons of using each of those. Also, really understand inline policies. You need to know that these are a one‑to‑one relationship with the identity that they are attached to, and they're best for specific one‑off scenarios. Generally speaking, if you can get away with using a customer‑managed policy or an AWS‑managed policy, you're going to want to do that over an inline due to the simplicity of using them. Inline policies are a lot more of a headache to manage, but they are there if you need them. Next up, you must understand how to read and interpret a policy document. A perfect example is here on the left, like we reviewed earlier. Trust me when I say you need to know what each of these fields and sections do. Now, I'm not going to review them again line by line. Feel free to pause here or review that clip that we had earlier, but it is extremely crucial that you know what each of these does. So please make sure that you understand them. Moving on, remember that IAM users when they're first created within IAM have no permissions attached to them. What that means is they can't do anything. Now, when you don't have something explicitly assigned, that's known as an implicit deny within IAM. That means that they are denied any actions unless you say that they can do those actions. Building off of that, remember this order, and we're going to cover this several times throughout this course, but it is really important that you remember it so I like to bring it up several times. First, explicit deny, second, explicit allow, third, implicit deny, which is the default. What this is referring to is in case there are any overlapping permissions or just a policy in general. So anytime there's an explicit deny, it will always win, even if you have an allow with the same resource and permission, if there's a policy that is also assigned to that identity that has an explicit deny, then it is denied. The second order of operation was that explicit allow. Remember the default is to implicitly deny all actions, so if you explicitly allow them, then they can perform those actions. Moving on to permanent IAM credentials. Remember that Access Key Pairs are meant to be used for programmatic access, which means no console access. If you need to access the console as a user, you're going to have to have a username and password, which is basic authentication. You've seen this demoed several times when we log into our Sandbox environment. It's a general best practice and recommendation that you rotate both your key pairs and your passwords based on your own best practice guidelines. A very common scenario is to rotate each every 90 days. I've seen some organizations recommend longer. It's really going to come down to your own compliance requirements. However, it is best practice to rotate these on a regular schedule. And speaking of credentials, IAM Credential Reports. Remember that these reports can generate new lists with new data every 4 hours, and the data contained is going to be stuff talking about your users, status of access keys, and even MFA. However, it's important to remember any reports generated within 4 hours of each other are going to contain the same existing data from before. We saw this in our demonstration clip where I generated the initial report. We added a new access key to one of our users, and then we downloaded a new report, but the information was the same. In theory, we would have to wait 4 hours from that first report in order to get new data. Regardless of that, though, these are very good for IAM user auditing and compliance information gathering. Now, that's going to do it for this module. Thank you so much for hanging in there. Let's go ahead and wrap everything up. You can take a break, grab some coffee, some tea, maybe take a nap. And whenever you are ready, I'm going to see you in the next module.

IAM Roles
What Are IAM Roles?
All right, let's get started with this next module where we're going to get started talking about IAM roles. So what are IAM roles? The short definition is that an IAM role is simply another type of IAM identity that you can create within your account and give it specific permissions. So it's similar, to a certain extent, to IAM users. But there are some key differences that we need to cover. Here are some of the distinguishing concepts that you need to know for the exam. A role is similar to an IAM user, except that these are to be assumed by whoever needs access to them. So they're essentially reusable by a bunch of different separate principles. With roles, you don't have any long‑term credentials that are generated and used. Instead how they work is they have a set of credentials that are generated, which are temporary, and then rolling. Now when we say rolling, that means they're always changing, expiring, and generating new ones. These are extremely useful for delegating access to many different users, different applications, and even different services within AWS. You control the permissions for the role and the access to the role via permissions policies and trust policies respectively. So both of these policy types have to be used with the role, and we'll explore these later on. For now, let's talk about some of the IAM role types. There's a service linked role, which is meant to allow AWS services to access other AWS resources in a secure manner. There's an instance profile, which is a type of role or resource that gets attached to EC2 instances directly in order to allow the instance to use the role credentials. And then we have roles for federated identities. So with this, which we'll cover in far more depth later on under an advanced section, is simply when you allow external identities that are federated and authenticated through an external service, like Google, Facebook, Amazon, etc., and you allow them to assume the role. Now this is all done via a trust policy configuration, which, again, we're going to look at here upcoming in some future clips. Now here are three common use cases you should be aware of for IAM roles. You can use them to grant access to lambda functions to allow them to access other resources, like, say, a DynamoDB database. You can leverage them to grant access to your EC2 instances to allow them to interact with services like S3 to get objects that are stored. And you can even use them to set up a role which is going to allow another account to access your account, and this is very useful for granting access to verified third‑party vendors, like, say, an auditor, for instance. Now, don't worry if you're not familiar with some of these terms of these other services just yet; these are going to be covered within either this course or another course within this learning path for this exam prep. For now, just be familiar with the concept that they're used to grant access. Now, the key underlying fundamental concept for how a role works as far as credentials go, is known as AWS Security Token Service, or STS for short. AWS STS is literally the backbone of how these roles generate their credentials that are temporary and rolling. This service is what actually enables roles within IAM to request short‑term, temporary credentials for users. The credentials can be set to last as little as 15 minutes before they expire, or they can last as long as 12 hours. Now, there's a lot of steps in between there, but those are the minimum and maximum. The service is commonly used with SAML federation and OIDC federation, and OIDC is short for OpenID Connect, which is another authentication standard. Another key feature is that STS supports requirements for things like MFA and even other conditional access statements, which we will explore in upcoming clips as well. Now, speaking of STS and roles, let's look at a session. A session is what is generated when you assume a role and leverage those STS credentials. And with the session, this is the example of what credentials will look like. Notice we have an AccessKeyId and a SecretAccessKey, which is typically a long‑term credential, but the difference here is we have an expiration date and time at the bottom, and I've highlighted this session token. A session token must be present in order for you to use IAM role credentials like this. If this is not present, then those Access Key and Secret Access Keys won't mean anything and they won't authenticate correctly. So just remember that a session token is required for assuming a role. Now, that's going to do it for this introductory lesson where we start diving into IAM roles and STS. Let's go in in the clip here, and coming up in the next clip, we're going to look at role trust policies.

IAM Role Trust Policies
All righty, let's dive into IAM role trust policies. If you recall, there are two different policy types specific to a role. We have the permissions policy, which we looked at in a previous module. But then there's also the trust policy, so let's look at what these are. A trust policy for a role is another resource‑based JSON document, so similar to the other policies we looked at again in the previous module. However, the big difference with a trust policy is that they define which principles are actually trusted to assume the role. So you're trusting some principle. Thus, the name trust policy. Now, principles can be things like IAM users, it could be another IAM role, it could be an entire AWS root account, and it can even be other AWS services. So it's very, very flexible. The key thing here is that they are required for a role to be usable in the first place. If you don't have a trust policy set, then you're assuming that nobody is trusted to assume the role. Therefore, it won't ever be assumed and it's not ever going to be used. Now, let's actually look at a trust policy example. Again, it's a simple JSON document, and it follows that similar format that we've explored before. We see the version, we see the statement list, and within there is where it gets a little bit different. W,e have the effect, which is to allow, and the big thing here is the principle. So who or what are we trusting? So in this case, this cloud user IAM user is the principle that is trusted to assume the role. No one else can. It's locked down to just this cloud user. The other big difference in this resource‑based policy is the action. The action here you'll notice is sts:AssumeRole. That's because, remember, you assume the role itself to access those credentials and permissions. So this is the big API action you need to remember for assuming a role, so please make sure you remember this particular action. Now the last thing here is another condition. So just like the other resource‑based policies we looked at, these also support conditions. So in this case, we're saying, hey, is MultiFactorAuthPresent. If true, then go ahead and allow them to assume the role. Okay, that's going to do it. We just briefly covered trust policies and looked at a quick example on what they look like. Let's go ahead and wrap this clip up, and in an upcoming demonstration clip, we're going to create an IAM role and a custom trust policy.

Demo: Creating an IAM Role and Trust Policy
Okay, welcome to this demonstration clip. In this particular clip, we're going to go through, we're going to create a new IAM role, and then we're going to create a custom trust policy, which allows us, the cloud user, to assume that role. So let's go ahead and get started. I'm going to navigate to IAM. And once I'm here, the first thing I want to do is I'm going to go to users, cloud_user, which is me, and I'm going to copy this ARN for later in a Notepad here offscreen. Okay, now that I have that, the next thing I want to do is I'm going to navigate to IAM roles. Now in roles, I'm going to create a new one. So when you're doing this in the console, they do the best to walk you through it and make it as simple as possible. I'm going to explain each part, so if you do this via code or some type of automation, you understand what's going on. Now for this, we're selecting the first thing, which is the trusted entity type. So who or what are we allowing to assume this role? Now, for us, what we're going to do is we're going to select Custom trust policy. But let's quickly explain these other options. There's AWS service, which is exactly what it sounds like. You're allowing things like EC2 and lambda to assume the role. You can specify an entire AWS account, whether it be this one or another one. You can specify a web identity, which is an externally authenticated identity, so maybe a place like Google, Facebook, Amazon. There's SAML 2.0, which is typically used with corporate directories like Azure Active Directory, or even Microsoft AD. And then we have custom, which is what we're going to do. So this is where we specify manually within JSON who we want to actually assume this trust policy. So what we'll do here is I'm going to walk through this statement editor here on the right. So the first thing we see is that we're able to assume a role. So let's go ahead and skip this because we have the action we want, and you can see it's checked down here as well. Now, one thing to call out. If you're manually doing this, notice that there are different access level actions based on what type of trusted entity you chose. So, SAML, Web Identity, those are two different API actions that's important to note. For now, though, what I'm going to do is go ahead and scroll down, and I'm going to add a principle. So, under principal type, I'm going to actually specify an IAM user. I'm going to paste in that copied ARN for us, I'm going to add it, and there we go, you can now see here on line 8, we've entered our IAM ARN for the user. So in theory, what this does is it's allowing this ARN, which is us, our IAM user, to assume the role. So I'll go ahead, I'll go down, click on Next, and let's go ahead and add a different AWS managed policy. I'm going to look for AmazonS3FullAccess. I'll select it. I'll click on Next. And let's give our role a name and description. Okay, so we gave it a name, we gave it an easy‑to‑read description, we see our trust policy, we see the permission policy that we attached, and this could be your own customer managed if you wanted, but for sake of simplicity, we're using an existing AWS managed one. I'm going to skip tags, and I'm going to click on Create role. Awesome! Let's view it. We see all the details, trust relationships, which is based on the trust policy. Okay, well, how do we use it? We have it, but how do you log into it? Well, remember, you don't log in to an IAM rule. You assume it from your already logged in identity. Now, a nice thing here is that they actually give you a link in console so you can copy this, paste it and it should allow you to switch roles and assume it. So what I'll do here is I'll copy and paste that . You see it fills out the information for us automatically. Now you can do this manually as well. But obviously, I suggest if you're following along, click that link, and then leverage this auto fill. It's our account ID, which is what we're in. You specify the role name, so cloud_user_role. You can give it a display, and if you wanted to, you could give it a color. So we'll just give it orange. I'll click on Switch role, and just like that, we've now assumed that role, which means we've essentially assumed permissions that the role has. Now, it's important to note we're in this role now. So we don't have the same permissions as the IAM user that we were just logged in as. We only have the permissions that were assigned to the role. You can see our display name with our orange color, and we're getting a lot of access denied. So in theory, we shouldn't be able to access anything except for Amazon S3. And you can see, hey, you don't have any policy permissions applied for this. So instead, let me navigate to simple storage service, and within this service, we should be able to do whatever we want. And it looks like we're pretty good. It looks like I can list buckets, I can likely create a bucket, etc. Now, I'm not going to do this. You could just take my word for it that this will work. I just don't want to clean up. But we've now successfully assumed that role we created, we locked that role down to a specific IAM user ARN, and we tested the permissions. So we only could do the permissions that were assigned via the permission policy for the role. We no longer have the permissions of the user that we logged into initially. Now it's very easy to get right back. You can just go to here to the drop‑down, there's a switchback button, and then after this is done, we've successfully logged out of the role or unassumed the role, and then we can log back into the console as the cloud_user just like that. So now we're back to these existing permissions, which are completely separate. Okay, and with that, the demonstration is done. Hopefully you saw how easy it is to create an IAM role, create your own custom trust policy, and then assume that role. Let's end this here, and in an upcoming demo, we're actually going to demonstrate how you can set up a cross‑account IAM role.

Demo: Creating a Cross-account IAM Role
All right, let's jump into this demonstration clip where we are going to work on creating a cross‑account IAM role. What we see here is the overall high‑level architecture design of what we're going to create. We're going to have a source account, and then we're going to have a separate target account. Within the target account, we're going to have a target IAM role that's going to have a trust policy, which is going to allow our source account role, or our source role, to assume it. Once the target role is assumed, it's going to grant temporary credentials so that we can now interact with S3 via read‑only permissions. So the overall flow is our source role, which is going to have a permission policy to only allow it to assume the target role. We'll attempt to assume it's going to get a returned set of temporary credentials, and then we're going to use the credentials within the console to interact with S3 within the target account. So let's go ahead and jump into the console now. Okay, I am in my Sandbox environment. I'm logged in as cloud_user, and I'm in the IAM dashboard. Let's go ahead and let's begin creating the pieces we need within the target account. I'm going to navigate to Roles. I'm going to create a new role. And for the trusted entity type, this time, I'm going to select AWS account. Now under the options, we can do this account or we can do another account. So I'm going to choose another AWS account because this is a cross‑account role. You'll see we need to put in the account ID here. So what account are we trusting? Well, let me jump to my other private window here, and you can see I'm logged into a secondary account, so this is going to be the source account here, the one that starts with 5371. So what I'll do here is I'll copy the account ID, I'll go back to my target account, which is our 9750 account, and I'll paste in this account ID here. I'm going to click on Next, and now we give a permission policy. So I'm going to look for S3. I'm going to find read‑only access. I'll select it, and we'll go to Next. For the role name, I'm going to call it, just like it was in our architecture diagram, target_role. I'll give it a simple description, and then let's review really quickly. We're trusting a completely separate AWS account to assume this role. We gave it read‑only permissions, and now we can create it. Perfect! Let's go ahead and view it, and there we go. Now, the second part we have to do is we have to give our source role permissions to assume this role. So this is only one part of a two‑part equation. So, what I'll do here is I'll copy the ARN of our target role in our target account, I'll go to our source account, I'm going to go to roles, I already created the source role to speed this up, and what we'll do here is I'm going to go into our policy, and I'll make sure that this is correct. And it is. So I already prepopulated this, and I just wanted to double‑check. This is the same account ID as our target account, as you can see up here. So what we've done here in the source account for the source_role is we've given it permission to assume the role which has this ARN, which is in our target account. So what I'll do is I'm logged in as an admin within the source account, I'm going to assume this role. So what I'll do here is I'm actually going to open this in the window here so we can sign in and switch to this role in the source account. I'm going to switch my role, and now we've assumed the role within the source account. So the next step is we want to attempt to assume the role in the target account. So to do that, we'll do it manually. The first thing I want to do, though, is show you we are still in our source account as you can see here. And notice we're getting access denied for pretty much everything. And that's because our permissions policy only allows this role to assume the target role. So everything else is denied. So what we'll do is I'll click on this drop‑down, we're going to switch role, and I need to enter this information. So let me go to my target account. I'm going to copy the information here, and I'm going to paste it and then set it as needed. We give the account ID, we give the role name, and we can give a display name. I'm going to select a color, so it's different. And now I will switch the role. Boom! Just like that. We are now assuming the target role in a different account. Notice we are currently active as the target role in the target account. We're no longer interacting with our source account, even though we're signed in as that user, that original user. So now everything we do within this console is in the target account. And if you remember, we should only be able to read with an S3. So if I look at IAM, we should get a bunch of access denied. And if I instead go to S3, I should be able to read buckets and get everything, but I shouldn't be able to create a bucket, which we can test now. I'll skip down, accept the defaults, let's create. Nope, can't do it, we don't have the permission. So, perfect, we've now created a cross‑account role, we assume that role, and we just tested those permissions. That's going to do it for this demonstration. Let's wrap up here, and then whenever you're ready, we can move on.

EC2 Instance Profiles
Okay, let's dive into EC2 instance profiles. This is an important subject you must know for this exam. What is an instance profile? An EC2 instance profile is an AWS resource that you use to pass an IAM role to an EC2 instance. These are how you allow your EC2 compute to leverage the temporary credentials that a role generates and uses. When you create an IAM role for EC2 instances within the console, that actually results in automatic creation of the instance profile itself. The reason I point this out is because if you do this programmatically via CLI or some type of infrastructure as code, well, then you have to create that instance profile separately. After you create roles for EC2 and you start trying to create some instances, you're going to see a list of IAM roles within the console menu. Now really this list of roles, when you're creating the instance, is truly a list of the instance profiles that are associated with those roles. So that makes it a bit confusing in my opinion because they label it as a role, but in reality, it's truly an instance profile that's attached to the role. Now, if that sounds confusing, don't worry, we're going to have a demonstration clip coming up in a bit where we will actually look at creating an instance profile and using it. It's important to note that instance profiles are 100% required for leveraging any role credentials with an EC2 instance. You cannot use a role for EC2 without these profiles. Now, you might ask, well, why is that? Well, the reason is because of how the hypervisor works. The hypervisor that is controlling and creating and managing all of these EC2 VMs doesn't have any insights into inside of the operating system. So because of this, this is a way that AWS came up with to essentially allow the operating system and the hypervisor to work together in order to leverage credentials for an IAM role in a much more efficient manner. Now, we'll look at something called the metadata service later on within this course, and this is going to show you a demonstration on how these credentials are actually referenced within an EC2. However, for now, just understand that these are required to use your role with an instance. Let's talk about some use cases really quickly before we wrap things up. There are two big use cases that in reality could be transferred to a bunch of other types of use cases, but I just wanted to point two out, so you have a general idea of how these could be used. You could easily use them to allow an application running on your E2 compute to access Amazon S3 and download, upload, or remove documents that are stored as objects. You could also use it for maybe a message polling application that you have running on compute, which needs to go ahead and pull messages off an existing SQS queue. The long story short version of these use cases is that it allows your EC2 compute to easily and securely interact with other AWS services and resources. Let's go ahead. We're going to end this clip here. And then in an upcoming clip, we're going to actually demonstrate creating an instance profile and then using that instance profile.

Demo: Creating an EC2 Instance Profile
All right, welcome to this demonstration where we're going to go ahead and create and use an EC2 instance profile. What we're looking at here is a high‑level view of the architecture. We're going to create an IAM role, and with that, we're going to trust the EC2 service, which is going to create an instance profile for us. The main point of this role is we're going to permit it access to interact with our instance profile demo bucket that I've already created and set up for us. After the role is done, we're then going to create a brand‑new EC2 instance using some basic default settings, and we're going to attach this instance profile to it so then we can test the s3.GetObject command, which will be allowed via the role credentials, which are passed into the instance via the instance profile. So without further ado, let's jump into the console now. Now, first thing I want to do is I want to show you really quickly this bucket that I've already set up. I set this instance‑profile‑demo bucket up, and you'll see we have two files here, a demo_file, which is what we're going to give the role permission to download, and then we have a not_allowed.txt. So in theory, we won't be able to download this one based on our controlled permissions that we grant the role. So with that out of the way, let's begin. I'm going to navigate to the IAM dashboard. I'm going to go to Roles. I'm going to create a new role. And for the trusted entity type, we're going to select AWS service because we're trusting the EC2 service. Now on the bottom drop‑down here, we can see this service at the top because it's commonly used, so they make it easy for us. But you can see there are tons of other services we can use. We're not going to review these. I just wanted to show them. So I select EC2. We'll leave the default use case for this, and I'm going to click on Next. Now, for the sake of this demo, and if you're following along on your own, we need to attach one managed policy before we go back and create our own inline policy. So I'm going to search for SSM, and I'm going to attach this managed instance core policy. Now without getting too in‑depth, this essentially is going to allow us to connect to our instance securely via Session Manager, which is a service that will be covered later on within the learning path. So for now, if again, you're following along, make sure you select and attach this policy. I'm going to skip down. I'm going to go to Next, I'm going to give my role my name. I'll leave the description as is. We can see that we're allowing sts:AssumeRole for the EC2 service. So this is perfect. We've attached my required permission policy, and I'm going to create my role. Sweet! It's done. Let's go ahead and view it. And you can see all of our permissions, trust relationship. And the big thing here is since we did this in the console, notice we have our instance profile ARN already created for us. This is awesome. So remember when you create it in the console like this and you trust EC2, they automatically create the instance profile for you. So with that being said, if you're doing it via command line, programmatic access, or infrastructure as code, you would have to create this separately, which is fine. With that out of the way, let's go ahead and add the last bit of permissions. Since this is a one‑off use case, I'm going to create an inline policy, which is exactly what they're good for. We'll do it visually to make it easier. I'm going to select S3, I'm going to look at Actions and look at Read, and I'm going to find GetObject and select it. So I'm not allowing any other permissions except for GetObject. So this is the least amount of privileges necessary, which is a well‑architected framework goal. So I select GetObject, and then we go down to Resources. So what specific resource are we allowing access to? Now, we could do all, but that's not very good. We want to be more specific. So I'm going to select Specific, and let's add an ARN. Now we'll do it the visual way, and the first thing we need to do is specify the bucket. So let me go back to S3, copy the name, paste it, and then our resource object name. And if you remember, we're going to do demo_file.txt, wo we will allow the downloading of this, and then we're going to test it on not_allowed and we should get a permission denied. So I'll paste that in. We see the overall ARN that will go in the policy, and I click Add ARN. That's it. Now we click on Next, we give our policy a name. I click on Create policy, and we're in business. So now let's test this all out. I'm going to go ahead and go to my EC2 tab. I'm going to launch an instance. And let me just go ahead and fill out some of the basic info, and then I'll skip to the important parts. We'll select Amazon Linux, the latest AMI. We'll leave all the other stuff default. I'm not going to use a key pair because we're connecting via Session Manager. I'll leave Networking settings the same, and I skip down, and I'm going to go under Advanced details. So now you can see the IAM instance profile section. So in here, we're going to see our role because it created that instance profile for us. So by selecting this instance profile, we're saying, hey, EC2 instance, we're going to allow you to use the role credentials for the role we created earlier, and those role credentials will give you access to only download that object that we granted permission to. So after I select my instance profile, I click on Launch instance. Sweet! It's successful. I'm going to run and select it. And what I'll do here is I'm going to go ahead and pause and then cut forward about a minute or so until this is actually listed as running. All right, so it took about a minute. We're up and running. And now what I'm going to do is select Connect. I'm going to make sure I select Session Manager, which is a secure way to connect to your instance, and I'm going to connect to it. Sweet! We are now in our instance. You can see the instance ID at the top. I'm going to zoom in a little bit, and I'm going to CD to my home directory. So now I'm in my home SSM user directory. I'll clear my screen, and let's go ahead and test our instance profile. Okay, so what I've done is I've copied a command here, I'm going to paste it to speed this up, and all this is doing is leveraging the AWS CLI to point at the S3 service. We're running a copy command. We're pointing at the bucket, and then the file or the object, and then this is the local file name at the end. So now when I hit Enter, there we go, we've downloaded that demo file. So now I can cat it. And this is locally reading that copied file, and we see that simple string. Hello from Pluralsight! Nice job using the instance profile. So it's worked. Our EC2 has permissions to interact with S3. Sweet! So let's test that not allowed object. I'm going to copy in another command here. I'm going to paste it. It's the same command. The only difference is the object name at the end, not_allowed.txt. So now in theory, when I hit Enter, we get a forbidden because we're not allowed to do anything else but get the object for that demo_file object ARN. So this is working perfectly. And that's going to be it. Hopefully you saw how easy it is to create an instance profile attached to a role that we trust EC2 service to assume and use the credentials with. We created it, we attached it, and then we tested it. So let's go ahead and wrap this demonstration up, and whenever you're ready, I'll see you in the upcoming clips.

Module Summary and Exam Tips
Okay, way to hang in there! We've reached this Module Summary and Exam Tips clip. Let's go and dive in and review some very important information for you to remember for the exam. First up, IAM roles. Remember, an IAM role offers an assumable IAM entity for performing AWS actions. It's not like an IAM user where you log in; instead, you actually assume the role, which makes it shareable by a bunch of different IAM entities and identities. A major thing to remember here, roles leverage temporary credentials to allow you to grant access to other AWS services and resources. With these roles and these temporary credentials, the fundamental backbone of them is the AWS Security Token Service, or STS for short. This service is the whole reason that roles can even work in the first place. It generates these short‑lived temporary credentials that you can leverage until they expire. When you leverage those credentials, you're essentially becoming this role. What this means is anytime you perform an action with those credentials, it's being logged as the IAM role actually performing those actions. The last thing here, within the exam and even in the real world, you should be leveraging IAM roles whenever it is possible. You want to avoid any long‑term credentials in every possible scenario. So if you see an exam question where it's asking to choose between access keys or using an IAM rule, I'm going to go ahead and tell you you'll likely want to use the solution with the role. They're a much more secure way to provide access. With that, try to remember these three common use cases for roles. You can use them to grant access to lambda functions to allow access to other resources, like maybe a DynamoDB database table. You can grant access to EC2 instances via instance profiles to allow them to interact with services like S3. And even more importantly, you can actually set up a role and allow cross‑account access from another AWS account. This is perfect for use cases where you need to allow temporary access to some type of company, like maybe an auditing or compliance company. You can quickly and easily set it up, restrict their permissions, and then take it away right when they're done. This is the way you want to set up cross‑account access, avoid anything with long‑term credentials. Moving on, you also need to know about role trust policies. Remember, these define which principles are trusted to assume the role. It could be a specific IAM user, it could be another role, it could be an AWS service, and it could even be an entire other AWS account. They're also required to actually assume the role. Obviously, if you don't have anyone trusted to assume the role, then, well, you can't assume the role. So this is part two of an IAM role. You have the permissions policy, allowing what the role can do. You have the trust policy, allowing who can assume and use the role. Remember those two parts. With that being said, please understand how to read and interpret a trust policy. It's very likely you'll see at least one example on the exam, and you'll have to interpret what it's allowing or denying and any type of conditions that might be present. So please, if you need to review that clip, watch the material again, but please understand how to interpret these JSON documents. The big thing to remember here is the sts:AssumeRole action. This is the key API call that must be allowed in order to assume a role. Moving on, let's talk about instance profiles again. Remember, these get attached to IAM roles, and they're exactly what allowed the instances to even obtain the role credentials in the first place. Again, these are required for an EC2 instance to leverage an IAM role. You must remember that for the exam. You don't attach a role directly; you use the instance profile, which is attached to a role. And with that, we are done with this module! Way to hang in there! I appreciate your time. Let's go ahead, we're going to end this here, and then whenever you're ready, I'll see you in the next module.

AWS CloudShell
What Is AWS CloudShell?
Okay, welcome to this module where we're going to talk about AWS CloudShell. Starting things off, let's talk about what AWS CloudShell really is. AWS CloudShell is simply a browser‑based shell that allows you to interact with your AWS resources in a much more efficient and easier manner. It works by creating a pre‑authenticated session within the console using your current credentials. So it could be the credentials for an IAM user that you've logged in as, or it could be an IAM role that you have assumed. The really neat thing about this service is that it comes with a ton of pre‑installed software and different toolsets so that you can get started very quickly. They've installed a ton of common programs for you to use, and it really helps with quick, simple tasks, so you don't have to set up some other third‑party toolset. In fact, I recommend people use this when they're doing CLI stuff because it's a little bit easier to just go ahead and use it in the console as opposed to configuring your local CLI. Now really quickly, before we wrap this clip up, let's take a look at what the view looks like when you open it. We can see that we've authenticated as cloud_user based on the top right portion of our console screen. And on the left side of that, more toward the top center of the image, we see the CloudShell button. So when you click that button, this is what it would look like. It pops up on the bottom, the CloudShell console And you can see that it is exactly what I described it as. It's simply a terminal session within your console. You can see we have the region listed, and then we've even confirmed our caller identity. So we're saying, hey, who am I? Who am I authenticated as within the IAM service? In this case, we see it's cloud_user, which checks out because we're cloud_user on the top right portion of this screenshot. Now that's going to do it for this clip. The big thing to remember here is that cloudshell offers you a quick and easy way to interact with your AWS resources via a CLI session all within the console. Let's go ahead and end things here, and I'll see you in the next clip!

Demo: Using AWS CloudShell
Okay, welcome to this demonstration clip. In this clip, we're going to use and look at the AWS CloudShell service. So I'm in my Sandbox Playground environment. I'm logged in as the cloud_user, which is an IAM user, and the first thing I'm going to do is I'm going to find the CloudShell button and I'm going to click it. And just like that, we now have a CloudShell session within our browser. So now if I run that get‑caller‑identity command, we can see, once again, that IAM cloud_user. So this is using the credentials that I have signed in with with this account. So now I can perform any actions that I normally would be able to in the console, but I can do it within this command‑line interface. So let's go and test. Before we jumped in here, I set up a few different resources so that we can verify how this works and just test the permissions. So in IAM, I created three additional users, as you can see here. And in EC2, I created two additional prod servers. So what we'll do is within our CloudShell session, we're going to check to see if we can see all of those resources. So the first command I'll do is I'm going to list all of the users. Sweet! We get every user that we have within IAM, and currently there are four. So that works. Now, let's go ahead and list our EC2 instances. I'll paste in my command here where I am describing instances, and there we go, we have our two instances that are described. So this is working really, really well. But can we actually create users? Can we write actions? Or can we only read actions? Well, let's test this out. I've created a very simple script, and this will be attached to this module's resources. So if you want to use it, feel free to use it. But all it does is it creates three additional IAM users. So to test it, let me go to IAM. I'm going to delete these existing users. I'll go back to CloudShell. We'll list users again just to make sure it's verified. There you go. And then I'm going to run my script here. We're going to get some output, and there we go. We've now created three additional users using our credentials for cloud_user all within this CloudShell session. So now if I go back into IAM, I refresh, there we go, we've now created three additional brand‑new IAM users. So within CloudShell, we've verified that we can do the actions we're permitted to do via console, but now we've done it with CLI credentials instead. And there we go. So that's going to do it for this demonstration. I hope you've learned quite a bit. It's easy to see that CloudShell is a very quick and easy way to interact with AWS while you're in the console, as it helps speed up command‑line interactions. Feel free to log into your own account or use the Sandbox environment if you have access and play around with CloudShell all you want. It's a pretty powerful tool, and it's really nice to have. But with that being said, let's go ahead and in here and then I'll see you in an upcoming clip.

Module Summary and Exam Tips
Okay, thanks for watching this very short module on the AWS CloudShell. Let's have a very quick module summary, and just a couple of exam tips that you should remember for your exam. First up, remember, CloudShell is a browser‑based shell that allows you to interact with your AWS resources. This shell is pre‑authenticated using your console credentials. That could be an IAM user. It could even be an IAM role that you've assumed. This shell comes with plenty of pre‑installed software and tools to help you get started quickly. And an exam pro tip regarding that last point, the AWS CLI is pre‑installed and obviously pre‑authenticated for you to use within this service. This is one of the easiest ways to get started with the CLI. You can be logged into the console, open a browser‑based shell session, run some commands, and you can verify by clicking around in the console. So just remember that for the exam if you need something quick, pre‑installed, pre‑authenticated, think AWS CloudShell. Now, that's going to do it for this module. Go ahead and wrap things up whenever you're ready, take your break if you need one, and then I'll see you in the next module!

Amazon Virtual Private Cloud (VPC) Overview and CIDRs
Amazon VPC Overview
Hello, and welcome to this next module where we're going to talk about Amazon Virtual Private Clouds. This first clip is going to be about an overview regarding a VPC. So, what is an Amazon VPC? An Amazon VPC is simply a logically isolated part of AWS Cloud where you're able to define and control your very own networking. Within a VPC, you have complete control of your virtual network. This includes the IP address ranges that you assign, the subnets that you create, the different route tables and the route customizations, and even the different network gateways that you can attach to your network. An example of a network gateway would be a VPN, for instance. The condensed summary of a VPC is you need to think of it as your very own virtual data center network within the cloud. You control it, you own it, it's isolated to you. There are two types of VPCs that you need to be aware of. There's a default VPC, which is automatically created by AWS when you create your account. The default CIDR, or the IP range of this VPC, is listed here. There's also a custom VPC Now, these are what you have to know how to create and how to manage for this exam. A custom VPC is created by you, and you specify a custom CIDR or an IP range for that network. These give you much more granular control over all of the settings within a VPC. Really important thing to note, never use the default VPC for anything but testing when you're starting off. Everything within your architecture should leverage a custom VPC that you build and manage. Now, I want to throw six different important components that you're going to have to be aware of for this exam. Now, don't worry if you don't know these just yet, we will cover these in upcoming clips within this course, but I want to get these terms out in front of you so you see them and you know what they are when they come up. Within your VPC, you create the CIDR block, so the IP range. You create different subnets to host your different compute. You'll create different route tables to customize routing of your network traffic. You're going to create something called a security group, which is essentially a virtual firewall for compute. You're going to put in Network Access Control Lists, or NACLs for short. And these are virtual firewalls at the subnet level. And then we also have network gateways. So these are going to include things like a VPN gateway, an internet gateway, and a NAT gateway. Now there's much more information coming your way here in upcoming clips, so don't worry about these just yet. Again, I just wanted to get the terms in front of you so you know what they are when you start seeing them. Moving on, let's talk about things that you should know when you're creating a VPC. When you're creating your custom VPC, you actually select the IP CIDR block, in other words, the range of IP addresses that you want to use for the network resources in the VPC. A big thing to know is that private IPv4 CIDR blocks are required whenever you're doing this, and they can range from a /16 to a /28 address range. IPv6 CIDR blocks are optional, and these can range from a /44 to a /60. Now, when you're designing your CIDRs for your VPC networks, they have to fall within the RFC 1918 range. What this means is essentially private IP space. So you can see the three normal and loud IP spaces that typically fall within the private IP space category. So when you're designing your networks, keep this in mind, they have to fall within one of these three network ranges. Last tip here, please carefully consider your IP planning when you are creating a VPC. I've been in organizations where they don't necessarily plan ahead too much and they assign too many IP addresses to a specific VPC, and then it becomes a big problem where they're trying to reclaim space because they've given out too much and they've wasted their internal IPs. So please carefully plan out your subnets and your entire networks before you deploy them. Now, a couple more things to call out before we start looking at IP spacing more in depth. You need to understand Amazon VPCs are regional resources. They only belong in the region you deploy them in. They're not global, they don't span regions, they are locked to one individual region. With that in mind, there is a soft limit of five VPCs per region per account. You can request this get raised, but to be honest with you, if you need more than five VPCs in a single region, you're probably architecting your account structure incorrectly. So before you request this limit, really take a step back, think about your planning, and see if you really truly need more than five. So let's look at three IP ranges that are considered private. Again, these fall into that RFC 1918 range. The first is going to be within the 10.0.0.0/8 range. We have that 172.16.0.0/12 range. And then the third is the 192.168.0.0/16. Now, on home networks, you've probably seen that last one before because that's usually the default IP space range for a private network, especially on a home router. The first one here, the 10.0 range, is a big one within enterprise organizations. I would reckon to say that you'll probably see that more often within your work than any other range. Remember, these are private address spaces. That means they are not publicly resolvable. That's very important to remember. You cannot resolve these addresses over the internet. Next up, let's break down an IPv4 CIDR example. You must know how to do this at a high level when you're planning your VPCs, and it could come up on the exam. So an IPv4 range is made up of 32 total bits. Each of the colored portions here that we've highlighted on the CIDR make up 8 bits. So, the 10, the 0, and the second octet, the third octet 0, and then the 4th octet 0. Basically each number separated by those dots. You will need, again, to know how to calculate correctly for your CIDR, so please pay attention to this part. This /16 on the end here actually indicates the amount of bits or the number of bits for the subnet mask. So, /16 takes up two full octets, which translates to 255.255.0.0.0 for the network address. What that means is that the first two octets are completely used for the networking address space. So in this case, the 10.0 are used for network addressing, so they're not usable within the network. What that leaves us are these last two octets, right? So there's 16 remaining bits for us to play with to calculate usable IP space. Now, to calculate the number of usable IP space within this network, you're going to go ahead and take 2 to the power of the number of remaining bits. So in this example, we have a /16. So out of 32 total bits, we have 16 that are used for the subnetting. So we're going to subtract 32, ‑16, and that leaves us 16 total bits for addresses. So then we take 2 to the power of those remaining bits, which is 2 to the 16th power, and that gives us 65,536 IPs that we can use with this specific CIDR range. Again, please take the time to practice this and calculate usable IP space because it could come up on the exam. Now that's going to do it for this introductory first clip where we went over VPCs and talked about what to expect when you're creating them in the address space. Let's go ahead and in here and then I will see you in an upcoming clip.

Demo: Exploring the Default VPC
Okay, hello and welcome to this demonstration clip where we're going to look at the default VPC. This will be very short. Again, I just want to show you what it looks like when you first have the default VPC in place. So when you create your account and you activate one of these regions, if you go to the VPC service within your region, you'll see under VPCs, we're going to have one in place already. This is the default VPC. You can see it even categorizes it as default here, and notice the IP CIDR that's available, 172.31.0.0/16. So this is created for you automatically with all of the default settings to easily spin up compute and start messing around with the networking within VPCs. Now, a big thing to call out here is anytime you deploy any compute within this VPC, by default, it receives a public IP address. So that means it's publicly reachable and publicly resolvable on the internet. That's why it's never recommended to use the default VPC because of severe security consequences. So within this VPC, there are tons of different resources that are attached. You have a route table, which classifies simple routes. You have a Network Access Control List, and you have a DHCP option set. The route table is going to have simplified routes. You have local routes and you have one that routes to something called an internet gateway. So this allows traffic to the internet. We'll explore internet gateways later on, so don't worry about that right now. Just understand default VPCs automatically allow internet access. In the network ACLs, we see the default access control list, and we see that it by default allows all traffic inbound and outbound. So there's really no rules in place. It just allows in and out however we want. It's also associated or applied to every subnet that comes along with this VPC. We'll look at subnets here in a minute. The next thing was the DHCP option set. So this is going to specify settings related to DHCP within your VPC. You can see the internal domain name, so any compute will receive this domain name. You can talk about what domain servers you want to point to. You can look at the owner of the DHCP option set, which is this account. And you can even set up other specifics, like customizing NTP servers and NetBIOS name servers. There's a lot of stuff you can do with these. The last thing I want to look at here was the subnets. You also get plenty of subnets. Notice the IP ranging is automatically created for us. These are /20 address spaces. What that means is we have 4091 available IPs. Now you're probably wondering, well, why isn't there 4096? And we will get to that in an upcoming clip in another module. There are some specifics to subnetting that you must know, and this will be covered later on. The big thing I just wanted to call out was that, remember, you have a default VPC within each region that's active, and it has public internet access. Again, never use these for anything critical. They're okay for testing really quick, small things, but they should never be used in production. With that being said, let's end this quick demonstration and I will see you in an upcoming clip.

Demo: Creating a Custom VPC
Okay, hello, and welcome to this next demonstration clip where we're going to create just at a high level, our very own custom VPC. Now we will create custom VPCs later on in upcoming clips throughout this course, and then we'll customize them even more in depth, but I just want to show you the overall process to even create one in the first place. So what I'm going to do here is I'm in the VPC dashboard. We see our default VPC. I'm gonna click on Create VPC. Now, when we're doing this the first time, I recommend that you only create the VPC and you don't do the VPC and more. You need to know how to create all this stuff separate from just this GUI, so in my mind, it's good to practice creating them all individually and then linking them together. I believe you will really understand at a greater detailed level how everything plays together. So for this demo, I'm going to select VPC only. We can give our VPC a name. We can scroll down. We'll select IPv4 CIDR manual input. So this is where we say, hey, what IPv4 space do we want to use? So we enter one of those valid ranges. We can allocate an IPv6 CIDR block, and with this, we can use some Amazon provided ones, or you can bring your own. What we'll do is say no for now. We'll have default tendency because we don't need hardware dedicated to ourselves as a single customer. But if you needed to, for compliance purposes, you could have a dedicated tendency as the default so that any compute is logically isolated to its own hardware and network, so it's not sharing anything with other customers. Now, there are prices associated with that. We don't really need to worry about that right now with this exam, so I'm going to select default. I'm going to click on Create VPC, and there we go. So check this out. We've created our new VPC. We gave it a name tag, but really you need to understand you should refer to your VPCs via the VPC ID. This is how you reference them in any infrastructure as code and any scripting. It's very rare that you'll ever just reference the name unless it's just within the console. Now, that being said, let's look at some of these details. We have our ID, we have our tendency. It's not the default VPC. It is available, and let's look at some of the resources that were created. It created our IPv4 space for us. We gave it a 10.0.0.0/24. And we have three primary resources that were created automatically. We have our DHCP option set, which we looked at with the default VPC, but this one is specific to our custom VPC. It created a main or default route table, and it created a main default Network Access Control List. Now, a cool thing here in the console is it gives you a resource map where you can see some of the resources that are a part of this VPC. And you can look at the CIDR. See we have the IPv4 space of 10.0.0.0/24 that is associated with the VPC. Okay, that's going to do it. I know that was a very short demo. I just wanted to show you how to create a custom VPC. And don't worry, in future clips I promise we're going to get much more in detail in upcoming modules where we start playing around with all of the different resources within a custom VPC. But for now, if you're following along, congrats on creating your first custom VPC. Let's end this here, and we will continue on with another clip.

Module Summary and Exam Tips
All right, way to hang in there. We're at the Module Summary and Exam Tips clip. This is a very short module, but we just wanted to introduce the concepts of VPCs before we start diving in in the upcoming modules. Let's talk about some of the important concepts you should remember for the exam. Remember, a VPC is a logically isolated network in the AWS cloud where you can define all of your own configs. You specify and must specify private IPv4 space, and IPv6 space is optional. Remember when you're defining your IPv4 CIDRs, they have to be between a /16 and a /28. Valid private IP space for your CIDRs are the following, 10.0.0.0, 172.16.0.0, 192.168.0.0. These are all private IP address spaces that you need to be familiar with. And remember because they're private, these cannot be publicly resolved. They only work internally. Last thing here, remember that VPCs are regional resources. They only span the one region where you deploy them. You also have that soft limit of five per region. And if you're approaching that, remember to take a step back and really think, am I planning my VPC architecture correctly if I need more than five? Moving on, big point here, you must know how to read CIDR block notation and calculate the number of hosts. Please be familiar with this process. Re‑watch that clip if you need to in the beginning where we broke this down, but make sure you understand how this works. Last thing here, I wanted to talk about default resources within a VPC. Now, we're talking about custom VPCs here. And remember when you create a custom VPC, you will get a default DHCP option set. This is used to configure DHCP options, so things like domain names, your different NTP servers, and the DNS servers you want to use for resolution. You can customize these or leave these as the default. You also get a main route table, which is a default route table, and it's going to come with a primary local route for VPC traffic only. The last thing here is the main NACL or the main NACL, and this is also the default access control list. You get two simple rules for inbound and outbound. And if you remember, it's essentially allowing all traffic in and out by default. Now, that'll do it for the module summary and the exam tips. Please remember all of these points before you move on and before you take your exam. But for now, let's end this module here, and then I will see you in an upcoming module.

VPCs: Subnets, Routing, NACLs, and Security Groups
VPC Internet Gateways
Okay, let's dive into this module where we're going to talk about Amazon Virtual Private Cloud resources, more specifically, subnets, routing, NACLs, and security groups. First up, we want to talk about VPC Internet Gateways. An internet gateway is simply a horizontally scaled, redundant, and highly available VPC component which allows you to communicate with the internet from within the VPC. It's pretty simple, it's exactly what it sounds like, it is a gateway to the public internet. Now there are some concepts you need to know for the exam, so let's go ahead and review those now. First things first, you need to understand that the internet gateways support both IPv4 and IPv6 traffic. So both of the IP range types that are supported by the VPC are supported by the actual internet gateway. The big thing with an internet gateway is it automatically scales for all of your traffic needs and it offers high availability within that region. You create it, you associate it with your VPC, and you're off to the races. The big thing to remember is that it allows public subnet resources to directly connect to the internet. You have to have a publicly resolvable IP address to directly work with an internet gateway. Now, in upcoming clips, we will have some resources where we can configure private compute to connect to the internet, but that's for a little bit later down the road. The big thing here is you have to have a publicly resolvable resource to directly work with an internet gateway. How they work is they simply give you a target within your VPC and you can route any internet bound traffic to flow through it. For instance, maybe you're on an EC2 in a public subnet and you want to route to pluralsight.com. Well, by specifying that all other traffic or all default traffic should flow through the internet gateway, you can easily do this. Another thing to keep in mind here, internet gateways typically are created separately from the VPC. Whenever you're creating a custom of VPC, which is the one you should be doing now, you're going to have to create a separate internet gateway for it to work. Within the default VPC, this is done for you, so that's why we recommend you actually work with custom VPCs. Another really key point is that they are only attachable to a single VPC at one time. Now, you can detach them and then reattach them to other VPCs, but they're only usable and attachable to a single VPC at that given moment. So keep that in mind, you can't share them between VPCs. Now, before we wrap this clip up, I want to give you a very high‑level architecture view of how these internet gateways work. So we have a simple VPC here, we have two different subnets, there is a private subnet on the left and a public one here on the right side. Now within those subnets, there are EC2 instances deployed. The one in the private subnet only has a private IP address. The one in the public subnet has both a public and a private IP address. So what we do here is we have the internet gateway attached on the right side to the VPC. Now, since the private subnets don't have publicly resolvable IPs, they cannot directly use the internet gateway. Our instances within our public subnet, however, do have publicly resolvable IP addresses, which are dictated or indicated at the top there, 52.46. So since they have a public IP address, they can use the attached internet gateway. The internet gateway will handle the traffic, resolve the addresses for us, and then send the traffic out. And it's that simple. I know it probably seems like it should be more complicated, but it's really not. You have to have a publicly resolvable IP address, then you can directly work with your internet gateway. Now, that's going to do it for this clip. Let's go ahead and wrap this up, and then we'll talk about subnets and other resources in upcoming clips.

VPC Subnets
All right, let's dive into VPC subnets. Subnets are a critical component within your VPC architectures. You have to know how to design them, you have to know how to create them and even secure them. So let's dive into some important concepts with them. The biggest thing here is that a subnet is going to be a range of IP addresses within your VPC so that you can host resources within there. Another key concept is that subnets are bound to a single availability zone, they cannot span multiple AZs. This is a very common exam scenario, and you do need to know this point. Now with subnets, you do get support for IPv4, IPv6, and you can even set them up to support dual stack. Now, if you're not familiar with that, dual stack is simply using both IPv4 and IPv6 at the same time. Lastly, there are four common types of subnets you need to be aware of for this exam. The first is public, which is going to allow public resolvable addresses to be used. There's a private one, so internal only with private IP space. There's VPN‑only, which is a little bit more specific and we'll cover that on later in a different module. And then there's isolated. Now, isolated means you're going to have no connectivity to anything, it's literally isolated. So isolated subnets are going to be good for things that are extremely critical and they need no network connections to actually work. Moving on, I need to talk about a special scenario within AWS networking. AWS actually reserves five IP addresses per subnet. What this means is you need to plan your subnet spacing accordingly. So if you recall in a previous module, we did the math on calculating the total number of available hosts within a CIDR, and I told you that there are some special conditions specific to AWS VPCs that you need to know about, and, this is that special consideration. Every subnet is going to have five less available host IPs than you would really calculate using the default calculation we practiced. Now, an example here to break down is let's assume we have a /28 subnet CIDR. So that means you really only have 11 available addresses. Typically it would be 16 based on the math, but we have to take away those 5 reserved spaces, so really we have 11 usable IPs. This is exactly what I mean when you have to take it into consideration; 11 usable IPs compared to 16 is a pretty drastic difference. Now, moving on, and let's actually talk about what those reserved subnet IPs actually are. Let's assume in this example we have a VPC CIDR with the CIDR range listed, and then we have a subnet with a /24 CIDR. Well within this subnet, here are the five reserved IPs and their use cases. The first is the network address, so, this is kind of typical to any other on‑prem network, you usually reserve that first one for a network address slot. The next is the VPC router. So within each subnet, there's actually a logical router behind the scenes that directs or routes your traffic within the subnets. Now, that router leverages this next one, which is the third reserved IP address, and that's the VPC DNS server. A lot of the time this is referred to as the +2 address. Now what this does is exactly what it sounds like, it's an Amazon‑provided DNS server that resolves traffic based on routing rules and other DNS configurations. So, this VPC router leverages that DNS server to say, hey, where do I send my S3 traffic? The fourth one is a future use address. Now, this has been labeled as future use for a very, very long time, and there are no indications on what this is actually going to be used for, but, it doesn't matter, it's reserved and we can't use it, so you need to know it. The last one is the broadcast address. Now, VPCs do not support broadcast traffic. Remember that for the exam, they do not support it. So with that, AWS just said, hey, I'm going to go ahead and reserve this, you can't use it anyways, let's just take it out of the calculation. So really take the time and remember that there are five addresses that are reserved, and do your best to remember what each one does. I can tell you right now the very two big ones here are going to be the router and the DNS server. Out of all five of these, you should really be familiar with those two. Moving on to public and private subnets, these are the two most common subnet types within an architecture. Your public subnets are going to be subnets that have a direct route to the internet via that internet gateway we just reviewed in a previous clip. These essentially require publicly resolvable IP addresses to be considered public. You also have private. Private is the complete opposite. You do not have a direct route to the internet, and it requires some type of NAT device to actually access the internet. These subnets will only assign private IP addresses from your VPC CIDR to any resources that are deployed. Now with that being said, let's look at a very high‑level subnet architecture diagram. We have our region, we have our VPC within our region, and notice we've split this between two different availability zones, us‑east‑2a and us‑east‑2c. Now within each of those availability zones, remember, you can only have a subnet span a single AZ, so these four different subnets are bound to their respective availability zone. And, because of this, if you remember the well‑architected framework you want to design for high‑availability and redundancy, that means we need to use multiple subnets spread across multiple AZs to achieve high availability. This is because if one of our AZs go down, we still have networking compute in the other AZ because we have subnets deployed there. So that's a very high‑level diagram for an architecture on how you can design subnets. Now this is very simple, it's just public and private, and these can get very complex. And, speaking of complexity, let's move forward to another example here called a multi‑tiered architecture. Now you're going to hear this throughout the course where we talk about a multi‑tiered approach. These are very important that you understand what multi‑tiered VPCs are. A tier within a VPC is meant to represent a logically isolated section of your network. By having multiple tiers within your design, it allows you to achieve a more secure network by using a security‑in‑layers approach. What this means is you can further restrict which layers are actually able to communicate with each other. So the web app, you could allow to the public, and then the database and application layers, or tiers, could be locked down to internal traffic only, so they're not public. So, multi‑tiered VPC designs are a best practice that you absolutely should follow. At minimum, you should have two layers of designs within your VPC. To make it even simpler, multi‑tiered means that you have two or more layers of subnets that are split between availability zones. The most common design that you'll probably see on the exam and in real‑world diagrams will be a web tier, which is public facing, an application tier, which only allows traffic from the web tier, and then finally, an internal database tier, which likely will only allow traffic from your app tier security groups. So you can see how that layered approach works. Now another important thing to call out for subnets here, they can communicate across availability zones automatically due to how routing works, and we're going to cover routing and route tables here coming up soon, but just understand subnets can communicate with each other, both in the same AZ and cross AZ automatically. The big thing to remember, however, is that there is a cost associated any time you're going across availability zones or across regions. To simplify this even more, remove all of the noise on the diagram. This is a three‑tiered network design. What we mean by this is we could classify these web subnets, the public web subnets as Tier 1, the internal private app subnets could be Tier 2, and then our third tier would be our internal private database tiers where we host our database resources. So, hopefully that makes sense. You really need to understand multi‑tiered designs within VPCs when you're designing them. So really make sure you review this and take the time to study it if you really need to. Now with that being said, I think that covers it for subnets. We talked about CIDRs, we talked about reserved IPs, public and private, and then we just wrapped things up with multi‑tier. Let's go ahead, we're going to end this clip, and then in the upcoming clip, we're actually going to look at routing with VPC route tables.

VPC Route Tables
All righty, next up, VPC route tables. Route tables are a resource within a VPC that contain rules, or otherwise known as routes, that tell your network traffic where to go. So, these route tables are going to work hand in hand with that VPC router within your subnets. Now let's look at some route table types. There are basically two types that you need to know for the exam. The first is a main route table. Now, if you remember in some of the demonstration clips when we created our VPCs or when we used the default VPC, there was a main route table already there, and that's because this automatically comes with your VPC when it's created, and it's meant to act as the default route table for anything that's left over, so, any subnets that are not specifically associated with a different route table. You can essentially think of this as a catchall for routing. The ones that you need to really know and understand are custom route tables. These are route tables where you fully define all of the different rules, and you have to associate them with the subnets that you want to leverage them with. So when you associate them with the subnet, those subnets use their internal VPC router to look up the routes based on these route tables. Hopefully it's all starting to come together. Let's look at some concepts in terms that you need to know for the exam and really any real‑world scenario. Within a route table you have what is known as the destination, and this is what it sounds like. It's the range of IP address space where you want to direct your traffic towards. You have the target. Now this is a little bit different. The target is going to be a different gateway, network interface, or some other network connection where your destination traffic would go. An example of this would be an internet gateway. So for instance, what you could say is you could have a default rule for all traffic to go out the internet gateway as the target. The all traffic would be the destination, the target would be the internet gateway. You also have a local route. Now these are not removable, and they're there by default for every route table. What this is is a local route for VPC bound traffic; in other words, any of your internal IP space within the VPC. The last thing here is an association. This is what you do when you want to leverage a route table for a subnet, you associate the subnets with the custom route table and that then applies those chosen rules for any matching network traffic in the subnet. Do me a favor, do yourself a favor, really understand how these four parts play together within route tables, they are very important. And speaking of important, generally, it's going to be best practice to have a 1:1 relationship for your subnets and your route tables. Now there are plenty of exceptions for this, and it's not the only rule to follow, but it is one of the most highly recommended rules to follow. It makes it a lot easier to be a lot more complex or strict with your routing rules, which could in turn lead to better security habits. Now let's move on to an architecture diagram. You probably remember this architecture, it was used in our VPC subnets architecture diagram, but this time, we're going to build off of it. So a similar structure, but we're going to remove one of the AZs for simplicity. So we still have our public and our private subnets in our availability zone, but now we have our VPC router and our internet gateway. Remember, the VPC router is part of the subnets as its reserved IP space. The internet gateway is attached to the entire VPC and can route internet resolvable traffic. So what we would do here is we would create a route table and contain or define our routes to where we want to send our traffic. So the destination here is 0.0.0.0/0, and the target is the internet gateway. So with this, our VPC router can look up that route because our subnet is associated with that route table. It says, hey, for any traffic, I'm going to send it to the internet gateway, and it handles all of that for you. So in this case, that traffic goes out, through the VPC router, through the internet gateway, to the internet. Now remember, again, it's best practice to assign a custom route table to each subnet. You want a 1:1 relationship for the most part. So for this example, our private subnet would have its very own custom route table with its very own custom routing rules. Also remember the main route table. If you do not explicitly assign a route table to a subnet, then the main route table will be assigned by default. Now that could be good or it could be bad, but regardless, just keep it in mind that this is how it works. If you do not assign a route table, the main route table is assigned by default. Okay, I think that's good enough for this architecture diagram review. Hopefully you've learned a lot about route tables and how they work with subnets. Let's end things here, and coming up after this is going to be a demonstration clip where we put these two different resources together and start testing things out.

Demo: VPC Route Tables and Subnets
Hello, and welcome to this demonstration clip where we're going to work on creating and setting up our very own custom VPC route tables and subnets. Before we dive into actually doing all of the work, let's have a quick architecture overview on what you can expect. Within the console, we're already going to have a VPC created for us, and that's going to be it as far as pre‑existing resources go. From there, we're going to work on creating our route tables and our subnets and then testing different traffic flows. Now, when we create the subnets, we're going to create public and private subnets in two different availability zones for a high availability. Now, when we create the resources for both the public and the private subnets, the private subnet route tables are only going to have a local VPC route for the VPC CIDR traffic. There will be no other routes so it won't know how to get out to the internet or go anywhere else at all, everything has to stay within the VPC. And, because of all of these settings, since there is no route to an internet gateway and there is no public resolvable IP address assigned to compute, this is why we consider this a private subnet, anytime something is considered private, that is going to mean typically that it only has internally resolvable IP space. In addition to the private subnets, we're going to also have our public subnets. Now the public subnet route tables are going to have that local VPC route, but they're also going to have a route to the internet gateway for all other traffic. What this means is if the traffic destination is for anything outside of the VPC CIDR block that we've assigned, then it's going to follow this other route, which is going to go out the internet gateway. The public subnets are going to have publicly resolvable IP addresses assigned to the compute. So these are the two things that make them public subnets, they have a route to an internet gateway directly and they have publicly resolvable IPs assigned to any resources that are within them. Now with that out of the way, let's jump into the console and let's get this demonstration going. All righty, I'm in the AWS console, we see our custom VPC here with our associated CIDR block, we see the default resources that go along with those, let's go ahead and jump in. First thing I want to do is I want to go to Internet gateways and I'm going to create a brand new internet gateway with a name that I can easily read. I'll create it, and then after you create it, you have to attach it. Notice it is in a detached state, so it's doing really nothing right now. So I'll go under Actions, Attach to VPC, and select my production VPC. There we go, so now it is attached to our VPC for us to use. The next thing I want to do is I want to create subnets for this VPC. So I'm going to go to Subnets, I'm going to create subnet, I select my custom VPC, and now I'm going to create all four subnets that we saw in that diagram. So what I'll do is I'll walk through the first one and talk about it, and then for the duplicates of each private and public, I'm going to skip through those and just review them. So let me go ahead and get going on the first one. The first one I'll create is our private subnet that will live in us‑east‑1a. Remember, subnets are bound to availability zones, so typically people like to follow naming conventions that help them identify what AZ they're in. So I give it a name, I'll pick my us‑east‑1a AZ, we see the CIDR block for our VPC, and now I define the CIDR block for the subnet. So for this, I'm going to follow a typical convention, private will have even numbers, and then public will have odd addressing spaces. So for private, I'll start things off with 10.1.0.0/24. Now notice really quickly, right now they say there's 256 IPs with that CIDR block, but after this, we'll review and you'll notice that's not quite true. So I'll give it my initial CIDR block for this, and I'm going to add a new subnet. Now let me populate this and then I'll talk about it, and you can see I've named it private_subnet_1c, I selected AZ 1c, and then I gave it a .2.0/24. So it's .0 and .2. Now the next one is our public subnet, so let me fill this in and I'll talk about it again. And for this public subnet, I follow a similar convention, public_subnet_1a, it's in us‑east‑1a AZ, I give it a 10.1.1.0/24 CIDR range, and now finally, I'm going to create our last public subnet; and there it is, public_subnet_1c in us‑east‑1c with this specific CIDR block. So I'm creating four subnets, two public and two private, with specific CIDR ranges, and I'm going to click on Create. All right, there we go. You can see our four subnets, there's two public two private, and really quickly before we move on, let me select one of these, I'll pull this up here, and notice right here, available IPv4 address space, 251. Now, if you remember from the previous clips, within subnets, AWS reserves five IP addresses. So that's why, yeah, we saw 256 under that calculation earlier, but we have to take five away, so we really only have 251 that we can use. So this will be the same for each subnet, and you can see that as I select them, 251 available IPs. Okay, with that out of the way, let's continue on. We have our four subnets, and now I want to follow best practice and create four specific route tables, one for each. So I'm going to go to Route tables, create new one, and I'm going to go ahead and copy and paste the name in here, private_rt_1a, and select my VPC. So what I'm going to do here is create this, and we see that it's active, it's created, and what I'll do now is I'm going to fast forward and create the three other ones and then we'll talk about them. Okay, so I fast forwarded really quickly just so I didn't bore you since it's all pretty much the same thing minus a naming, but I've now created a single route table to map to a single subnet, so a 1:1 relationship. We have our two private ones for 1a and 1c, and our two public ones for 1a and 1c. Now, the big thing here is that if we look at the private route tables, if we go to Routes, we have only a local route, so this is only for local VPC traffic, so our CIDR for our VPC. The big thing here is we don't have any subnets associated. Right now, these subnets have not been explicitly associated so they're using the main route table that comes with the VPC, which is no good. So I'm going to click on Edit subnet association. This is for our route table 1a, so I'm going to select the 1a private subnet and then save. I'm going to do the same thing for 1c, except for I'll select private_1c, and when I go back here, I can do the same thing for public. So for public_1a, I will associate the public subnet in that AZ, and I'll do the same thing for public_1c. Awesome. So now we've associated our subnets for each route table. Now, the private route tables are fine because we don't need any other routes, remember, we're only going to use the local internal route. The public ones, however, we need to add a route to the internet gateway to make them truly public. So what I'm going to do here is select public_rt_1a, go to routes, I'm going to click on Edit routes, and you'll notice we have that local route like the private route tables, but I want to add a new route for all other traffic, so 0.0.0.0/0. Under this drop‑down, I'll select the internet gateway we created, and save changes. So now, this subnet, public subnet 1a, has a route for both the local VPC CIDR and all other traffic will be sent to the internet gateway. Now, I've got to do the same thing for the other public route table. So let me select it, go to Routes, and I'll add that exact same route. And, there we go, local route, and then everything else will default to go to the internet gateway. So now we have our route tables in place, we've created our four different subnets, let's test everything out. To do this, I'm going to create two EC2 instances. We're going to have one that lives in the public subnet and then one that lives in the private subnet. So let me go to EC2, and what I'll do here is I'll create these EC2 instances really quickly. I'm going to go to Launch Instance, we'll call this public, I'm going to leave majority of the defaults here, but under Network settings, I need to select our custom VPC resources. So, our production VPC, I need this to be in one of the public subnets, so I'll just choose public_1c, we need a public IP address, otherwise it won't work with the internet gateway, and I'm going to create a new security group. Now, this is out of scope for this particular demo so don't worry about this right now, I'm only creating this for a demonstration purpose, you'll learn about security groups coming up later in this module. All this is doing at a very high level is allowing all traffic to be allowed within the VPC, so anything that comes from within the VPC will be allowed to talk to this instance. Now, I'll skip down here, fill out a couple more details, I'm going to select an instance profile that I already created so I can connect to this instance, and then I will go down here, verify everything else is good and click Launch Instance. Awesome. So now I need to launch a private instance. I'm going to go back, create a new one, I'll use a majority of the same settings here. Under Network settings, production VPC, so our custom VPC, I'll choose private_subnet_1a, we won't have a public IP because we want this to be a private subnet resource, and I'm going to select that same security group. So allow_All_vpc, and then I'm going to go ahead and click on Launch Instance. Now if I go to my list of instances here, I'll refresh, we'll see our public and our private. So what I'll do is select public, we see we get a public IP because we put it into our public subnet and we said yes, associate or deploy a public IP to this instance, and we also have a private IP. So any internal communications will use the internal private IP space, any external communications will use our public IP. So now what I can do is connect to this instance, and all I'm going to do here is I'm actually going to go ahead and traceroute to pluralsight.com. In theory, this should work because we have attached an IGW to the VPC. We deploy this instance into a public subnet with a publicly resolvable IP, let's test it. Perfect. So you can see we're hitting all of those publicly resolvable IPs along this route to pluralsight.com, so this is working. Now one thing I did want to show you is even though our instance that lives within our private subnet, if I go back, so the private EC2, you'll notice it only has a private IP space. In addition to that, remember, the route tables for the private subnet only have the local route. So because of this local route and the way subnets work, these instances can talk to each other, assuming all of the security protocols are in place to allow it. From a networking routing standpoint, it is allowed. So what I can do is go back to Instances, copy this private IP, go to Systems Manager here, clear my screen, and I should be able to ping this instance, and I can. So notice, our public subnet resource, or our public server, can reach that internal IP space because it has that local route, it knows where to send that traffic. Okay, now that's going to do it. We've deployed our public and private subnets, we tested both private and public resources, and we configured the route tables to go ahead and route our traffic. Let's go ahead in this demonstration here, and then when you're ready we can pick up with another clip.

Network Access Control Lists (NACLs)
Okay, welcome to this clip, where we're going to start discussing Network Access Control Lists, also known as N‑A‑C‑Ls, or NACLs, for short. A network ACL, when defined by AWS, is a resource that allows or denies specific inbound or outbound traffic at the subnet level. Now, I can tell you, you need to know that these work at the subnet level, please remember that for this exam. Let's look at some important concepts and terms regarding network ACLs that you need to know. Essentially, what these are are stateless firewalls that control traffic, again, at the subnet level. I really stress the idea of subnet level because it is very important. You'll see why when we compare it to another resource later on, but again, you need to know this. Now you might be asking what stateless means. Simply put, stateless means that you have to explicitly define both your inbound and your outbound traffic rules separately, they don't work together, they are separately controlled. With these resources, you assign one NACL per subnet, you cannot do any more than that. You also do get a default NACL in place if it's needed, so it's similar to that main route table that gets created for us as well. Now, of course, you don't want to use the default, you want to avoid that, and you want to use your own custom ones whenever possible. With that in mind, any newly created NACL resources deny all traffic by default. You have to explicitly allow any traffic that you want to be inbound or outbound. All this list is is a list of ascending numbered, prioritized rules, and the first match wins. So it'll go from lowest number to highest number in terms of prioritized, and then as soon as there is a match with one of your defined rules, the processing stops, it doesn't look for any more specific ones or any other matching ones, it found the match, it stops processing. So really be sure that you're doing the correct rules when you're implementing them. Now one thing to keep in mind is that NACLs don't work with everything. Here are a couple examples of different traffic that NACLs can't work with at this current time. You can't filter for Amazon Domain Name Services, or Amazon‑provided DNS. They don't help filter Amazon DHCP traffic. They can't work with EC2 instance metadata, and if you're not familiar with instance metadata, don't worry about it, we cover that in a different module, just keep in mind that you cannot actually filter this traffic. And then lastly, you can't filter on those reserved IP addresses that are used by that default VPC router that we covered earlier. So these are four instances of traffic that don't work with NACLs. Keep those in mind. Moving on, I want to bring up something very important and something that's very tricky for a large majority of people that are learning about these resources: ephemeral ports. An ephemeral port is strictly a short‑lived transport protocol port where operating systems allocate them for client‑side transmissions when they connect to a server. Long story short is that it's a random port within a specific range that is used in a temporary connection. They change and they can be different every single time, that's the important thing. The big thing here is that ephemeral ports do change or vary in range depending on the operating system. So depending on what OS you're on, they might change slightly for connections, but they're always going to fall within 1024 to 65535. So an example of configuring traffic for an ephemeral port, if you allow an inbound HTTP connection on port 80, you need to allow the outbound connection to be on the ephemeral port range, which is here, 1024‑265535. This is tricky for a lot of people because they are under the assumption that you're allowing port 80 in for a web server connection, so why doesn't it connect or respond with port 80 on the client side. Well, that's not how the networking stack works, that's why this is very important. That client‑side connection port is actually going to be between this ephemeral port range. This is a common scenario that can trick you on the exam. Now keep in mind this is just one specific example, it's not just for HTTP connections, it could be for SSH connections, etc. Just remember ephemeral ports. And speaking of ephemeral ports, let's look at an architecture diagram. In this example, we have our VPC, and then we have an NACL that's wrapped around our public subnet because remember, NACLs are assigned at the subnet level. So for this example, remember that NACLs require both inbound and outbound rules because they are stateless. So with this, we have two different rules, we are allowing inbound TCP on port 80 from the internet, or 0.0.0.0/0. And then we also have to allow outbound TCP traffic on the ephemeral ports, because remember, this is how those client‑side connections work. So we're allowing outbound TCP on the ephemeral port range to that internet destination. Remember, these are two separate rules on the same NACL resource, you have to go ahead and define both because it is stateless. So in this example, from the internet, we might have someone connect to our web server, destination port of 80, source port of 45535. Well that means the outbound needs to match our rule, and lucky for us, it obviously does. So we have our source port of 80 in our web server, but notice the source port is not the same as the destination port in the original call. Again, this is an ephemeral port. So this is the exact scenario you have to keep in mind when you are defining your NACL rules, be very careful planning these rules out. Now, I know I keep stressing this, and you're probably getting tired of me saying ephemeral ports, which I understand, but I can promise you this is a popular exam scenario. It's also a very popular scenario to make you frustrated when you are configuring your VPCs. So really learn and understand it. Also, please remember, NACLs are stateless and applied at the subnet level. Those are two concepts that trick people all of the time. That's going to do it for this clip. We just got done talking about Network Access Control Lists, please review this if you need to, but if you're ready, we can stop here and move on to the next clip.

Security Groups
All righty, next up on our VPC journey, security groups. A security group is a resource in a VPC that controls the traffic that is allowed to reach and even leave the resources that they are associated with. At a slightly deeper level, let's look at what these really are. Security groups are stateful virtual firewalls that get attached at the EC2 or network interface level. Notice that we've really highlighted the stateful part. That's because these are easily confused with network access control lists, which are stateless. Stateful is a completely different approach to this virtual firewall. Another big thing is that they are attached at the resource level, not the subnet level. Essentially, this works as a layered approach with network ACLs. With security groups, you only define allowed traffic, you don't ever put deny rules within the rule list. With that being said, if you don't allow the traffic, then it's implicitly denied. So that's why there's really no need to add a deny rule. All rules within the rule list get evaluated before the logic is applied to the network traffic. Whatever is the most restrictive match will win. Lastly, just to reiterate, there is an implicit deny in place for anything not explicitly allowed. That's a very important concept. If you don't allow it, then it's just denied. Moving on, let's look at some components that are important to making up a security group. When you create one, you have to specify several things. One of the big ones or the first ones is the protocol. In other words, what protocol are you allowing? TCP, UDP or ICMP traffic? You then define the port range, so what port or ports are you allowing, 22, 443, maybe you're setting an ephemeral port range that's modified, etc? And then lastly, the source or destination. What source or destination IP ranges are you allowing in or are you allowing out? Building off of this, a pro exam tip. You can reference other security group IDs within your security group rules. When you create a security group, it gives you a resource ID, so you can reference it, and instead of having to track down the IPs within that security group, you can easily just leverage that security group ID within a rule set. It might seem confusing now, but in upcoming demonstrations, you'll see exactly what we mean. Moving on, let's talk about architecture. So we used a similar architecture diagram when we talked about the network access control list in a previous clip, and we're going to leverage pretty much the same thing, except for now, you can see we have a security group here. Remember, NACLs are the first line of defense. They're applied at the subnet level, and then security group rules are applied within them. Now this is assuming we're talking about incoming traffic, not outgoing, so just also keep that in mind. Now with the security group, it's attached to a resource, so in this case, our web server. In that security group, we can create an inbound rule that allows port 80 from the internet, or 0.0.0.0/0. So with this, remember that security groups are stateful, so we define the source port and the destination port, and because they're stateful, that means any responses are automatically allowed as long as the request is allowed. So we allow port 80 to connect inbound, that means we're going to go ahead and allow any outbound connection related to that request to go out. In shorter terms, if an incoming request is allowed, then the outgoing response is automatically allowed, and vice versa of that, if an outgoing request is allowed, then the incoming response is automatically allowed. This is what stateful means when we talk about a stateful virtual firewall. You don't have to go and create two different rules for the same traffic flow like you would in NACL, it's done automatically for us, and it's a lot easier to use. So really make sure you understand that these are stateful in how they work, and of course, coming up next, we're going to have a demonstration talking about creating NACLs and security groups and testing traffic flow. So let's end things here, and I'll see you in the next clip.

Demo: Creating NACLs and Security Groups
All righty, welcome to this demonstration clip where we're going to put all of our previous knowledge together, and we're going to go ahead and create a brand new network access control list, as well as a security group to create security within layers for a web server in our VPC. Before we jump in the console, here is a high‑level architecture view of what to expect. We are going to have a VPC that was already created, as well as a public subnet. Now, we are going to be responsible for creating a public NACL and a web security group. These two resources are what we are going to focus on for this demonstration. The first thing we'll do is we'll look at the public NACL, and we're going to create two separate rules. We're going to create an inbound rule, which is going to allow all HTTP traffic, so port 80, from anywhere in the world. Now remember, we have to create a separate outbound rule as well because NACLs are stateless. So we'll create the outbound rule that will allow all TCP traffic for the ephemeral port range, so 1024 to 65535. Remember, for standard web connections like HTTP, the client‑side port will not be the same as the destination port, so this is why we add that ephemeral port range. Now after we set up this public NACL, we're then going to set up a web security group. Now, in this security group, we're only going to define an inbound allow rule for HTTP traffic from anywhere with an IPv4 address. Now remember, security groups are stateful, so we only have to allow this inbound rule, and then that outbound connection for the request response is going to be allowed by default. This is probably the biggest difference between security groups and NACLs. Now once this is set up, we'll spin up an EC2 and we will test using a very simple web page to verify. Now let's go ahead and jump in the console. Okay, I'm in my AWS sandbox, I already have a production VPC with our CIDR range, and in this VPC, I've created one single public subnet, you can see that here, and we have one route table for that public subnet. We have our local route, and then a route to an IGW, which I created and attached, and of course, we've associated this route table with our subnet. Now, if I go back to the VPC, the one thing I want to show you here is we have a main NACL. Remember, you get a default NACL when you create your VPC, and it's used as a fallback. You can see default, yes, and it's associated with our subnet, but we don't want that, we want more specific inbound and outbound rules. So let's create it. I'll create a new one, I'll give it a name, we select our production_vpc, and click on Create. Boom, just like that we have a new NACL. Now, if I refresh this page here, and select our public NACL, there we go. Now, remember, we have to still create our inbound rules and our outbound rules and our associations for our subnets, so let's do that now. The first thing I want to do is go to inbound rules, and notice, again by default, we have a deny rule in place. So all traffic inbound and outbound is denied by default for an NACL when you create it. So I'm going to go to edit inbound, I'm going to add a new rule and give it a rule number. Now remember, these numbers are prioritized from least to highest number, so for this, I'll just do rule number 100, as that's a typical default, I'm going to go under type here and select HTTP. You can see the protocol is filled out for us, the port range is filled out for us, port 80, and I'm going to allow it from anywhere. So I'm going to click on Save, and now we have our inbound rule, but, that's only part one. Remember, these are stateless, so I have to create my outbound rule. I'm going to go to edit outbound rules, add a new rule. I'll give it 100 as well as the rule number, and I'm going to leave it as Custom TCP. If we selected HTTP, this would not work, remember. We need to allow ephemeral ports for outbound connections like this. So I'm going to go back, I'm going to do Custom TCP, and I'm going to fill out that range. I'll say the destination is anywhere, and we're going to allow it. I'll save changes, and there we go. So now we're allowing port 80 to connect inbound, and we're allowing outbound the ephemeral port range, which is what would be connected to on the client side. Last thing here is we need to associate our subnet, so I'll click on Edit subnet associations, I'll select our one lonely public subnet here and save my changes. Awesome, so our NACL is set up and in place, we've associated it with our subnet, let's move on to our security group now. So I'm going to go down to Security groups under NACLs, I'm going to create a new one, and I'm going to give it a name. I'm going to give it a description, and then you must select the VPC for this security group. These are locked to a specific VPC, they can't be shared between VPCs. So I select my production VPC, and now we get down to our rules. First thing I want to call out is to look at the default outbound rule, we're allowing all outbound traffic. So by default, since this is a stateful resource, if we were on an EC2 with this security group attached, and we had no inbound rules defined, if we initiated the connection from the EC2 itself, that is allowed out. So because these are stateful, that means the request response would also be allowed, regardless of us defining rules. Since the traffic initiated from the inside, then it's allowed coming back in because we already allowed that outgoing traffic. With that out of the way, let's add our inbound rule. I'm going to go ahead and click Add rule, I'm going to define HTTP, it defined the protocol and the port for us, and then for source, I'm going to say anywhere IPv4, and let me go ahead and fill in my description, and then I'm going to click on Create security group. So now we've created this security group, and we added an inbound rule to allow HTTP from anywhere on the internet. Another thing I wanted to show you here is, remember, there are no deny rules, you only allow, and if you remove one of the allowed rules, then implicitly that traffic is now denied. So keep that in mind as well, you only specify allow rules. Okay, so, our networking resources are in place, let's go ahead and load up EC2, and I'm going to create an EC2 instance here that will serve as our web server for us to test with. I'm going to give my instance a name here, I'm going to select a personal AMI that we have created on the back end, which is just a simple web server. So I've selected my custom image here, I'm going to skip down, fill out some of the other required information, and I get to network settings. So I'm going to edit the network settings, select our production VPC, we only have one subnet, I want a public IP obviously because I need to navigate to the web page, and then we get to our firewall or security group settings. I'm going to select an existing security group, our web server security group, and then I'm going to skip down. Now, I'm going to paste in some user data here really quickly, and all I'm doing here is I'm restarting the Apache web server service on the instance. I'm going to click Launch instance here, and there we go. So what I'll do here is I'm going to select this, I'll fast forward until this is up and running, and then we will test. All right, so it's up and running, and now we have our public IP, let's test this out. I'm going to open this address, make sure I make this HTTP, and, boom, there we go, we've now successfully configured a NACL and a security group and tested our web server. Now, really quickly, let's just go ahead and test our network access control list out really fast. I'm going to go back to my VPC dashboard, select my network ACL, select my public NACL, and for inbound rules. Let's test this out. I'm going to add a new rule here, we'll give it rule number 90, we'll say deny all traffic from my personal IP address, and then let's save changes. All right, so now if I go to inbound rules, we have rule 90 before rule 100, so even though we are specifying that we allow all traffic for HTTP inbound, I've added this other rule before that to deny my specific IP, so let's test it out now. And just like that, we've denied traffic from a specific IP address. You can see it's just stuck spinning, and you can take my word for it that this is going to time out. So that shows how rule processing works within your network access control list. It found that first match and then it stopped processing any other rules. Okay, that's going to do it for the demonstration. I hope you've learned a lot on how to layer security groups with NACLs for good network access controls. Let's end here, and I will see you in an upcoming clip.

DHCP Option Sets
All right, let's talk about DHCP option sets. What are DHCP option sets? A DHCP option set is a group of network settings that are used by your resources that are deployed within your VPC. What these resources do is they allow you to control the following aspects of network configurations within your VPC. You can control the DNS servers that you want to use for name resolution. You can customize the domain names that are assigned to your compute. You can specify custom NTP servers for time protocol. And you can even decide if you want to have DNS resolution turned on or off within the VPC. Now with a DHCP option set, when you create them, you can associate a single option set with multiple VPCs. On the flip side of that, or the other side, you can only have one associated DHCP option set per VPC, so a VPC cannot have more than one associated. However, you can associate one option set with multiple VPCs. Let's look at some important concepts regarding an option set. Each AWS region has its own default DHCP option set when the account is created and the region is activated. Each VPC is going to use the default option set for that region unless you specify otherwise. In order to specify otherwise, what you can do is you can create and associate your very own custom option set with the VPC. Or, if you absolutely wanted to, you could specify no DHCP option set. Obviously, that's very rare, and it's more often you're going to create your own and associate your own. A very important exam tip here. You cannot modify a DHCP option set once it is created. I know that sounds weird, and it really stinks, and it causes a lot more work, but once you create it, those settings are set in stone. With that, a common exam scenario. If something comes up where you need to modify an option set's configuration settings, you can't do it. Instead, you have to create a new option set with the updated settings, and then you associate that new DHCP option set with the VPC. Just remember, you cannot modify the settings on these resources. you have to create a new one and then reassociate. Now that's going to do it for this clip, let's end things here, very short and simple one. Coming up next, we're actually going to go ahead and do a demonstration where we create our very own custom DHCP option set.

Demo: Creating a DHCP Option Set
Okay, welcome to this demonstration clip where we're going to work on creating and associating our very own DHCP option set. I'm in my VPC dashboard here, and for this demonstration, I'm just going to use the default VPC. Now currently, it's using the default DHCP option set that comes within the region provided to us by AWS when this region is activated. So in theory, right now, based on this option set, our EC2 servers would have this domain name and they would use the Amazon provided DNS servers. So, we're going to pretend that we don't want this, and instead we're going to create our own. So I'm going to go to DHCP option sets under the VPC menu, I'm going to click on Create option set, we're going to give it a name, and then we're going to look at the options that we have here. So the first thing we could do is specify a domain name. So let's go ahead and make our own custom domain name now, so we don't use the default. The next thing we can do is specify the domain name servers that we would want to use. Now, what I'm going to enter here is not a necessarily great example as far as what you would really use; however, I want to show you how this works when we deploy an instance. So I'm going to enter two different DNS servers here. Now, if we wanted to, we could enter our own custom NTP servers via IP addresses; however, I'm not going to do that for this demo; the big thing to remember is you can customize this. We can do the same thing for NetBIOS name servers, I'm going to skip this, and we could choose a NetBIOS node type. So I'll leave those three things as defaults in blank, and I'll leave the IPv6 preferred lease time blank. I'll go down here, I'm going to create my new option set, and there we go. Now this is created, however, it's not associated with a VPC. So what I want to do is go to VPCs, go to Actions, Edit VPC settings, and in here, I can select my DHCP option set I want to use. So remember, we can't change the settings for an option set, we can only create a new one and then associate that new one and remove the old one. So that's what I'm going to do here. I'm going to select production_dhcp, I'll leave the DNS resolution and hostnames the same for the VPC, and I'm going to click on Save. Okay, so now we're using that new DHCP option set, let's spin up an EC2 instance. I'm going to load up EC2 and create a very basic instance just to test. Now, what I'll do here is I'm going to give it a name, I'm going to accept pretty much the default settings for all of the other stuff including the VPC, I'm going to skip down, and under Advanced details, I'm going to select a custom instance profile so I can connect to this instance. I'm going to click on Launch, and then what I'll do is while this is actually spinning up, I'm going to wait for it to become ready and running, and then once I can connect to it, I will resume. Okay, so this is running now, I'm going to select it and click on Connect. I'm going to go to Session Manager and then connect to this instance. Let me go ahead and zoom in for readability, and the first thing I want to do is I'm going to run the hostname command. There we go, we have our IP address as part of our actual hostname and then we have our domain, pluralsight.corp. Sweet, so that's working, it's pulling from the DHCP option set. Now if I clear this, I'm going to check out the resolv file. This is the config file for resolving DNS, and you'll notice here our name servers, which I just highlighted, are exactly what we set in the DHCP option set: 1.1.1.1 and 8.8.8.8. And then underneath those, we have our domain name that we customized. So our DHCP option set is in use, it's working as anticipated, and you are now all done. Hopefully you saw how easy it is to create your very own DHCP option set, as well as the values you can configure. Let's end this clip here, and when you're ready, I will see you in an upcoming one.

Module Summary and Exam Tips
All right, let's go ahead and wrap this module up with some quick summary and exam tips for you to take with you into your exam. First thing we want to review, subnets and route tables. Make sure you understand how these work and the differences that they have with one another. Remember, subnets are ranges of reserved IP space within a VPC. Another very important thing to remember, subnets get deployed within a single availability zone. They never span more than one AZ. When you're creating subnets, they support IPv4, IPv6, and even dual stack addressing. Remember, dual stack addressing is when you have both IPv4 and IPv6 assigned to your resources. You have to understand the difference between a public and a private subnet. Public in these terms means that the resources that are deployed within those subnets have publicly resolvable IP addresses and have a direct route to an internet gateway. Private resources within private subnets are supposed to only have internally resolvable private IP addresses assigned to them, and they should have no direct route to an internet gateway. Last key point here, subnets get exactly one route table assigned to them. They can't have any more than one route table at a time. Moving on, let's talk about route tables. Route tables are simply a resource with a set of rules on how you want to actually route your traffic, so a route table. With route tables, the most specific or longest prefix match is going to win when you're routing. What this means is if you have a general IP range set as a route, and then you have a specific IP address set as a route, and those overlap, well the more specific one is going to win. Remember that each VPC has a main route table. This is meant to serve as the default route table for any non‑associated subnets within the VPC. The two big things you need to remember for route tables: destination and target. Remember that the destination is essentially the traffic or the IP range that you're looking at or wanting to send to, and the target is the actual target for that network traffic. So a perfect example of this is if we have a destination of a public website, then the target is going to be the internet gateway. The destination is reached through the target, that's an easier way to remember that. Lastly here, you can associate with many subnets. So a single route table can be associated with multiple subnets, it's just that only a subnet can have exactly one route table associated to it. Now obviously these work hand in hand with one another to correctly route and set up routing of traffic, so it's important you understand how they actually work. Building off of this, remember that AWS reserves five IP addresses per subnet CIDR block. What this means is if in calculation you would have 256 IPs there, well, that means you only have 251 usable IP addresses. Make sure you remember what those five addresses are reserved for. Moving on, let's talk network access control lists and security groups. You have to understand the difference between these, these are critical. Security groups are a stateful firewall. That means if something is allowed in or out, then the response is also allowed. In other words, it remembers the state of the network connection. Security groups get assigned at the ENI or EC2 level, so they're attached to the actual resources themselves. Within them, you only create allow rules, you do not create deny rules. Remember, if there's not an explicitly allowed rule, then everything else is implicitly denied. With these, you specify the source, you specify the port range, and you specify the protocol for each rule. So in other words, where are you allowing traffic from, what port are you allowing that traffic to hit, and what is the protocol, TCP or UDP? Lastly, it's important to know that you can aggregate rules from multiple security groups at one time. So a resource could have multiple security groups attached, and in doing that, you aggregate or put together all of the rules into one combined big rule. The most restrictive rules will win. Now let's compare those to network ACLs. These are stateless firewalls, so they do not consider the state of the traffic flow. What that means is, remember, you have to define both inbound and separate outbound rules. It's very important. You assign these at the subnet level. That is another humongous difference compared to security groups. These are assigned at the subnet network level. Again, you specify inbound and outbound rules separately. These are stateless. We repeat this because it is very important. These can get tricky, and it's very easy to confuse them with security groups. Remember that when the rules are getting processed in your list, the first match is going to win. We had a demonstration clip where I showed you, we allowed all HTTP, I then added a more specific deny HTTP for my specific IP address, and that first match, which was the deny, ended up winning. So all processing stops once that first match is hit. And the last thing here is you can only assign one network access control list to a single subnet at a time. So don't confuse this with the fact that you can have a network access control list shared to multiple subnets. What this means is that a subnet can only have one NACL. Now, a pro tip here, remember that you can reference security group IDs within your security group rules in place of IP ranges. So when we're defining a source for an allow rule, you can actually reference another security group ID instead of the IP range. This makes it much more efficient to manage your inbound rules. Now, speaking of this, here is a example screenshot of doing just that. We have a private app servers security group here, and you'll notice we have one inbound ruleset. Now the important thing here is look at the source. Instead of specifying an IP range, we specified a source security group. What this does is it allows us to better manage controlling the sources. Instead of having to manage, track, and update the IP spaces or reference an entire subnet that the web servers might be living in, instead we can say, hey, any web servers that belong to this security group ID are allowed to hit port 80 inbound traffic. Referencing an existing security group ID as a source in another rule for another security group is the best way to handle that type of situation. You don't have to track the IP addresses, it's just much more efficient. Please remember this for the exam, this is a common scenario. Moving on, DHCP sets. Big thing here, remember, you cannot modify a DHCP option set once it is created. If you have to make changes, well, that means you have to create a new one and then associate that new one to your VPC and remove the old one. Unfortunately, you just can't modify in place. Now, that's going to do it for this module summary and exam tips clip. Hopefully you learned a lot. Remember, go ahead and review this stuff if you need to, the stuff we just covered is critical for the exam. Let's go ahead and end this here, and then whenever you're ready, you can take a break, do whatever you need to, and I will see you in an upcoming module.

VPC Peering, Network Gateways, Endpoints, and AWS PrivateLink
VPC Peering
All right, let's get started with the next module in this course. Throughout this module, we're going to cover several topics, including VPC peering, we'll look at network gateways, we'll talk about VPC endpoints, and we'll briefly discuss AWS PrivateLink. First up, we're going to cover what is known as VPC peering. What is VPC peering? VPC peering is a feature to enable secure and direct communication between VPCs. It works by establishing a secure connection that allows traffic within the network to remain within the AWS network infrastructure alone, so it doesn't have to traverse over the public internet. The neat thing about this is that it enables private resources within the different VPCs to communicate and interact as if they were on the same network. The feature also ensures that there's no single point of failure for a connection. So, in other words, it's highly available and redundant when you establish it. Now, for the exam, some key things to remember for VPC peering. You can connect VPCs cross‑account, you can connect VPCs within the same account, and you can even peer VPCs across completely different regions. These are all supported for VPC peering. Now to help break this down, we came up with a little VPC peering architecture diagram to kind of simplify what this looks like. In this example, we're going to assume VPC A at the top wants to establish a peering connection with VPC B at the bottom. So, in this case, VPC A is going to go ahead and be classified as the requester VPC. This means that this is the VPC that is actually sending the peering request to a target VPC, so that means for this, VPC B is the acceptor VPC. This VPC owner is deciding, hey, do we want to accept this peering connection or not? So they can allow it or they can deny it. Now, assuming they accept it, each VPC peering connection receives a unique peering connection ID for reference, as well as its own individual ARN, so it's easy to put into Infrastructure as Code and script because it has this unique identifier. Now, building off of this diagram, once this peering connection is then actually successfully set up, you need to update the VPC route tables in both VPCs in order to correctly route the traffic between the two. In addition to this, you also have to correctly configure the NACLs in the security groups that are in place to control access to your different network resources. So it's three parts, you have to set up the routing, you have to configure the network access control list at the subnet level and the security groups at the resource level. Assuming all of those are set up correctly, that means compute and the private resources can now start communicating via the peer with the peered VPC resources. And remember, this is all done over a fast, secure internal connection. Now, we also took a snapshot of what a peering request looks like, so this is part one of a two‑part thing. When you initiate the request, we can see here, the requesting account ID, the peering connection ID that's created, we see the accepting account ID, as well as the requesting VPC ID and name. We also have the requesting VPC CIDR, so what address range is being used? We see the peering connection ARN, which we talked about, as well as the accepting VPC ID and name. Now you'll notice since this is just a request that hasn't been accepted yet, the acceptor CIDR is actually hidden from this request. So after reviewing all this information, we accepted this peering connection, and that's what this looks like now. You'll see the only difference in here is that we can now see the accepting VPC CIDR within our console screen, so we can see it's 10.10.0.0. So those are really the highlights of the two parts of creating a peer, you request it, and you accept it. Now, another exam tip here is you can enable VPCs to resolve their IP DNS hostnames to the private IP addresses when they're queried from instances in the peered VPCs. Now to accomplish this, both VPCs have to enable DNS hostnames and DNS resolution. This is an option you have to turn on. Moving on, let's talk about some VPC peering must knows. So you need to know this information for the exam and any architecture design. First off, probably one of the biggest things to know, peered VPCs cannot have any overlapping IP CIDRs. Even if it's just one usable IP that overlaps, peering will not work, they cannot overlap in any sort of way. Also, peering does not allow for a transitive routing scenario. If you don't know what transitive routing is, hang on, we'll talk about that here in a minute. Just remember that peering does not allow for it. Lastly, we discussed this briefly already, route tables have to be updated to correctly route the traffic destined for a peered VPC. Now, talking about what does not transitive mean. Let's assume this very simplified diagram. We have three VPCs, A, B, and C. A is peered with both VPC B and VPC C, with separate peering connections. So in this instance, I'll ask you, can VPC A talk to VPC B? Yes, it can because there's a peering connection. Can VPC A talk to C? Also yes, there is a separate peering connection. But can B talk to C through A? No, so this is what we mean by not transitive, B cannot talk to C even though A is peered to both of them individually. However, we can accomplish this by creating a third peer. So we can peer VPC B with VPC C and now they can talk directly. So please remember this for the exam, VPCs are not transitive by default. Moving on, another exam pro tip for you; when you're peering VPCs in the same region, you can actually reference peered VPC security IDs as needed, so for instance, in other security group rules. This is a really handy feature and it speeds things up drastically. Remember, referencing security group IDs in the rules allows for a much more efficient management of access controls. Now, wrapping things up with some peering use cases, what would you use VPC peering for? Well, a common scenario is for centralized shared services. Maybe you have one VPC which hosts a company's shared service application where you can peer hundreds of VPCs with this single centralized VPC, and you're also isolating network connectivity. They can't talk to each other, but they can all talk to the central VPC. It's also really good for multi‑region internal app deployments. Remember, you can peer across region and take advantage of the AWS backbone network. And lastly, they're good for cross‑account integrations for collaborations or maybe some type of merger and acquisition. So, maybe you've acquired a company or you're a company that's been acquired itself, and the parent company wants to own or start controlling the application in AWS. Well, they can set up cross‑account peering to communicate with that application. This is a supported feature. Now, that's going to do it. I think that's good enough for VPC peering. Let's go ahead and wrap things up, and coming up shortly, we're going to have a demonstration where we actually set up a VPC peering connection.

Demo: Peering VPCs
Hello, and welcome to this demonstration clip where we're going to work on peering VPCs. I'm in my VPC dashboard within my sandbox environment, and the overall goal here is I want to peer VPC A to B, and A to C, and then we want to test communication. So what I'll do is I'll go down to Peering connections, I'll click on Create peering connection, and I'm going to give this a name. The next thing we see is we select the local VPC, so the requester VPC. So in this case, I'm going to choose VPC A, we see our CIDR range, and then we can select some other options. We can say, hey, what account is this in, my account or another account, where we can specify the account ID? And then we select the region. Remember, you can do same region or cross‑region, so if we selected another region, I would just choose the region and then give the VPC ID here. Now, we can combine these settings, which is awesome, but for this demo, I'm going to select my account and this region. Underneath that, we choose the Accepter VPC, so which VPC are we asking to peer with? In this case, we're going to choose B. We see the CIDR range, and then I create the peering connection. Perfect. So now it's pending acceptance. So understand this workflow. You initiate the request or create the request, it goes into pending acceptance, and then the accepter VPC can either accept it or deny it. That is the primary workflow that you need to understand. There's a request, there's a pending, and then there's an accept or deny. Now, we created the acceptance request or the peering request, so what I'm going to do here is I'm going to go under actions because it's in the same account, and I'm going to accept the request. And just like that, we now have our peering connection established. So we see all of our information, we can go down here and look at DNS settings, and I'm actually going to go ahead and edit these. So I'm going to choose Edit, I'm going to enable DNS resolution for both the requester and the accepter VPCs, and click on Save changes. Now just a caveat there, your VPCs have to support DNS hostnames and resolution at the VPC level for this to work. So if you get any errors, if you're following along, remember that, check your DNS resolution at the VPC level. Now, we still need to do the route tables, but I'll do that in a moment. I want to go back to peering connections, and I want to peer A to C. So let me go ahead and refresh this, we see our active connection, I'm going to create, and let's give this a name. I'll select A as the requester, it's the same settings, and then C as the accepter. Now really quickly, if you're paying attention, you've probably noticed one very key element here, we have an overlapping CIDR block. Notice the network ranges here, these are overlapping because 10.0/16 means all of these are used for address spacing for our host; however, that overlaps with our /24 down here where we have 10.0.30. So if we try and create this, no, can't do it, they give you the warning why. So I just wanted to demonstrate what would happen if you even tried in the first place, it doesn't work. Now with that out of the way, let me cancel, and we're just going to use our VPC A to VPC B peering connection. So the next thing we need to do here is I need to update my route tables. Now, these VPCs already have route tables and subnets deployed, so you're going to see those as we walk through these, and I'll briefly cover them as we touch them. So first things first, I want to look at subnets. Now you'll notice there are several subnets created. We have two subnets within three different AZs for each VPC. So A, B, and C, public and private. For this demonstration, we're only going to use the two private subnets in AZ A. In reality, you would set the route up for all of the chosen subnets that you need, but for the sake of simplicity, we're only going to do two subnets here. So with that being said, I'm going to go to Route tables, and we see our two private route tables for those AZs. So what I'll do is I'll select this first one, which is for VPC A, as we can see here, and I'm going to go to Routes, and then edit my routes. I need to add a new route, and remember, VPC B is a 10.10 network, so I go ahead and put the destination in there, and then I can select a peering connection, VPC A to VPC B. I click on Save changes. Perfect, that route table is now configured correctly. So now any destination traffic falling under this CIDR will get sent to the peering connection. Let's go back to Route tables, I'm going to do VPC B's private route table, go to Routes, and add that same route, but the destination this time will be 10.0. I choose the exact same peering connection here, and I save changes. Awesome, so now our routing rules are in place for these two subnets, so let's go and test this out. What I'll do is I'm going to load EC2, and once this loads, I've already created two instances, these instances are running in the respective AZs that we just worked with. So this VPC A server is running in the subnet, Private Subnet A, and B is running in Private subnet A for VPC B. Now we want to see if we can communicate with each other, so what I'll do here is I'm going to load up VPC A and connect to this. Okay, so I'm in my screen now, let me zoom in just a little bit more, and I want to go back to my instances, find VPC B server, find that private IP and copy it, and let's test ping it. Interesting, it's not working. Well we set up the route tables, what else could there be? Well, remember there are several network resources that have to be in place for this. So, what we'll do is we checked the route tables already, those are correct. The next layer is the subnet layer, so I'm going to go to Route tables here in this tab, I'm going to find Network ACLs, I'm going to find the one that's attached to VPC B, which is here, inbound rules, okay, we're allowing all, outbound rules, we are allowing all. Okay, what about VPC A? Outbound all, inbound all. Okay, so it's not the network access control list. What's the next layer inside this? The security group. So let me go to my instances, I'll zoom out here, and let's look at our security for VPC B for right now. We see our security group here, and we have one rule, ah, we're only allowing our own VPC traffic, it looks like. Remember, we have to update the security groups to allow this peering connection as well. Now what I'm going to do here is scroll over, and then scroll up here, and we see our security group ID. So what I'm going to do is click this, I'm going to open it up in a new tab, and I want to copy this security group ID. What you can do, remember, with peered VPCs is leverage security group IDs within security group rules for the peering connection. That's one of the big benefits of same‑region peering. So I'll make sure I copy that, I'm going to go back to security groups, find security group A, inbound rules, edit my inbound rules, and I'm going to add a new rule for all traffic coming from this security group ID. So even though this security group is in that other VPC, we can reference it. So I'll save this rule, and now I want to do the same for VPC B security group. So I'm going to copy the security group ID from VPC A's security group, so let me go ahead and copy this, I'm going to edit VPC B's security group to allow traffic from VPC A. I'm going to add my rule, all traffic, I'm going to paste in my security group ID and save it. So now, in theory, we're allowing VPC B to have inbound traffic to VPC A's security group here, and vice versa. So as long as our instances belong to these security groups, they can communicate. So I'll go back to Systems Manager, clear my screen, let's try pinging, and there you go, now it works. We have the correct network resources in place for access control to allow this traffic. We updated the route tables, we made sure the NACLs were correct, and then we updated the security groups to allow the peered VPCs to communicate. Now, one last thing I did want to test here is the DNS resolution. If you remember, we enabled that for the peering connection. So in theory, what I'll do here is find VPC B, I selected it, I'm going to go to Details, and we should be able to use the DNS name. So if I copy the DNS name, go back here, and I ping this, it's working, so it is resolving the DNS correctly because we enabled that setting. So now we can use private DNS names instead to communicate and not have to use the IP addresses, which is a much bigger headache usually. Now that's going to do it, we went ahead and created our very own peering connection, we set up all of the network resources to allow the connections, and then we tested traffic between the peer. Let's go ahead and end things here, and then I'll see you in an upcoming clip.

Public NAT Gateways
All right, welcome to the next clip here where we're going to start discussing public NAT gateways. First off, what is NAT? NAT, which is short for Network Address Translation, is simply a service that operates on a router or edge platform, and it connects private networks to public networks. With NAT, organizations need basically one IP address or one limited public IP to represent an entire group of devices that are connecting outside of their internal network. So it's essentially a centralized point to allow access to an external network. Now, with NAT described, we can move onto AWS NAT gateway. A NAT gateway is a network address translation service in AWS. It's a managed service that you can use so that instances in a private subnet can connect to services outside of the VPC, but it stops external services from initiating connections with those same instances, so it allows connections out, but they can't start from the outside in. Now, why would you want to use NAT in the first place? Well, two primary reasons. The first is because it's private, it allows resources and private subnets to connect to the internet to peered VPCs and even on‑prem networks. It's also highly secure, so these private resources cannot receive unsolicited connection requests from outside the network that they belong in. So, it will allow, again, connections outbound, but it's not going to allow connections to initiate from outside the network. Now, on the exam, you're probably going to be tested on NAT gateways and something called a NAT instance. A NAT instance is outdated and you really should start to avoid this. It's essentially an EC2 instance that you own and manage, and it runs some type of software that provides network address translation. The problem with these is that there's a lot of overhead, you have to manage the operating system, security and patching, network access controls, etc. There's just a bunch to worry about. In addition to that, if you have to use a NAT instance, you have to disable the source and destination check option on your EC2. Otherwise, the traffic will be dropped because it won't know where it's really going. So if there is a scenario where you have to use a NAT instance, remember this setting needs to be disabled. Really what you want to use, though, is a NAT gateway. These automatically scale as needed from 5 Gbps all the way to 100 Gbps. They are insanely fast. And in addition to that, you need to know that they get deployed to a single availability zone, so they're not like an internet gateway which is attached to a VPC. NAT gateways belong to a single AZ at a time. Also, within the subnets that they're deployed to within their AZ, they leverage an elastic IP address for reference. So you attach a static public IP address, and then you can easily reference the NAT gateway in the future. This is very useful for allow lists for external connections. Now, I like to repeat this, you're likely going to choose a NAT gateway over a NAT instance whenever you can for your exam scenarios. It is extremely rare that NAT instances are ever used in place of a NAT gateway with your exam. And then the last thing here, for true resiliency, since you deploy NAT to a single AZ, if you need true high availability, you have to deploy a NAT gateway in multiple AZs and set up the route tables appropriately. That way if one AZ goes down, your private resources still have access to the other NAT gateway that's running in the other AZ. Now for this scenario, however, you need to keep an eye on costs because they can get very costly very quickly. So for exam scenarios, you need to outweigh the cons and the pros, and really look at the true requirement. Do you need actual high availability or do you need to do something with the lowest cost option? Now moving on, another very important thing to remember, you have to deploy a NAT device within a public subnet in order to allow internet access. In other words, it has to have direct routes to the internet gateway to work appropriately. Please remember that, you deploy these in public subnets. Now I mentioned previously an elastic IP address, and I just want to cover what that is very quickly. An EIP is simply a static IPv4 address designed for your cloud computing in AWS. You get an EIP by allocating it to your AWS account, and at that point in time it is yours until you release it back into the wild. So as long as you've allocated to your account, you can hold on to it for as long as you need. Now you do get billed for using these, so, keep that in mind, but they're very good for quickly swapping statically mapped resources. Another key thing to remember here with an EIP, these are regional resources that do not change over time. So that means they're bound to the region they're allocated in, and the IP address stays constant always. So if there's ever a scenario on the exam where you need a static IP address, this would be a great option for that. Moving on, let's have a quick highly available NAT gateway architecture diagram. So you can see here we have a simple VPC in our region and we have two subnets in two different AZs for high availability. In this diagram, we have compute in the private subnets, and we've deployed NAT gateways within our public subnets. Remember, they have to be deployed to a public subnet that has direct internet gateway access. So since they're only redundant within their respective availability zones, that means we need to deploy more than one. So we put one in each region within our public subnets, and now we have multiple AZs covered for a highly available solution. So now if us‑east‑2a went down, you can still use the NAT gateway in us‑east‑2c to gain internet access privately and securely. Okay, with that out of the way, two really quick exam pro tips for you to take into your exam. Remember, if you need private resources to have secure internet access, then NAT gateway is probably going to be the best choice for you. Number two, you do not assign security groups to your NAT gateways. This is a very important thing to remember, you don't control access to the NAT gateways via security groups attached to them, that's not how they work. You can control access to them via network access control lists, but again, those are assigned at the subnet level. So once you spin up a NAT gateway, you don't use security groups to control them. Now that'll do it on this clip about NAT gateways within AWS, let's end things here. We'll take a quick break, and then we'll pick back up with another clip.

Demo: Deploying a NAT Gateway
Okay, welcome to this demonstration where we are going to deploy our very own NAT gateway. Real quick architecture overview before we dive in, we're going to have a VPC already created for us with some public and private subnets, as well as an EC2 instance deployed into one of those subnets. Now, what we're going to work on is we're going to create a NAT gateway, and we're going to deploy it to a public subnet within a single availability zone. Remember, NAT gateways are only redundant within their AZ that they're deployed to. In addition to that, they need to live in a public subnet to allow public internet access. Our private compute in our private subnet is only going to have private IP space assigned, it's not going to have a publicly resolvable IP for us to use, in other words, we can't route directly to the internet gateway. Now, after we deploy our NAT gateway, we have to do some route table updates. The first thing we'll do is we'll update our public subnet route tables, or verify them really, that they have a route to the internet gateway for all default traffic outside of the local VPC. So it'll be 0.0.0.0/0. The next part we have to do is update our private route tables, so we're going to have to add a route for our private route table to point to the NAT gateway. That way when our traffic goes to the NAT gateway, it looks up its route and it says, okay, this traffic goes to the internet. Now with this overview out of the way, let's actually jump into the console and begin. Okay, I'm in my AWS console here in my sandbox environment, and you can see we have our VPC already created for us. Now, under the Resource map tab here for our VPC, we can see we have six subnets, two in each AZ, one public, one private, and we have some route tables and an internet gateway. So the internet gateway is set up to be used with the public route table, which is associated with three of the public subnets. In addition to that, each private subnet has their own route table assigned as well, but they have no route to the internet at all. In addition to this, we have an EC2 instance running here called private‑server, and this lives within the private subnet C, as you can see down here. So this has no route to the internet right now, so we can't even connect to it with Session Manager. So what I'll do is go back to VPCs, and let's begin. I'm going to go down to NAT gateways here, and then I'm going to create NAT gateway. We'll give it a name, and then we select our subnet. So within this drop‑down, we see all of the subnets from all of the VPCs within this region. Luckily for us, we named our subnets for easier readability, so I'm going to find Public Subnet C and select it. Remember, these have to live in public subnets to give you public internet access. We select public connectivity type, and then we go down and we can select an elastic IP. Now what I'll do is I'm going to allocate a new one to show you that process, and just like that, AWS has given us a new elastic IP address, as you can see up here at the top in this little banner. So we make sure it's selected via that ID, and then I'm going to go down and create NAT gateway. Perfect. So now we have a NAT gateway in a pending state, and here very shortly it's going to be available. So we've deployed it into our public subnet, I'll go back here to NAT gateways and refresh, and then once this is available, we'll continue on. So let me go ahead and pause really quickly and fast forward until this is ready to go. Okay, it's now in a state of available, which is perfect, and I'll tell you right now that took roughly a minute, maybe a little bit longer. So just be prepared to wait if you're doing this yourself. If I bring up this Details pane here, we see it's available, we see our public IP, which is our elastic IP address that was just assigned to us, and we also get a private IP address because it's deployed into a subnet. So it has a private IP and a public IP. So now we can use this to go ahead and start setting up traffic. So what I'll do is I'm going to go to my route tables, I'm going to find public here, which is already selected, lucky for us, and I just want to make sure our route is in place for the internet, and it is. So we are pointing to the internet gateway, and remember, our subnet we deployed our NAT gateway in is using this route table, so it has access to the internet. So I want to find private table C, and I need to add a route. So I'm going to edit routes, I'm going to add a new one, and for all traffic besides local VPC traffic, I'm going to find my NAT gateway, I'm going to select it, demo‑nat, and I'm going to save changes. Perfect. so now we have a route in place to hit that NAT gateway, which lives within the public subnet, which has a route to the internet. So now if I go back to VPCs, I look at the resource map, we should have a route in place for that route table C, and we do. So the easiest way to test this out is to connect to our EC2 and then start trying to ping some external websites. So I'm going to go to my instance and I'm actually going to reboot this to restart the SSM agent, and while this is rebooting, I'm going to pause and then I will resume when it's ready to go. All right, so that took roughly a minute, it's ready to go, I selected my EC2 here. I'm going to click on Connect, I'm going to choose Session Manager, and then Connect. Perfect, so this is already a good sign, we can now connect with Session Manager, which needs public internet access or VPC interface endpoints deployed in order to work. So let me zoom in here. So the easiest way to do this is let's just ping a simple website that's public. So let me ping google.com, and there we go, we have outbound traffic. However, we can never reach this internal instance because it doesn't have a public IP. The NAT gateway is handling all of this for us. Notice if I go to the instance, we still only have a private IP, there's no public IP. So we've securely implemented a NAT gateway. So now I can clear my screen, and I can even run a sudo yum update. Now there's nothing to download, which is fine, but it's actually reaching out and looking at those public repositories to see if it does need to download anything. So this is working. We could securely patch and update our OS without allowing external communications to initiate from the outside. Let me clear one more time, and I want to do one more thing. Let's actually trace our route to google.com. Now, I want to point this out. If you notice this first hop here, 10.0.2.80, where do you think this is actually at within our network? Because this is an internal IP space. Well, it's our NAT gateway. So if I go back to VPCs, I find our NAT gateway here under our list, select it, you can see it is our private IP assigned to the NAT gateway. So the first hop is the NAT gateway based on our routing rules we implemented, and then from there, the NAT gateway sends it out to the internet gateway and over the internet. So that's what we're seeing. It hits the NAT gateway first, goes through the internet gateway, and then traverses the internet. Okay, that's going to do it for deploying a NAT gateway, let's end this demonstration here, and I will see you in some upcoming clips.

Transit VPCs
All righty, welcome back to the next clip. In this clip, we're going to look at transit VPCs. So just remember, when you're peering VPCs, we discussed that they're not transitive by default. Just because VPC A is peered with B and peered with C, that doesn't mean B can talk to C. Now you can achieve transit VPCs by using a VPN solution. Now, we're not going to talk about VPNs in depth during this module, as that's way out of the scope, but we will talk about VPNs later on. However, this is an important concept you should know, so we're going to cover it at a high level here. Take this architecture diagram, for instance. We have three separate VPCs on the left, and then we have what is known as a transit VPC in the middle here. On the right side, we have an on‑prem network that we might want to connect to our VPCs. Now, in the transit VPC, you can see we have an EC2 router or a custom VPN running on our EC2 instances within their respective availability zones. Now, to connect to these routers or VPNs, within the children VPCs on the left, we deploy what is called a virtual private gateway, and we'll discuss that here in a moment. This virtual private gateway allows us to actually set up IPSec VPN tunnels to our VPN running on our EC2. So for true high availability and redundancy, we have one connection to each subnet into the transit VPC from each gateway, so a total of six tunnels; two from each VPC. Now, we can also do the same thing from on‑prem, we can set up separate IPSec VPN tunnels, and with this architecture now, we can set up that router and VPN running in the transit VPC to allow all of our VPCs and our on‑prem to talk to one another. So all of the traffic will flow through that central transit VPC, we can leverage the router or custom VPN software to say, hey, this traffic needs to go to this other VPC, and it will work. Obviously, you can see there's a lot of management overhead that goes along with this solution; however, you need to know that this is the way you can do it, you can't do it via basic VPC peering, but you can do it by setting up a transit VPC with a custom routing and VPN solution. Now I told you we'd cover virtual private gateways, so let me do that really quickly before we move on. A virtual private gateway, or a VGW for short, is simply a VPN concentrator on the Amazon side of a site‑to‑site VPN connection. So remember that it is allowing VPN connections on the Amazon side. Because it's on the Amazon side, how they work is you attach them to your VPCs whenever you need access to that VPN connection. So remember, you attach a virtual gateway to your VPC, and that's the Amazon side of the connection for an IPSec VPN; in other words, a site‑to‑site VPN. Now that's going to do it for this lesson on transit VPCs. Just remember, if you need a transit VPC solution, it's going to require some type of custom EC2 router or custom EC2 VPN software. Let's wrap things up here, and I'll see you in the next clip, shortly.

VPC Endpoints and AWS PrivateLink
Okay, next up, let's talk about VPC endpoints and AWS PrivateLink. AWS PrivateLink is a highly available, scalable technology that you can use to privately connect your VPC to services as if they were in the same VPC. A big concept to remember for the exam and when you're designing architectures, AWS services are going to leverage the public endpoints by default. What this means is that traffic is going to try and go over the public internet by default. This is a key concept that you have to know. Now, this is where AWS PrivateLink comes in. Maybe you don't want that. Well, the too long didn't read version of PrivateLink is that it allows your private resources to communicate with the services and stay entirely within the AWS private network infrastructure, so the traffic never leaves the AWS private network infrastructure, it never goes out to the internet. What this allows you to do is remove things like an internet gateway, a NAT gateway, you don't need VPNs or even direct connections. It works by creating endpoints within your VPC that you can control and secure traffic for. A really neat feature is that they allow you to host actually your own private services that you might be running via an app by setting up a service provider and a service consumer, and we'll look at that here right now. In this diagram, we have our Service Provider VPC at the top and our Service Consumer VPC, here, at the bottom. So the service provider is the VPC that's hosting an application that you want to share out. So how this works is a service provider creates a network load balancer within their own VPC, and that's meant to front the shared private service. After setting up sharing permissions and granting those permissions, you can share this service. When you do this, it works by deploying an interface VPC endpoint within the consumer VPC that connects to the provider VPC's network load balancer. After this is in place, the consumer compute within their VPC can now start using that shared service over a secured private connection. So they point traffic to that network endpoint, which is a virtual network interface, and that is actually leveraging on the back end the network load balancer, and it all traverses over the AWS network infrastructure and never leaves the AWS network. A huge tip here to remember, PrivateLink is what powers VPC interface endpoints. We just talked about those a little bit, but we're going to dive into those very soon in an upcoming clip. Just remember, PrivateLink powers interface endpoints. Now, that's actually going to do it for a PrivateLink. Just remember, you can use this to share services and connect to services in a secure, private manner. Do your best to remember that architecture we just reviewed, where there's a provider VPC and a consumer VPC. And with that, we'll end here, and coming up shortly, we're going to look at gateway endpoints.

Gateway Endpoints
Okay, let's dive into VPC gateway endpoints. A gateway VCP endpoint is an endpoint that provides reliable, secure connectivity to Amazon S3 and Amazon DynamoDB without the need for an internet gateway or a NAT device. These are very powerful features. A few things to keep in mind regarding gateway imports. These are perfect for keeping S3 and DynamoDB traffic from traversing the internet. So if there's ever a scenario where you need to reduce traffic over the public internet for these two services, whether it be for costs or security purposes, these are a perfect solution, you should think gateway endpoints. A key thing to keep in mind is these don't leverage PrivateLink. PrivateLink uses interface endpoints, and these don't do that, they just give you a gateway for you to use that you configure. Another key benefit and thing to keep in mind, they're free to use, so they are perfect for cost‑effective solutions for S3 and DynamoDB. Interface endpoints cost money, just keep that in mind. One thing to know also is that they require updates to route tables in order to direct traffic correctly. So once you deploy and attach a gateway endpoint, you have to update the route tables for your subnets to leverage them correctly before they're actually used. By default, if you recall, traffic to AWS services like S3 and DynamoDB will use the public endpoints, so that means they're going to want to go over the internet. You bypass that by using a gateway endpoint and updating the route tables to use that gateway. A nice thing about them, though, is as complicated as they sound, they're actually super easy to use. You deploy them, you attach them, and then you can easily reference them via a managed prefix list for your routes. Now, a managed prefix list is a little out of scope for this entire course, but at a high level, it's essentially something managed by AWS, which contains a list of IP addresses that are constantly revolving. So instead of having to track the IP addresses for your routing tables, you just reference the prefix list, and you never have to worry about the changing IPs. Now moving on, two exam tips before we have an architecture review. Number one, gateway endpoints do not require security group updates. They don't work like a network interface, remember, they're completely separate. They are a gateway that you can just configure via routing. Number two. Right now, they only support Amazon S3 and Amazon DynamoDB, no other services in AWS are supported by a gateway endpoint. That, right there, should be enough for you to knock out a ton of exam scenarios that deal with endpoints. If there's any other service mentioned, well, it can't be a gateway endpoint, period. Now with those exam tips out of the way, let's have a very quick architecture discussion. We have our simple VPC here with a single AZ and a single private subnet. Now, this private server, we're going to assume has no or restricted access to the public internet, so it's not supposed to go over the internet at all, or it even can't go over the internet at all. So instead, we deploy a VPC gateway endpoint that's specific to Amazon S3. So we use this for any S3‑destined traffic, and when we configure our route tables, traffic will automatically be sent via the endpoints using the routes in the route tables, which point at the prefix lists that are managed by AWS. So now, if our private server makes an S3 upload call, it's going to say, hey, what route should I use? The route table says, okay, well, I have this managed prefix list for anything S3 related, so send it here. That traffic is then sent to those managed IPs, which are all within the AWS backbone infrastructure, and that traffic never leaves the AWS cloud. So it's sent over the internal network, it's very fast, it's free to use, and it's super secure. So hopefully you can see how easy they really are to implement in an architecture. Now coming up next, we're actually going to have a demonstration where we set one of these up and we actually test how they work. So let's go ahead and wrap up here, and I will see you in the demo, up next.

Demo: Gateway Endpoints
Okay, let's go ahead and get started with this demonstration clip where we're going to work on deploying and using our very own gateway endpoint. Before we dive into the console, let's just have a quick architecture overview. We're going to have a VPC already deployed for us with several private subnets. Now for this, we're going to focus on one subnet just for simplicity, and within that private subnet, we're going to have a private server that's already deployed. Now, this private compute in EC2 is going to have no or really restricted access to the public internet. In fact, we're not even going to have an IGW attached to the VPC at all. What we're going to work on doing because of this is we're going to deploy a gateway endpoint to our VPC, and then update our route tables to point to that gateway endpoint for any and all S3‑bound traffic. So if the destination is S3, we're going to say, hey, use this gateway endpoint to go ahead and make your connection. After all of these resources are put into place, we're going to test copying an object from an existing Amazon S3 bucket. So with that out of the way, let's end this here, and let's jump into the console. Okay, we're in our cloud sandbox environment, I'm in my VPC dashboard, and let's go over really quickly the existing resources to get that out of the way so you know what's already created and being used. I'm actually going to start with the S3 bucket. So we have an S3 bucket here, this templates bucket that has a gateway.txt document in it, and this is what we're going to test for to go ahead and download. We also have an EC2 instance that's already deployed into a private subnet, and this EC2 instance has an IAM role that allows session Manager connections, as well as S3 access. So let me close this, go back to EC2, and now moving on to VPCs, let's show some of these resources. So if I click on Resource map, you're going to see we have three private subnets. Notice there are no network connections to the public internet, so this is a perfect demonstration. How can we get to S3 if there's no internet access? Well, we'll use a gateway endpoint. Before we begin, however, there is one thing I want to show you, and we're not going to go too far into depth on this because these will be covered in a different demonstration, but under endpoints, I've created three interface endpoints, which are required for Systems Manager. So these three endpoints are deployed into the same subnet as our EC2 instance, so 5580, and if you notice the networking here, 5580. Now these interface endpoints have a security group set up that allow me to connect. So this is how I'm able to connect to my EC2 instance via Session Manager. This doesn't work if there's no internet access or interface endpoint access. So in case you're following along, I wanted to call that out. This is the only way I can connect to this instance. However, let me zoom in on just this particular window, and let me try and ping something. Notice we can't because there's no internet access. That also means if I run aws s3 ls, we should get zero answer because, again, we can't hit that public endpoint. So let me go ahead, clear my screen, I'll go back to my VPC screen here, zoom back out just a little bit, and then let's continue on. The first thing we want to do here is under our VPC, we want to deploy a gateway endpoint. So what I'll do is I'll go to Endpoints, I'm going to go to Create endpoint, and I'm going to give it a name. So I'll just call it S3, and then under Services here, we can search for a service. So let me look for S3, and we can filter down. Now notice, there's two S3 service endpoints here, a gateway and an interface. For this demonstration, we're going to choose the gateway endpoint because we want a free, cost‑optimized way to keep our traffic off the internet and interact with S3. So under Services, I select the gateway endpoint for S3, I scroll down, and then I need to choose my VPC. So I'll choose my DEMO VPC, and now we have to update our route tables. So, what I'm going to do is just select all of them because I want every route table to have a route to S3 through this private connection. So I select all of them, I scroll down, and we have a policy. Now, this is a little out of scope for this exam and this clip, but we can set a policy here to restrict access on who can actually use this endpoint. For this, we're going to leave it at full access, I'm going to scroll down here and click on Create endpoint. Sweet. So now we have our S3 endpoint here, we can see it's available, and if I click on my route tables here, we see the associated route tables. So we're using Private Route Table A, so let me go ahead and click on that. I'll select it, let me refresh, and under Routes, there we go, we have our local route and now we have another route that points to a prefix list saying, hey, any destination under this prefix list, send it to our gateway endpoint. So you might be wondering, well, what is this pl? This prefix list is the managed prefixed list we covered in the previous clip. What this contains is a list of IP addresses or CIDRs that map to the S3 service. So how it works is whenever we make an S3‑related call to hit the S3 service, our DNS router is going to look up the IP addressing that falls or belongs to the S3 service, and assuming they fall under this list of entries, which they should, our route table is going to say, okay, well, any destination that falls within these CIDRs, I want to send it to the target of our VPC gateway endpoint. So it's all handled for us automatically, which is awesome. So now let's actually test this out. In theory, I should be able to reach S3 now even though I have no internet access. So what I'll do is I'm going to go to my console screen here, let me zoom in on my Session Manager, and I'm going to run an aws s3 ls. Boom, just like that. We can now list our buckets because we can reach the S3 service, and all of this is done without internet access. If you notice under VPCs, if I go back to our VPC again, we still don't have an internet gateway attached. If you notice, there's no network connection minus our S3 gateway now. So let's actually test really quickly again, downloading an object. I'll clear my screen, I'll go to my bucket here, I'm going to select my gateway object, I'm going to copy the URI, and I'm going to use that to copy. So I'm going to aws s3 copy, paste the URI, and then the local file. Now if I go to my home directory, we can see here this gateway.txt that we just downloaded, and there we go, we've now used that gateway endpoint to bypass the public internet securely. Hopefully that helped drive home the idea on how gateway endpoints work, you can see we didn't even need an internet connection at all for them to work, and we just downloaded an object using them. Let's go ahead and wrap this up here, and I will see you in some upcoming clips.

Interface Endpoints
All righty, welcome back. We just got done looking at gateway endpoints. And in this clip, we're going to look at VPC interface endpoints. So what are interface endpoints? How are they different than a gateway endpoint? Well, interface endpoints actually deploy an ENI into a chosen VPC subnet. Now, really high level, an ENI is basically a virtual network interface for a VPC. We cover these much more in depth in an upcoming module. Just remember that they use and deploy network interfaces into subnets. Because of this, they require management of an attached security group. So they function like an EC2 instance because security groups get attached to network interfaces. So you control access via these security groups for networking. A really nice thing about these endpoints is that they actually support way more services than gateway endpoints. However, they cost money because you're deploying an ENI, which takes up compute and network resources within your VPC, and you are billed for that. So there is a tradeoff. You get much more support from a service standpoint; however, you're paying money for these endpoints. So it really comes down to the requirements for your scenario. Now when you use interface endpoints, you really want to turn on DNS resolution within your VPC. This way, traffic gets sent to your AWS services via these private endpoints, which are going to use regional and zonal DNS names. Now let's break down what that means. A regional DNS name is exactly what you see here, but what do we see here? Let's break down each part. The first portion of this DNS name is actually the VPC endpoint ID. So you get a unique identifier for your interface endpoint. The next part is the service or, in other words, the AWS service that you're using. So in this example, this is for CloudWatch logs. So so far, we have a VPC interface endpoint for the CloudWatch logs service. Next, we have the region. So in this case, the us‑east‑1 region is where we're deploying this VPC endpoint. And then the following last portion is the typical nomenclature or syntax for these endpoints, vpce.amazonaws.com. This will always be the same for every interface endpoint. Now on the flip side, this is a regional DNS name. So you can easily leverage the service, but they also make zonal DNS names for more specific use cases. Now the only difference here is there's a ‑az name appended to the VPC ID. So notice in the beginning, that's literally the only difference. Everything else is the same, the service, the region, and then the ending of the name. So in theory, you could have several different zonal names for your endpoints, depending on which az you deploy these endpoints into, us‑east‑1a, 1b, 1c, etc. Please be familiar with the DNS naming syntax for interface endpoints. It's something you really should know. Remember, there's a regional one for simpler architectures or there's zonal for more control. Now, we mentioned private DNS and DNS resolution, but what does that mean? Well, private DNS in a VPC is simply a feature that enables you to make requests to an AWS service using the public endpoint DNS name. But when you do it, it allows you to leverage private connectivity via the interface VPC endpoint. So in other words, for our previous example of CloudWatch logs, when you just make a CloudWatch logs service call using maybe an SDK or the CLI, if you have private DNS enabled, it's going to automatically leverage those endpoints for you, which keeps the traffic all within the AWS network backbone. So that's what we mean when we say private DNS being enabled. Typically, you're going to want to turn this on for your VPCs if you're going to use endpoints. It makes things way simpler in terms of management. Now let's actually have a really quick review on some architecture. We use our typical example with a VPC and a single availability zone with two subnets. Now in this case, we have two private subnets. Now the reason for that is because you deploy interface endpoints into a private subnet. So on the left, we have our private server, which is going to have no or restricted access to the public internet. And then on the right side within the other private subnet, we deployed our VPC interface endpoints into that subnet. So that endpoint uses an ENI, and you direct traffic to that ENI via the endpoint service. Now remember from an earlier clip, we briefly mentioned that PrivateLink powers interface endpoints. Remember that. The underlying service for VPC interface endpoints is AWS PrivateLink. And if you remember with PrivateLink, it works this exact same way. You deploy a network interface into a chosen subnet in your VPC, and you can leverage the services that are hosted on it. The only difference is that AWS is managing the services on the back end. So once we deploy the interface endpoint and we get the ENI set up, we can then direct traffic to be routed via the internal AWS network and avoid the public internet entirely. Now a really, really big exam tip here. Amazon S3 can actually leverage both types of endpoints, so interface and gateway. I know this might be very confusing, and it can be a little annoying, but choosing the right endpoint type is likely going to come down to a very specific requirement within the exam. A very likely scenario that could come up is going to be cost efficiency. If that's the case, remember that gateway endpoints are free to use, and they're simpler to use. If you need much more control over the network traffic like security group control, etc, well then maybe you want to look at an interface endpoint. Those are two possible scenarios to choose the right endpoint. Now that's going to do it for interface endpoints. Just remember they're powered by PrivateLink, you deploy them into subnets, and they use ENIs to direct traffic. Let's go ahead and wrap things up, and I will see you in the next clip.

Demo: Interface Endpoints
All righty, welcome to this demonstration clip where we are going to work on creating interface endpoints for our VPCs. Quick architecture diagram overview before we jump in. We're already going to have a VPC created for us. And within that VPC, we will have only private subnets. Now within that subnet, we're going to have a private server that is deployed. We're going to try and connect to that server with Systems Manager Session Manager. Now the first time we do it, we'll see it won't work. It's not going to be available, and that's because we won't have access to the endpoint. We won't have access to the endpoint because there's no internet access in place, not through an internet gateway nor a NAT gateway. So, to leverage the private compute and connect to it securely, we're going to deploy the required VPC interface endpoints for Session Manager so that we can connect to it. We're going to deploy those endpoints into the same subnet, and they're going to have a security group that allows VPC traffic inbound. After we deploy the endpoints, we need to make sure that Private DNS is enabled for the endpoints and the VPC because we want to easily leverage these interface endpoints and not have to point to them directly. Remember, by enabling private DNS resolution, we can just make the normal call to Session Manager or the AWS service, and they will automatically leverage our endpoints for us. This is the easiest way to do it. After we put everything together, we're going to see that we can now connect to the private instance via the Session Manager session. So let's go ahead. Let's jump in the console now. All righty, welcome to the cloud sandbox. Let's get started in the console with this demo. We're in the VPC dashboard, and let's go over the pre‑existing resources really quickly. We have here our DEMO‑VPC, and you'll notice under Resource map, we only have Private Subnet, so there's no internet connection. We also have an EC2 instance. It's a private server, and from a networking standpoint, we deployed it into the Private Subnet AZ A. And for security, we have an inbound rule that allows all VPC traffic, which is really not going to be used; however, I just included it just to have it. And then really, I have an EC2 role here that's going to allow SSM permissions. So we can connect to this instance with SSM once the connection is set up correctly. So before we even begin, if I click on Connect, we're going to notice that Session Manager is not online. The agent is not reporting online. Well, that's because we don't have access to the Session Manager endpoints. So let's fix that. I'm going to click Cancel. I'm going to go back to my VPC, and let's begin. The first thing I want to do is I'm going to go down to Endpoints here, and I'm going to click on Endpoints. I'm going to create a new one, and we can give it a name. Now the first thing I want to tell you is there are three endpoints that are required for Session Manager to work correctly, and I've listed them here in a callout, and we'll also see this in an upcoming module as well. But for now, if you want to follow along, just make sure you're selecting the same three endpoints. I'm going to call this first one ssm. I'm going to select AWS services, and let me filter for ssm, and I'm going to find the ssm endpoint here. It's an interface endpoint. I'm going to select it, and then we select the VPC we want the endpoint to be deployed to. I'll select the DEMO‑VPC, and then if I scroll down, we now select which subnets we want this to be deployed to. So general best practice would be to select all of your subnets; however, for this, I'm only going to select the one subnet we're using, so us‑east‑1a Private Subnet. We can see we have the IP address type set to 4 because that's what the VPC CIDR is set to. And next, we come down to Security groups. So because this is an interface endpoint, you control access via two things, the security group and the policy down here below. Now we're not going to get into policies because that's out of scope for this particular section. So we're going to leave Full access, but I do want to create a new security group to use with this. So what I'll do is scroll back up really quickly. I'm going to open up VPC in another tab. I'll scroll back down. Let me open up Security groups over here in this new tab, and let's create a new one. Now I'll name it interface. I'm going to give it a description. I select our DEMO‑VPC, and now we have to add an inbound rule. Now for the sake of simplicity in this demo, I'm going to allow all traffic from our VPC. So let me type in the CIDR here, 10.0.0.0/16, allow all vpc. But if you really wanted to, and a general best practice would be to lock this down even more. So if you want, you could lock it down to specific custom TCP traffic, or you could say all traffic from the security group. So we can say hey, I just want the app server security groups to leverage these endpoints. This way, no one else can use them but the resources that belong to that security group. So what I'll do is I'll delete the second rule. We're going to allow the entire VPC to use these if they wanted to. I'm going to scroll down, and I'm going to click Create. Awesome! So now we have an inbound rule, allowing all VPC traffic to use or hit this security group. I'll close this. Go back to the create endpoint screen, I'll refresh, and there we go, interface endpoints sg. Perfect! So I select this, I go down, and I'm going to click on Create endpoint. Now I have to do this two more times because again, there are three interface endpoints required for this to work. So Ill go back to Create, I'll call this one ssmMessages. It's an AWS service. I'll filter for ssm again. I'll select the messages endpoint, select the DEMO‑VPC, that same private subnet, select the same interface endpoint sg, and then I'll click on Create again. Now, third and last one. This one is actually called ec2Messages. So I name it appropriately. It's an AWS service. I'll type in messages, and you can see it here. So this is the third and final required interface endpoint for Session Manager to work. So I select it, and I'll set the same settings as before. Awesome! So now I've created all three required interface endpoints, and they've been deployed to our subnets. So while these are pending, what I'm going to actually do here is go to our instances page, and I'm going to scroll down, and I'm going to find Network Interfaces. Now within here, you're going to see a few things. We're going to have our app‑server. So this belongs to our actual EC2 instance. But notice there are three other ENIs. These belong to the VPC interface endpoint, and you can see that here. So these all map to one of those endpoint services that we deployed to. You can even notice the security group ID here that they belong to, which is the interface security group. So remember how these work. Interface endpoints get created, and then they deploy an ENI and network interface into the chosen subnets for you to use. So we created three endpoints in one AZ each, so we have three ENIs. Now let me go back to my endpoints here. I'll refresh, and now these are all available. So now if we go back to EC2, I'll find my instance. And to restart the SSM agent to speed up the connection, I'm actually going to reboot this instance. So while this is rebooting, I'm going to cut here, and then I'll resume once this is up and running. Okay, so I fast forwarded. We're in the future now. Let me refresh. I've selected my EC2, and now I click on Connect. There we go. This is already a better sign. We're not getting that same warning about the agent being offline. So I select Session Manager. I connect, and there we go. We are now logged into our EC2 instance. So our interface endpoints are working as expected. We've connected to this EC2 instance with 0 public internet access set up for our subnets. Now one last thing before we wrap up. I just wanted to show you on these endpoints here that we do have under Details Private DNS names enabled. So this was set up with the VPC and when we created the endpoints. So this is why we can so easily leverage this service because it's trying to make the call to the service name, which is the public endpoint name, and it's leveraging these private DNS VPC endpoints. Now that's going to do it. Hopefully you saw how easy it is to deploy an interface endpoint into a subnet and then connect to that endpoint for your service from private compute. Let's wrap things up here, and I will see you in an upcoming clip.

Module Summary and Exam Tips
Okay, way to hang in there. You've made it to the Module Summary and Exam Tips clip. Let's get started. First up, reviewing that gateway. Remember, these are only redundant within the AZ that they are deployed to. They're super powerful because they scale automatically to support up to 100 Gbps in network bandwidth. Another amazing feature is that there's no infrastructure for you to manage. You deploy it to a public subnet, and then AWS handles everything else for you. You just have to set up your routes. The key use for a NAT gateway is to allow private resources to reach the internet securely. That is their entire purpose. And then lastly, if there's ever a scenario for high availability with NAT gateways, you must deploy them into multiple availability zones. They're only redundant within their single AZ that they're deployed to. Another humongous thing to remember, NAT gateways have to be in a public subnet with internet gateway access to allow internet access. This is the whole point of a public NAT gateway. Shifting subjects, VPC peering. Remember that this is an easy method to securely connect VPCs via a direct network route. You can peer across regions and even across accounts. You also need to remember there is no transitive routing when you use VPC peering. This is a very important concept. And then lastly, just remember when you do peer your VPCs, compute will behave as if it's on the same network. You just need to make sure you have the appropriate network access controls in place like the NACLs, and security groups, and route tables. Other than that, compute should be able to communicate just fine. Another exam to peer with peering. VPCs cannot be peered if there are any overlapping CIDR blocks. It will not work. This is a popular exam scenario. This is also why you should be fairly good at calculating CIDR ranges. The better you are at calculating CIDR blocks in the available IP space, the better and faster you will be at picking this issue up. Lastly here, remember there are two parts to VPC peering. The requester VPC makes a request for the peering connection, and then the acceptor VPC can either accept or deny that peering connection. So remember, it must be requested, and it must be accepted. Next up, transit VPCs. Now I'm not going to review this entire diagram again. Just be familiar with the concepts. The big thing to remember is that if you need a transit VPC, you're going to have to deploy some type of custom EC2 router or EC2‑hosted VPN solution. Moving to PrivateLink, if you see a question about peering VPCs to tens, hundreds or even thousands of other customer VPCs to access a service, you should think AWS PrivateLink is involved in some fashion. The nice thing about PrivateLink is it doesn't require VPC peering. In fact, you don't even need to update the service providers' route tables, you don't need to implement NAT gateways, you don't need internet gateways, etc. It allows you to securely access services via the AWS network infrastructure. And lastly, remember, if you're using this to deploy your own shared service, it requires a network load balancer on the service provider's VPC, and then there's a shared ENI on the consumer VPC. They work hand in hand to provide that secure access. Moving on to VPC endpoints, you need to know how to use these and when to use these. Endpoints are perfect for when you need to connect to AWS services without leaving the Amazon internal network. So if there's a question or a scenario where you need to restrict public internet access for AWS service calls, think VPC endpoints. Remember the two types of endpoints. There's an interface, which deploys ENIs into subnets, and this costs money, but you also get more control. And then there's a gateway endpoint. These are free to use. You deploy the gateway and update your route tables to leverage the routes to the gateway. Now with gateways, you only have two currently supported services, so that should really help narrow down answers on your exam. You can only use them for Amazon S3 and Amazon DynamoDB. However, with that in mind, also remember S3 can leverage both types of endpoints. We discussed this earlier, but do remember it's going to come down to the requirements. Do you need more control or do you need something more cost efficient? That should be the really deciding factor for which endpoint you use. Now that's going to do it for this Module Summary and Exam Tips clip. Go ahead, review whatever you think you might need to before you move on. But when you are ready, I will see you in an upcoming module.

Amazon Elastic Compute Cloud (EC2) Overview
Amazon EC2 and AMIs
In this module, we're going to start looking at EC2. During this first clip, we're actually going to discuss EC2 in general in a concept called AMIs. So what is Amazon EC2? EC2 is what essentially AWS is most well known for. It's an on‑demand, scalable computing service within AWS that allows you to run your very own virtual machines. The nice thing about EC2 is you get exactly what you need exactly when you need it. It is on demand. Now, this is classified as an Infrastructure as a Service because you're using their infrastructure to host your VMs. The key thing to remember with EC2 is it allows you to wait minutes, not months. So in traditional data center virtual machine configurations, that could take a long time. There's teams you have to talk to There's networking configurations that take a while to set up, etc. Well, with EC2, you have your VPC and you can deploy your EC2 exactly when you need it. So you just wait for the operating system to boot up, and you are ready to go. Now why would you use Amazon EC2? Well, it's really nice. Since it's on demand, what this really means is you get exactly what you need when you need it, and you only pay for what you use while it is running. So if your EC2 is shut off or shut down, you don't pay for that compute. You're only paying for the compute when it's actually up and running and doing something. The beautiful thing about it is that you get to avoid any wasted capacity because you also have the ability to grow and shrink the size of your EC2 instances as you need to. So if you realize you're overpowered or underpowered, well, the nice thing is you can change that to meet your growth or your current requirements. Now let's discuss some EC2 instance basics. When you are using the EC2 service, there are four primary categories of resources that you need to understand. The first is the compute. So this is going to be the amount or number of virtual CPUs that your machine has. You then have memory or RAM. This is the amount of RAM that you're assigning to your EC2 compute. The third is network. So what kind of bandwidth capabilities do you need? Do you require a high amount of bandwidth, or can you get away with something that's a little bit more limited because you're just testing something? And lastly, can't forget storage. So this is going to be some type of virtual hard drive, some type of temporary storage like a cache, or maybe you want to use a network storage. These four primary categories are extremely important when you're designing and deploying EC2 instances. Now, another important feature of EC2 are EC2 key pairs. A key pair is a set of regional security credentials that are used to prove your identity when you are connecting to your instances. How they work is they are comprised of a public key and a private key, thus the name key pair. So these are asymmetric keys. When you create a key pair to use for your instances, the public key portion is actually stored on your EC2 instance. And then you download and use the private key to connect to your instances. You'll use key pairs to do the following, SSH into Linux instances and decrypt admin passwords for Windows instances which are used to remote desktop in. These key pairs are used for nothing else. They're not used to encrypt data, etc. So on the exam, if there's ever a trick question like that, remember a key pair is only ever used to connect to an EC2 instance. Moving on, when you create an EC2, you have to choose what is called an AMI. And AMI is short for Amazon Machine Image. All these are are images for VMs to use that are provided by AWS, which contain the necessary information required to launch your instance. So what we mean with this is that AMIs contain things like the operating system configuration settings, maybe host names, other things and features on your instances that you want to capture for repeatability. Now you have to select an AMI before you launch an EC2 instance. This is how you reference the underlying operating system. And with that being said, an AMI can be as simple as just a base Ubuntu image, a base Windows Server image, etc. It doesn't even have to be complex. There are two categories or types of AMIs that you need to be familiar with. There's a public AMI. So anyone with an AWS account can explore these and then launch them as needed. And these are going to be things like your generic operating systems, so Amazon Linux, Ubuntu, Windows, things of that nature. You also have private AMIs. So these are where you create your very own or someone else creates their very own. And you can choose to keep them private, or you can even share them with the world or specific AWS accounts. So a private AMI is perfect for a golden image where maybe you have to create a new image each month with the most up‑to‑date patches on them or some type of software update and you want to share it very quickly. So let's talk about a few more things you need to know about AMIs. They are perfect for prebaking. This is a concept where you embed a significant portion of an application artifact within the image so that it's loaded at instance launch time and it's ready to go much faster. A use case for this would be like we kind of mentioned earlier, creating a golden image. So maybe you have a single image with your operating system and software that contains all of the necessary security patches and updates that you want to deploy monthly. A second use case is it's useful for standardizing your deployments to really make sure that each of your instances have the exact same installation and exact same custom software configurations. Now that's going to do it. Let's go ahead and wrap this clip up here. I think we've learned enough about EC2 and AMIs. So whenever you're ready, we can go ahead and pick up with the next clip.

Amazon EC2 Sizes and Instance Types
All righty, welcome to the next clip where we are going to talk about Amazon EC2 sizes and instance types. When you're choosing your on‑demand EC2 instance, it's very important that you choose the correct instance that you actually need. And there are a few different things that you need to understand that go into this decision‑making. The first is the class and the generation. So what this means is what type of instance are you launching and what generation of that type are you launching? Now we'll discuss types here in a moment, so don't worry if you're not really sure what that means. But the second portion here is the instance size. So this is a little bit more simple. It's how big or how small do you need your resources to be? Do you need quite a bit of RAM and CPU or can you get away with a little bit of both? Now when you're picking out an instance type, there's a specific nomenclature you need to be familiar with. So let's break that down. Right here in front of you is an actual real‑world example of an actual EC2 instance that you can launch. The first portion is the family or the instance type. So in this case, it's a type T‑family. Now you'll notice there are tons of different instance types when you're going through and launching them, and you don't need to necessarily memorize all of them. But you should be familiar with a large majority, at least at a high level of what they do. So a T‑family is a general purpose burstable instance type. The next portion here is the generation, so 4g. This is the fourth generation of the T type instance that we're looking at. And then lastly, we have the size of the instance. So this could be micro, it could be extra large. There are a lot of different sizes that are specific to the amount of memory and CPU that you need. Now it's also important to understand that the sizing of your instances also could directly relate to your network bandwidth. So you really need to spend the time and understand what your requirements are when you're choosing an EC2 instance size. Now this is just one simple example. Again, you will see there are numerous possibilities when you're choosing an EC2 instance. So really take your time and choose the right one for you. Now like we mentioned, there are many different categories of instance types that you should at least be semi familiar with, so let's explore those now. The first one is general purpose. So this is going to offer a balance of compute, memory, and networking. Now these are going to be primarily M and T‑type instances. So we just looked at that T‑type nomenclature a second ago. That would be a general purpose instance. It's also important to call out that T‑family instances are burstable. When we say burstable, that means they can temporarily go beyond their resource limits that are assigned. Now it's not unlimited. You get credits for this, but it's important to know that you can use them for small bursts past those limits. Next is compute optimized. So these are perfect for compute‑bound workloads, which is why they're called compute optimized. So think of these instance types when you need high performing processing. These are good for modeling and batch processing, and they're primarily going to be the C‑type instances. Next, we have memory optimized. So these are perfect for workloads where you're processing large datasets in memory, so maybe things like caching. These offer you a high memory per CPU ratio. So you're getting a lot of RAM for a little bit less CPU. These are primarily R, X, and Z‑type instances. Next up, we have accelerated computing. So these are good for hardware acceleration needs, so things like video transcoding or graphics rendering. These will incorporate GPUs for performance boosts. So if you see something that requires some types of transcoding or GPU requirements, you might think of this instance type. These are primarily going to be P and G instances. Then we have storage optimized. So you use these for workloads that have or need high sequential read and write access to local storage. These are going to be perfect for low latency requirements or applications that have high IOPS requirements. These are going to be I instance‑type family. And lastly, HPC optimized. So these are the best price performance for running HPC workloads at scale. So if there's scenarios involving genomics projects, running simulations or even running your own machine learning models, you want to look for an HPC instance‑type family. Now let's have a quick summary because we learned a lot there. That's a lot of information. Here are three things you should take away from this lesson. Really make sure you take the time to choose the correct instance type and the correct instance family for your use case. Go back and review those types of instances we covered if you need a refresher. Remember, you can scale the power up or down based on the size of your instance that you choose. So do you need a ton of memory? Do you need a ton of CPU or do you need just a little bit of both? There are plenty of options to help meet your requirements. And lastly, remember, the bigger the instance, the more expensive it's going to be. You're paying for compute and the RAM that are assigned to your instance. So the more that are assigned, the more money they're going to cost. Please keep that in mind. All right, let's go ahead and end here. I think that's enough. That's a ton of information. We'll go ahead and move on to the next clip whenever you're ready.

Demo: Launching an EC2 Instance and Creating an AMI
Okay, let's get started with this demonstration clip where we're going to launch an EC2 instance. We're going to customize it, and then we're going to create our very own customized AMI to launch a new instance off of. To start things off, we're going to be a user, obviously, within the console, and we're going to spin up an original instance. So we're going to launch a new EC2 instance. We're going to connect to that instance, and then we're going to install an application, customize it, and then create a simple text file to show how things carry over into the image. Once we stand up this instance and customize it, we're then going to go through the process of customizing or creating a customized AMI. Now we'll view the process in the console, of course. But when your EC2 is ready, you're going to want to stop the instance. This really helps with the data integrity portion of your image. And then we create that AMI, and we can then reuse it to deploy a new instance with those customizations. So let's go ahead and jump out of here. We're going to get into the AWS console, and let's begin this demonstration. All right, hello, and welcome to the AWS sandbox environment. I'm logged in. I have navigated to my EC2 dashboard here, and let's go ahead and get started. First thing I need to do is I need to launch an instance. So I'm going to title my instance here. I'm going to scroll down, and then now we decide what AMI we want to use. So in the console, there's a quick start menu where you can select the base operating system, so Amazon Linux, macOS, Ubuntu, Windows, etc. And then within these quick start menus, there are more specific images for you to pick from. So you can pick a Windows image with SQL Server. You can pick a desktop with Office on it. There are tons of different options. Now this is just one way to view them all. What you could also do is click Browse more AMIs. Now, I'll tell you right now. Obviously, we're not going to go through 11,000 AMIs. That's way too much, but feel free to poke around here in this menu. And you'll notice there are a ton of different AMIs that are selectable for you to deploy on your EC2s. There's quick starts. There's AMIs created by your own accounts or your own organization. There's Marketplace AMIs, so people that sell specific configurations like OpenVPN, for example. And then there's even community AMIs. So these are images that people have customized and then shared to the general public to use for their own needs. Now what we're going to do is actually cancel out of all this. I'll go back to launch an instance. We have our name here, and I'm just going to go ahead and select Amazon Linux. So I'm going to confirm my changes. I'm going to make sure I just use 2023, the base AMI. And after I do this, I'm going to scroll down to Instance type. Now for this, I'm going to go ahead and do a t3.small. So we're using the T family third generation and then a small size. Big thing here before I do select this. As you can see, the sizing matters the most here. The micro only has one virtual CPU and just over half a gigabyte of memory. If we go to small, which we selected, notice which is bigger, we have 2 CPUs and 2 GiB of memory. So I'm going to select t3.small. I'm going to scroll down, and I need to create a new key pair. So I'm going to click Create key pair. I'm going to give my key pair a name. I'm going to use the .pem format because I'm on macOS. And I'm going to create it. I save the file here, and then I continue on. So I'm using my new key pair to connect. We'll leave the default network settings for now. This is using the default VPC in a public subnet, which is perfect. And I'm going to create a new security group. I'm going to allow SSH from anywhere. Again, this is not generally a best practice, but I'm just doing it to speed things up. I'll scroll down. We'll verify we don't need anything in Advanced details, which we really don't. And then I'm going to go ahead, and I'm going to launch my instance. Awesome! So our instance is now created. And what I'm going to do while this is initializing and getting started on running is I'm going to load up my terminal session. So while this is initializing, let me skip to my terminal. Let me clear my screen here, and I need to change the permissions for my key that I just downloaded. So let me do that really quickly. Okay, so we have that changed. Let me go back to my console. I'm going to select my server. I'm going to refresh, and this should be up and running now. So what I'm going to do is copy my public IP, go back to my terminal, and let me ssh in. Awesome! So I'm on my Amazon Linux server that I just spun up. Let me clear my screen here. Let me look at the directory. I'm in my ec2‑user home directory, and there's nothing here. Now what I'll do is I'm going to clear my screen, and at first I'm going to verify that Apache web server is not installed. So let me paste this in, and I should an error. Perfect. That's because this program and this application is not installed. So what I'll do now is actually install it. Awesome! So that's installed. The next thing I want to do is I'm going to demonstrate like we have a license file that we want to be created and maintained on every image that we take. So let me go and make a new license file. I'm going to put in some gibberish here. I'm going to write and save it, and there we go. So now I have this license file here, and we have our Apache web server installed. So these are additional configuration steps that we took off the base image. Now what I'm going to do is I'm going to exit this, clear my screen, go back to my console, and I'm going to take an image of this EC2 because we now customized it. So what I'll do is I'll go to Actions, I'll go to Image and templates, and I'm going to create an image. Now from here, we can give our image a name. So let me go and name this. We can give it a description, and then we have a Reboot instance checkbox. So, this is what we were talking about when we said we're going to stop the instance. This is recommended by AWS because it ensures data consistency, which is important, especially for application configurations. So I'm going to reboot the instance, and you'll notice we can even add volumes to the instance. Now I'm going to leave the default here. We're going to match all of these defaults or leave them as is, and I'm going to create my image. Awesome! So now we can see it's creating our AMI here. I'm going to click on this. It'll load me into the AMI list. And if you need to, you can navigate to Images over here on the left‑hand menu. Now we see our apache_web_server AMI. And if I scroll right here, we're going to see that the status is pending. It's also pending here. So what we're going to do is I'm going to pause here, I'll fast forward into the future once this is available, and then we will continue on. All right, we are now at a status of Available. So I'll give you a quick heads up. That took several minutes. It's not an extremely fast process. Just keep that in mind. But now that we're here, let's look at the details. You can see on the Details pane, we get our AMI ID. We have our AMI name and a bunch of other information like the platform, architecture, etc. Now we also have a Permissions tab. So under here, this is where you can share your AMI. You can make it publicly available. You can share it with accounts, as you can see here. And at the bottom, you can even share with specific organizations and organizational units, which is a topic that we cover in another course within this learning path. The key thing to remember is you can share AMIs privately, publicly or not at all. The next thing we have is our storage, so what devices are attached to this image by default. We see we have one here, and we see the size and the type. So what I'm going to do now is I'm actually going to go ahead, and I'm going to select Launch instance from AMI. So I'm going to click this. We can give it a name, and you'll see it already populated the AMI for us via this AMI from catalog. So apache_web_server, we have our description, our AMI ID here, and now we can do pretty much the same thing. So let me select the image size here or the instance size, I should say. I'm going to scroll down. We'll use the same key pair. We'll use default network settings. I'm going to select that existing security group that we had. And then I'll go down here, and I'm going to click on Launch instance. Okay, so now we have our new instance launched from our EC2 AMI. I'll select this. Let's go to this menu really quickly. And what I'll do again is I'm going to pause here, cut forward. And then once this is actually available, even though it says running, so let me just go ahead and pause really quickly, and then I will restart when we're ready to go. Okay, it's been just under a minute. We should be good to go now. Let's go and test this out. I'm going to copy my public IP, go to my terminal, and let's ssh to this instance. Perfect! So now I'm on this new EC2 that leveraged that instance image that we made. So let's go ahead and test something out. I want to make sure that our Apache web server is installed and we have our license file. Remember, those two things were done after we launched off the base image originally, so we're hoping they were captured in this image. The first thing I'm going to do is I'm going to try and start my Apache web server. Okay, that's a good sign. We didn't get an error. Now we can check in our home directory for our license file. Perfect. So if we cat that, the data is all there. This has worked successfully. So now we've created our very own custom AMI. We launched a brand‑new EC2 referencing that AMI, and everything checked out appropriately. Let's go ahead. Let's end this demonstration. And then when you're ready, we can move on.

Amazon EC2 User Data
All right, welcome to this clip where we're going to start talking about a concept known as EC2 user data. User data is passed into an instance to perform automated configuration tasks and even run scripts right after the instance starts. Using these scripts is a process that is commonly referred to as bootstrapping. So we looked at prebaking instances using AMIs. Well, this is the counterpart known as bootstrapping where you're running scripts at boot time. There are two formats that user data can come in. There's simple plain text, and then there's Base64‑encoded text. Let's look at some examples of each. In this first example, we can look at plain text. This is exactly what it sounds like. You pass in your commands or your scripts in plain text so it's easily readable. So in this example, we're running a simple bash script for a Linux instance to install and then start and enable a specific service on the host. Now the other format, remember, was Base64. And this, if you're not familiar with it, is what Base64‑encoded text looks like. It looks jumbled up. It looks like a big mess. But I can tell you this, both of these examples that we just looked at do the exact same thing. That Base64 encoding is just a method to encode the text into a single string. So Base64 is really good for extremely long scripts that might cause some issues or have some formatting issues, or you can Base64 encode them and have a single long string instead. The EC2 instances are able to identify and use both formats, so choose whatever one works best for you. It's also important to call out that Base64 is not encryption, it is encoding. Please be aware of that. Now let's talk some use cases for user data. User data is perfect for bootstrapping applications. So say you want to deploy a web app on your instances and you also need to install and configure the application automatically during the boot. Well, you can do that using a simple script. It's also very useful for configuring instance settings. So maybe you want to configure operating system settings like time zone, locales, and even maybe a keyboard layout on a fleet of EC2 instances all at once. And lastly, it's very useful to join a custom domain. You can pass in your user data as a script to help the instance join to one of your domains and even configure the necessary settings required to join that domain. These are all examples of use cases for user data. Now that's going to go ahead and do it. Let's end things here. Coming up next, we're going to have a clip where we actually have a demonstration on using the user data.

Demo: Passing in EC2 User Data
In this demonstration clip, we're going to work on passing EC2 user data into our EC2 instances. Now during this clip, I'm going to perform two tests. The first test is where we're going to pass in a simple user data script that's going to be written in plain text. Once we do that, we will launch the instance, and you'll see that it gets executed by the instance at boot time, and we'll verify it works by navigating to a simple web page. The second test we will perform is we're going to use that same script, but we're going to put it in as Base64‑encoded text. We're going to see after we've launched the instance with that user data, the EC2 instance that we use with this is going to execute the exact same data, and it's going to boot and result in the exact same server being hosted. Now with that being said, let's jump into the console now. All right, I'm in my AWS sandbox here. I'm in my EC2 dashboard. Let's go ahead and get started. I'm going to launch a new instance. I'm going to give my first instance the name of PlainText, and I'm going to scroll down. I'll select Amazon Linux. I'm going to leave the instance type. I'm going to say I don't need a key pair because I'm not connecting to the instance. And then under Network settings, I'll leave most of the defaults here, but I want to create a new security group. I'm going to unselect or deselect SSH, and I'm going to allow HTTP instead because we're going to test a simple web server. Moving down, I'm going to skip down to Advanced details, and I'm only going to change two things here. The first thing I'm going to do is I'm going to change the metadata service here to allow version 1. And don't worry about this. This is covered later on within the course. This is just for demonstration purposes. The next thing I want to do is I want to change the user data. So this is where we could either upload a file or copy and paste a file in. And remember, it's either plain text or Base64 encoded. So what I'll do is copy and paste in my file here, and you'll notice it's just a simple bash script. We're updating our repositories. We're installing a package. We're enabling and starting that package, and then I'm customizing my web page before I restart my web server. We will include this script in the assets for the module. So if you do really want to use this, feel free. It'll be included. For now, what I'm going to do is select Launch instance, and we get a successful message. So I'm going to go back to my EC2 dashboard, go to running instances here. And what I'm going to do is wait until this instance is up and running and showing up, and then we will resume. So let me go ahead and pause really quickly, and then I'll fast forward. Okay, I fast forwarded to the future. Welcome. It's almost the exact same as the past. Let's go and continue on. So we have our PlainText server. It's running. What I'm going to do here is I'm going to navigate to this public IP address. There we go. We get our simple web page. So this web page, remember, was passed in via a user data script. We ran that script automatically at boot time. It created a simple web page with information about our instance. So if we compare this to our instance, it should be the same ID, and it is, and it should be in the same AZ, and it was, us‑east‑1e, us‑east‑1e. So our script worked. Now let's go and do part two. I'm going to go back to Instances. I'm going to launch a new instance. I'm going to call this one Base64. I'm going to select pretty much all of the exact same settings as before, except for this time, I'm going to choose a different subnet. So the last one was in us‑east‑1e. Let me just choose us‑east‑1b to make sure we get a different web page so you can see that it's actually working. I'll scroll down. I'll use that existing launch‑wizard‑2 security group that we used just a second ago. And then I'm going to do the same thing under Advanced details. So I'm going to select under the Metadata version to allow V1 and V2. And then I'm going to copy and paste in the Base64‑encoded version of that same script. Notice this is a lot less friendly to read. It just looks like a bunch of mumbo jumbo, and it really looks like to the naked eye that it wouldn't do anything at all. But this is the Base64‑encoded version of that same script. Now to use Base64, we're going to go ahead and say yeah, hey, this has already been encoded, so you don't have to worry about encoding. Just decode it after it gets passed in. So I check here. I click Launch instance, and we get the same successful message. So let me go ahead and open up this in a new tab, and I'll do the same thing for this. I'm going to pause, wait until this is available, and then I will resume. Okay, I fast forwarded through the future again. Congratulations! You've time traveled twice in the same clip. Let me select my Base64 instance here. We see it's running. We get our public IP. Let me go ahead and open this in a new tab, and there we go. The same web page, but now we have different information. Our instance ID is different, and our AZ is different, and these will match our instance that just launched. So that'll do it. We've used user data to pass in a simple script that gets executed at boot time. We tested it both with plain text and with Base64 encoding. Let's go ahead and wrap things up here and we can move on.

EC2 Hibernate
All right, let's go ahead and talk about a feature called EC2 Hibernate. EC2 Hibernate is simply a feature of EC2 that signals the operating system to perform hibernation. This is also known as suspend to disk. This feature works by saving any content within the RAM, so temporary memory, and it saves it to an attached long‑term volume, so one of the attached hard drives on your EC2. This feature is extremely useful for quick recovery failures where maybe an application takes a long time to boot up. Well, you can use this to prewarm the instance and skip a lot of that waiting time. Now that we understand what the feature is, let's talk about the process on how it works. There is a specific order for starting up an EC2 after you initiate hibernation. The first thing that happens, your root volume or your root local hard drive is restored to its previous state. After that, the RAM contents are actually reloaded from that volume because they were saved, remember, on that volume and pulled out of RAM. So the operating system reloads the contents to RAM, and then any processes that were previously running are now resumed. The last thing that happens is that any previously attached data volumes are reattached, and the instance retains the instance ID that it was assigned. Now if you're not familiar with data volumes, don't worry. We cover EBS and those different types of volumes much more in depth later on. Just really understand this process. The base root volume is restored, the RAM contents are pulled off and reloaded to RAM, your processes resume, and then any previously attached extra volumes get reattached. Okay, with that out of the way, let's go ahead and wrap things up. Coming up next, we're going to have a quick demonstration on actually using this feature.

Demo: Hibernating an EC2 Instance
Okay, let's get started with this demonstration where we're going to show just how the EC2 Hibernate function and feature actually works. I'm in my console here. I loaded up EC2. Let's launch an instance. I'm going to give my instance a name. I'm going to leverage Amazon Linux 2023. I'll leave the instance type the same, and I actually already have a key pair created to speed this up. So I'm going to select my hibernate key pair, and let's get down to Network settings. Now, I'm going to use the default here. This is perfectly fine. The only thing I do want to change is I have an existing security group as well, which locks down SSH to my particular IP. So I'm going to select that existing security group, and then I'm going to go down here. Now, one thing I want to call out, with the Hibernate feature, you must encrypt your volumes that are attached to your instance. So to do that within the console, we're going to click on Advanced. I'm going to drop down this menu here, and then there's a section for our volume, one that says Encrypted, Not encrypted. I'm actually going to change that to Encrypted and select this default value that pops up. Now, we talk about EBS volume encryption in a different module, so don't focus on that right now. Just understand if you are following along, you do have to select this Encrypted option and choose one of these drop‑downs. I would recommend the default. After I have encrypted my volume on my instance, I'm going to scroll down here. And under Advanced details, if we scroll down, there's going to be a Hibernate behavior section right here. So this is asking, do you want to enable hibernation for stopping? I'm going to say yes, I want to enable hibernation. And that's going to be it. We've now turned on this feature for our instance. I'm going to click on Launch instance here. I'm going to select my instance, and then what I'll do here is I'm going to wait just a couple of seconds until this is available and running. So once this is running, we'll connect to it, and we'll continue on. Okay, so it's now up and running. I'm going to copy my public IP, and then I'm going to navigate to my terminal session here, and I'm going to ssh into my instance. So I'm going to go ahead and use that key pair as my identity file. I'll accept that I want to connect, and there we go. Let me clear my screen to reset. And what I'll do to demonstrate this is I'm going to execute a very simple bash script, and we'll look at the script once I create it. So let me go ahead and create my script here. I'll paste. Let me go to the top. And all this is is a simple bash script that will be available in the assets document. But we're essentially just outputting a count, and then we're looping through it until we manually break it. And that counter is getting output to a log file in the same directory, count.log. So it's very simple. But the goal here is to demonstrate, hey, if this process is running in the background, it should not continue if we didn't have Hibernate enabled. But since we have Hibernate enabled, when we do put it into the hibernation phase, it should continue once we restart. So, let's go ahead. I'm going to write and quit out of this file. I need to make it executable, and let's test this out. I'm going to go ahead and execute my script, and we can see it's looping through and updating our counter. So what I can do here is put this into the background with Ctrl+Z. And really quickly, I'm just going to go ahead and cat that count.log file to make sure it's working, and it is. So let me go ahead and resume this script. We know it's counting out to our log. I'm going to go back to my console. I'm going to go and select my instance. I'm going to go to Instance state, Hibernate instance. It's going to ask, hey, are you sure you want to do this? I'm going to say yes, and we get success. So now it's in the stopping state, and eventually, we're going to get it into the stop state. So if I go back, you're going to see a broadcast message, which we see here. And eventually, we're going to lose connection to this instance, and there you go. So it just stopped. If I go back to the console, I refresh. Eventually, this is going to say Stopped, and there we go. So now this is stopped. So, I've lost connection. In theory, our script should be running and continue to be running once we pick back up. So before I start, let me go to my terminal, and we ended roughly around 39. So we should pick up roughly around 39. I'm going to go back, Instance state, start my instance. So what I'll do now is once this new public IPv4 is available because these are ephemeral IP addresses, I will go ahead and connect to it in my terminal. So I'm going to fast forward this refreshing so I can immediately grab this and we can pick back up. Okay, I have my new IP. I'm going to go to my secondary terminal here, ssh back in. We'll say yes. Let me go ahead and look at my processes. We should see that count, and we do. So it is still running it looks like, but let's verify. I'm going to tail ‑f my count.log. There we go. Our script is still running. So that process, that bash process running this script is picked back up where it left off. So hopefully, that demonstrates how the Hibernate feature works. It's perfect for pausing applications and then picking back up right where you left off if you need to stop. Let's go ahead. I'm going to end this demonstration here. Hopefully you learned a lot, and when you're ready, we can move on.

Module Summary and Exam Tips
Okay, way to hang in there. We've reached the Module Summary and Exam Tips clip. Let's get started with some stuff that I think is important for you to take away before you move on and before you go ahead and take your exam. Remember, EC2 instances are a virtual machine that is hosted in AWS instead of your own data center. The wonderful part of this service is it allows you to select the exact capacity you need right now. Then you can either grow and shrink on demand as needed as well. And lastly here, you only pay for exactly what you use. This is one of the benefits and one of the big driving factors of using EC2. If you don't have an instance running, then you're not paying for that compute. Moving on, remember how key pairs work. These are a private and public key pair that are used to connect to EC2 instances only. They do nothing else. If you SSH into your instances, then you use the private key directly. That private key matches the public key, which is hosted on your operating system, and then you can connect. If you're using Remote Desktop for Windows instances, for example, well then you use the private key to actually decrypt the admin password and then connect using those credentials. Moving on to user data, you need to know how this works. Remember that user data allows you to pass in a bootstrap script to your instance. A bootstrap script is a script that runs when the instance first starts up. So it's running at boot time, thus the term bootstrap. These are extremely useful for installing applications, as well as updates and more. So maybe you have some configurations that change each time an instance needs to start up, well then this is perfect for that. Next, remember there are an enormous amount of EC2 instance family types to choose from. Really make sure you're narrowing down your requirements for your scenario before you choose one. In addition to that, do your best to review the different instance family types that we looked at earlier. Moving on to EC2 Hibernate, just remember this is a feature for your EC2 instances that performs a suspend‑to‑disk task. That suspend‑to‑disk task saves data in your RAM and then attaches it to a long‑term volume. So it saves that temporary data and makes it long term essentially. This feature is extremely useful for fast recovery failures of an app that might take a long time to boot up. Now that's going to do it for this Module Summary and Exam Tips clip. Let's go ahead and wrap things up here. Take a break if you need it. And then whenever you're ready, I will see you in the next module.

EC2 and Amazon Elastic Block Store (EBS)
Amazon EC2 Storage
Welcome to this module where we're going to discuss Amazon Elastic Block Store. In this first clip, we're going to talk about the overview of EC2 storage options. There are two primary categories for EC2 storage options. There's block storage and then there's file storage. Now file storage at a very high level are things like Amazon EFS and Amazon FSx. Block storage includes Amazon EBS, Elastic Block Store, which is going to be a durable block‑level storage volume. They also offer what is called an instance store. This is a temporary block‑level storage for your instance. Now file storage services are going to be in their own section later on within this course. For this module, we're going to focus on the block storage options specifically. And with that being said, let's end this clip here. And we're going to pick up with Amazon EBS in an upcoming clip.

Amazon Elastic Block Store (EBS)
Okay, just like I promised, let's go and talk about Amazon Elastic Block Store. Amazon EBS is a scalable, high‑performance block storage resource that is used with Amazon EC2 instances. When you create an EBS volume and it is created and attached at launch, this is referred to as the root data volume. You must have a root data volume attached to an EC2 instance. You can't launch an instance without one of these. Some important concepts regarding EBS. What these are are network‑attached drives, but they function like normal local drives so like how a normal hard drive or SSD would work. It's just network‑bound, but they make them appear as if they're local. The benefit is these allow you to persist your data even after an instance is terminated. So just because you get rid of an instance, you can still save that volume and use it elsewhere. Now normal EBS volumes are bound to a single EC2 instance at one time. They don't belong to more than one instance. Now we have here (non multi‑attach) in parentheses because that's a special volume type that we're going to cover later down the line. But for now, remember, normal volumes are bound to a single instance. With that being said, the volumes are deployed and bound to a single availability zone, which really makes sense. They're bound to a single EC2, and EC2s can't span more than one AZ. Well, their volumes can't either. You deploy them to a single AZ, and you can only use them in that AZ. When you're working with EBS, there's some information you have to know. The first is the capacity. So you get to set a desired amount of space or capacity. And if you need to, you can increase them whenever you want. Keep in mind, however, pricing. You do get billed for any provisioned capacity. Now this is regardless if you're using that space or not. So if you create a 500 GiB EBS volume, well, even if you're not using any of it, you're getting billed for that volume. So keep that in mind. Moving on to an important point that you need to know. The default behavior for the root EBS volume that gets created and attached to your instance at launch time is deleted when the instance is terminated. We say it's the default behavior because you can optionally tell AWS that you want to keep the volume by deselecting Delete on Termination. So you can go in and say hey, I actually want to save this. Don't delete it on termination. So when we delete the EC2, the volume will stay put. Additional volumes, however, are the complete opposite. By default, they're not deleted on termination. You have to go in and manually select that. So just keep it in mind, root volumes by default are deleted on termination. Additional volumes are not. Now let's look at some use cases regarding EBS. You would use these the same way you would use any typical system disk on your laptop, on your home server, etc. They're designed for production or critical workloads. The reason being is that they're extremely highly available. They're redundant within their AZ. It's very hard to lose an EBS volume. Some use cases for an EBS, you can create a local file system on your server, you could run a database on your server. Obviously, you can run an operating system, so you can run your Linux or your Windows or whatever it may be. They're meant to store data on your EC2 instances, so long‑term data. And you can easily install applications on them. Again, these are just like any typical normal system disk that you would attach to your personal computer. Before we move on and close this out, let's have a couple additional important concepts for the exam. EBS volumes are automatically replicated within that single AZ that you deploy them to, and this helps automatically protect against hardware failures. So Amazon Web Services is doing their best to make these highly redundant and extremely durable. Also remember, you can dynamically increase capacity, and you can even change the volume type with no downtime or very little performance impact to your life systems. We are going to talk about volume types here coming up next. So let's end this clip, and I'll see you in the next one.

EBS Volume Types
Welcome to this clip where we're going to start looking at volume types for Amazon EBS. Within the EBS service, there are several volume types that you need to be aware of for this exam. The first primary category is general purpose. These are good for web server hosting, and they're good for workloads that just require a generally good balance of performance and cost effectiveness. You then have provisioned IOPS. These are going to be perfect for things like hosting EC2‑hosted relational databases where you have high input/output demands. IOPS stands for input/output per second. So if you need high input/output, well, think provisioned IOPS. The last primary category is magnetic. These are going to be very good for data archiving and data backups. Think of these anytime you need something EC2 based and you have any type of any large data that's infrequently accessed and needs to be stored cost effectively. You need to also be familiar with the syntax here or the names, gp2 and gp3 for general purpose, io1 and io2 for IOPS, and then sc1 and st1 for magnetic. Now let's dive into the different types a little bit deeper here. We'll start off with general purpose. Gp2 is a previous generation of the general purpose drive. These are good for boot disks and general apps. They offer up to 16,000 IOPS per volume. They offer 99.9% durability, etc. They're very good for general purpose workloads. Then we have the gp3, which is the successor or the newer version of the general purpose SSD. These are very good for high performance applications. They offer a predictable 3,000 IOPS as a baseline performance and 125 MiB regardless of volume size. So these are a little bit more performant than gp2, which makes sense because they're newer. Now they offer the same durability as gp2, so there's no difference there. But in general, you're going to want to use gp3 over gp2 nowadays. Next, we have provisioned IOPS. The previous generation is io1. These were suitable for things like online transaction processing, so things like online shopping, and other latency‑sensitive applications. They offered 50 IOPS per GiB and up to 64,000 IOPS per volume. So, they're very high performant, they're very quick; however, they are the most expensive option for EBS volumes right now. With that in mind, they do offer the same durability, 99.9%. Next, we have io2. So this is the newest generation. These are just like the last one, suitable for the same workloads. But the nice thing of these is that they're even more high performing. They offer higher IOPS per GiB and the same amount of IOPS per volume. With these, you actually get even more durability, so that's a huge selling point. They're more durable. They're the latest generation, and they're cheaper than io1. You really have no reason to use io1 over io2 in any scenario. Moving on to the next subject, multi‑attach volumes. We briefly touched on these in a previous clip. Now let's talk about them more in depth. EBS multi‑attach volumes are provisioned IOPS volumes that you can attach up to 16 instances at once. There's two big things there. There has to be a provisioned IOPS volume, and you can only do it with up to 16 instances. Those are key factors to keep in mind. Another important thing here is that they only work within the same AZ. This really shouldn't be a surprise as EBS volumes, remember, are availability zone‑dependent, and they're AZ‑bound. These type of volumes are no different. They only work in the same AZ. The nice thing is that all of the attached instances can read and write to the volumes, which makes sense. They're attached so they can read and write. However, to use these, you have to use a cluster‑based file system. A typical file system like XFS, ext4, etc, those aren't going to work. Just trust me on it. You have to use something like GlusterFS or another cluster‑based file system in order to take advantage of these. Let's look at some use cases for multi‑attach. They're very good for shared file systems based on that Gluster file system or a cluster‑based file system. They're good for distributed databases where you have a lot of different interactions going on, and they're also really good for high‑performance computing. Since they have to be in one AZ, that means all of the attached instances are in one AZ. And that means the networking performance between all of those instances and these drives is going to be extremely high performant. Now let's end things with a couple comparisons before we move on. IOPS versus throughput. You need to know the difference between the two. IOPS is meant to measure the number of reads and writes per second, so input/output per second. This is a very important metric for quick transactions, low latency, and any transactional workloads. These offer the ability to action reads and writes at a very fast pace. The higher the IOPS, the faster you can read and write. For these, anytime you need input/output performance, choose a provisioned IOPS SSD, io1 or io2. Throughput is actually measuring the number of bits that are read or written per second. So an example would be MB/s. This is very important for large datasets, large sizes of input/output, and complex queries. Those are key differences in metrics between the two. With throughput, they offer the ability to deal with large datasets. That's the point of throughput. How much throughput or how much data can you put through? If there's a throughput question regarding EBS, you're likely going to want to choose a throughput optimized HDD, which is st1. That's one of those magnetic disk types that we looked at earlier. Now with those comparisons out of the way, I think that's enough information for now. Let's go ahead and wrap this up. We'll end the clip here, and I'll see you in the next one.

Encryption of EBS volumes
Okay, let's talk security, specifically encryption of EBS volumes. The neat thing about Amazon EBS is that you can encrypt both the boot and any attached data volumes of an EC2 instance. AWS gives you this security feature. When you encrypt EBS, the encryption operations happen on the servers that host the instances. This means there is very little performance impact to those VMs. When you enable encryption of your EBS volumes, the following data types get encrypted, data that is at rest on the volume, any data moving between the volume and the EC2 instance it's attached to, any snapshots that are created in the future from that volume, and any volumes that are created from the snapshots that were created earlier. The encryption used leverages a service called AWS KMS, and it performs AES‑256 encryption. Now we cover KMS in a different portion of this learning path, so don't worry about it right now. Just understand that it uses KMS for encryption of EBS. We will remind you of this when we talk about KMS later on. Real quick exam tip. If you have an unencrypted EBS volume, you can create a snapshot to create a new encrypted copy of the volume. This might seem like a lot of work to do something so simple. However, the reason we do this is because you have to. You cannot directly encrypt existing unencrypted volumes or snapshots. Keep that in mind. If it's not encrypted when you create it, you can't directly encrypt it afterwards. Now you might be asking me, Andrew, okay, this is cool, but what is a snapshot? Well, I'm glad you asked. Snapshots are coming up next. Let's go ahead and wrap this encryption clip up, and then let's dive into those snapshots here in a second.

Amazon EBS Snapshots
Have no data fear, snapshots are here. Let's talk about Amazon EBS snapshots. What is a snapshot? Well, an Amazon EBS snapshot is simply a point‑in‑time photograph or copy of an EBS volume that is incremental in nature. When you create your first snapshot of a volume, it's going to take quite a bit of time compared to the future ones because that first snapshot is a full capture of all data. After that initial one though, all of the future ones only capture the delta changes. In other words, they're incremental, so they should go faster. When you take a snapshot, the snapshots capture all data and all configurations at the time of creation. What this means is every single configuration item is captured. However, if you start a snapshot at a certain point in time and then while it's pending you make some more data changes, well those new data changes aren't captured in that snapshot. Keep that in mind. It's only capturing at the time of creation. A nifty feature of snapshots is you can share them. You can share them with other accounts, and you can even share them between regions. Now to share between regions, you copy them, and we'll do that here in upcoming demonstrations, but just know you can do this. And the last major point here, snapshots are stored in Amazon S3, Simple Storage Service. Now it's very important to understand you have no access to Amazon S3 in terms of where these snapshots are stored. Understand that for the exam. That's a popular scenario. You can only manage the snapshots from the EBS menu. You cannot access the underlying storage where the snapshots are held. Now let's talk about snapshot lifecycles. When you take a snapshot, there are some things you can do regarding the lifecycle of those snapshots. One of the first features is archiving. You can archive snapshots, which allows you to store them for lower costs if you don't need them quickly. So an example use case here, maybe your snapshots are greater than 90 days and you're not going to restore from them any longer, well you can archive them. It's perfect for compliance purposes. So maybe you can archive to store for lower costs because you can't delete them. We also have a recycle bin. This is exactly what it sounds like. It's a data recovery feature to restore accidentally deleted EBS snapshots and EBS‑backed AMIs. This would be perfect if you're doing some maintenance and cleaning up and you accidentally delete an important snapshot. Well, don't worry. It's going to be in a recycle bin that you can restore. Next up we have fast snapshot restore. This is a feature that allows you to create a volume from a snapshot that is fully initialized at creation. What this means is that it eliminates any latency of input/output operations on the blocks when they are accessed for the first time. Essentially, you're getting delivery instantly of all of the provisioned performance for that EBS once that snapshot is restored. This is exactly what it sounds like. it restores them very quickly so you can use them very quickly. This is perfect for extreme scenarios where you have to have very low recovery point objectives or very low recovery time objectives. Now this feature can be enabled on snapshots with a size of 16 TiB or less. Anything bigger than that is not currently supported, so keep that in mind. It has to be 16 TiB or less. Two exam tips before we close this off. Exam tip one, remember you have no access to the underlying S3 storage that hosts your EBS snapshots. Please remember that. You can only manage the snapshots at the EBS level. Next, use snapshots if you need to copy data from an EBS volume to a different AZ or a different region. Remember one of the features of this is that you can do that. You can share your EBS with other accounts, and you can copy them to different regions or different availability zones. And with that, let's end this clip here. Coming up soon, we have a couple demonstrations to show you how to work with EBS snapshots. So let's go ahead and wrap this up, and I'll see you whenever you're ready.

Demo: Creating an EBS Volume and Snapshot
In this demonstration clip, we're going to work on a few things. We're going to create a new instance, and we're going to create, obviously, a root volume. And then what we're going to do is we're going to write some data to that volume. We're going to create a snapshot of the volume, and then we're going to show some different ways we can use that snapshot to restore the data. So let's get started. I'm going to launch an instance here. I'm in my EC2 dashboard. I'm going to give my server a name. We'll call it OriginalServer. I'm going to use Amazon Linux. We'll use the default t2.micro, and I've already created a key pair here that I'm going to use for connecting via SSH. I'll scroll down. We'll use the default network settings minus the security group. I created a new security group that is locked down to my IP address for SSH only. So I'm going to select that, and let's look at our storage here. I'm going to go to Advanced so we can see more info. We see our EBS Volumes, and our root volume here has all the information we need. So we see it's EBS type, the device name, the snapshot that is used to create it. So this is going to be a base image, and we see the size. Right now it's 8 GiB. We see the volume type where we can select Provisioned IOPS, Magnetic, etc. We can set our IOPS, we can flag if we want it to delete on termination, and we can set the encryption. Now we're going to leave the defaults here, and I'm going to scroll down and click on Launch instance. I'm going to select the instance, close my other tab here. And what we're going to do is I'm going to go ahead and copy this public IP. And once this is up and running, we're going to go ahead and connect to this instance. So let me cut forward into the future really fast. Okey dokey. It's up and running. Let's go to my terminal session. I'm going to SSH into this instance. So let me go ahead and get rid of this IP. And it's a normal command. We're using an identity file, which is my pem, as ec2‑user at that IP address. I'll connect, and there we go. Let me clear my screen. We are the ec2‑user. We are in my home directory, and there's nothing in there. So let's actually create a file. I'm going to create a file here called quote. And in this, I'm going to paste a simple quote for my good friend, Benjamin Franklin, and then save and exit. So now we have this file on the root device underneath the ec2‑user home directory. And it has some text within the file. Perfect. So I'm going to clear this. Let's go back to my console, and let's begin taking a snapshot of that drive. So I selected my server here. I'm going to go to Storage, find my volume, and click on the volume. So this is a shortcut to get to the attached volume if you need to. You can actually go to Elastic Block Store here on the left and select Volumes and then find this manually. When we get in here, we see our details, the Volume ID, the Size, the AZ, etc. So I'm going to select it. I'm going to go to Actions, and I'm going to create a snapshot. Perfect. So we see the source volume and the AZ. We can give it a description. We can see the encryption status, and then we create it. We get a successful banner here. I'm going to click on this shortcut. And again, if you need to manually get there, you can do it via the EBS menu here on the left and click on Snapshots. For now, our shortcut worked. I selected it, and we can see here in the top that it is actually pending, and the progress is at 4%. And as soon as I say that and refresh, it actually hits 100%, which is perfect. So we don't need to do any fast forwarding. Let's go ahead and continue on. So we have our snapshot ID, and then we see some other relevant information like the owner, status, description, source volume, etc. Now under the Actions menu for the snapshot, we have Snapshot settings. So you can modify permissions, aka, share it with other accounts, you can lock it, and you can manage the fast snapshot restore feature, which we already covered in another clip. We can also archive the snapshot. So if you get to a point where your snapshot is very old and you don't really need it anymore, but you can't delete it, then you can archive it. If you realize you need it, well then you can restore it from archive. However, what we're going to do is two different scenarios. The first we're going to do is create a volume from the snapshot. So when we get here, we see the volume settings. Now it's our Snapshot ID that we're creating it from, as you can see up here at the top. And notice we can change pretty much everything else. We can change the volume type, so I could select a provisioned IOPS volume. I can change the size, so I can go up to 10. But one thing to note here is you can't go below the original size. Keep that in mind. You can only go the same or bigger. So I'm going to make this 10 GiB. We'll leave IOPS the same. If it was applicable, we could change Throughput, and we can even change the Availability Zone. So what I'll do is I'll deploy this to us‑east‑1c instead. Basically, we could pretend that maybe we want to restore an instance to a different AZ for some particular reason. We can make it a multi‑attached volume because it is now an io2 volume, and we could turn on encryption. But we'll do encryption in a different demonstration, so we're not going to do this now. I'm going to click on Create volume, and there we go. Let me click on this, open it in a new tab, and we have our new volume. It's already available. Volume status is Okay. We see the new type, which we changed by restoring the snapshot to a new volume, and we increased the size of the volume. The neat thing is this is all done for us. We just set the settings, and then AWS Elastic Block Store handles it for us. Now from here, we could attach this volume to a new instance or a running instance. The one thing is I don't have an instance running in us‑east‑1c, and remember these are bound to availability zones. So if you had an instance running in this AZ, then you could attach it, and you could actually go ahead and mount it and then do whatever you needed to do on that instance with that volume. I'm going to cancel out of here. And instead, I'm going to go back to my snapshots here, select it, and we're going to do another method. Under Actions, I'm going to create a new image or an AMI from the snapshot. So this is a really cool feature. We can take the snapshot, create a new image, and then use that image to launch instances. You'll see we have our snapshot ID, we give our image a name, we give it a description, we can set the architecture, root device name, etc. I'm going to leave all of these as default. And then we get down to Block device mappings. So, this is the volume that's being created from our snapshot. Notice the snapshot ID matches our snapshot ID up top. So we could in theory also make changes here. We can make it bigger, we could change the volume type, etc. Now what I'll do is I'm going to leave it as is. I'm going to go down and create my new image. Sweet. So now if I select my AMI here. And if you need to, again, just like before, there's a menu here on the left under Images to find this AMI. I just took a shortcut. So we have our snapshot image. We see it's actually available because it's such a small amount of space. And what we can do now is launch an instance from this AMI, so let's do that. I'm going to call it NewServer. I'm going to scroll down. We see our AMI is already populated for us. I'll keep the default instance. I'll use the same key pair. Then I'm just going to select my existing security group. We'll leave the other network settings the same. And you'll notice here under EBS Volumes, we have our root volume, which references now our snapshot ID. So remember that original instance had a different snapshot ID for the base image. Well, we customized that, made a new image, and it uses this snapshot as the founding volume. So what we'll do here is I'm going to click on Launch instance, and we're going to test to see if our file is still in the home directory. I'll select my instance here. And just like before, what I'll do is I'll refresh this until it's running. I'll copy this public IP address, and then we'll go ahead and ssh into this new instance. So let me fast forward until this is up and running. Okay, it is actually up and running right after I said that, which is nice. Let's go ahead and go to my terminal. I'll quit out of this. Clear my screen. Let's ssh into the new EC2 instance. We'll accept it. Okay, we're logged in. Perfect. Well, what directory are we in? Ec2‑user. I'm imagining we are ec2‑user, which we should be. And is our file there? It is. Our quote.txt carried over from that snapshot. So now if we cat it, is the data there? Perfect. Our quote is in our text. We've now successfully restored a snapshot to a new volume, and we use that volume on a new AMI and a new EC2 instance. Hopefully you saw how easy it is to create a snapshot and the different methods to restore those snapshots. Let's go ahead and cut this demo here, and we'll pick back up with another clip.

Demo: Copying Snapshots Between AWS Regions
All right. In this demonstration clip, we're going to work on copying snapshots between AWS regions. In this scenario for this architecture, what we're going to do is create an instance in us‑east‑1. We're going to put some data on that root volume, and then we'll create an EBS snapshot. Then to simulate a type of disaster recovery effort, we're going to copy that snapshot to a completely different region. After it's copied to the new region, we're going to restore that to a new instance, and we're going to go ahead and make sure that the data copied over as well. So let's go ahead and jump into the console now. All right, we've jumped into the AWS sandbox in my console here. I've loaded up my EC2 dashboard. Let's launch an instance. Now this is going to be a very, very simple quick walkthrough, so let me speed through this. I'm going to give it a name. I'll select the default AMI here, default instance type. I have a key pair already created that I'm going to use. We'll use the default VPC, and I also have an existing security group locked down to my IP address. So what we'll do here is we'll use the normal default volume settings here. So our EBS root volume will be a gp3, 8 GiB, etc, and I'm going to launch my instance. Perfect. Let me select it, close my other tabs. And what we'll do is once this is up and running, I'm going to go ahead and ssh into this instance. All righty, we're up and running. Let me jump to my terminal. I'm going to ssh into this instance using my pem identity file. I'll click on yes. Perfect. So now we should be in our home directory, and we are. There's nothing in there. Let's create a test file. We'll insert a simple text string. I'm going to save it, and there we go. We now have our test file with our string. Let's see if this works. I'm going to jump back into my AWS console. I'm going to select Storage. I'm going to find my volume and select it. And once here, I'm going to create a new snapshot. We'll give our snapshot a description, and then we'll create it. Awesome. So let me go ahead and select the new snapshot. And what we'll do here is I'll wait until this is available and then we'll continue on. So let me go and fast forward really quickly before we move on. All right, we're now 100% available. I'm going to click on Actions. I'm going to Copy snapshot. Now, this is how we can change the region. You see the source snapshot and the source region. But what we want to do is copy it to a different region. So under here, we can give that copied snapshot details. I'll leave this as the default, and then we can choose the destination region. So in this sandbox, I'm restricted to us‑west‑2. So I'm going to select that. And another thing we can do here is we can encrypt the new one. Now I'm not going to do this. I'm just going to worry about copying it. So don't worry about encryption for now. But if we review really quickly, our source snapshot is in us‑east‑1. We're copying it to us‑west‑2 with a new description. I'm going to click on Copy snapshot, and there we go. It says it was created. Let's go ahead and check it out. I'm going to click this link here. It should take us to us‑west‑2, which it is, and it automatically loaded up our snapshot. There we go. It's looking pretty good. We have our snapshot being created. Progress is unavailable and it's pending, which is fine. I'll tell you what though. I'm going to pause here. I'll wait until this is ready, and then we will pick back up once it is available. Cool. So our snapshot is now available. It's completed. We see our snapshot ID here, as well as our description. Now one thing I want to call out is notice the Source volume. Now this is not important for the exam. It's just important to understand when you're actually doing stuff in the real world, so that's why I'm calling this out. The volume ID here is an arbitrary ID. It is not meant to be used at all. So don't reference the volume ID for these snapshots. Second thing to call out, since this is cross region, you are incurring cross‑region costs for moving data in and out of a region. Also keep that in mind. Now this took roughly four minutes I would say to complete, and it's not really a big snapshot. So I can only imagine if it's a massive snapshot that's a lot bigger, it's going to take a lot longer. So really keep that in mind as well. With that out of the way though, let's move on. Underneath this snapshot in our us‑west‑2 region, I'm going to go to Actions, and I'm going to create a new image from the snapshot. We'll give it an Image name. We'll give it a Description. We'll leave the other stuff, the defaults here. We see our Block device mappings, which is leveraging our copied snapshot. We'll leave the settings the same here, and I'm going to create my new image. Perfect. So I'm going to select my AMI here. We see it's available. I'm going to launch an instance from it. We'll call it our CopiedServer. We see we already have our AMI populated for us. We'll leave the defaults here. For key pairs, I've already created one called NewKeyPair while I was waiting for the snapshot to go ahead and become available. However, keep in mind, you have to create a new one within this region because key pairs are regional resources. So I select my key pair. I created my own VPC in this region. So we're going to select the Public Subnet here. I'm going to actually edit this and enable public IPs. We're going to create a new security group here. I'm going to call it the same as the previous one. I'll leave the description as is. I'm not worried about that. Now what we'll do is we'll leave this rule in place just because it's going to get deleted right away. We'll allow SSH from anywhere. I'll scroll down, and let's look at storage. I'm going to click on Advanced. We see our root volume here. It's using our snapshot that was copied over into this region, and we have our settings. So let's launch the instance. We see it launches. I'm going to click on it. I'll select it, and once this is up and running, I'm going to do SSH into this instance, and we're going to make sure our data really carried over with that copied snapshot. So I'll fast forward. And then once this is running, we'll pick back up. All right, it's up and running. I'm going to copy my IP here. What I need to do is change my permissions for my key pair really quickly. And then I'm going to use that to go ahead and ssh into my new instance. We accept. We're logged in. Things are looking good here. We're in our home directory. Let's check this out. There we go, Hello from us‑east‑1. Awesome. So we've now created a snapshot in us‑east‑1. We copied the snapshot to a completely different region, restored that snapshot to an image, launched a new instance of that image, and all of our data was there. With that being said, I hope you learned a lot in this demonstration. Let's wrap up, and then we can move on when you're ready.

Demo: Creating Encrypted Version of Unencrypted EBS Snapshots
Welcome to this demonstration clip where we're going to work on creating an encrypted version of an unencrypted EBS snapshot and volume. Let's have a real quick architecture run‑through before we jump into the console. We're going to have a running EC2 instance that's going to have an unencrypted EBS volume already in place. We'll ssh into it, and I'll show you some of the data that's already there. After we do that, we're going to take a snapshot of the EBS volume. After we view all of those pieces, we're going to go ahead and create a copy of that EBS volume snapshot, and we're going to specify that we want to encrypt it using a KMS key. Once that snapshot is ready to go, we're going to restore it and attach it to a new EC2 instance as a data volume. And we're going to see that the data is still there, but we'll notice that the data is encrypted within the details of that volume. So let's go ahead and jump in the console now. All right, welcome to the in‑console demonstration portion. Let's dive in. Right now real quick, let's review what we have running. I have an instance that I've called Unencrypted, and that's because under Storage, I have a root device and a data volume. Now you'll notice both of them are not encrypted right now. So what we're going to do is we're going to actually create a snapshot of our data volume here. And then we're going to create an encrypted version of the volume after we do that. So real quick, let's connect to this EC2 instance. We'll see 54. I'll copy this. Let me go to my terminal. I'm going to go ahead and paste in a command here. So I'm SSHing in using an existing key pair that I created for that instance. I'll hit Enter, and we're logged in. Let me clear my screen, and right now we're in home/ec2. Now what I can do here is I'm going to run lsblk, and this shows the mount points for my two disks that I have. So we see the first one here at the top is our 8 GiB disk, so this is the root device. It's also mounted at root. The second one here is our data volume. So this is the one that's mounted, and it's 10 GiB as opposed to 8, which if I go back to the console and I look at Storage, we could see the size difference. Let me go back, and you'll also notice we've mounted it at this location. So let me clear my screen here. I'll change directories into that mount. And inside here, we have a quote.txt file. So let me go ahead and cat that quote file, and you'll see we have just an inspiring quote here. Perfect. So this is what we're going to use to verify that the copied snapshot worked after we even encrypted it. So I'll clear my screen. I'm going to go back into the console. And the first thing I want to do is I'm going to navigate to my volume. Okay, so now that I'm in my volume screen here, I'm going to select our data volume. And the first thing I want to look at is you'll notice under Encryption that it's not encrypted down here. So this is perfect. So what I'll do now is I'm going to go to Actions, Create snapshot. We see our Availability Zone, which is fine, and we see our Description. So I'm going to add a description, and I'm going to click on Create snapshot. Okay, our snapshot has been created. I'm going to navigate to it. And what I'll do is I'll wait until this is available. So I'll just keep refreshing, and I'll fast forward, and we'll pick back up once it's ready. All right, it's now available. Snapshot is completed. We see it's not encrypted. So let's go ahead. We're going to make an encrypted copy of it and restore that to a volume. So what I'm going to do here is I'm going to go to Actions, and I'm going to click on Copy snapshot. We see our source here. We see our description. I'm going to change this to this is now encrypted. We'll keep the same region, and I'm going to encrypt this. So I'm going to select Encrypt this snapshot. We'll use the default KMS key because then it's free and easy to use. We see the KMS key information here, which is fine, and I'm going to click on Copy. Okay, we have our not encrypted version. But if I select my encrypted version, if I bring this up here, we're going to see, hey, this is now encrypted. And speaking of that, here's the encryption information right next to that. So what I'll do here is I'm going to refresh this, and then once this is ready, we'll pick back up. And speaking of that, it was very quick. It's now ready. And a big thing I want to point out again is I've called this out in other demonstrations. Never use this volume ID in anything else. It's just an arbitrary volume ID to record within the console. With that warning out of the way, we now have an encrypted snapshot. So let's go ahead and create a volume from this snapshot. So I'll make it general purpose. I'll leave everything the same. We'll change the AZ it was deployed in. So this will be in us‑east‑1a. We see the KMS information, which we could change if we wanted to. So we could change what kind of encryption is on here. But notice you can't deselect encrypt. This is encrypted. You can't undo that any longer. So once it's done, it's there. So I'm going to scroll down here. We're going to click on Create volume. I'm going to navigate to this volume, and here we go. Now in another tab here, I'm going to open up EC2 Dashboard, and I'm going to start a new instance in that same AZ. So let me go ahead and start this process. We'll call it Encrypted. I'll use the same operating system, AMI. I'll use a t2.micro. I'm going to select my existing key pair here. We'll use default network, except I'm going to actually specify AZ a as my subnet. Now this is because, remember, volumes are AZ‑bound. So wherever they're deployed, they can't span across AZs. So I select the same availability zone. I'm going to use my existing security group that I've already created here. Now under Storage(volumes), I'm going to expand this, and we see here that we have our root device. So we're just going to keep this for now, and I'm going to manually attach our new volume after this. However, for this demo, I'm going to encrypt this existing root volume. I'll use the default key here. And it's good to know that this is not required. You can have an unencrypted EBS volume and an encrypted EBS volume attached to the same instance. That is perfectly doable. But for the sake of total security, I'm selecting to do the root also. I'll go down, I'll launch my instance, and there we go. Let me go ahead and select it. I'll close my existing tab, select this, refresh. And then once this is running, what I'm going to do is I'm going to actually attach the new EBS volume that we created, which was encrypted, using that encrypted snapshot. So it's running. I'm going to go to Storage, and you'll notice we only have the root device. This is perfectly fine. So what I'm going to do is instead I'm going to go to Volumes. I'm going to look for encrypted volume, which is in us‑east‑1a, and you'll notice it's an available volume state. The other ones are attached and in use. So I select my new volume here. I'm going to go to Actions. I'm going to attach the volume because you attach the EBS volume to the instance. We see the Volume ID, the AZ. So that means I should be able to select our new instance, which I can. Now when you do this, you select the device name. So I'm just going to do my typical standard of /dev/sdf. And you'll notice this is really irrelevant within the operating system. I click on Attach volume, and there we go. Now if I go back to my instance here, if I go down and I look at Storage, boom. We see our secondary data volume. It's 10 GiB, and it is encrypted. Perfect. So let's test this out. What I'm going to do here is I'm going to connect this instance, so let me go to my terminal. I'll break out of my old session, and I'm going to ssh to the new one. I'm connected. Let me clear. Now, if I run lsblk again, you're going to notice that we have two drives, xvda and xvdf. So this actually did take, which is awesome. A lot of the time it won't necessarily be the exact same lettering. Just keep that in mind. In this example, it did, which is awesome. But you'll notice our root device is obviously mounted, you can see here on the mount point, but our data volume is not. So I need to mount this first, and then I can actually demonstrate interacting with that file system. So what I'll do here, I'll clear my screen. I'm going to make a new directory here called /mnt/my_new_drive. I'll clear my screen again. I mount the new drive to my new directory, and then I ls in that new drive. We see our quote.txt. So if I change to that mount, we can see our existing quote. So this worked perfectly. We're now on an encrypted EBS volume, which was created from an encrypted snapshot. And that snapshot was a copy of an unencrypted snapshot. After all of this was done, we still have our data intact. We're able to mount that data volume and leverage it on our new operating system. Hopefully you see how easy it is to create an encrypted version of a snapshot and an EBS volume when the parent or source volumes and snapshots are unencrypted. Let's go and stop this demo clip here, and I'll see you in the next one.

EC2 Instance Stores
In this clip, we're going to discuss EC2 instance stores. This is the other block‑level storage we mentioned earlier on within this module. The simplest definition for an EC2 instance store is that this is a temporary block‑level storage for your instance. When we say temporary, we do mean temporary. Here are some concepts that are important for the exam and just real‑world architecture decisions. An instance store volume is located on disks that are physically attached to the host of the EC2 instance, so the underlying host of the instance. They're perfect for temporary storage, so things like buffers, caches, and even scratch data that you don't really care about. How they work is they're made up of one or more volumes that get exposed as block devices for your operating system. So they look similar to an EBS volume, but they behave way differently. One of those key differences upfront is sizing. Sizing of these is going to be directly dependent on the number of available devices, as well as the EC2 type and EC2 size that you choose. These all directly impact the sizing for your instance store. Another humongous thing, these are only ever attached at EC2 instance launch. You cannot create them and attach them after you launch an instance. With those understood, let's talk about data persistence. With an instance store, if you reboot your instance, then the data will persist. You can use it again. It'll be there when you restart. You're good to go. However, if you stop, hibernate or terminate an instance using one of these, that means that every block of the instance store volume is cryptographically erased. In other words, it is gone for good. So if you stop, hibernate or terminate, your data is gone. If you reboot, your data will be fine. Really though, long story short, never use these to store critical data. This is a terrible idea, it's a terrible design, and you should never do it. If you need something long term, you should be using an EBS volume. And with that out of the way, we now know what instance stores are. Let's end this clip, and coming up shortly, we're going to have a demonstration on creating an instance store volume.

Demo: Creating an EC2 Instance Store
Okay, let's dive into this demonstration on EC2 instance stores. First thing I have to do is I'm in my EC2 dashboard, so I need to create a new instance. I'm going to launch a new one. I'll give it a name. I'll select Amazon Linux base AMI for now. But it's important to understand your AMIs need to support instance stores, as well as your instance types. So for instance, if we go down to our storage here, you're going to notice there's no support for instance stores with a t2.micro. And even m5.large doesn't support them, so let me show you really quickly, m5.large. I'll go back down here. There's no instance store menu. So you need to make sure your instance type supports it. So for this example, I'm going to search for m3 and choose m3.large. These do support instance stores. There's a massive list. I will include a link to supported instance types here in the module assets. But there are several other instance types that support this. With that being said, I'll choose m3.large. I'll go down to Key pair. I created an instance_store key pair already for me to use. I'll use default network settings regarding subnet and IP. I have an existing security group that is locked down to my IP address, and let's get down to Storage. So we see here, we have our root volume, which is an EBS volume, and we see the device name. I'll minimize this. And now we see Instance store volumes. So let's show details, and we see Volume 2. So we have an instance store automatically attached to this instance for us, which is awesome. I'm going to select sdf as the custom device name. But notice we can adjust the size. Remember, the size is dependent on the instance size and the instance type. You really have no control over the size of the instance store. It also says, hey, this is ephemeral. So be cautious. So with that being said, we have volume 2, which is our instance store specific to our instance type. I'm going to go down here. I'm going to click on Launch instance. I select my instance. I'll close my other tab. I'll refresh. And what I'll do here is I'm going to go to my terminal, and I'm going to connect to my instance here. Okay, I have SSHed in. That's perfect. Let me clear. And the first thing I want to do is I want to run an lsblk. I want to see what devices I have on my machine. We see here xvda, which is our root device, so our root volume, which is an EBS volume. But then we have this xvdf, and it's 32 GB, and it's mounted on /mnt. Well, remember when we had that instance store volume there, we selected the sdf device, which is what this maps to. So this is our instance store volume. Now if I do a quick clear and I run a permissions check on the / directory, we're going to see mnt is owned by root, and we can't read or write. So I want to change the permissions just for this demonstration. So I'm going to run a command here where I'm chowning. So ec2‑user owns that /mnt. Now if I run ll on that, which is a long list, okay, ec2‑user. Perfect. So what I'll do now is I'll change to mnt. I'll clear my screen. Nothing in here but a lost and found, and let's see if I can create a file. Perfect. So now I should be able to edit that file. Let me enter some text. I'll save it, and there we go. So now we have this file on that mounted instance store. I'm going to break out of this. Now remember, with instance stores, if you reboot, your instance store should maintain its data. You only lose it if you stop, terminate or hibernate. So let me go here. I'm going to reboot, and I'm going to give this about a minute to go ahead and come back up. So I'm going to pause, and then I will resume, and we're going to reconnect to this instance. Okay, so I've given it enough time. I'm going to go to my terminal. I'm going to ssh back in. I'll clear my screen. I'm going to run a quick lsblk again, and there we go. So we have our two volumes again. And just a heads up. If you do this too quickly, if you're connected too fast and you run this command, it takes a minute for this to mount. So it might not show up as being mounted just yet. Just keep that in mind. But we waited long enough, so it is showing up. I'm going to clear my screen, and let's look in /mnt. There it is. We have our test.text. Let's cat it. All your drives are belong to me! So this worked. Rebooting did not kill our text file. Now let's go and disconnect, and let's try stopping and then starting the instance. So I'm going to stop it. And what we'll do here is I will fast forward until this is stopped. And then once it is, I'll show you, and then I'll start it and I'll fast forward again until it's started. Okay, so a real quick cut. We've now reached the stop state. I'm going to start it. And then while this is starting, I'm going to go ahead and fast forward until it's up and ready. Okay, so I've waited about 30 seconds. It should be good to go. I'm going to copy my IP. Go back to my terminal. I'm going to run the ssh command with that new IP, which is here because I've already copied and pasted this to test. We have a new host fingerprint, which makes sense. I'm going to stop it. I accept. I'll clear my screen. We're going to run lsblk. Okay, we see that same instance store volume. Let's check out that mount. There we go. Our file has disappeared because we stopped our instance, which means our ephemeral storage, which is our instance store, deleted everything on it. So now you can see why you don't store critical data on those drives. Let's go ahead. We're going to end this demonstration here. Hopefully you learned about instance stores. We'll take a quick break, and then whenever you're ready, I'll see you in an upcoming clip.

Module Summary and Exam Tips
Okay, welcome to this Module Summary and Exam Tips clip. Way to hang in there. Let's review some important stuff that you should remember before we move on. First up, EBS volumes are simply virtual hard disks that get attached to your EC2 instances. With that, you need a minimum of one volume per instance, and remember this is called the root device. You can have many other additional devices attached, but you have to have that one single root device. You also need to remember the different volume types. They're important when you're making architectural decisions. You have general purpose, which are gp2 and gp3. You have provisioned IOPS, so io1 and io2. And then you have magnetic, sc1 and st1. Let's review these very quickly. So let's compare general purpose volumes first, gp2 and gp3. The biggest thing here is that gp3 is more suitable for high‑performing applications. With that being said, it's still meant as a general‑purpose disk. It also offers a predictable 3,000 IOPS baseline and a better throughput. And this is all regardless of volume size. Their ability is going to be the same. Moving on, let's compare provisioned IOPS. We have io1 and io2. It should be easy to remember because it's I, O, input/output. Now, in your scenarios, you're hardly ever going to need to choose an io1 disk. I'll just be honest. Io2 is much higher performing, and it's much less expensive, and it offers better durability. Pretty much everything about it is better. So if you get a question with input/output performance being the primary requirement, you want to think io2. Now moving on to the last category here, magnetic. So these are hard drives. They're not SSDs like the other ones are. We have st1 and sc1. Each of these are going to have their own specific use case, and sc1 will be best for less frequently accessed data, so in other words, legitimately long‑term data that you don't need to access. It is also going to be the lowest cost option. Other than that, they're pretty similar minus the fact that throughput is a little bit better on the st1. So if you need throughput performance, you would lean that way for your scenario. With all that in mind, do your best to review these and really understand the key differences between the comparisons. Remember, multi‑attached volumes can attach up to 16 instances in the same availability zone at one time. However, they require a cluster‑based file system. These bits of information are very important because in the future, we're going to cover other storage systems that could be similar in scenarios. So remember, you can only do 16 instances, it has to be the same availability zone, and you need a cluster‑based file system. These are majorly important to remember. Next up, volumes and snapshots. Remember, volumes exist in EBS, snapshots live in S3, and you have no access to that S3 storage. They're point in time and are incremental after the initial one. That first snapshot is going to take some time because of the size. And you can share the snapshots with other accounts and copy them to other regions. Lastly here, with your volumes, you can resize them on the fly, and you can even change the volume type, which is awesome. Moving on, let's talk about encryption. Remember, when you enable encryption on a volume, all data at rest is encrypted inside the volume using KMS. With it, all data in flight between the instance and the volume is also encrypted. In addition to that, any snapshots you take of the volume are encrypted, and volumes created from that snapshot are also encrypted by default. In reality, you should always be encrypting your disks whenever possible. There's really no reason not to at this point in time. There's no performance impact, and it's a far better security measure. Next up, a quick tip. Remember, you can use snapshots to create an encrypted version of an EBS volume that is not currently encrypted. Also remember this is required. You can't directly encrypt an unencrypted version. Moving on to instance stores. Remember, these are volumes that are essentially ephemeral storage, so they're temporary. They go away when your instance is stopped, hibernated or terminated. Now you can reboot, which is perfectly fine. But because of all of these, they're perfect for things that are temporary. So think buffers, cache, and scratch data, things that are not necessarily super important. That's going to do it for this module. Let's end things here. Please do your best to review those important concepts again, and then whenever you're ready, I'll see you in the next module.

EC2 Security Features
Connecting to EC2 Instances with Bastion Hosts
All right. Welcome to the next module, EC2 Security Features. In this first clip, we're going to look at connecting to your EC2 instances using what are known as Bastion hosts. What is a Bastion host? A Bastion host is simply a server running at the edge of your network, which is meant to allow secured and controlled access into your private network. Sometimes these get referred to as jump servers or jump boxes. So if you see one term or the other, they are generally going to mean the same thing. So we know what they are, but what do they do? Again, they're set up to control and allow external connections, so hosts and people on the outside of your network, to connect to the inside of your network and actually resolve private traffic. They're meant to serve as a secured middle point or a jump server to connect to your private resources. So that's why they're called a jump box sometimes. Now typically, these are deployed into public subnets. I say typically because there are instances where you might have some special edge cases where you use one of these in a private subnet. But typically, for this exam, in most scenarios, you would deploy these into a public subnet. Also, a majority of the time, and I would venture to say a large majority of the time, they're going to leverage SSH via port 22 for the initial connection. So you connect to your jump server via SSH. And then from there, you can connect to other resources. What that means is you need to be sure to set up your other internal security group rules correctly if you are using this approach. You want to be very restrictive on who can actually connect. And when they can connect, what kind of traffic or port ranges are you allowing from that server? Three primary concepts to remember. When you use a Bastion host, you should be locking these down to an extremely small set of allowed users and even IP ranges. Generally speaking, you don't want to just open this up to the world. One of the benefits of a Bastion host is that you can leverage their operating system firewalls to actually perform customized port forwarding and implement even further security control. So maybe you have a custom OS firewall like IP tables that you want to use, well, you can do that and you can leverage it. The third benefit here is that you can force authentication and security requirements using these. There are some AWS services that act similar to this. But if you need the fullest amount of control and you need to log connections with custom settings, Bastion hosts might be the way to go. Now let's look at an architecture example on what using a Bastion would look like. We have a very simplified VPC here. On the left, we have our Bastion host in a public subnet, and that subnet has our Bastion NACL assigned, and a Bastion security group is attached to our Bastion host. On the right, we have our private subnet. And in the private subnet, we have an app and a database tier. And then we have servers within their respective subnet and tier. Now the subnet on the right on the private side has its own other specific NACL, and they have their own security group. The first line of defense or control would be the NACL. So this Bastion NACL we can set up to allow specific source IP inbound requests over a specific port. The next layer of security is the security group. So we can have the Bastion security group have its own custom inbound rules to even further customize and restrict. Within the actual host itself, that's where the server configuration lives. So we can set up very specific auth requirements, we can customize IP tables, you can install some type of agent on there for monitoring, etc. The possibilities are endless, but just remember there is overhead associated with going this route. On the right side or the internal side of our diagram, our EC2 and RDS security groups for our app and our database could be set up to only allow inbound traffic for their specific ports from the Bastion security group. Remember, you can reference security group IDs within rules, and this would be a perfect example on when to do so. Moving on, let's look at port forwarding with the Bastion host. What we could assume is we have our NACLs and our security groups and everything all set up correctly. And if we have a malicious user trying to brute force their way in or send some type of unauthenticated request well, we could stop it at the subnet level. Or if somehow that got through to the security group, we could catch it there. And if they got through that layer, then hopefully our host is set up with a good authentication system and it can deny the request. But if we assume we have a good user who's SSHed in correctly and they're wanting to enable port forwarding, let's say specifically to our database host, well, we can do that. Using a command like you see above, you can set up remote port forwarding so that the port is forwarded from our client through the Bastion host and forwarded to the end host, which would be our database instance in this example. The command you see at the top of the diagram is a real‑world example on port forwarding that you might use. It's setting up port 8080 on our client host, and we're connecting to our RDS instance on port 5432, and we're connecting with our credentials on the right, user@bastion. You need to be familiar with this particular example. Moving on, last thing here. There is a service called Systems Manager Session Manager. Typically, you're going to want to favor this Session Manager over using Bastion hosts. Now, I'm putting this here early, but we're going to cover Session Manager coming up within this module. I just wanted to make this point known as it's very important. A large majority of the time, you're going to want to use Session Manager and not a Bastion host. Unless there is a extremely specific requirement for the use case, you're likely not going to want to use a Bastion. But with that being said, let's go ahead and wrap this up. And in an upcoming clip, we're going to have a demonstration on how to set up a Bastion and log in to the Bastion and connect to a private resource.

Demo: Connect to EC2 Using SSH
All right, welcome to this demonstration clip where we're going to connect to our EC2 instance using SSH. Real quick diagram overview of what to expect before we jump in the console. What we're going to have already existing is a simple custom VPC that has only public subnets in place. What we're going to create is a brand new EC2 instance running Amazon Linux, and we're going to create a brand new key pair. We're going to use that key pair to SSH into our instance. Now before we can do that, we're going to create a custom security group for the instance itself. And we're going to restrict access to only our IP for port 22. Once that is in place, we're going to demonstrate how you use the private key within the key pair to SSH into your EC2 instance. This is very important for connecting to any instances, and it's also very important for setting up a Bastion host connection which we will look at later on in an upcoming clip. So without further ado, let's end this. Let's jump in the console, and we will begin. Okay, let's dive into this demo. I'm in my EC2 dashboard in my AWS sandbox environment. First thing I want to do, I'm going to create a brand new instance. I'm going to give my instance a name, and then I'm going to select my base AMI which I'm just going to leave as the default, which is Amazon Linux 2023. Remember, we're SSHing into our instance, so we want to choose obviously a Linux‑based operating system, and Amazon Linux is just probably the easiest to get started with. So I select my AMI, I'll scroll down, I'll leave the instance type the default, and now we come to our key pairs. So I don't have any key pairs, and we're going to create a new one. So I'm going to select Create new key pair. I'll give my key pair a name, and then I can select my key pair type. And I'm going to select ED25519. I select pem for my key file format. But if you're using Windows and PuTTY, you'd want to use PPK. So I select PEM. I create my key pair. I save it to my local machine, and we're good to go. So now, if you remember what's happening with the key pair is AWS is creating the key pair, which means they create the private and the public key. We download the private key. The public key is injected into the operating system as a trusted key. So that's how we can connect. And I'll tell you what. I'll show you the trusted key file on the EC2 instance once we get in. Now that we have this in place, I'm going to go to Network. I'm going to choose my custom VPC. And I only have public subnets here to make this easier. So I'm going to leave the default, I'm going to auto assign a public IP address, and I'm going to create a new security group. So let me go ahead and give this a name. I'll give it a description. And then we get down to our inbound rules. So, luckily for us, the default is SSH for this first rule. So we'll leave type. Obviously, protocol and port are filled in. And then we have source type. Best practice here is to lock this down to a custom set of IP ranges. Or for this, I'm going to choose my IP. So now it's locked down to my specific IP address. I will leave the description blank, and that's really all we need. So I'm going to click on Launch instance. Our instance is launched. Let me go ahead and select it, close my other tabs here, and we see it's running. So what I'm going to do here is refresh one more time. I'm going to copy this public IP address. I'm going to jump into my terminal, and I'm going to connect to this EC2. So the first thing we have to do here is I have to change the permissions for my private key that I downloaded. So let me do that quickly. And just a reminder, we've done this in previous demos, but this is a required step. Private key pairs cannot be too open or too permissive. Otherwise, you will get an error. So make sure you change the permissions appropriately. The next thing I'll do, I'll clear my screen. I'm going to use this key pair to connect to our instance. So I'm going to run ssh, I'm going to pass in ‑i for identity file, and I'm going to point to that private key. So this is saying, hey, I'm proving who I am with this SSH private key for the key pair that you trust. We then specify the user, which is going to be ec2‑user. We say at and then that public IP, so at the host. I click on Enter. This message pops up saying, hey, we don't know who this host is. Here's the fingerprint for the public key. Do you want to trust them? I'm going to say yes. It adds it to my trusted file, and we are connected. So now I can clear. I could run to whoami. I'm ec2‑user. I'm on my EC2 instance. We can tell by the private IP here, which I can compare in the console. And now, we're connected to our instance via SSH from my local machine. So everything is set up appropriately. Now, before we wrap things up, I want to show you that trusted host file that I talked about. So to do that, I'm going to change to my local SSH directory, and you're going to see we have an authorized key file that's in this hidden directory. So if I cat this, we're going to see the public key here that was generated for us by the EC2 service. So notice it has the public keys string here. And then the ending comment is just saying the key pair name, which is ssh‑demo, which is what we named it. So that's how this works. EC2 injects this into this file, and this is what allows us to connect as our EC2 user, which is what we did. Now that's going to do it for this demonstration. We've connected to our instance using SSH. Let's go ahead and wrap things up, and then I will see you in an upcoming clip.

Demo: Connect to EC2 Using RDP
Okay, welcome to the next demonstration clip. In this clip, we're going to connect to our EC2 again. But this time, we're going to use Remote Desktop Protocol, or RDP. We're going to do the same thing to a certain extent. We're going to create a new EC2 instance, but this time it's going to be Windows. After we do that and during the process, we're going to set up a new security group to lock it down to our IP address to only allow RDP traffic inbound. We're also going to create a new key pair to leverage for this. Now you're probably asking, well, why do we have a key pair if we're not SSHing? Well, we're going to show you, but you use it to get the password, the admin password, to connect via RDP. That's the only reason you initially use it. So once we set it up, we spin the server up, we're going to use that key, retrieve the password, and then start an RDP session. So let's go ahead and we'll wrap this up here, and I'll see you in the console. Okay, I'm in my EC2 dashboard here in my AWS sandbox environment. Let's get started with this rdp‑demo. First thing I need to do, I have to create a new instance. So I'm going to Launch instance. I'll give my server a name. We're going to scroll down, and I'm going to select the Windows AMI. Now I'm going to leave the default here because we're not really doing anything on the actual operating system, which is perfectly fine. So I'm going to select 2022 Base. I'll scroll down. I'm going to give this a larger instance type just because this typically takes more resources compared to Linux. So I'm going to select m5.large, and then we get to our key pair. So I'm going to create a new key pair for this Windows instance. I'm going to give it a name. Now notice something interesting. With Linux, we can choose our EC2 key pair type. With Windows. that's not the case. Now this probably won't come up on an exam, but I think it's a pretty interesting thing to call out. They restrict it to RSA, which is one of the more tried and true methods and has been around for a very long time. I'll choose my pem format, and I'm going to create my key pair. I'm going to save it, and we can continue on. So, this is not the same as SSH when we were setting that up. They don't use the key pairs in the same way. We'll show you how to use them here in a moment. But for now, we'll move down. I'm going to select my custom VPC, leave it at Public AZ B, give myself a public IP, and let's create a new security group. I'm going to give it a name. I'll give it a description. And then we come down here, and you can see the inbound security group rule for a Windows instance is RDP. So this is the default rule. It was SSH for Linux; it's RDP for Windows. So that's kind of neat. So we'll leave type the same. I'm going to restrict it to my IP, and I'm going to click on Launch instance. So I'm going to select this instance. Close my old tab here. And while this is actually booting up, I want to show you the difference on that security group. So SSH is port 22, but RDP is port 3389. Now I can tell you, you need to be familiar with these basic ports. Please understand SSH is 22, RDP is 3389. There's a very good chance you'll have to diagnose something for a scenario on the exam that references one of those port numbers. Now that was the big thing I wanted to show. So let me go back here. I'll refresh. I'm going to go back to Details. Select my public IP. And the next thing I want to do here is before I can connect, I need the password. So what I'm going to do is I'm going to go to Actions, go to Security, and then Get Windows password. So this is where that key pair comes into play. You can see it saying, hey, the key pair you used was rdp‑demo. So please upload the private key or paste the contents in this empty field here to decrypt the password. So what I'll do is upload the private key. So let me go ahead and go to my downloads. Select the rdp‑demo pem. You see it pasted in here for us. And I can now decrypt this password. Perfect. So I'm going to ignore the private IP because we're not going to connect via private IP. I'm going to use that public one I copied earlier. But I do need to reference these. So notice username is admin, and our password is quite long. So let's go ahead and try and connect. I'm going to jump into my remote desktop client. Now, before we begin really quickly, this is just a macOS client for Remote Desktop Protocol sessions. So you might look different if you're doing it yourself. Just keep that in mind. For me, I'm going to go up. I'm going to create a new PC. And for PC name, I'm going to enter the public IP that I copied. User account I'll leave Ask when acquired, and I'm going to go ahead and select Add. Now it's going to be under Saved PCs for me. I'm going to double‑click this. It's going to try and connect. And it says, okay, well, I established the beginning of a connection. Where are your credentials? Well, lucky for us, we have those. So let me copy this, paste in admin, go back, copy my password, enter that, Continue. It's going to give us a similar message to the SSH style where it's like, hey, we couldn't verify this. Do you want to actually continue? I could show the certificate, but I don't need to. I trust it. I'm going to click on Continue. And just like that, we've RDPed into our Windows server. You can see all the stuff getting set up. And we are good to go. Now, I'm not going to wait this entire time for the operating system to set up. You can see that we're logging in. This will come up eventually. You could take my word for it. In fact, you don't even have to take my word for it because it just came up. But now we've connected to our EC2 instance hosting our Windows server in our VPC. Remember, you use the key pair to decrypt the admin password for the RDP session. I think that's good enough for now. Let's go ahead, we'll end this clip here, and then I will see you in an upcoming one.

Demo: Using EC2 Instance Connect
In this EC2 instance clip, we're going to look at connecting to an EC2 instance using a feature called EC2 Instance Connect. Real quick diagram overview before we dive into the actual console. In this demo, we're going to leverage the default VPC and then a public subnet within that default VPC. We are going to create a new Linux‑based instance using Amazon Linux. Now it's important to call out the reason for this is that this feature currently only supports specific Linux‑based AMIs that can run the EC2 Instance Connect software. Now Amazon Linux comes with it by default, and I'll show you a list of others that support it by default. But this is the reason why we're spinning up a Linux instance. A big requirement for this service is that your instance needs to have some type of public network connectivity so that the feature can connect. Now if it doesn't, you can set it up to have private connectivity via a VPN, for example. But our demonstration is going to require public connectivity. I'll say, for the most part, if you have a VPN in place, you might as well just be able to SSH in directly as opposed to using this feature. However, they do provide the option of having connectivity over something like a VPN. The next thing is the security group. The security group attached to this instance must allow inbound SSH on port 22. The underlying fundamental way this works is it leverages SSH to connect. The big difference though is that it's all done via a browser‑based session. So once we have all these parts in place, we're going to use this console feature, and we're going to connect to the instance using it and you'll see how it all works. So let's go ahead and jump in the console now. All right, I'm in my sandbox environment. I've loaded up my EC2 dashboard in our US East 1 region. Let's get started. First thing I have to do, I'm going to launch an instance. Let me give it a name. And then we scroll down here and we can choose our AMI. Now, I'm going to use the Amazon Linux 2023 base image because I know this is supported. However, if you want to use a different AMI or a different operating system for this, I have this page pulled up, and I will include this as a URL within the module files. But you'll notice, here are the default‑supported operating systems. Notice Amazon Linux is on there. There's CentOS, Red Hat, Ubuntu, etc. Now, if it's not on this list and you want to use your OS, then you have to follow directions to install that. That's out of scope for this particular module in this course, so we're not going to do that. Just understand that it is a prerequisite that the software is installed. So let me close this. Go back to my EC2. I'll select Amazon Linux, and let's move on. I'm going to leave the instance type the same because it doesn't matter. For the key pair, I'm going to choose Proceed without because we don't need this. We're not connecting to it via our own SSH client. We're going to use the EC2 Instance Connect software instead. So I proceed without. Network settings look good. Default VPC. Don't care what subnet as long as it's public because we're enabling public IP. And I actually have an existing security group that I want to show you. So I'm going to select existing. I'll select AllowAndru. And all this is doing is allowing SSH from my specific IP, and we'll look at that as our instance is booting up. Now this is really all we need to do. There's nothing else that we have to configure here, so I'm going to skip through. Click on Launch instance. And then I'm going to select this instance. So we see it's pending. And while it's coming up, I'm going to select this and I'm going to show you that security group real quick, AllowAndru. We have our port 22 TCP so SSH, and then my source IP range. So this is locked down to my specific IP, and this is not going to work. I'm calling this out right now so you understand before we move on, this is not going to work. And I'll show you exactly why when we start connecting. So let me go ahead, I'll refresh. It looks like this is running, which is perfect. I'll go back to details, and it looks like we're good to go. So let's go ahead and test this out. What I'm going to do is select the instance, which I've done. I'm going to go to Connect. And we see EC2 Instance Connect as an option here. However, look at the warning I highlighted. It's saying that hey, our IP addresses that we use for this can't actually access your system. So yeah, I allowed SSH. However, we need to allow SSH from this particular IP CIDR. Now this is an AWS‑managed CIDR block, and this is available in a document called IP ranges. So let me show you that really quickly. The AWS IP address range document is a JSON‑formatted document that has all of the current IP addresses that AWS services use. So again, I'll include this URL within the module documents as well. But you'll notice that you can download it, and then you can go ahead and find ranges within it. Now I have downloaded this already. I've ran this command But let's go ahead and search for this address. So I'm going to click on Find address ranges, and then we're going to look for IPv4 for a specific service in a specific region because we want to get this as secured as possible. So what I'm going to do here is copy this command. Now jq is a command line interface program that you have to install, and it just queries JSON output. So I have this installed, and I'm going to run this command. So let me copy it. I'll go to my terminal session. And we're in my downloads, so this is where that document is at. Now what I'll do here is I'm going to cat the file, and you're going to notice just how big it really is. So I'm going to cat it, I'm going to pipe it to less, and I'm going to go through this really quickly. Don't worry about the actual information. I just want to show you how long this document really is. There are tons and tons of CIDR ranges within this document. So these are all Amazon‑owned and managed. Now let me break out, and what we're going to do instead is I'm going to paste in that command we had and I'm going to change the service name. So you'll notice here we're looking in the region US East 1, which is good. But we need to change this service. Right now, it's looking for global accelerator. So what I need to do instead is enter EC2 Instance Connect. So let me do that now. I'm going to hit Enter, and there we go. So we found the value for EC2 Instance Connect in US East 1. And if I go back to my console, go back to my connect, it's the same IP range. So they gave it to us, which is nice, but I wanted to show you how to use that document. So what we need to do instead is allow SSH from this IP range. So let me copy this. I'm going to go back to my instance in a new tab here. I'm going to find my security group for this. And we're going to edit these rules. So let me go ahead and edit inbound rules. I'm going to remove my specific IP. I'm going to add that CIDR that they gave us, and I'm going to click on Save rules. So now, we're allowing SSH over port 22 from their AWS‑managed IP range. So now if I go back, let me refresh this. Perfect. We don't get that warning anymore because we are now allowing their service IP ranges to connect. So let's continue on then. We have our instance. We're going to connect via EC2 Instance Connect. You see the public IP and then the username. Now we're going to leave this because ec2‑user is the default AMI username. However, like the note you see here on the bottom, make sure you know how to use your AMI. Your user might not be the same. So I'm going to leave all of the defaults, and I'm going to click on Connect. And there we go. We now are connected via SSH to our EC2 instance using EC2 Instance Connect. We see our public IP on the bottom. We see our private IP the instance ID. I can run a whoami. I should be ec2‑user. And I should be able to ping amazon.com. Perfect. So our connection is working. Now before we wrap this clip up, check out the URL at the top. Notice the different parameter options that it's passing in. It's saying us‑east‑1. The connection type is standard. It's saying the instance ID. We have the username, and then we have the port. And notice it's SSH port because it's over port 22. So that's why you have to allow port 22. Now that's going to do it for this clip. Hopefully you've learned how to use EC2 Instance Connect Let's go ahead and wrap things up here and then we can move on.

Demo: Deploying and Using a Bastion Host
Hello and welcome to this demonstration where we're going to deploy a Bastion host and connect to a private server using that Bastion host. Let's have a quick architecture review before we actually jump into the console and start doing all of this. We're going to have some precreated resources for us to speed this up. We're going to have a VPC. We're going to have several public and private subnets. And we're going to use those to deploy two different instances. We're going to deploy a Bastion host in the public subnet, which we're going to use to SSH into our network. We're going to have a private server in a private subnet, and we're going to configure a brand new security group to only allow SSH traffic from the Bastion host security group. So we're going to lock it down in two different areas. The Bastion host will only allow our IP to SSH into it. And then on the internal side, the private server is only going to allow the Bastion host network connection. So to do this, we're going to use some forwarding, and I'll address that when we get started. But for now, let's jump into the console and let's get going. All right, I'm in my sandbox here. I'm at the VPC dashboard looking at my custom VPC that I've already deployed. And let me give you the lay of the land real quick before we move on. If I click on Resource map, you're going to see we have six total subnets, one public and one private, spread between three different AZs. The privates have their own route tables, and then the public share one single route table that goes to an internet gateway. The private uses a NAT gateway, which routes to the internet gateway. Now with that out of the way, let's get started. The first thing I want to do here is I want to go to Instances under EC2, and I'm going to create the Bastion host first. So let me click on Launch instance. I'm going to call it Bastion. I'm going to scroll down. I'll leave the AMI the same, but I am going to make the instance a little bit bigger. I found this just works if we can speed it up a little bit or give it some more resources, it seems to operate a little better. So I'm going to click on t3.small, I'm going to go down to key pair, and I'm actually going to select to create a new key pair. So remember, key pairs, if you're not familiar with, them are used to securely connect to your instance. They're required for SSH and RDP. So what we'll do is we'll create a brand new one that is managed by AWS. We'll download the private key and use that to connect to our instance. So I'm going to give it a key pair name. You can select your key pair type and your format. So I'm on macOS, so I'm going to use a .pem. But if you're on Windows, you're likely using PuTTY and you'd want to select ppk. So let me select my format, I'll click on Create key pair, and there we go. Now let me download this. I saved it to my local download folder. And we can continue on. So now what's going to happen is the operating system is going to automatically get configured with this key pair information, and it's going to add it to the trusted SSH key list on the operating system. So that's how this works. It adds that to the local files, essentially trusting the public key portion of the pair. We have the private key portion of the pair, and we use that to SSH in. Now moving on, we selected our key pair. We need to change our network settings. So I'm going to go under VPC. Choose VPC‑A. I'm going to choose public AZ A because we need this to be in a public subnet. It needs a public IP address. And I'm going to create a new security group. So let me call this Bastion security group. We can give it a description. And then we get down to our inbound rules. So the default is SSH, which is actually perfect because we're going to SSH into this host. So I'll leave the type. It fills out the protocol and port for us, which is fine. But source type, I want to lock down. Now you could do custom, but what I'm going to do is choose My IP. So now we can see that this is locking it down to only our IP address for SSH incoming traffic. Now I don't need to do anything else, so I'm just going to click on Launch instance. And after this launches, what we'll do is we're going to go back to EC2 here. I'm going to click on Launch, and I'm going to create my private server. So I'll call it private. We'll scroll down. I'm going to select a t3.small again. And I'm going to use the same key pair. Now I'll show you why here in a little bit because we're going to forward our authentication information through our Bastion host. In reality, you could create a brand new one and then try and configure the Bastion host to contain that, but we're not going to do that. I'm going to use the same key pair, so secret, and then I need to configure the network settings. So under VPC, we're going to use our custom. I'm going to put this in a private subnet instead. So we don't have a public IP and no direct route to the internet. And we're going to create another security group. So I'm going to call this private. I'll give it a description. And now we get down to rules. So again, we're going to use SSH because that's how we're going to test the Bastion host. But this time, we're going to do a custom source, and I'm going to specify the Bastion host security group. So I'll select Bastion security group. And now what we're saying is we're going to allow only resources with this security group attached to SSH into our private server. Nothing else will be allowed. I'll click on Launch instance, and there we go. So now what I'll do is I'll go back to Instances here, and I'm going to look at my running instances, and we see our Bastion and our private. So what I'll do is select Bastion, and we have our public IP. So I'm going to use this here in a moment. For now though, what I need to do is get over into my terminal. So let me switch to that. And the very first thing you need to do with a key pair, if you're not familiar with it, is change the permissions on it. It can't be too permissive. This is due to security reasons. If you have it too permissive, it actually won't even work. So what I need to do is chmod it. So I'm going to change the permissions to a 600, and I'm going to point to that key pair. Perfect. Let me clear my screen, and now we can attempt to SSH into the Bastion host. So to do this, I have SSH running obviously on my machine. And I'm going to add this key pair to my SSH agent on the back end. You can see it has added that identity to my keychain, and I can actually verify this running ssh‑add ‑l. There you go. So you see the key pair added to my key chain. Perfect. Let me clear. Now, since it's already added in my authentication chain, I can just SSH into that instance. So I'm saying ssh, the username ec2‑user, at the public IP. Now before I do this, let me break out because I want to show you another way to use that key pair. This is a common scenario. People like to do it this other way instead. So if I did want to, I could do ssh ‑i for identity file. And then what you do is you point at that key pair. After this, you can do the same format. So ec2‑user at that public IP. You'll see it ask if you really want to authenticate. I'll say yes and we're in. Now, that's just one way. I wanted to show you that. I'm going to break out clear my screen, and I'm going to leverage my agent instead. The reason we're doing this is because of a flag that we can pass in, which I'll show you right now. So let me ssh as ec2‑user at my public IP, and I'm going to pass in the ‑A flag. What this flag does is it's going to forward our authentication credentials into our next host. What that means is we have our private key here that we added to our authentication chain. It's going to forward it and do the same thing automatically on the Bastion host once we connect. Now, the reason this is important is because, if you remember, we used that same key pair for our private server. So now if I list my identities on the Bastion host, we see that same exact key pair has been forwarded for us. So now, what we can do is get the private IP of our app server, and we should be able to ssh as ec2‑user again. We get yes, and there we go. We are now on a completely different instance, which is our private internal instance, all connected through our Bastion host. So that's going to do it for this demonstration on creating and using a Bastion host. We created the Bastion host. We created the security group for it. We also created a shared key pair between the instance. And then we locked down our private app server within its own subnet and its own security group, which only trusted the Bastion host. I hope you've seen how you can use a Bastion host to connect now. Let's end this demonstration here, and then I will see you in some upcoming clips.

Connecting to EC2 via Session Manager (SSM)
Okay, let's dive into Session Manager. This is a critical service and capability you absolutely need to know how to use. What is Session Manager? This is a capability or a feature that's offered within the AWS Systems Manager service that provides us an agent‑based connection to managed EC2 instances, edge devices, and even managed on‑prem VMs. Essentially, it's a way to connect securely to your different compute, and it's all done via an agent. Here are some important concepts to remember for Session Manager. Again, connections are established via the SSM Agent. What this means is you don't need to open up ports for SSH or RDP. So it's a lot more secure in that aspect. Using this allows you to centralize access control via IAM policies to decide whether or not you want to allow Session Manager to even connect. A third thing here is you can log and audit your connections. So when there is a connection established or it fails to establish, you can log the API calls to Session Manager in CloudTrail, and you can log sessions and different metrics to both S3 and CloudWatch. These all integrate very nicely with this feature. Now I mentioned the SSM Agent in that first portion there, but what is that? The SSM Agent is the Systems Manager agent. And simply put, it's just a software that runs on EC2 instances, edge devices, on‑prem servers and VMs, and it's just an agent that makes it possible for Systems Manager to update, manage, and configure the resources. This is used in a ton of different scenarios for this exam, and it will be covered extensively throughout this entire learning path. So you really need to be comfortable with it. For this particular clip, it's used to actually connect securely to our instances. There are three major requirements for using this. First, Session Manager supports Windows, Linux, and macOS. Now, obviously, that's the majority of all operating systems, but you need to make sure your flavor of that operating system is truly supported. IAM permissions are required for connections to even be established in the first place. You'll need to remember this for EC2‑related questions on the exam. It's not just strictly a network access issue; it's a permissions issue as well as far as authentication and authorization go. You have to allow IAM permissions to connect. Thirdly, the SSM Agent has to have network access to the Systems Manager service. This could be via directly through an internet gateway, or you can set up VPC endpoints, which we discussed in a previous module. So just remember, the agent has to have network access to Systems Manager to connect. Speaking of endpoints, learn to love these three endpoints for Session Manager. These are the base requirements for connecting via the agent. You have the SSM, you have SSM messages, and then EC2 messages. Now I highlighted region in the black text here, the second portion of the addresses, because those are going to be dependent on the interface endpoints that you deploy based on their region. So it could be US East , US West 2, EU West 1, etc. That's a changing component. However, the rest of the endpoints is static. So please remember these three endpoints. Now I'll say this here. Typically, you're going to always want to use Session Manager to connect to your instances. We cover Bastion hosts. We cover SSH and RDP because you have to know how that works. But if you need to connect to your EC2 or your managed compute, you should honestly always try and leverage Session Manager. This is extremely important for the exam. Now with that being said, let's go ahead and we're going to end this here. Let's jump into a demo where we connect to an EC2 instance using the Session Manager agent.

Demo: Connect to EC2 via Session Manager in Console
In this demonstration clip, we're going to connect to an EC2 instance using Session Manager within the AWS console. Real quick let's discuss architecture at a high level before we actually dive in. We're going to have a VPC already created for us in the US East 1 region. And within that VPC within the region, we're going to have an existing set of public and private subnets, so a two‑tiered architecture. And we're going to have an internet gateway and a NAT gateway already in place. What we're going to work on creating is everything pretty much on the right side of this diagram. We're going to spin up a brand new Amazon Linux instance. And while we're creating that instance, we're going to work on creating a new EC2 security group. But in this security group, I'm going to show you we're going to have zero inbound rules defined. So we're not going to allow any network traffic in the traditional sense. I just want to show you that this will still connect because it's an agent‑based connection and it's not using ports. In addition to the security group, we're also going to create a brand new IAM role and an instance profile of course. And with that profile IAM role credentials, we're going to allow it to communicate with Systems Manager Session Manager. And I'll show you the AWS‑managed policy that you can use to do the same. Once all of these pieces are in place, we're going to see that Session Manager is going to have a network connectivity, and it's going to be able to connect to our agent installed and running on our instance, and it will be all done via a NAT gateway. This is a perfect scenario for a NAT gateway. Now once all of these pieces are in place, we're going to make a Session Manager call within the console, and we're going to connect to our instance, and we're going to poke around a little bit. So let's go ahead. Let's jump in the console now. All righty, I'm in my AWS sandbox console here. Just want to show you the VPC architecture before we dive in. I've deployed this VPC, and it's a two‑tiered architecture split between three AZs. So if I look at resources, you'll see public and privates in three different availability zones and then all of the route tables. We also have our NAT gateway for the private subnets, which goes out the internet gateway, so a typical architecture. With that out of the way, let me close this. I'm in my EC2 dashboard here. Let's create our instance. First thing I'm going to do is give it a name. I'm going to scroll down, and I'm going to select Amazon Linux 2023 because this has the agent installed by default. Now I'll include a URL for installing and working with this agent if you need it. But for this demo, I'm choosing Amazon Linux because it has it installed and running immediately. So I choose this. I'll choose an instance type. I'll just leave it as t2.micro. And then we get to key pair. Now we don't need a key pair because we're not using RDP and we're not using SSH. So we're not connecting in the traditional manner. So I'm going to say proceed without a key pair, and I'm going to go down to Network settings. So let me change my networking now. Let me choose my custom VPC. I'm going to choose a private subnet here. So we'll just choose private subnet C. Make sure Auto‑assign public IP is disabled, and let's create a new security group. Let me give it a name. I'll give it a description. And what we're going to do is I'm going to remove all inbound rules. So we have no inbound traffic specified on the security group. Now the next thing I need to do here is I need to go ahead and create a new IAM role as well. Remember, Session Manager and Systems Manager in general require IAM permission. That's how you control partial access. If you don't allow it in IAM, then the instance and the agent cannot even communicate with the Systems Manager service. So I'm going to click on Create new IAM profile, I'm going to open up this link, and let's get this created for us. First thing I need to do is click on Create role. I'm going to choose AWS service, and I'm going to choose EC2. Now, for the sake of simplicity, I'm going to choose the second option here, but I'm going to show you what this does. So I'm choosing role for Systems Manager. I'm going to click on Next, and this is all I really did. It's adding this AWS‑managed policy. So you can do this manually as well. This is obviously in there for you to use. But all it does is it skips that part for us. So it attached this AWS‑managed policy. You'll notice all of the SSM permissions that are granted within the JSON. These are all required for our instance in our agent to communicate with Session Manager. If these are not in place at a minimum, then you will not be able to connect to your instance. So I'm going to minimize, I'm going to go Next, I'll give it a name. I'll leave the description the same. We can see our assume role policy or our trust policy is set up, and I'm going to go down and click on Create role. Awesome. So now our role is in place. Our instance profile exists. To pass those credentials in, I'll close this. I'll refresh down here, and let's select our new EC2 role. Perfect. So now what I'll do is I'm going to skip down, and I'm actually going to go ahead and click on Launch instance. Perfect. I'm going to open this up. We see it's already running. I'll refresh. I'm going to select it. And just to verify under our Security tab, we have no inbound rules. So any traditional network traffic would not be able to work in the first place, which, in addition, we're in a private subnet. So we don't only have public IPs in place to communicate from an external client. So let's go and test all of this out. I'm going to select my instance. I'll refresh one last time. Click on Connect. And we're going to see Session Manager here. So this is an option under Connect. Notice that we can connect. Now if we did not have either network access to a NAT gateway and internet gateway or if we didn't have a VPC endpoint set up in place, this would be grayed out and we would not be able to connect. Also, in addition to that, if we didn't have the correct IAM permissions in place, we would not be able to connect either. So these are all required to use this feature. So I'm going to select it, I'll click on Connect, and let's see what happens. So you'll notice this is a big difference. It's not ec2‑user. We're ssm‑user. Remember, the default username for connections like SSH and Instance Connect are ec2‑user. We are ssm‑user. So that is a big difference. Keep that in mind. So if I change directories to my home directory, we're now here. Awesome. So we've connected now via an agent, and we had no inbound rules specified at the network level for our security group. So that shows how we're using this secure agent to connect as opposed to an exposed port. Let me clear this, and I just want to show you a few things regarding the agent. So let me go ahead and become root real quickly. I'll clear this. And there are three things I want to show you. The first thing I want to show you is the agent running. So I'm going to paste this command in here, and then we're just running systemctl status amazon‑ssm‑agent. And you can see now it's loaded and it's active. So the SSM Agent is a service that's loaded at boot time to start trying to connect to Systems Manager so that we can leverage this agent. Now if I break out of here, clear again, I want to show you a shortcut to configure the agent. So again, we're running system control, but we're editing it. And this is where you can make some changes to that service. Now this is way out of scope for this exam, so I'm not going to touch any of this stuff. But just understand that you can edit the service by doing this. Let me break out of here, I'll clear this, and one last thing I want to show you. I want to show you the log output. So let me change to the log directory because I want to show you the log files. So let me go and change to the log file directory. And when I ls, we have our SSM Agent log here. Let me go ahead and I'm going to cat this and pipe it to less. And you can just notice all of the different output that goes along and is logged with this agent as it's starting up and as it's connecting. We can see the instance ID here. You can see different messages about it trying to connect, etc. Now again, this is kind of out of scope for the exam, but I do like to just show you where these files all live as they're pretty usable and they're required for real‑world scenarios. So feel free. Go ahead and poke around if you're following along. Check out all of these files, the logs, etc. But for now, I think that's a good demonstration on connecting to an EC2 instance that's in a private subnet using no exposed ports all via this SSM Agent. Let's wrap things up here, and we can move on.

Demo: Connect to EC2 via Session Manager via CLI
All righty, in this demonstration clip, we're going to connect to EC2 using Session Manager, but we're going to use the AWS CLI to do it. Now in the previous clip, we had a demo where we used the console, so we walked through a lot of configuration steps. So if you're skipping that, please go back and watch that clip before you watch this one. There is a lot of setup that needs to be done, and we're not going to explain it in this particular clip. In this demo, we're going to have everything pretty much already set up in place. So we're going to have our EC2 instance running. It's going to have IAM permissions, the agent is up and running, and there will be network connectivity via a NAT gateway. The only difference this time is instead of using the console, we're going to leverage the CLI on our local machine to establish that agent‑based connection. So with that being understood, let's jump into the console now. All right, I'm in my EC2 console now. I've loaded up my instances. I've selected my Session Manager instance. And again, I'll review really quickly what's already set up. However, if you want to watch these pieces get set up, please watch the console demo that's also in this module. Moving on, let's have a quick review. So I have my EC2, and I've set up this EC2 to live within a private subnet. So we have no public IP, but the private subnet has NAT gateway connectivity. From a security standpoint, we have our EC2 role here. This role, which has an instance profile attached, has a managed policy here of SSM managed instance core, which has the minimum required permissions to use Session Manager. Going back to my instance, we also have a security group with no inbound rules. So we have zero rules to allow inbound network connectivity. We did this to demonstrate how you don't need to expose any ports for this to work. Now the last thing I want to show you is specific to the IAM user that I'm going to use. I'm in our hands‑on playground, and I'm signed in as cloud_user. So I'm going to go ahead and configure my CLI to use these access keys. So when you see me configure the CLI, this is what I'm configuring it to use. I just don't want there to be confusion on that. Now let me close this, go back to my EC2, and let's begin. So I'm going to jump to my terminal. I'm going to run aws configure. And let me copy and paste in my credentials. So I'll paste in my access key. I'm going to paste in my secret access key. And these are all off screen here. I'll accept these defaults, and there we go. So now I'm configured as cloud_user as the default session. Now one last thing before we actually dive in. If you are doing this via CLI, let me jump back to my browser, you have to install the Session Manager plugin for the CLI. I'll include this URL in a text document for the module assets. So please feel free to go ahead and copy and paste that. But you do have to use this as a prerequisite. This will not work if you don't install this. So I'm on macOS, so I already followed these instructions. But I just wanted to give you a heads up that you must do this before this will work. Now with that being said, let me go back to my terminal, and let's connect. Connecting is super simple. All we have to do is run one command. So we call the AWS CLI. We're going to specify the SSM namespace. And then we run a start‑session command, so aws ssm start‑session, and then we pass in a target. Now the target is going to be the instance ID. So let me go ahead and copy and paste that. Go back, copy my instance ID, I'll paste this in, and then that is it. Now we're using a default region of us‑east‑1, so just keep that in mind. If you're targeting a different region that's not default, you might have to pass in this ‑region flag. This shouldn't matter for us, so I'm going to hit Enter. And we're starting session, and there we go. Just like that, we've created a new session and connected to our private subnet EC2 instance with no inbound rules in place. So if I run an ifconfig, we see our private IP 10.0.12.101, 10.0.12.101. And we should be logged in as ssm‑user, and we are. So this is working perfectly. We've now used the AWS CLI to establish a secure connection to our EC2 instance in a private subnet using the SSM Agent. That's going to do it for this demonstration. Let's go ahead and wrap things up here, and then we can move on whenever you're ready.

Using the Instance Metadata Service Version 2 (IMDSv2)
All right, welcome to this clip where we are going to look at the Instance Metadata Service. So what is metadata? EC2 metadata is simply data about your EC2 instance. We'll look at some examples of what that means here shortly, but this is all it is. It's literally just data about your instance that you can reference. Now in AWS, to get this metadata, you use the Instance Metadata Service. For short, it's IMDS. Now, for IMDS, there are two versions currently available. Version 1 is the original method. And to reference this information, you would make a simple GET HTTP call with a request and response. The newer version, version 2, is more secure. It uses a session‑oriented method requiring headers in a similar GET HTTP method call. The long story short here is that version 2 should be your preferred method whenever you're deploying an instance and you want to leverage this service. Version 1 is easier to use per se, but version 2 offers you a much more secure way of obtaining that instance metadata information. You should use it whenever possible. Moving on, let's explore some of the information that you can get by referencing this metadata. Some examples of the information that you can get are the user data itself, so the user data scripts that get passed in. You can view the host name of the server you're on. You can look at the security group information. So what security group does this resource belong to? And you can even view instance IAM credentials. So if you have a role and instance profile associated with this instance, you can view the credentials that are currently used. Now a big thing to know for the exam and when you're designing architectures in the real world, you can set it so that you require only version 2 be used. And when you do this, it will disable version 1, which really makes sense. You're saying, hey, you can only use version 2, so we're going to go ahead and disable version 1. The big difference to remember, again, is that version 2 requires a session token to be able to successfully get the data you're looking for. It's not just a simple HTTP GET call. It requires both the GET call with a header that contains the session token. Moving on, let's talk about retrieving that metadata. In this example, we're going to look at the IPv4 method. First thing I want to call out, you will always use the same local URL/IP address to obtain this information. So even if it's version 1 or version 2, or if you're changing the different underlying host. No matter what kind of operating system you're running and no matter what instance it's on, all instances leverage this same URL and IP address, and you can see that here. You must remember this IP address for the exam. This is a static address, and what that means is that it's never going to change no matter the instance you're on. So if you have hundreds of instances, this URL will be the same for all 100 of them. Now you can also use an IPv6 local address as well. So this is an example of what this would look like. This is the static never‑changing address that you would use for IPv6 calls. One important thing to call out, however. If you're going to do this, you obviously have to have a subnet with IPv6 enabled, but you also have to have an instance that is belonging to an AWS NITRO system. Now NITRO instances are way out of scope for this particular course, so we're not going to talk about those now. But understand that is a specific classification of an instance type. So if you see this anywhere on the exam, remember these two requirements. Let's talk about now the instance settings that you'll see in the console. We're going to have a demonstration where we talk about this and walk through these, but I like to include them here in the slides for future reference as well. On the right side is a actual screenshot of instance settings when we're creating a new instance. You'll see the first part here is that you specify if you even want the service to be reachable. So this is a big tricky exam question. You can disable the metadata service. So if that ever comes up, just understand you can do that. The second here is do you want to enable the IPv6 endpoint? And that's obviously only if it's supported. The image in this image capture was from a subnet that didn't have IPv6 enabled, so it's grayed out on this instance. Thirdly, you choose your version. So in this case, we're saying version 2 only. And because of that, remember, that means we're disabling version 1. Now you can have version 1 and version 2, or you can require only version 2. Fourth thing here is the maximum amount of hops the metadata response can actually travel. I believe this is up to a 64 hop limit, but I don't really expect this to come up on the exam as that's a very specific question, but I like to call out the settings since we're walking through them. Fifth and last thing here on this image, do you want to allow the instance to get tag information through the service? So in other words, can the instance reference its own tags by hitting the metadata? So those are the fields when you're configuring the metadata service. Let's go ahead. We're going to end this clip here. And then coming up shortly, we're actually going to go ahead and jump into the console and demonstrate using the metadata service.

Demo: Exploring the IMDSv2 Information
All right, let's get started with testing the IMDS or, in other words, the Instance Metadata Service. I'm logged into my EC2 dashboard here. And to test this, I'm going to create two instances, one specific to version 1 and then another where we lock it down to version 2. So the first thing I'll do is create a new instance. I'm going to call this v1. We'll leave the defaults for the AMI and the instance type. And I've already created a key pair that I'm going to use. So I'll select that. I'll go to Network settings. I'm using the default network settings except for the firewall or security group. For the security group, I have an existing one that allows me to SSH so I'm going to select that. After that, we'll skip down to Advanced details. And under here, I want to select an instance profile. So I created this as well. I'm going to attach it. And we're not even going to use it, but I want to show you how we can reference it. After I've selected my instance profile, I'm going to scroll down until I find the Metadata section, and here it is here. So let's look at configuring these fields. The first is, do you want to make it accessible or not? So you can disable this access entirely. So if this comes up on the exam, remember, you can turn this service off. Or you can do what we're going to do and enable it. The next is the IPv6 endpoint. Now we don't have IPv6 for the subnet, and we're not on a NITRO instance so this wouldn't work anyways, but this is where you would enable it. Next up is the version. So there are two options. Currently, you can select v2 only, or you can allow v1 and v2. So I'm going to select v1 and v2 for this instance because I want to demo using version 1. We then have our hop limit and if we want to allow tags or not. So we'll set these settings. I'm going to go down to User data, and I'm going to copy in a very simple script because I also want to show you how you can reference this. So we're running a bash script. I'm echoing a text field into a data.text file. I'll click on Launch instance, and there we go. Now I need to launch version 2. So I'm going to launch another instance here. I'm going to call this v2. And we're going to use pretty much the identical settings to the first one, so same key pair, same instance type. We're going to select the same security group. And then under Advanced details, I'm going to select the same instance profile, and I'm going to set similar metadata service configurations. However, this time I'm going to leave v2 only. So we're not going to allow v1 at all. Let me enable tags and metadata. I'll paste the same script in, and we'll click on Launch instance. Perfect. So now if I go back, we'll refresh. We should see both instances here in a moment. We'll let v2 spin up. And there we go. So let's demonstrate version 1 first. I'll select it, copy the public IP, go to my terminal, and let me SSH into that instance. Now just a reminder, I already set up my public key. So the permissions are there for me to use this. So I'll SSH with my identity file into my instance as ec2‑user. We'll clear this. We'll list. There's our data.text. Perfect. So our user data script executed. That's great. Let me clear my screen, and let's get going. The first thing I want to show you is the command. So for version 1, remember, it's a simple response and request. So we're curling a local a HTTP address, which is what you should be familiar with, 169.254.169.254. Please remember that address. Now when we curl this, we get a list of different years and dates. So these dates contain different referenceable metadata based on the availability of that year. Typically, you're going to use latest, so let's use that. I'm going to repeat that command, clear my screen, paste in latest, and you'll notice we get another list. So, hopefully you're understanding that this URL is path‑based. We see dynamic, meta‑data, user‑data. Let's use meta‑data. So I'll clear my screen, and we get an even bigger list. So notice all of the information that we can reference via this URL. We can look at the AMI ID, we can look at the host name, we can look at the instance ID, etc. Now to hit these, you simply pass in the path. So let's say we want to see the security group that we're going to reference or that we are using. Well, to do that, let me clear. I'll paste in security groups. There we go. We can see now the security group that was configured with our EC2 instance. I'll clear again, and maybe we want to check out something else. Let me paste that latest in there again. And let's say we want to look at our security credentials. Okay, well, we have IAM here so let's try that. I'm going to paste in iam. We see info, and we see security credentials. So I'm going to take security credentials, append that to the path, and we see our role name, EC2‑ROLE. This was the instance profile and IAM role that we used if you remember. So again, I'll copy, I'll append it, and look at that. So this is how an EC2 instance is referencing the IAM role credentials that we've associated with it. So this is what the instance profile allows us to do. We're able to look at a secret access key, access key, and the token required for that role session, etc. So if there's ever a question on referencing IAM credentials on your EC2 instance, this is how you would find it. It's easily referenceable via that URL. This is maybe a good option to see why you might turn the service off as well. You might not want someone to have access to read these credential informations. Perfect. So hopefully now you can see that it's pretty easy to reference the different metadata that is available. Feel free to try this on your own. There's tons of information in here, and we're not going to go through every one of them. The one thing I did want to show you, however, was the user‑data. So if I go here, I'm going to go to latest, we have user‑data. So let me go and enter that. Check that out. We can actually see the script that was executed via the user‑data field that we passed in when we were creating this instance. So this is really neat. If you ever have to look at the user‑data, understand this is an option that you can us for those scenarios. Now with that being said, let me go and kill this connection. I'm going to go back to my console, and let's try version 2. I'm going to select version 2. Copy the IP. Now let me connect to this one. So I'm going to repeat. Just change the IP address here. We will connect. And the first thing I want to show you is let's test version 1. So remember, it's the same IP address on every instance. Let me copy and paste the command. We're curling 169.254.169.254. Uh oh, unauthorized. Well, remember, because we only enabled version 2. So let's go ahead and test out how that works. What I'll do here is copy and paste this in, and I want to break down this command for you. What we're doing here on this first portion of this command, we're setting an environment variable of token. And to set the value, we're curling the local metadata address latest, the api option instead so a different path, and it has a token option. We're saving that token and passing in a TTL. So essentially, hey, let this expire in 21,600 seconds. We're then using that token that we just set to make a new curl. So we're saying, hey, here is the metadata token, the token variable, on that same address. So now if I hit Enter, we get that list. Let me clear my screen, and now we can leverage token whenever we need to. So if I go in and I echo token, you'll see this is our API token that's being used that we can reference for 21,000 seconds. So let's go and test some more out. Let me copy and paste another command. And for this command, instead of setting that token again we're just curling, we're passing in a header specifying that token. So remember, this is set. Now we can reference it. We're curling that same URL, and then we're saying latest/meta‑data, and you'll notice the options are the exact same. Nothing has changed as far as availability of the information. It's just the security on how you're able to obtain it. So one of the fun ones here is we can look at the public keys that are assigned to our EC2. So if I paste this, we're saying, hey, latest/meta‑data/public‑keys, what's the key pair name? There we go, imds‑demo. Remember that's the key pair we configured for this instance. Okay, now that's going to do it. Hopefully you see the differences between version 1, which is a simple HTTP GET call to our local address, and then version 2 where we actually had to set this token before we were able to successfully hit the metadata service. A big takeaway, remember, is you can disable version 1, and you can disable the service entirely. Let's go ahead and wrap this demo up here, and then I will see you in an upcoming clip

Module Summary and Exam Tips
Okay, way to hang in there. Welcome to the Module Summary and Exam Tips clip. Let's review some important information you should remember before moving on. First up connecting to our instances. Remember, you can set up Bastion hosts or jump servers to set up controlled access to private compute. In order to connect to instances, you need to remember the available options, SSH, which is port 22, RDP, which is port 3389, EC2 Instance Connect, which, if you recall, leverages port 22 underneath the hood so we had to allow that AWS IP range, and then there's Session Manager, which is an agent‑based connection. Remember we had in the demos, a security group with zero inbound rules, and we could still connect. The big thing to take away, Session Manager is more than likely going to be the absolute best choice for any scenario on the exam. It offers the most secure connection. When you're using it, however, remember you need IAM permissions and network connectivity in place. So you have to allow the EC2 the correct IAM permissions, and you have to have network connectivity to Systems Manager in order for this to work. Now that can be through the internet gateway, a NAT gateway, or it can be via VPC interface endpoints. Regardless, you need connectivity. Also remember, the secret sauce is the SSM Agent. This is what you're actually connecting to when you're using Session Manager. We looked at how it starts and where it's configured and some of the logs. And speaking of connectivity, let's talk about Session Manager interface endpoints. Remember that these are critical for SSM to function if you're not leveraging it over the internet. There's SSM, SSM messages, and EC2 messages. You must use these three for Session Manager to work appropriately if you're not going over the public internet. Moving on, remember that SSH and RDP both require key pairs in order to set up initial remote connections. So SSH, of course, uses the private key directly, and then you connect because your EC2 instance trust the public key. And then RDP uses it to create and encrypt a private admin password that you reference when you're setting up that RDP connection. Next up, we have the Instance Metadata Service. Remember, you can hit this service on your EC2 instances to gain a lot of information. You can pull things like the user data script that was executed. You can look up the host name, you can check out the security group you belong to, and you can even look at the temporary instance IAM credentials currently active. Now, obviously, this is a very small list of everything that's available. So I do recommend you review the Instance Metadata Service and play around with it so you can get comfortable with what values you can actually pull. And with that being said, please, for the exam, remember this URL for any IMDS scenario, 169.254.169.254. Regardless if you use version 1 or version 2, this URL is the same on every single piece of compute. Now that's going to do it for this Summary and Exam Tips clip. Let's wrap this module up now. You can take a break if you need it, and then I will see you in the next module.

EC2 Networking and Performance Scenarios
Configuring an Elastic Network Interface (ENI)
Welcome to the next module in this course where we're going to discuss EC2 networking and performance scenarios. In this clip, we're going to look at configuring an elastic network interface, otherwise known as an ENI. Elastic network interfaces are just a logical networking component that is part of a VPC, and it's meant to represent a virtual network card for compute resources. So if you've been following along throughout the other modules, you've seen ENIs get created automatically for us when we are creating an EC2 instance. But in this module, we're going to dive in a little bit deeper into the concepts and the process of using these. Speaking of concepts, three big ones to remember for ENIs. They have to be created and bound to a single availability zone. If you think about it, it really makes sense because they are attached to EC2 instance compute, and EC2 instances can't span between multiple AZs either. They're bound to a single AZ. A neat thing with them, however, is you can attach, detach, and then reattach them as needed to any other EC2 living in that same AZ. So you can move them around if you need to do so. Thirdly, they offer many different attributes that can be customized, and let's take a look at those now really quickly. Six major attributes that you should be aware of. When you create an ENI, you give it a primary private IPv4 address from the subnet CIDR, or you can give it a primary private IPv6 address from the subnet CIDR. It depends on the subnet properties that you've configured. Now you can also give them a secondary private IPv4 from your subnet CIDR. You can give them a single elastic IP address for each private IPv4 address that's assigned, and we'll talk about EIPs later on. You can obviously customize security groups that they are belonging to with your own custom rules, and it's important to call out here. When you assign an EC2 instance to a security group, what you are really doing is you're assigning the network interface that's attached to that EC2. So in reality, if you move the ENI to a different EC2 instance, well, then that security group travels with it. Last thing here is you can give it one ephemeral public IPv4 address to reference. Now what is an ephemeral public IPv4 address? Well, I'm glad you asked when you assign a public IP to your instances in one of your public subnets, the default behavior is to pull an IP address from Amazon's pool of public IPv4 addresses that they have. Now when you disassociate that IP from the instance, what happens is it gets released and put back into that pool of IP addresses. Now you might be wondering, well, when is an IP released? Well, your IP is released whenever one of the following occurs. If you stop, hibernate, or terminate your instance or if you decide to assign an elastic IP address to your ENI, any of those scenarios will release that IPv4 that was assigned from Amazon's pool of addresses. Long story short, in other words, these are temporary, which is also known as ephemeral. They're always changing. Speaking of it, let's look at a quick diagram explaining the process on how this works. When we start a new EC2, AWS has their own large pool of available ephemeral public IPv4 addresses that are usable. So when we decide to go ahead and start our EC2 instance, what happens is we get assigned a public IP address that's pulled from the Amazon pool of available addresses. The IP is associated to your ENI. It'll also receive a public DNS name like you see here. In addition to that, the ENI also receives an internal private IPv4 from your subnet CIDR. So it has a public and a private IP. When you decide to stop your instance, that process is similar but in reverse. What happens is your IPv4 is released and put back into that pool of addresses. However, stopping the instance does not affect the private IP that was assigned. So you'll notice the private IP will remain the same. Now if we restart that instance, well, then the process begins again. We get a new reassignment of an available IPv4 address from that pool. Now you need to assume when you're using these ephemeral addresses that you will more than likely never get the same IP address. While it is technically possible you could, you should never rely on that. So always plan that this will change. Now to solve for that, we're going to look at elastic IP addresses coming up here in an upcoming clip. So let's go ahead and wrap this up, and I will see you in the next clip.

Assigning Static IPv4 with Elastic IP Addresses (EIPs)
All right, let's dive into assigning static IPv4 using elastic IP addresses. Elastic IP addresses are simply public static IPv4 addresses that you can map to different compute. So let's talk about some concepts regarding these EIPs. The biggest thing to remember is that an EIP is a static IP address that will never change. So it's not ephemeral. Also, they can only be used within the specific region that they get created in. So you can't move them between regions. How they work is you first allocate an EIP, and then you can associate that EIP address to one of your resources. When you associate an EIP, it means that the public DNS of your instance will also change. So obviously, the public IP changes. And then with that, the DNS will change. Now there is currently a soft limit of five EIPs per region per account. What soft limit means is that you can request an increase if you absolutely need to. With that being said, if you need more than five EIPs, you might want to rethink your architecture. Now while these are usable for masking failures of compute because you can reassociate them to different instances and move them around and they never change, you really should avoid using them unless it is absolutely necessary. So there might be scenarios on your exam where you need a static IP address that never changes. Well, then that would be a perfect use case. However, if you can avoid using them in real‑world scenarios, it's highly recommended that you do. Instead of leveraging a static IP, you should, instead, be leveraging DNS names that you can map to your ephemeral IP addresses. So whenever those ephemeral IPs change, the DNS stays the same, but it will map to the different resource automatically. But with that being said, if there's ever a scenario where you need a static IPv4, then this is probably the resource for you. So with that understood, remember, EIPs are public static IPv4 addresses that you can move around as necessary. Let's go ahead and wrap this clip up here, and then I will see you in an upcoming clip where we're going to demo associating an elastic IP to an EC2 instance.

Demo: Associating an EIP
Let's go ahead and dive into this demonstration clip where we're going to work on associating an elastic IP address to an EC2 instance. In this demo, we're going to have a publicly reachable EC2 instance already in place. And we're going to show that it's leveraging an ephemeral public IPv4 address from the Amazon pool of addresses. So what we'll see is since it's launched, we're going to have both our private IP, as well as our ephemeral public IP assigned. To demonstrate why these ephemeral IPs can be a slight headache, we're going to stop and then start our EC2 instance. And we're going to notice that the public IP has more than likely changed. Now to solve for this issue, what we're going to do is we're going to go ahead and we're going to allocate a brand new elastic IP address within our region for our account. After we allocate it, we're going to work on associating it with our instance, which is going to disassociate the original public IP and then map the new one to our instance. So now whenever we stop and start, it won't ever change. So let's go ahead and let's jump into the console now and let's demonstrate how you can do this. All right, let's dive in. I'm in my EC2 dashboard here. I've loaded up my instances, and I've already created a publicly reachable EC2 instance. It's called eip‑demo, and you can see the details below. We have a public IP address that's assigned, and this is an Amazon‑provided address. We also have a private IP as well. To show this, .et's go to Networking. I'm going to scroll down here. I'm going to open up this ENI in a new tab. And let's view this ENI. Perfect. So when we look at the details, we can see what VPC, subnet, attached security groups, etc. But the big thing I want to show you down here is we can notice the elastic IP is Amazon‑owned. So the public IP is owned by Amazon, and this is ephemeral. Now I know this might be a little bit confusing, but this is not a true elastic IP in the sense of the exam. So while they call this an elastic IP, just understand it's because this is owned by Amazon and that's how they refer to it as well. The big thing to remember for the exam is this is ephemeral unless we create our own. So let me close this, and I'll just show you really quickly if I navigate here, we just have a simple web page running, it says what AZ it's in and the instance ID. So let me go back to my instance. The first thing I want to demo is how this IP is going to possibly change. So I'm going to go to Instance state. I'm going to stop this. I'll click on Stop. And what I'll do while this is stopping is I'm going to go ahead and fast forward. And then once this is actually in a stopped state, we'll go ahead and resume. So let me go ahead and cut forward now. Perfect. So that took a little bit under a minute, but notice now we don't have a public IP address that's assigned anymore. So what I'm going to do is I'm going to go back up, and I'm going to start our instance. So what has happened here is that we stopped the instance. So when that happens, remember, you disassociate those ephemeral IP addresses. So it was released back into the pool of IPs that Amazon owns. Now, eventually, when this loads back up and it's running, we're going to probably have a completely different IP address. Now it is possible you could get the same IP. Remember that. But again, you should never count on it, and this is exactly why. Notice it's a completely different IP address. Now, however, though, if I navigate to this, eventually, this is going to load that same server because this is the new IP. There you go. So if I go back to Instances, let's solve for this changing IP. What I want to do is on the left‑hand side here, I'm going to find Elastic IPs under Network & Security. I'm going to load it in a new tab, and I'm going to allocate an elastic IP because you allocate them to your account and your region for you to use. Now notice there are four different options currently. You can bring your own IP, which is a little bit of out of scope for this exam. We can do a customer‑owned pool for our own on‑prem networks using outpost, which is a completely different technology. And we can use IP address management, which is out of scope for this particular course. Now the next thing I want to show you is the network border group here. So even if I x this out, it doesn't matter. You have to choose us‑east‑1. Remember, this is because elastic IPs are bound to the region. So this is usable in all the AZs, but it has to be the region you're creating it in. I'll click on Allocate, we get a successful message here, I'm going to refresh. And now if I select it, we scroll down here, we can see it's allocated but it's not associated yet. So we need to associate it with our instance and our network interface. So I'm going to go under Actions, Associate, and now we can choose our resource type. So you can do it directly to a network interface, which sometimes is a little picky. But for this demo, we're going to do an instance. So I'm saying, hey, I want to associate this elastic IP that's static with an instance that's running, which we'll choose our eip‑demo. And then we choose the private IP that it's also associated with because, remember, the network interface has both the public and a private IP assigned. So I select those two. And then the last thing here is a reassociation. So you can select this if you want the elastic IP to be allowed to be reassociated, essentially pulled off of the old resource and put onto a new one. I'm not going to do this because I want to keep it in place and I don't want it moving around. So I'll click on Associate. I'm going to go back to my Instances tab here and refresh. And notice our public IP has changed. So now, if I refresh here, it's just going to time out because this is not the public IP that maps to our instance. So let me go back. And now if I navigate to the new elastic IP instead, we should get that same server. There we go. Notice the old IP is timing out. I'll close that down. The new IP works. So now if I stop the instance, this IP should never change. We've now statically assigned an elastic IP address. So what I will do is, again, I'm going to fast forward until it's actually stopped, and I'll show you how the IP stayed the same. Okay, so it's now into a stopped state, but check it out. Our public IP doesn't change. This is the benefit of an elastic IP address here. So now I can start it back up, and it's going to have the same statically mapped public IP address to reference the instance. So this is perfect. This is exactly how they work. Hopefully, you've seen how you can allocate and associate. Let's go ahead and wrap this demo up here, and I'll see you whenever you're ready to move on.

Dual-home EC2 Instances
Next up, let's look at dual‑home EC2 instances. The typical definition for a dual‑home instance means that the instance has multiple network interfaces that are attached, and those different ENIs are living within different network bands. In even simpler terms, it means that the EC2's networking is split between two different subnets or tiers. Remember, we talked about multitiered subnets in a previous module. So what that means here is that we have EC2s split between two of those tiers. An example of this would be having an EC2 instance that lives in a management subnet for restrictive access. So maybe you have maintenance you have to get in and do and you want to lock it down to internal networks only, and the internal network might be on a completely separate network band. And then maybe you have a public subnet ENI for allowing only public web access. Let's look at a diagram example of how this might work. So the subnets obviously have to be in the same availability zone. Remember EC2s are bound to AZs. When you set this up, your server is going to have two network bands so it can talk to a public resource using the public subnet ENI where you can assign a public IP address or use an elastic IP address for that network interface. The server also would be deployed into a private subnet on the right here where it would receive an additional private IP that is entirely separate from the other subnet's IP CIDR. So this is a perfect example. The left side of the diagram could be open to the public; the right side of the diagram where that ENI lives could be restricted to something like a corporate data center. Now, building off of this, removing some of the other stuff, you would also have two different security groups. There would be an internal security group attached to the internal elastic network interface, and you could restrict this to allow only internal traffic over very specific ports for secure management of the resource. So maybe you have a VPN in place and you want your corporate office to talk to this resource, well, you could do that. And then the external security group over on the left side attached to the public ENI could allow any type of application traffic from the internet, so HTTP, HTTPS, etc. What this accomplishes is it deploys your instance with two completely separate network bands that you have full control over. Now this might be a little confusing right now, but don't worry. In the upcoming clip, we're going to have a demonstration where we actually create a dual‑home EC2 instance. So let's end this here, and then I will see you in the upcoming demo.

Demo: Creating a Dual-home EC2 Instance
Let's get started with this next demonstration. In this clip, we're going to work on creating a dual‑home EC2 instance. Before we jump into the console, let's have a quick architectural overview on what we can expect to accomplish. We're going to see that I've already deployed a VPC with several subnet tiers. We're going to have a public subnet, which is going to be used for a public ENI. We have a private subnet, which is going to be used for our internal management traffic. And then we're going to have a management band or a management subnet, which is going to be used to host a management EC2 instance, and we're going to allow this management instance to connect to only our private ENI. So we will accomplish a few different things. I've already deployed the management instance here, but we're going to focus on the left side here. We're going to create our public EC2 instance first. So when we do this, we're going to give it a publicly resolvable IP. It'll have a private IP as well for that subnet. And we're going to attach a primary security group that will AllowHTTP from anywhere. No other traffic will be allowed. After this, after we test, we're then going to create a brand new ENI, and we're going to attach it as a secondary interface to the existing web server. This ENI will live in a private subnet so it won't have any publicly resolvable IPs. And we're going to attach a secondary security group that's restricted to only allow internal SSH from our management subnet. Now a big thing to remember when you're doing something like this, the ENIs have to be in the same availability zone. This is a strict requirement because EC2 cannot stretch across AZs. So as we're doing this, we need to pay attention to what AZ we're deploying in in the beginning for the web server. Now after these two ENIs are set up and we've verified everything is in place, we're going to test a connection from the management subnet using our management EC2. With that out of the way, let's jump in the console now. Before we get going, I want to go over some of the existing resources that we talked about in the architecture review. I've deployed a VPC here, and if I show you the resource map, you can see we have a public and a private subnet split amongst three AZs right now. I also created a Singular management subnet in AZ A. And if I go to this, the big thing I want to show you is the IP CIDR, 10.0.20.0/24. Now this is important because we have a security group that's only going to allow traffic from this particular CIDR for the VPC. Now, in that management subnet, I've deployed an instance called management. So it lives in the management subnet as you can see here, and it has an IP within that CIDR range. Now this has no security group inbound rules right now except for default, which is fine. And I've set up an SSM role to allow me to connect via Session Manager. So I'll connect now just to get that out of the way. And once this connects, we'll jump back into the demonstration. Perfect. So I'm going to go back to Instances here. And the first thing I want to do is create our EC2 instance hosting our web server. Now remember, the first portion is going to be for our public‑facing site. So I'm going to call this WebServer. I'll lead the default AMI in instance type. I'll choose no key pair. And the reason I'm saying no key pair here is I'm going to show you what I'm doing in the user data. So just keep that in mind. So I'm not proceeding with the key pair. I'll edit the network settings. I'll select my custom VPC. We'll do public AZ C for this. We get a public IP from Amazon, and I've already created an existing security group called AllowHTTP, and we'll look at those rules here in a minute. I'll scroll down to Advanced details. I'm going to scroll down to my user data. And I'm going to paste in some user data here. All we're doing is creating a very simple web server, which you've probably seen before. And then I'm also echoing a trusted public key into the authorized_keys file for ec2‑user. So this is going to allow me to import my own private key that I have to use to connect to this from the management band. Now, I'll make this available in the module files. So feel free to use it if you need to. But understand, the public key portion won't work for you. So I'm going to pull this out. You have to use your own public key. Now I'll click on Launch instance, and there we go. So I'll go back to EC2. I'll filter on running instances, I'll refresh, and there we go, WebServer. So now we get this public IP here, which we'll use in a second. You can see we're in the public subnet in AZ C. And if we look at security, we have our AllowHTTP rule, which allows all internet traffic over port 80. And from a networking standpoint, we have one ENI attached, which is the primary. So what I'll do here is go back. I'm going to load this public IP. And once this registers and maps to the resource in the back, this will load a simple web server. So what I'll do is I'll give this a few seconds, and once it's available, we'll resume. Okay, so it took probably 5 to 10 more seconds. It's up and running, and you can see our information. We're in our us‑east‑1c and our instance ID. Perfect. So I'm going to leave this up. I'll go back to my Instance tab, and now let's add that dual‑home portion. So we're going to add a private separate ENI to this web server. So what I'll do is go back here. I'm going to find Network interfaces. I'm going to create a new one. Now I'm going to give this a name, and I'm going to call it management. This way, I know it's used for management traffic. The next thing you do with your ENI is choose your subnet. Now remember, this is important because we deployed the public EC2 instance in us‑east‑1c. So in order to attach this to an existing instance, it has to live in that availability zone. So I'm going to look for private subnet AZ C, which is right here. I select it. I'll accept the defaults here. And then I'm going to attach this other security group, which we will review here in a moment. I'm going to create it, and there we go. I'm going to select View network interface. We see it's available, which means we can now attach it to an EC2. We see the subnet it lives in, which is the private AZ C. We see our private IP from that CIDR, and we even see our security group. So let me load our security group in another tab. And this is the rule that we attached, AllowManagement. So if I go down here, we're allowing ping or ICMP and SSH from our management subnet only. So remember that CIDR we looked at previously, 10.0.20/24. So this will work with our management subnet, which will work with our management EC2 instance. I'll close this and go back to my network interface, and let's go ahead and attach this. So I'm going to select Actions and then Attach. Now we have to choose the VPC. We want to put this resource in. Well, it's already in our subnet. So I'm going to select VPC A. I'm going to choose my WebServer instance because that's in the same AZ. So that's why it's showing up in this list. I'll click Attach. It's now in use, meaning it's attached. And we can see the instance idea it's attached to. So I'll select this, I'll show our web server here and refresh, and you can see now we have two private IP addresses. So we have a primary, which is the top one, and then our secondary one, which was just attached. So now if I go to Networking, we should see two ENIs and we do. We have device index 0, which is the primary with our public IP, and we have ENI device index 1, which is our secondary, which was our management ENI. So now if I scroll over here, we're going to see the VPC, the different subnets. We even see the security group attached to the individual ENIs. So, we can allow management traffic on this ENI, the secondary one, but we can't SSH into our primary ENI address. So now we've created, in theory, a dual‑home EC2 instance. Well let's test it out. I'm going to go to Details here. I'll copy this private IP, the new secondary one. Let's go to my Session Manager, and let me ping it. Perfect. It's working. We're able to communicate with that secondary interface. Now to prove that this won't work on the primary, let me copy the primary private IP and let me try and ping that. Perfect. It's not working because we have a completely separate network band that we've deployed the two network interfaces into. And remember, security groups get attached to network interfaces. So now, that management traffic can only communicate over that secondary interface, which is perfect. We've isolated it to be not public. So now I can use an SSH key to try an SSH because our network connection is in place. Now I've already gone through and made a management key. So if I cat this, you're going to see my private key. Now obviously, it won't matter because once this clip is out, this will be erased. But I'm going to use this because I made that public key a trusted authorize key on our web server using that user data, if you remember. I briefly talked about it. So now, remember, we do have that SSH rule in place for our management network. So what I can do here is SSH, point to my management identity file, ec2‑user, at that secondary private IP. So I'll copy this, I'll go back, I'll paste it, and there we go. So now we've connected over that management network band to our secondary dual‑home ENI. Now that's going to do it for this demonstration. Hopefully, you saw how you can create a dual‑home EC2 instance using two different network interfaces on two separate network bands. Let's wrap things up here, and I will see you in an upcoming clip.

Increasing Performance with EC2 Placement Groups
All right, let's talk about increasing your performance with EC2 placement groups. A placement group, by definition, is just an optional grouping of EC2 instances to better influence their placement. There are three types you need to be aware of for the exam. First, you have a cluster, there's a spread, and then there is a partition placement group. Let's break down each of these now. First up, cluster. When you use a cluster placement group, it puts your EC2 instances extremely close together, all within the same availability zone. So it groups them into the same AZ. This is a key characteristic. The reason these are really good is that they offer extremely low latency for your network performance. Since they're all in the same AZ, that means that their networking components are extremely close as well, which means low latency. Typically, these are going to be used for high‑performance computing scenarios. So if you have something on your exam where you need to have high‑performance computing on your EC2 instances with low latency, you might think of a cluster placement group. However, it is important to call out not all instance types support this type of placement group. So just keep that in mind. Now, they're not going to test you on the exam on which do and do not support this because that's hard to know, but just understand the other key characteristics. Next up, we have spread placement groups. This is where you place a small group of instances across different underlying hardware. So there's no hardware being shared. These are meant to reduce common hardware failures for the underlying host. Spread placement groups are going to be recommended for critical applications where you require uptime. So if you have a small number of instances that are hosting a very critical application and you want to protect against failure, you would use a spread placement group. The easiest way to remember this is that they spread the instances out. Now let's talk about partition. Partition placement groups are why you spread instances across logical partitions. What this does is it ensures groups do not share underlying hardware. So it's similar to a spread placement group. Each partition is going to have its own set of racks hosting the hardware that have the underlying host for your EC2 instance on them. The key difference between these and spread to make it easier is that these are useful for distributed or replicated workloads. So if you see scenarios with things like Hadoop, Cassandra, or Kafka on your EC2 instances, you'll likely want to think of a partition placement group. That will be one of the key indicators between choosing this or a spread one. Moving on, let's talk about some exam tips you should remember. It's good to know that you can add a stopped instance to an existing placement group. So if you stop the instance, you can edit the config and add it to an existing group. Now when you create a cluster placement group, AWS is going to recommend homogenous instances. So homogeneous instances for those who don't know means similar or same. So for instance, if you deploy an m5.large, well, then they recommend that you deploy all m5.larges within that cluster placement group. This is going to help with consistency and performance since all of the virtual hardware is the same. And the last thing here, you can merge placement groups. No matter what kind they are, even if they're similar or not, you can't merge them. So just remember that as well. Now that's going to do it for placement groups for EC2 instances. Definitely review these if you need to. They are a critical factor when you're deploying your EC2 for specific applications. But let's go ahead, we're going to end this clip here. And in an upcoming demo, we're going to use a placement group.

Demo: Launching EC2 Instances in a Placement Group
Okay, let's dive into the next demonstration clip where we're going to launch some EC2 instances in a placement group. Now for this demo, we're only going to be able to leverage two of the different placement groups because our sandbox doesn't support the instance types that support a cluster placement group. So just keep that in mind. So instead, we're going to launch a partition and a spread. Now the first one we'll do is a partition placement group. So when we do this, EC2, the service, is going to do its best effort to evenly distribute instances that you place in the placement group across the different logical partitions. So in this case, we'll launch four instances with two partitions. So ideally, it'll have two and two. Now remember, the way logical partitions work is that they separate the instances between different underlying hardware. So in theory, if Partition 1 goes down and the hardware fails, then those EC2 instances fail. But the other partitions' instances should continue to run. This is one of the benefits of these placement groups. Now after we demo launching a partition placement group, once we demonstrate how easy it is to leverage a partition placement group, we're going to move and demonstrate a spread placement group. So with this placement group, remember, EC2 instances get placed on a completely separate or distinct server rack. What this means for you is that each instance is going to have its very own networking source and its own power source. So it's not relying or sharing any of the same underlying hardware that is very important. That means in this case, if we lose an entire rack hosting one of our EC2 instances, well, then the rest will continue to run. This is why this is perfect for critical small applications. Each EC2 is completely isolated from one another. Now let's go ahead, let's end this here, and let's jump into the console and launch these placement groups. Perfect. I'm in my EC2 dashboard here in my AWS sandbox. I've loaded up the EC2 dashboard screen. Let's get started. The first thing we'll do is let's launch a partition group. I'm going to create each placement group a little bit separately just to demonstrate the two different ways that are common. The first way I'll do it is from the Launch instance screen. So I'm going to launch some instance. We're going to call this Partition. I'm going to go to Number of instances and click on 4. So we want four instances ideally spread between two partitions. I'll go down. I'll leave all of this stuff the same, so a t2.micro I have a key pair already created, which we won't really need, but I'm just going to select it. And I'm just going to leverage the default network settings since we're not connecting to these instances at all. So I'll allow it to create a new security group. We'll use a public IP. That's all good. And let's skip down to Advanced details. This is where we're going to start creating and then setting up our placement group. So if I scroll down, you're going to see placement group here and we can select. Now I don't have one. So what I'll do is select create placement group, open it in a new tab, and let's create a partition placement group. I'll create it. I'll call it Partition. I'm going to choose my strategy. So remember, there are three different types. We have Cluster, Spread, Partition. So for this, I'm going to choose Partition. After that, you can choose the number of partitions you want. Now it's important to call out there's a limit. You can only have seven partitions. So keep that in mind. For this, we're going to select two, and I'm going to create my group. Now this is available. We see two partitions, and then we have an ARN. So I'm going to go back to my instance, refresh my list, and select my partition placement group. Now if we wanted to, we could select the target partition. So you can manually say, hey, I want you to put this instance on this partition. We're not going to do that. Instead, I'm going to let EC2 place them because I am spinning up four instances. So I'm going to skip down here. I'm going to launch my instance. And now we have four instances that should be created and running here in a second. We see the list. Let me go to View instances. I'll filter on running. And then here in a moment, we should have four different instances. So what I'll do is I'm going to go ahead and fast forward. And once these are up and running, we'll continue. Perfect. So that took literally just a few more seconds. We see we have four partition instances here, and they're all going to belong to the same placement group. So if I scroll down, Host and placement, we see Partition. Now this is on partition number 2. So let's look what the other ones are, Partition 1, Partition 2, Partition 1. So EC2 perfectly balanced these between our two partitions. So now if our Partition 1 instances go down here, well, we still have our Partition 2 instances running our workloads. So this is why it's perfect for distributed workloads. Perfect. Okay, we've launched a partition. Let's work on spread. So this time what I'm going to do is I'm going to go down. I'm going to select Placement groups here under Network & Security, and I'm going to create this first. I'm going to call it Spread. We're going to choose a spread placement strategy. And now we could choose the spread level. So the big deal here is we're going to have to use rack, which means no restrictions on our end. However, you could use host if you were leveraging a service called Outposts, which we cover later on. So what I'll do is choose the default of Rack, and I'm going to create my group. Awesome. So what I'm going to do now is I'm going to create two instances here. I'm going to go to Launch instances. I'm going to call these Spread. I'll increase this to number being 2. We'll select the default here. I'll select my key pair. We'll leave the default networking settings, but I will choose that launch wizard. And then we're going to scroll down to Advanced details, and we're going to find Placement group again. Now this time, what I'm going to do is select Spread. Notice, there's no other option types like there were for the target partition for our partition placement group. This is going to automatically spread instances across different groups so they're completely separate. So after I choose Spread, I'm going to launch my two instances. Perfect. So now if I go to my instances here, look at running. Once these are up and running, we'll take a look. All right, so that took about a minute, and now we see our two spread instances. So if I go down in details on our first one here, you're going to see we have a placement group ID, the spread placement group type, etc. So it's not partitioned like it is for a partition placement group. Instead, you have to assume that the underlying hardware is completely isolated from one another. Now what I want to do is last thing here, show you how you can add a stop instance. So what I'm going to do is I'm going to go to Launch instance. I'm going to create a new one. I'm going to call it Spread Add so we know it's different. I'm going to select the default here. Same network settings. And then under Advanced details, I'm going to make sure that there's no placement group selected and there isn't. So let me launch this. I'm going to filter on it. And then once this is up and running, I will continue on. So let me cut forward into the future really quickly and then we'll continue. Okay, so this is up and running. If we go under details, you're going to notice there's no placement group being used, which is perfect. But now, I want to try and add this to an existing group. So under Actions here, you're going to notice one thing. We can't modify this because it's running. Remember, you can only add an instance to a group when it's stopped. So what I'll do is stop this instance. I'll fast forward very quickly until this is in a stopped state. And then let's add this to our existing spread placement group. Okay, it's now stopped. Let's go ahead. I'm going to go to Actions, Settings. I'm going to modify instance placement. We're going to say, hey, this is actually a very critical application. We would want it to be in that same spread group. So I'm going to select Spread, I'm going to save, and there we go. Now, when I start this back up and I refresh, ignore this bug. It's just filtering weird. So let me go back here and I'll clear this out. I'll find our Spread Add here. I'll go down, and now it's in our Spread placement group. So now this is the third instance running on completely separate hardware with the other spread instances. So that's how easy it is to add an existing instance to a different placement group. Let's go ahead. Let's wrap this demonstration up. Hopefully, you saw how easy it is to create and use your own placement groups. And with that, I will see you in an upcoming clip whenever you are ready.

AWS Outposts for Localized Compute
All right, let's talk AWS Outposts for localized compute. What is AWS Outposts? AWS Outposts allows you to bring the AWS data center directly to you on‑prem. It allows you to have a large variety of AWS services essentially hosted in your own data center. Now when you use AWS Outposts, they come in different sizes. They offer Outposts servers, which will come in 1U or 2U size. And they offer Outposts racks. So an Outposts rack is going to be 42U. So the rack can host several servers. It's really going to come down to your use case. Now the above, if you're not familiar with them, are just industry standards for measuring servers and the amount of space that they take up in a rack. So a 2U is going to be larger than a 1U. So what are some benefits of using Outposts? Well, they help you easily create a hybrid cloud where you can leverage AWS services, but it can be all inside of your own data center. So you don't have to go over the internet and talk to AWS. What that means is that they bring the management console, the different APIs for services, the different SDKs that interact, and they're all consolidated into your very own data center, allowing for uniform consistency in a hybrid environment. So like we mentioned, it brings AWS to you. You have all of the same benefits. Thirdly, AWS can actually manage this infrastructure for you. You don't need to have a dedicated team to look after the infrastructure. AWS can do it for you if you desire them to. Now let's dive into family members a little bit more in depth. We talked about Outposts racks and Outposts servers, and I just want to show some information regarding both of those Outposts racks are available starting at a single 42U rack. Outposts servers, remember, are individual servers in 1U or 2U form factors. Now with Outposts, you can scale up to 96 racks. So you can build a massive Outposts internal data center. So this is very useful for a lot of AWS capacity on‑prem. Outposts servers are useful for small space requirements. So maybe you have a small retail store, maybe you have branch offices you need to use them for, maybe an individual health care provider, etc. If you don't have or need massive data center footprint sizing, then an Outposts server might be more than enough. Now racks are what allow you to host all of these servers and bring on the compute, storage, database, etc. The servers are going to be better for local compute and local networking. And the last thing here, Outposts racks give the same AWS infrastructure services and APIs in your data center. So the racks are the bigger picture within the AWS Outposts. It allows you to use a lot more internal services for AWS. That's a lot of information. But the key thing to take away from this lesson is not the size, etc; it's to remember this. If there is a scenario where you have to extend AWS to your data center, you're likely going to want to use AWS Outposts. Remember that this allows you to have AWS essentially running on‑prem. Now that's going to do it for AWS Outposts. Let's go ahead and wrap this clip up, and then I will see you in the next one.

Enhanced Networking for EC2
Okay, let's talk about enhanced networking for your EC2. There are three networking card types you need to know for the exam. The first is an ENI. Now we covered those already in a different clip, so we're not going to review them here. But I just wanted to point out it is one of the three types you need to know. You then have enhanced networking, and this will be abbreviated as EN. And the last is Elastic Fabric Adapter, or EFA. Now let's dive into these a little bit more. ENI, remember, is best for basic day‑to‑day networking tasks. So maybe you're hosting a web server, etc. Enhanced networking uses a technology called single root I/O virtualization or SR‑IOV, and it allows for high‑performance networking to occur. EFA is meant to accelerate things like high‑performance computing and machine learning applications. Now we mentioned SR‑IOV, and what it does is it allows a single physical network device to go ahead and present itself as multiple virtual devices to the underlying operating system and the hypervisor hosting the EC2. In simpler terms, this allows a much higher and better I/O performance while minimizing CPU utilization. So you get better networking performance with less stress on the operating system. Now when we talk about input/output performance, just understand this is only measured in packets per second in latency. So it's network‑specific. Let's go ahead and look at enhanced networking a little bit more. Now your instance type is going to dictate the supported enhanced networking options. So, the two that we're about to cover might not be available depending on your instance type. The first is Elastic Network Adapter, or ENA. This supports network speeds up to 100 Gbps for supported instances. You will always want to choose this whenever possible. The second is Intel 82599 VF interface. This is an older version, and it supports network speeds up to 10 Gbps for its instances. This is typically going to be used on older instances that don't support the newer ENA. Based off this info alone, I think it's pretty obvious that ENA is the best choice for enhanced networking scenarios. But with that being said, you need to be familiar with both choices. So make sure you remember ENA and these Intel VF interfaces. Next, Elastic Fabric Adapter. This is a network device that you attached to your instances, and it helps greatly accelerate HPC and ML applications. The key feature that EFA offers is called OS‑Bypass. What this does is it allows your applications to directly communicate with the network interface hardware. So it's essentially skipping the virtual layer and going directly to the underlying hardware. That's how it allows for such great performance. These are going to provide extremely low, more consistent latency. They also offer much higher throughput than the TCP transport that is traditionally used in cloud‑based, high‑performance computing systems. So if you see something regarding ML or HPC and you need extremely high‑performance networking, EFA is the go‑to. Remember that. Last thing here, it's only going to work on Linux. Now, typically anyways, high‑performance computing is run on Linux and not Windows, but I need to call this out. These are not supported for Windows at this time. So if there's ever a scenario where they're talking about needing high‑performance networking, unfortunately, if it's a Windows platform, EFA is going to be ruled out. Now that's probably a good thing for the exam because then you know it has to be something else, but just remember that it's only for Linux. Now I think that's enough on the enhanced networking capabilities that are offered for your EC2 instances. Remember, EFA is good for HPC and ML applications. Remember that for enhanced networking interfaces, the ENA version is preferred over the Intel VF interfaces if your instances support them. And then we covered SR‑IOV, which allows network devices to essentially present themselves as multiple virtual devices. Now that's going to do it for this clip. Let's wrap things up. And then when you're ready, I will see you in the upcoming module summary.

Module Summary and Exam Tips
Okay, way to hang in there. We've completed this module. Let's have a quick summary and exam tips clip here so we could take away some of the more important information that I think is good for the exam moving on. Remember, ENIs, elastic network interfaces, are the logical networking component for your VPCs that represent network cards for your compute. Next up, we have ephemeral public IPs and static EIPs. Ephemeral public IPs, remember, are from the Amazon's pool of public IPv4 addresses that they offer. Remember that you lose your IP address when instances are stopped, hibernated, or terminated. So when you perform one of those actions and you restart your instance back up, it's going to have a different IP assigned. You also lose your IP whenever you use an elastic IP address. So if you associate an EIP to your ENI, then that public IP from their Amazon's pool is gone. And speaking of EIPs, remember you will use these if you need a static, never‑changing public IPv4 address. So if there's a scenario that falls into that category, think EIP. Dual‑home instances, remember, will have networking split between two different subnet layers. Now even though they're in different subnets, remember it has to be in the same AZ. EC2 instances and their ENIs are bound to a single availability zone. Next, remember the three placement groups and their use cases and their benefits. Cluster offers low network latency, high network throughput, and they're good for HPC applications. It groups all of your instances in a close cluster, and the majority of the time they're sharing underlying hardware. Spread placement groups spread your instances apart. These are better for small amounts of instances that are hosting critical applications. So remember, you want to spread your instances apart because they're critical. Partition is similar to spread, but it's meant for more EC2 instances, and it's more specifically designed for distributed databases and distributed compute. So think things like HDFS, HBase, or Cassandra, for example. If any of those are present in your scenarios, I would immediately think partition placement group. Just remember these logically partition your instances across hardware. Then we have AWS Outposts. If there's ever a scenario about extending AWS to your on‑prem data center, think Outposts. Remember that Outposts offer racks for large deployments, and they offer servers for smaller deployments. And then the last thing here, please remember the EC2 networking card types. We talked about ENI before, and remember these are best for basic day‑to‑day things. There is enhanced networking, or EN, which uses single root input/output virtualization to offer high‑performance networking. Remember that SR‑IOV essentially allows the networking device to present itself as multiple virtual devices. And then the last one here is EFA, Elastic Fabric Adapter. This offers OS‑Bypass, and it's used to accelerate specifically high‑performance computing and machine learning applications. If you see HPC and ML in your scenario for any type of enhanced networking scenario, then you're going to probably lean toward EFA. That OS‑Bypass is a major distinguishing factor. Now that's going to do it for this module. Let's end this clip here. Hopefully you've learned a lot. Please go back and review if you need to. But for now, we'll end this clip, and I will see you in an upcoming module.

EC2 Price Optimizations
Reserved Instances and Capacity Reservations
All right, moving on to the next module, let's talk about EC2 price optimizations. This first clip is going to be regarding Reserved Instances and Capacity Reservations. Real quick, let's have an EC2 review. Remember that EC2 is an on‑demand, scalable computing service offered by AWS. You get the compute that you need, when you need it, and you pay for what you use. With that in mind, we do have on‑demand pricing. On‑demand pricing is exactly what it sounds like, you're paying for the capacity by the hour or second, usually with a minimum of 60 seconds being charged, and this is all without any long‑term commitment, so you're paying on‑demand for what you're using only. This is the most common billing type for EC2. When you spin up an instance, by default, you're on‑demand. It's important to remember this is the baseline cost for EC2. What that means is that this is going to be the most expensive option for running your compute in the EC2 service based on a per‑hour or per‑minute basis. Now, AWS has introduced multiple options to allow you to save money or save cost on your EC2 pricing. One of those options is called Reserved Instances, so let's dive into those right now. Here are some important concepts regarding Reserved Instances. What these are is a purchasing option that allows you to save up to 72% based on the on‑demand prices, so you can save a lot of money using these. Now, what you do is you commit to a usage period, so you commit to a year or a three‑year amount of time. What you're saying when you do this is saying, hey, AWS, I'll guarantee you for one year, I'm going to go ahead and use this EC2 compute, so can you give me a discount since I'm guaranteeing that I'm going to run it? AWS is more than happy to give you a discount if you're buying into a locked‑in amount of time. So, essentially, you're trading flexibility on the on‑demand side by saying, hey, I don't want to run this right now, but you're getting a cost break instead. When you use Reserved Instances, there are three potential purchasing options, the first is No Upfront, there's Partial Upfront, and there's All Upfront. What this means is when you make a Reserved Instance purchase, if you do No Upfront, that means you get billed monthly for the cost of that agreement. If you do Partial Upfront, that means you pay a little bit right upfront immediately on your bill and then you break that down into a monthly payment for the remaining balance. And lastly, All Upfront, this is what it sounds like, you're paying all of the bill immediately upfront so you get the best discount. Now these go in order from least‑to‑most amount of discount, so All Upfront gives you the most amount of savings, No Upfront gives you the least amount of savings. Also, when you're choosing Reserved Instance option, you get to choose if you want zonal or regional Reserved Instances. Zonal means it's locked down to a specific availability zone, this will give you slightly more of a break in your cost compared to regional because regional offers more flexibility because you can spread that EC2 instance throughout that region. Typically, it's easy to remember, if you're committing to a more specific use in a longer amount of time, you're going to get the best cost savings. Now, there's also a option called a Convertible option. The Convertible option allows you to do things like changing the instance type, the family, operating system, etc. A typical Reserved Instance means you're usually locked down to the specific instance type, on a specific operating system, for a specific instance family. This is why they offer a Convertible option. Now again, you get more flexibility so you're getting less of a discount if you use this. However, if you're not 100% sure on what exact instance type you need, etc., then you might go ahead and choose this just to be safe. You're still getting a very good cost break, but you're not getting a full cost break. Reserved Instances are typically going to be perfect for long running, critical applications that you always have to have up, they're not a good option for any small or short amount of time because you're paying for that one year or three‑year commitment. Another thing to keep in mind, you can buy and you can sell Reserved Instances within the AWS Marketplace. So, if you realize, hey, I actually don't need the rest of these Reserved Instances, well, then you can sell that remaining bid to someone that might want them. And that goes the opposite way, you can buy Reserved Instances off the Marketplace if you realize, hey, I just need Reserved Instances for a shorter amount of time, let me see if someone's selling their leftover Reserved Instances. With all of that in mind, however, Reserved Instances are slowly being phased out for a thing called a Savings Plan. We'll discuss Savings Plans later on in this module, but you do need to know, for right now, what a Reserved Instance is. Moving on to the next topic for this clip, Capacity Reservations. A Capacity Reservation allows you to reserve compute capacity for your instances in a specific AZ for any amount of time that you need. The key thing to keep in mind here is that you are charged for On‑Demand pricing, even if you're not running the instance. So, essentially, you're saying, hey, reserve capacity for me at an on‑demand price so I always have it if I need it. You're essentially guaranteeing that you can spin up your compute exactly when you need to for your set amount of time. But you need to remember that you're getting charged for that no matter what, so if you make a Capacity Reservation and you're not using it, well, that's not very smart because you're getting charged for it. These are perfect for things like business‑critical workloads or maybe an application that requires a specific long or short‑term capacity need. So, maybe you don't need a year or three years, instead, maybe you need eight months, well, then you could do a Capacity Reservation to guarantee that you're going to get that exact amount of compute, but you're not getting a break in price, so really remember that difference. If there was one thing that you should remember regarding Capacity Reservations, you are charged for them, even if you are not using them, please remember that. Now, that's actually going to do it. Let's go ahead and wrap up this clip on Reserved Instances and Capacity Reservations. In some upcoming clips, we're going to explore other cost‑saving options.

Savings Plans
The next topic that we're going to discuss is Saving Plans within EC2. A Saving Plan offers you a flexible pricing model that provides savings for your AWS usage specifically to compute. You'll probably notice that they're extremely similar to Reserved Instances, in the fact that they offer you a break in price, and that's because they are the successor to Reserved Instances. There are three plan types that you need to know for the exam and if you're planning a proper architecture, the first is a Compute Saving Plan, there's an EC2 Instance Savings, and then there's a third one called a SageMaker Saving Plan. Let's break each of these down really quickly so you understand what each one offers. First up, Compute Saving Plans. These are going to offer the most flexibility for choosing different instances. Because you get more flexibility, the prices are up to 66% off compared to On Demand. Now, the reason we say you get more flexibility is because these automatically apply to many different characteristics of EC2, so it's going to apply to any EC2 instance usage, regardless of the family, the size, the region they're running in, the operating system you're hosting, and even the tendency of the EC2 instance, so you're still getting a great discount with a lot of flexibility. Now, also included in that flexibility, you get other compute services called AWS Lambda and Fargate. Now, Lambda is a serverless service within AWS and so is Fargate, but Fargate is specific to containers. We'll cover these two services in a different part of this learning path for this exam prep. The next type of Saving Plan is an easy an EC2 Instance Saving Plan. Now with these, you're actually committed to a specific instance family within a specific AWS Region that you choose. Because of this more strict nature of this type of Saving Plan, you're getting the most savings, so these offer that 72% discount similar to Reserved Instances based on your On‑Demand pricing. With an EC2 Instance Saving Plan, you can only change the instance size, the operating system, and the tenancy, so you can see there's far less flexibility compared to Compute. And because of that less flexibility, these provide the most savings, right? We talked about this previously in a different clip. Generally, if you're giving up flexibility, you're going to have the most cost savings, that's just how things kind of go within the AWS cloud. The third type was the SageMaker Saving Plan. The first thing I want to call out before we get into this is that SageMaker is a machine‑learning service, and we will cover it much more in depth in a completely different portion of this learning path for the exam prep, so just please keep that in mind. With the SageMaker Saving Plan, these automatically apply to any SageMaker instances that you're using with that service, regardless of the instance family, size, Region, operating system, and tenancy, so they're very similar to Compute Saving Plans. The difference though is that these only provide up to 64% off On‑Demand pricing for your SageMaker‑specific instance costs, so it's still a really good discount, but it's not as cheap as the Compute Saving Plans offer. The real key thing here is that these are only relevant to you when you need to save on costs specific to Amazon SageMaker. Compute Saving Plans don't cover SageMaker instances because it's not classified as EC2, it's not classified as Fargate, and it's not classified as AWS Lambda. So, if there's a scenario and you need to save money on SageMaker instances, think this plan. Next thing we want to talk about is just some general concepts regarding Saving Plans. Just remember, these allow you to gain savings by committing to usage over a span of time. Now, lucky for us, the commitment periods are the exact same as Reserved Instance, so it's really easy to remember. You have one year and you have three‑year commitment periods. So with that in mind, don't get tricked if they talk about four year or two‑year commitments for any exam scenarios. Remember, it's one and three year. These are going to be extremely similar to Reserved Instances regarding the payment options. You can do No Upfront, which offers the least amount of savings, you can do Partial Upfront, and you can pay All Upfront, which gives you the most amount of savings. Last thing that we want to cover here, cost‑saving scenarios involving long‑term usage for any static compute will likely involve one of these plans. Reserved Instances are slowly being phased out, however, they do appear on the exam still, so we covered them, but the new thing that you should be using is a Saving Plan. So if anything, really focus on understanding what these three plans offer and how they work. But with that being said, let's wrap up this Saving Plans clip, and then when you're ready, we can move on to another one.

Dedicated Hosts and Instances
Okay, next up, let's talk about Dedicated Hosts and Dedicated Instances. There are two dedicated options you need to be familiar with for the exam, the first is a Dedicated Host, this is when you get a physical server that is fully dedicated for your own use. The next type is a Dedicated Instance. Now, Dedicated Instances are EC2 instances that run on hardware dedicated to a single account, so it's a pretty key difference there. Let's break down Dedicated Hosts a little bit more. A Dedicated Host is perfect for better, stronger compliance requirement needs regarding your underlying hardware. It's extremely helpful for addressing any binding software license issues, or Bring Your Own License, BYOL, that are related to physical servers. So, some software companies and vendors require that you bind software licenses to a specific physical host, well, then this is perfect for that use case. These, however, are going to be the most expensive option for running compute because you're billing per host, you're actually paying for the host hosting your VMs. With that in mind, these offer much more control and visibility into what's going on. The next type was the Dedicated Instances, these are a little bit less strict. However, what you do get with these is that your instances will not run on hardware shared with other AWS accounts, so you don't get visibility or control over the placement on the host, and you don't get host affinity, but you are not sharing underlying hardware. So, in my mind, it's kind of a step down from a Dedicated Host because you don't get as much control or visibility. However, you can move your instances between hosts by starting and stopping them. So if you do need to move between a host with a Dedicated Instance, just remember you can stop and then start it. Now these, only partially support Bring Your Own License requirements, so keep that in mind. If you need full support for BYOL, you'll likely need a Dedicated Host. Choosing between one or the other is going to matter about the amount of control you really need. Do you need instance placement control fully, do you need visibility into the underlying host and hardware, well, then you need a Dedicated Host. Can you give up on some of that control and get a little bit more flexibility and not worry about it to that specific of a level, well, then maybe a Dedicated Instance is perfect for you. Really make sure you just read through the scenarios and pick out the specific requirements. Let's go ahead and wrap things up here. We just covered Dedicated Hosts and Instances. And coming up next, we're going to explore some other options.

Purchasing Spot Instances
Okay, up next, we have Spot Instances. A Spot Instance in EC2 is simply an instance that uses spare capacity at a much lower price than On Demand. In fact, it can be up to 90% off On‑Demand pricing, so you can save a ton of money by using this. Now let's explore some concepts that you need to know regarding Spot Instances. How these work is you pay the hourly price for the instance, and this is called a Spot price. The Spot price for each instance type is going to vary based on the availability zone and the type of instance, and the price is set by EC2 itself. Now, the Spot price is adjusted gradually based on the long‑term supply and demand of the spare compute that AWS has. So, if there's a little supply and a lot of demand for the same type, well, then the price is going to be much higher than if it was the opposite. With that in mind, you can actually only even run these type of instances if capacity is actually available in the first place, so it's very possible you might not even be able to leverage these instance types. But, if you can, they're perfect for cost‑effective applications. So if you have something that's extremely flexible, like some type of batch processing that can stop and start whenever or some type of analytics job that can stop and start whenever, then these might be a good option. The big thing with Spot Instances is yes, you're getting a significant discount, but they are subject to interruption, meaning they can shut down without any guaranteed notice. What this means is that AWS will do its best to give you a two‑minute warning before it's reclaimed, but they can take back that compute at any point in time, so you really need to be careful with what kind of workloads you are running on these instance types, just make sure that it can handle an interruption. In other words, don't run a database on these and don't run business‑critical applications, only run things that can actually be interrupted. Now, another concept with Spot Instances is a Spot block, so this is where you have a Spot Instance and you specify you want to run it for one to six hours, and it's supposed to help you better avoid interruptions of that compute. Because you're giving up that flexibility, which is a common recurring theme, these are typically more expensive than a straight up Spot Instance, that's because you're giving up the capability or opportunity of it going away whenever AWS needs it, so just really remember that. If you need a Spot Instance for a significant amount of time or a set amount of time, then you might use a Spot block. Next, we have EC2 Spot fleets, these are a feature that are used to launch a fleet of tens, hundreds, and even thousands of EC2 instances in a single operation. The launches are a mix of several things. You can have multiple instance types, you can distribute them across multiple AZs, you can use a combination of On Demand and Spot Prices if you want to, and these handle automatic replacement of your Spot Instances, so this is a key feature to keep in mind. You can mix a combination of On Demand and Spot, and you can handle automatic replacement using this feature. Now, with Spot fleet, there is a thing called an allocation strategy, so this is how you're allocating your compute. So there are four things you need to be aware of here. The first allocation strategy is price capacity optimized, so what this means is that you're saying, hey, EC2, I want you to choose the pool of instances first based on capacity that's available. And then, after that, I want you to look for the lowest prices within those chosen pools. This is the recommended approach by AWS. You then have capacity optimized, so this is going to select a pool of instances that actually have the best amount of capacity for you to use. So this doesn't take price into effect at all, it's only looking for the most amount of capacity for your Spot Instances. The third is diversified, so this is where you have your Spot Instances get distributed across all Spot capacity pools, so this is better for any availability requirements you might have. The reason for that is because you're saying, hey, I don't care what I'm running, just give me a Spot Instance and choose whatever capacity you can get, so this is going to offer the most amount of uptime. And the fourth one here, lowest price, I'm guessing you probably can pick up what this means based on the name, but this is going to choose the pool for instances based on the lowest price available, and that's it. Just keep in mind, this is the most risky because it comes with the highest risk of interruption because you're only looking for a price point. Typically, the best options are going to be price‑capacity optimized or diversified. However, try to be familiar with all four options. Now, that's going to do it for EC2 Spot Instances. We talked about Spot blocks, Spot fleets, and we talked about why you should never run critical applications on your Spot compute For now, let's go ahead and wrap this clip up. And then, when you're ready, I will see you in the next one.

Demo: Creating a Spot Instance
Okay, welcome to my EC2 Dashboard. I'm in my sandbox environment here, and the first thing I'm going to do here is I'm going to go to Launch instances, and let's create a Spot Instance. I'll give it a name, and then I'm going to allow the defaults for the AMI, the type, I'll say no key pair, and we'll leave the network settings as the default as well. These are not really relevant for this demo. I'm going to skip down to Advanced details, and I'm going to find our Spot purchasing option. So, see here, under Purchasing, we could do Capacity Blocks, which is for capacity reservations. But we want Spot instances, so I'm going to click on Spot instances, and we could just accept all the defaults. So, right now, I could just go ahead and launch an instance with default settings for our Spot instance. But that's not the point of this, I want to show you what you can customize. So, I'll choose customize, and let's look at some of these options. The first is the price, so what maximum price are you willing to pay. The default option is No maximum price. What this is saying is, hey, I'm going to pay the Spot price, whatever it is, and it's always going to be capped at On Demand, at most. So you'll never pay more than On Demand, but you might pay On Demand. So this is going to give you the most flexibility, but you're really relying on that Spot price, whatever it is at the time. Now, you can also set a maximum price, for this, let's assume that the current Spot price is $0.045 cents per hour for this instance. Well, if we entered 0.043 cents per hour, per instance, well, then right now, we would not fulfill our request because it's under the Spot price. However, if that Spot price ever went below this or at this amount, then it will fulfill that Spot instance. Just understand when you do this, you could lose and gain your instances depending on the Spot price at the time, so we're going to choose No maximum price and really just hope for the best. The next thing we have is a Request type. Do you want this to be a one‑time request, so it's going to try once, and once it's filled, that's it, or do you want it to be persistent? So persistent is saying, okay, I want this to constantly be trying to fulfill this every time that it can. So maybe it goes away, and then it comes back, and then it goes away again, and then it comes back, that's what persistent means. It's going to constantly try and keep your instance running as long as it can. If it's one time and we set a maximum price or maybe the capacity is not available and our instance goes away, well, then our Spot instance won't be fulfilled ever again. It's only one time. So, for this, I'll choose Persistent. But for now, let's skip to Valid to. This is saying, hey, do you want to expire this request? So the default is no, I don't want this to ever expire. But you could say, yeah, I want this to expire on a specific date and a specific time, so maybe you only need this for a month or less than a month, well, you can set the expiration. For this, we'll choose none because I want this to run as long as it can. And the last option we have is interruption, so do you want your instance to hibernate, do you want it to stop, or do you want it to terminate? So what these are saying is, hey, if the Spot price is above your maximum price or the capacity is not available, how do you want to handle that? Do you hibernate your instance, do you stop it, or do you just terminate? For mine, I'll choose Stop. Now, what I'll do is choose Launch instance. So I just launched my instance, and now if we go to it, we're going to see this is Pending, and it's going to start running. Now, you're probably going to say, well, this looks no different than any other compute and you are exactly right. It's a normal EC2 instance, but now, we're paying the Spot price, which is typically significantly cheaper compared to On Demand. So you'll see here, the lifecycle is actually spot. So now, this is going to run at a much cheaper price, and it's going to always run until we stop it. Now, another thing, though, to keep in mind is if the spot capacity goes away, then this will stop. So, even though we're having no maximum price, it does matter if the capacity is even available, just keep that in mind. So what I'll do here is I'll refresh. And the one thing I want to show you before we wrap up, on the left‑side here under Instances we have Spot Requests, so this is another way you can create a Spot Request or view your existing Spot Requests. So, we'll see here, we should have one, the Request ID, it's an instance type, our t2.micro, it's fulfilled, and it's persistent, so this is just the details of what we did. And check this out, the max price right now is $0.012 for this instance. So this is where you can see the list of your Spot Requests. You can break it down by instance types, etc. So, right now, we're actually saving 64% compared to On‑Demand pricing, which is very significant. Just keep in mind this can change. But with that out of the way, hopefully you saw how easy it really is to launch a Spot instance and try to remember the best you can the different options when you're doing so. Let's go ahead and wrap this demonstration up, and then I'll see you in an upcoming clip.

Reducing Spend Using AWS Compute Optimizer
Okay, let's talk about reducing spend using a service called AWS Compute Optimizer. First up, what is this? AWS Compute Optimizer is a service that leverages machine learning to actually recommend you the most optimal AWS Compute resources for your workloads. The goal of this service is to help you reduce costs and also improve your performance for the compute that you've chosen. Now, that's nice and handy, but what does this actually do? Well, there are four key points here. Compute Optimizer goes in and it analyzes configurations and utilization metrics of your different supported resources. After it does that, it's going to report on the current usage optimizations and it's going to give you potential recommendations. With those reports, it gives you graphical history data and projected utilization metrics, so it's doing all of this for you on the backend by analyzing all of your configurations and past usage. Now, when you use these, this service uses graphs, metric data, and recommendations for moving or resizing your resources, so it can tell you essentially, hey, you're using a resource and you're only using 10% of it, so why don't you downsize it, or maybe vice versa, hey, you're running at 98 capacity all the time, why don't you scale up a little bit? That's the benefit this service offers. Now you might be asking, okay, but what resources does this service work with? Let's break those down. Currently, there are five supported resources. The first are EC2, it works with EBS volumes, it works with a service called Elastic Container Store, which is a container‑orchestration service in AWS, it works with AWS Lambda functions, which is their serverless platform, and it works with their managed database service called Amazon Relational Database Service. Now, these last three are going to be covered in different courses within this learning path, however, just understand that they work with Compute Optimizer right now. And speaking of working with it, you need to know you must opt‑in and enable this feature in order to use it. Now, this probably won't come up on the exam at all, but I like to call this out for real‑world applications. Remember, you're going to have to opt‑in and you have to enable it before it's going to give you recommendations, so make sure you do that. Now, with that all out of the way, we just talked about Compute Optimizer and what it does and what resources it works with, I think that's enough for now, let's go ahead and end this clip here. And then, coming up next, we're going to have a summary and exam tips review.

Module Summary and Exam Tips
All right, way to hang in there once again, let's have a quick module summary and some exam tip reviews before we move on. First up, EC2 instance pricing options. You have to be familiar with these for your exam and any real‑world scenarios. Remember, On Demand, this is where you pay by the hour or the second, depending on the type of instance you run. This is perfect for flexibility because you get exactly what you need when you need it, and it's always going to be available. The next is a Spot instance, this is where you purchase unused capacity at a discounted price of up to 90%. Keep in mind that the prices will fluctuate with the supply and the demand of that capacity. These instances are perfect for applications that have flexible start and end times. Big thing to remember is do not ever run critical workloads on these instances. Please don't do it. Thirdly, we have Reserved Instances. Remember, these are the original way to reserve capacity for one or three‑year time periods. They offer up to a 72% discount of the hourly charge based on On‑Demand pricing. They're going to be great if you have a known, fixed requirement, but they are being phased out for Savings Plans. Remember, with these as well, if you give up flexibility, you're generally getting more savings and vice versa. If you want more flexibility, like less time commitment, no cost upfront, well, you're giving up some of that savings. Last thing here, Dedicated Instances. Dedicated Instances are instances that do not run on hardware shared with other accounts. They partially support your Bring Your Own License requirements, and these are the most expensive instance option. Now, I know that might be confusing because we say On Demand is the most expensive, but we're talking about the baseline price. Dedicated Instances are a specialty purchase option so that's why they're more expensive. When we're talking about strictly EC2 compute, well, then On Demand is the baseline price. Moving on to Spot Blocks. Remember, you can use Spot Blocks to run Ec2 instances for one to six hours, and you're better avoiding interruptions, but you're not going to get as big of a savings as typical spot instances, so it's somewhere in the middle usually. The next thing we talked about was Savings Plans. The first is the Compute Savings Plan. Remember, this is the most flexible offering because it covers EC2, Fargate, and Lambda. And you have more flexibility on what type of EC2 instances you're using, these offer up to 66% of a discount. The next is EC2 Instance Savings Plan. This is only for EC2 instance coverage, and because of the less flexibility, you get more of a discount, these offer up to 72% off. And the third one here we covered briefly, SageMaker Savings Plans. These are only for SageMaker‑specific instances. They offer up to 64% off. The easiest way to remember compute in EC2 Instance Savings plans and the differences is I like to think Compute covers a lot of different things, so it's more general. EC2 instance is very specific, they're calling out only for EC2, hopefully, that helps. Next up, we have Dedicated Hosts. We talked about Dedicated Instances and now we need to cover Hosts. Remember that this is a physical server that you get assigned to you for your EC2 instance capacity. It's perfect for scenarios where you have licensing issues that require per‑socket, per‑core, or per‑VM license bounding. Just remember, this is by far the most expensive option for hosting any Compute. However, on the flip side of that, or on the other side of that, it gives you the most control and the most insights for hosting your compute. Next up is AWS Compute Optimizer. Remember that this service is recommending you optimal compute and EBS resources for your workloads. It's perfect for scenarios about analyzing the best compute resources to optimize your costs. That's the easiest way to remember it, you're optimizing your compute. So, it's going to tell you, hey, you should scale up, or maybe, hey, you should scale down because you're not using as much as you think you are. Last thing I want to cover here is a pricing example, so this pricing example table is comparing different purchasing options and showing you the different prices based per hour. Now, the first thing here is it's an M5a.large instance type running Linux in the us‑east‑1 Region. I also need to call out real fast is that these are liable to change. That's not the major issue, don't worry about that, the real whole point of this comparison is to show you how cheaper different options are compared to others. So, if we look right now for On Demand for this instance type, you're getting billed $0.0860 an hour. If you were to do a Reserved Instance with one year all up front, this is the significant savings you get, you're almost cutting it in half. Now, if you did three year all up front, you get even more savings. So this is way bigger of amount of savings, just look at the difference between one and three year. The next is, let's say, we're doing an EC2 Instance Savings Plan for one year. Well, currently, you would pay $0.051 hourly. And then, let's say we do an EC2 Instance Savings Plans for three years. Well, again, we're losing flexibility for a longer amount of time, so we're getting more savings. So, in this case, we're paying $0.0372 an hour. So, in this case, we're paying just over three cents an hour. And the last one here I want to show you is a Spot Instance. So, right now, if I spun up a Spot Instance with this exact configuration, I would pay just over $3.5 cents per hour. So hopefully, this breaks down in an easy‑to‑read manner the different savings you can go ahead and get if you're willing to commit to a certain amount of time or deal with interruptions, like a Spot Instance. For now, let's go ahead and wrap this module up. Thanks for hanging in there. Let's go ahead and take a break, get some caffeine, take a nap, go for a walk, whatever you got to do. And then, when you're ready, I will see you in the next module.

Network Storage and Elastic File Systems
What Is Amazon Elastic File System (EFS)?
Okay, welcome to the next module in this course, where we're going to discuss and explore network storage, as well as Elastic File Systems. During this first clip, we're going to explore what Amazon Elastic File System is, otherwise known as EFS. Amazon EFS is simply a managed Network File System, or NFS, that can be mounted by multiple EC2 instances at a single time. Amazon EFS actually works with multiple EC2 instances across multiple AZs, so this is a major key component of this service, it can span multiple availability zones. EBS, remember, is actually bound to a single AZ, while EFS can span multiple AZs. This service is highly available and it is easily scalable, it scales automatically. However, due to this convenience, it is much, much more expensive than using general EBS. There are some numbers where it could be up to three times as expensive as a typical, general‑performance volume in EBS. So it is a lot more expensive, but you're getting a huge benefit for being multi‑AZ, highly available, and scalable. Here are some important concepts regarding the service that you need to know for the exam. It uses the standardized NFS version 4.1 protocol, so this is the industry standard for NFS. Right now, at the point of this recording and probably for the foreseeable future, it's only compatible with Linux, so only Linux‑based AMIs can be used when you're connecting to EFS. One of the key benefits of it is that you have the capability to actually enable encryption at rest using the AWS Key Management Service, so you can encrypt your data using cryptographic keys. Another major factor and a huge benefit, and probably one of the biggest things to look for on the scenarios in your exam is that this scales automatically, so there's no capacity planning that is required for it. In other words, this is a perfect solution for any file system that you're not sure what size you need. This will scale for you automatically. One thing to keep in mind though is that it is pay per use. Now, usually that's a good thing, however, with EFS, it is much more expensive. Remember that this is roughly three times more expensive than using a general purpose EBS volume, but you're getting a ton of more benefits to use this service. Moving on, another important thing to remember, to control access to your EFS resources, so your different file systems, you actually use VPC Security Groups, so a majority of the control is done via network controls. Now you can have permissions obviously on the file system within the resource, but to actually get to the resource, you control it using security groups. Now let's actually look at an architecture diagram on what this might look like within AWS. We have a simple VPC here split between two availability zones in us‑east‑1. We have two instances, and then we have an NFS File System. Now, the big thing here to remember is that EFS can span multiple AZs, this is what allows it to support highly‑available architectures. So, if one AZ goes down, well, you can still reach it because it's spread amongst multiple availability zones. Another key factor is that EFS supports POSIX file system standards. What this means is you can make standardized API calls to your files. Now from an architecture standpoint on the left side of the diagram. What happens is your NFS clients mount a target that's created for the shared file system in NFS, so these clients will mount the mount target and that mount target is actually attached to your NFS File System. Last thing to remember, this is only for Linux‑based AMIs at this point. Now, this could change in the near future, however, I wouldn't expect it to anytime soon. So, at the point of this recording, remember, this is only supporting Linux‑based instances. Next up, let's look at some use cases for EFS that might come up on your exam. EFS is very useful for content‑management systems, this is because you can easily share content between multiple instances as you need to. It also is good for simple web servers where you have a simple folder structure for the website. Along with the simplified aspect, you also get, remember, the scalability, so this is why it's perfect for these two use cases because it can also scale for you automatically. Exam pro tip to wrap things up. If there's a question about needing a scalable file system or NFS protocol, you're going to want to think Amazon EFS, Elastic File System is perfect for those use cases. Now, that's going to do it for an overview and an introduction into Amazon EFS. Let's go ahead and move on to some more concepts in some upcoming clips.

EFS Performance
All right, we know what Amazon EFS is now, let's talk about performance of using it. Three big points here from an overview perspective. Remember, EFS is scalable, and with that, it allows up to thousands of concurrent NFS client connections, so this is really amazing. This is an easy way to distinguish when you would choose this over something like a multi‑attached volume. Remember, multi‑attached volumes in EBS can only have up to 16 concurrent connections. NFS via EFS can have thousands of connections. By default, EFS can provide up to 20 Gbps per second of throughput for your networking requirements. In other words, it is blazingly fast. And in addition to that, remember that it automatically scales. But one of the biggest things to remember is that this can scale to petabytes sizes as soon as you need it, so there is very little limitation in the amount of stores that you can use when you use EFS. Again, these benefits are also one of the reasons why it's more expensive than using a traditional EBS volume. Moving on, let's talk about performance modes. There are two primary performance modes that you need to be aware of here. The first performance mode is General Purpose, and this is the default option for when you're creating an EFS file system. This is going to be good for things that are latency sensitive or things like web servers and content management systems like we talked about in a previous clip. This General Purpose mode has the lowest per‑operation latency between the two options. Now, speaking of the second option, we have Max Input/Output or Max I/O. First thing to call out, this is a previous generation. It was originally meant for highly‑paralyzed workloads that can handle a little bit higher of latency. Now, when we say higher latency, we're meaning compared to General Purpose. Now, really to cut all of this short, Max I/O performance mode has a higher per‑operation latency than General Purpose performance mode. I know it seems odd because you see Max Input/Output and you're thinking, well, how can that not be the fastest? Try to remember this? It's a little tricky. General Purpose performance mode is recommended over Max I/O, that's because General Purpose has better latency. Again, I know it's confusing, just do your best to remember that. Moving on to Throughput modes. So, Throughput modes determine the amount of Throughput that is available to the file system. Currently, EFS offers three different modes that you can choose from. The first mode is Elastic. Now, this is the recommended mode for a majority of use cases with EFS. It's going to be the mode you should choose for any spiky or unpredictable workload, so if you're not sure the amount of throughput needed and the storage size, etc., this is a perfect choice, and that's because it automatically scales up or down with the throughput as needed. We then have Provisioned Throughput. Now, this is useful if you actually know the performance requirements for your workloads. So, if you're a perfect‑solutions architect and you know exactly down to the smallest detail what you need, then this might be perfect for you. The big thing is that it does not take into account the storage size to provision the throughput. This doesn't work like Elastic where it's going up and down based on the size needed, it's going to just have a simple, Provisioned Throughput set, regardless of the amount of storage. The last here is Bursting. This Throughput mode is useful for when you want throughput to actually scale with the amount of storage in EFS, so it's exactly what it sounds like, it's bursting in throughput to scale as needed. Now, again, the default method that they recommend is to use Elastic Throughput, and typically that's going to be the best option for everyone. Now, that'll do it for EFS performance. Try to do your best to remember the different performance modes, General and Max I/O. Remember that General is going to be used primarily over Max I/O, and then spend some time reviewing the three different Throughput modes. Elastic is good for spiky workloads, Provision is useful if you know requirements, and Bursting is useful when you want throughput to scale based on the amount of storage in EFS. Now, let's go ahead and wrap this clip up here. And then coming up next, we're going to look at storage classes.

EFS Storage Classes
All right, let's move on to EFS storage classes. There are three different storage class options that EFS provides us to use when you're creating an EFS file system. The first is EFS standard. Now this is going to offer you high‑speed SSD storage and it's meant to deliver sub‑millisecond latency performance for your application data, so it's very fast and it's very responsive. This storage class is going to be useful for very frequently‑accessed data. It is the most expensive option that they have. We also have Infrequently Access options, so EFS‑IA, or Infrequent Access. This is a more cost‑optimized storage class that's meant to offer a combination of lower costs with a mix of high performance. It's going to be useful for files that are needed to satisfy audit requirements or maybe performing historical analysis. You really want to use this for data that's accessed a couple times a year or maybe a few times a quarter. And then, lastly, Archive, also known as EFS Archive. This is the cost‑optimized solution for data where you only need to access it a couple times a year or even less. In other words, it's perfect for use cases where you do not need sub‑millisecond latency and you can just let data sit there. Prices can actually be up to 50% lower compared to infrequent access. It's important to call out with Archive, you have to at least pay for storage duration of 90 days, so it's archived for a reason. The other ones you don't have to have a minimum storage duration time, but it is important to call out for archive. You're paying for data to be there for 90 days, no matter what. Moving on, let's look at the file system types. There are currently two different file system types you can choose from for your EFS file system. The first option is Regional. This is also sometimes referred to as Standard, however, the new term is Regional. This is where you're going to spread your data across multiple Availability Zones for much better durability and availability. This is going to be useful for critical workloads or important workloads like production systems. The other type is One Zone and this is exactly what it sounds like, your data is only redundantly stored within a single Availability Zone. Now, one thing that's a positive from this, minus the cost, is that backups are turned on automatically for you by default, just in case. This type of file system type is useful for noncritical workloads, so think development, test, etc. Regardless of the file system type you choose, each file system type is said to deliver up to 11 nines of data durability within their respective AZs. So even with one Zone chosen, you're still having a really, really good chance of never losing data. But obviously, if it's important, you want to use Regional because then you're increasing your chances even more so by spreading this across multiple AZs. Last thing before we wrap up, a quick exam pro tip. You can leverage lifecycle policies with EFS to move files between different storage classes. So, for instance, maybe you have files in the standard file system storage class and after 90 days, it's probably not going to be used a lot, well, then you can go ahead and shift that or migrate that to infrequent access and then eventually archive. This is all a real scenario that you could run into and it's good to know. So just remember, you can leverage lifecycle policies to move files between storage classes. Let's go ahead and wrap this clip up here, and then when you're ready, I'll see you in an upcoming clip.

Demo: Deploying an Elastic File System for EC2
In this demonstration clip, we're going to look at deploying an Amazon EFS System and then mounting it using EC2 instances between two different Availability Zones. Let's have a quick architecture diagram review before we actually jump into the console. What we're going to do is what you see here. We're going to spin up two different EC2 instances, split between two different AZs. We will work on creating our EFS file system resources, as well as the security group that's only going to allow specific traffic within our VPC for the NFS protocol. After we do that, we're going to spin up two different EC2 instances, again, spread between different AZs, and we're going to work on setting those instances up so that they mount the new file system, and we're going to test writing different files and showing how it really works. Now, when we make these new mount targets and we mount the mount targets, we're going to do it in two different ways, one manual and then one automatically via the EC2 configuration screen. I just want to show you the different options that are there. For now, though, let's actually cut here, and then I'm going to see you in the AWS console here in just a moment. All righty, welcome to the AWS sandbox console here. I'm in my us‑east‑1 Region and let's go ahead and get started. First thing we want to do is we need to work on creating the EFS resources and then the VPC security group, so what I'll do is I'm going to create this security group first. What I'll do is I'm going to navigate to my VPCs, and for sake of simplicity, we're going to use the default VPC just to make it a lot quicker. So, what I'm going to do is I'll jump to Security groups, and I'm going to call out this, I have an AllowAndrewSSH rule here. This security group is the security group I'm going to attach to our EC2 instance's network interfaces. So to secure our EFS file system, we're going to set this up so it only allows NFS traffic from this security group. So I'll go to create, let's give it a name. I fill in the Description, we're using the default VPC, and then let's add our single inbound rule. I'm going to search for NFS. I'll select it. We see the Port range here, which is 2049. And then, for source, I'm going to leave it as Custom, and I'm going to look for our SSH group here, AllowAndrewSSH. With this, we're only allowing NFS traffic from resources that have this security group assigned so we don't have to track IP addresses, etc. I'll scroll down. I'm going to click on Create. And there we go, we now have it created with our one inbound rule. I'll close this tab. I load up Amazon EFS, and you'll see we have the file systems loaded, so let's create our first file system. I'm going to click on Create, and I'm going to go to Customize. Now we could do the defaults, but I want to show you the different options, so I'm going to click on Customize and let's walk through this. We can give it a name, so I'll just call this Demo, and then we choose our file system type. Remember, there's Regional and there's One Zone. With Regional, remember it's the highest level of availability and durability because it stores data across all AZs. With One Zone, you have to choose the single Availability Zone you want the mount target to live in. So, for this, we'll choose Regional, and then we come down to Automatic backups. Now, I'm just going to leave this checked. But if you wanted to save on storage costs for some odd reason, you could easily disable this. Now, I'm going to leave it enabled, but it's good to call out that this is enabled by default. Next up, we have Lifecycle management. So, remember we talked about how you can use life cycle policies to transition data to different storage classes, so the defaults here are Transition into Standard is None because that's what we're going to use, but you could turn it on for first access. I'll leave it at None, and then let's look at the other two. So, the default here is, hey, after 30 days since the object was last accessed on the file system, transition it to Infrequent Access to save on cost. And then, after that, when it's 90 days since the last access, we're going to go ahead and transition into Archive because that means we're probably really not going to use it, and we want to save on cost even more. These are the defaults, but notice there are some custom options you can select if you wanted to, I'm going to leave it as is. Next up, we have Encryption so you can encrypt at rest like we called out in the previous clip. So for this, we're going to use the default service key here. But if you wanted to, you can customize and use a custom KMS key for better security practice. We're going to skip this and use the default encryption‑at‑rest method, and let's get down to performance. All right, so with performance settings, remember there are different Throughput modes. They give it two general categories, Enhanced and Bursting. But remember, with Enhanced here, we have Elastic, which is recommended like we called out, there's Provision where you set the Provisioned Throughput regardless of the storage size, and then there was that Bursting that we looked at. Now I'm going to choose Enhanced and Elastic because that's the recommended method, and underneath Additional settings this is where you can change the performance mode. So, notice now with the current version, they don't even allow you to select Max I/O for enhanced Elastic. But if we did Provisioned, well, you could choose it, and set your settings that way. And if you did Bursting, you could do the same. So I'm going to choose General Purpose and Elastic. Remember, General Purpose is actually faster than Max I/O. So, with our settings in place here, we'll review really quickly and I'll click on Next, and now we choose the Network settings, so you choose the VPC where you want your EFS to live. We're going to use the default here. And since we chose Regional, we have Mount targets by default in every subnet that is active within this VPC, so for us it's a, b, c, d, and f. You can see the subnet IDs in those Availability Zones, and we're going to allow the IP addresses to automatically get assigned within that subnet. I don't want to have to deal with tracking that. So we'll leave this blank and then we get this security groups. So remember, you control access to your file system via security group controls. So what I want to do is remove all of the defaults, and I'm going to add in our EFS security group for each subnet here. So let me go through, I'll fast forward and select this for each one. Perfect. So now I have our EFS security group assigned for each subnet, which is going to be assigned to each mount target in that subnet. And as a refresher, this allows NFS traffic from only our other security group that we had created, which will be assigned to our EC2 instances. So I am going to click on Next, and this brings us to our file system policy. Now, the file system policy is an optional policy written in JSON where you can set certain permissions. So, for instance, you can say, hey, I want to prevent route access by default, it'll put that in place, I want to enforce read‑only access by default, it'll put that in place, etc. I don't want to set any policies, but it's good to know that you can, and I'm going to click on Next. So, we review here. We see all of our settings. We see our mount targets and our subnets. We have no policy, and I'm going to click on Create. Awesome. So it's creating right now, I'm going to go ahead and select it, and then what we can do now is we can view it. You see it's available. We have all of our settings. What I'll do is refresh and we should get a DNS name here in a second. There we go. And this is what we're going to use to mount on our EC2 instances, so let's actually test that out now. I'm going to go to EC2. I'm going to create a new instance. We'll call this Client1. I'll leave the default Amazon Linux. I'm going to select a different instance type just to make it a little bit bigger. And then for key pair, I've already created a key pair before this demonstration to use so I'm going to select that. And let's specify our network settings, I'll use the default VPC. I'm going to deploy this into us‑east‑1a, and then I want to leave Auto‑assign public IP, and select my security group that allows my SSH, which is also trusted in that EFS security group. So now this is all we need for this first test. I'm going to click on Launch instance, I'll select it, and then let's go ahead and connect to this instance. So I'm going to copy this public IP, I'm going to go into my terminal, and let me go ahead and SSH in. Awesome. So I'm now in my EC2 instance here, let's work on mounting this EFS file system. The first thing we want to do here is we want to make a new mount directory, so what I'm going to do is run sudo mkdr and I'm going to make an EFS directory in the root directory. Okay, so now that should be there, and we can see efs here. And the next thing we need to do is we need to make sure we have the NFS utilities even installed on our server itself, so I'm going to go ahead and run a yum update, looks like we're good there. I'm going to copy and paste in this command here, and we're just looking for and installing nfsutils. Now, I'm going to say this is going to have Nothing to do because on Amazon Linux 2023 this is already installed. However, I wanted to show you this command just in case you need to install it. Now these commands will be in a text document for you to reference as a module asset, so feel free to look at those there if you need to. But let's go ahead and move on. The next thing we need to do here is I need to make sure that the NFS service is even running, so let me copy and paste this command in. I'm going to try and start it, and now I'm going to try and check the status. Perfect. So NFS is running and now we should be able to mount our EFS target mount point. I'll clear my screen here, and the only other thing we need to do now is try and manually mount this. Let me paste this command in, and we'll talk through it. We're running sudo obviously to hit a root permission, running the mount command, we're saying nfs file system type, and then we're passing in some options. Now these options here that I've highlighted are the recommended options by AWS for mounting an EFS file system. With that being said, I need to change a few things. This last portion here is actually our local mount point, so I need this to be /efs, and then I need to change this DNS name, so what I'll do is I'm going to get rid of this entire DNS string. I'm going to go back into my console. I'm going to find my EFS file system, and we're going to use this DNS name here that we looked at earlier. So I'm going to copy this, I'm going to go back, let me paste that in, and now I can hit Enter. Perfect. So now, in theory, we should see that when I look for it in our mount screen. So let me go ahead and run mounts, I'm going to grep for efs. Perfect. We can see we've mounted it here on /efs, and it's nfs4. So now if I do an ll on route, we have a mounted directory. Let me clear my screen. I'll change into that directory, and now we can do a list of permissions. And one thing we need to note here is that we don't have permissions to write, so I need to go ahead and change the permissions of the file system. So to do that, I'm going to copy and paste the command in here. What this is doing is it's changing the permissions to allow read‑and‑write access with a couple other nuances that we don't need to cover. The big thing here is to realize you have to change permissions to allow read‑and‑write access outside of root. So I'm going to run this, I'll run ls ‑al again, and now we see we can write. So now, in theory, I should be able to go ahead and create a file. I should be able to write in it. And there we go, so now our file is on our NFS file system. Perfect. Now let's test this out with another instance. So what I'll do is go back to my console, I'm going to go to Instances, and I'm going to launch an instance like this one. So under Actions, Image and templates, Launch more like this, I'll change the name here. I'll leave everything else the same as far as instance type. I'll choose my key pair. We're going to select a different AZ, so I'll just select 1f to prove that this is multi‑AZ. We'll use the same security group, but I want to show you another method to mount this. So we did it manually via the file system last time, but you can actually go to Advanced, load up file systems here, and we can mount it automatically this way. So I can select EFS, Add shared file system. And for us, it's the only one there, so it automatically loads. We see our name, our ID, it's Regional, and we can set the mount point automatically. So I want to change this to be /efs just like the other one, so I'll set that. We can say, hey, I want you to automatically create and attach security groups as needed. We don't need to do that because we already did this. However, the next one I do want it to do, it's going to run a user data script to automatically mount it at boot. So, if you look at the user data here at the bottom, you'll notice it's already in there automatically, which is awesome. So this is just another format you can execute user data with, but I want to show you just some of this similar stuff. We're installing nfs‑utils, they're running a couple of other optional ones. They're setting our file_system_id for us, and then they're running a similar command here where they're mounting with all of those different options like we did manually. The only difference is they're setting this in fstab, which means this is going to mount automatically at the boot time. So right now, on our Client1 instance, we have to manually mount this each time. But since in Client2, it's getting set in fstab, that means every time this reboots, it's going to remount this mount point. So this is the way you really should do it, but I wanted to show you the manual way and the automatic way. So we'll launch this instance, and I'll select it. I'm going to copy the IP and go back to my terminal here, load up my second tab, and what I'll do is I'll ssh into this instance. We're now connected, but now it might take a second for us to actually view our EFS. We can see the directory is there, let's see if our file is actually there, and it is. So now if I change directories, and I cat that file, perfect. So now we have mounted that NFS file system between two different EC2 instances in two different Availability Zones. So now what I can do to test from this side is clear this, and I'm going to download the PDF version of the user guide for EFS into the EFS directory. Okay, it's showing up over here. Let me go back to my original instance. Clear my screen. I'll ls in here, and we see that PDF so this is working. We're now able to go ahead and use files concurrently via these connections. So that's how easy it is to spin up an EFS file system, create the required security group resources, and then mount it on an EC2 instance. Again, the commands I used for this demo will be available in the module assets so feel free to follow along if you want. But with that being said, I'm going to go ahead and end this demonstration clip here. I hope you learned a lot, let's wrap it up and move on.

Amazon FSx for Windows
All right, shifting gears just a little bit, let's start talking about Windows workloads. In this clip, we're going to look at Amazon FSx for Windows. What is Amazon FSx for Windows? What this is is a fully managed, native Microsoft Windows file system that's meant for you to use to easily move your Windows‑based applications that require file storage up to the AWS cloud. In shorter terms, it's an easy‑to‑use service for Windows‑based file systems. Let's review some concepts that are important for the exam. Really remember, FSx for Windows, while it seems pretty self‑explanatory, is a fully‑managed Windows file system for Windows migrations. The key giveaway here is always going to be in the name of the service, which is FSx Windows. It supports the typical protocol, so Windows NTFS file systems and Server Message Block, or SMB. Again, it's designed for Windows and, accordingly, Windows applications. That seems self‑explanatory, but it can be a little confusing sometimes because they might trick you on the exam and try and mix some things in there. So remember, this is for Windows and Windows applications. Now since it is designed for Windows, it does support things like Active Directory users, Active Directory access control lists, security groups, and even security policies, so all of the normal things that go along with managing users in a Windows domain. And lastly, it even supports things like a Distributed File System namespace and replication, so DFS. If you see anything regarding DFS with namespaces and replication, you're likely going to need to choose FSX for Windows. Now I will call out, you're not going to necessarily need to know super in‑depth knowledge regarding AD users, Active Directory security groups, and DFS, etc, but you need to know that this service supports those. With that being said, let's look at performance and durability. This service allows for throughput of up to 10+ GB per second, so it's very, very fast and high performing for storage throughput. Speaking of storage, it can store hundreds of petabytes of data so it can scale to a massive amount. And lastly, it allows capability to be multi‑Availability Zone, so it's highly available, and it can backup to Amazon S3 on a daily basis. If you're not familiar with S3, we're going to cover that later on within this course. But, just for a quick, high‑level knowledge, it's a storage system in AWS. Big thing here, just think Amazon FSx for Windows when you need a centralized storage service for a Windows‑based application. So if you see things like SharePoint, Microsoft SQL Server, Workspaces, IIS web servers, or other native Microsoft applications, this is going to be the perfect choice for that scenario. Now, that's going to do it for this clip on Amazon FSx for Windows. The biggest thing to remember from this clip is if you need a centralized storage solution for Windows, this is probably going to be the best choice. We can go ahead and emd this clip here, and then when you're ready, we can move on.

Amazon FSx for Lustre
All right, next up, Amazon FSx for Lustre. Amazon FSx for Lustre is a fully managed, parallel, distributed file system that is optimized for compute‑intensive workloads. So let's actually break down what this actually means. Here are some important concepts relating to Lustre. This is geared toward Linux‑based operating systems. Now it's not restricted to Linux‑based operating systems like EFSs, but it is geared toward them, and the main reasoning for this is because it's useful for high‑performance computing and machine‑learning workloads, which, a majority of the time are going to run on a Linux‑based operating system. FSx for Lustre easily supports millions of input/outputs per second and sub‑millisecond latencies, so this is why it's highly performant and why it's very good for HPC and machine‑learning workloads. In addition to the input/outputs and the sub‑millisecond latency, it allows you to process massive datasets at up to hundreds of gigabytes per second, so again, it's highly performant. A neat feature of Lustre is that you can leverage Amazon S3 as a file system for it. So remember, similar to a previous clip, S3 is just a storage service in Amazon that we're going to cover later in the course. But for now, just remember Lustre can use it for a file system. Some examples of when you might use Lustre is video‑processing workloads or maybe electronic‑design automations. Again, this is all compute intensive stuff so that's why FSx for Lustre is very useful. Really, just think FSx for Lustre when you need high speed, high‑capacity distributed storage for your apps. What we mean by this, again, is anything where you're talking about high‑performance computing, financial modeling, etc., anything that's compute intensive for a storage system this is a perfect solution. Last tip here, also remember that Lustre can store its data directly on S3, this is an important feature to keep in mind. Now that's going to do it for this lesson on FSx for Lestre, biggest thing to take away here, again, remember that it's useful for high‑performance computing workloads or anything else that's compute intensive. Let's go ahead and wrap this up, and then I will see you in another clip.

Amazon FSx for NetApp ONTAP
Okay, next up on the FSx playlist, we have Amazon FSx for NetApp ONTAP, or O, N, T, A, P. What this service is, is a fully managed, shared storage service in AWS, with the popular data access and management capabilities of ONTAP. If you see anything related to needing a NetApp ONTAP compatible service, this is going to be it. A big thing here is that it works with a majority of major operating systems, so there's not a big compatibility issue that you have to worry about when you're using it. One of the key capabilities is that this storage can shrink and grow as you need it, so that's one of the major benefits of using this in AWS. Another key concept is that it supports the industry standard NFS, SMB, iSCSI, and even NVMe‑over‑TCP. Now, a majority of the time for the exam, you shouldn't need to know too many details about this service. But just remember, if you see anything about needing to migrate or use NetApp ONTAP file systems within AWS, then you'll want to include this service in that solution. With that being said, let's go ahead and wrap up here, and then when you're ready, we'll move on.

Amazon FSx for OpenZFS
All right, next up for the FSx playlist is FSx for OpenZFS. Amazon FSx for OpenZFS is another fully‑managed file storage service, but it's meant to make it easy to move data to the cloud from on‑prem using ZFS file systems or even other Linux‑based file servers. Let's look at some important concepts related to the service. This is usable by multiple OSs, including macOS, Windows, and Linux, so again, there's not too many restrictions on what OS is using it. Now this supports the industry‑standard NFS protocol, including version 3, 4, 4.1, and 4.2. Right now, it offers multi‑AZ, which is highly available, Single AZ a highly‑available mode, and Single AZ non‑highly available. So obviously, the Multi‑AZ highly‑available mode is going to cost a little bit more, but it offers the best availability and durability, and then we work down the list to Single‑AZ HA, which is a little bit better durability, a little bit less cost, and then single AZ non‑HA is going to be the lowest amount of durability and availability, but it's also going to be the cheapest option. Now, a key factor here for this service to keep in mind is that fully‑managed file system backups get stored on Amazon S3. I'm sure you're seeing a recurring theme here that a lot of these can leverage S3, specifically the FSx services, so just be sure to remember that key component, the backups get stored in S3. A big thing here, too, is that there are near‑instant point‑in‑time OpenZFS snapshots so it's really good for RTO and RPO purposes. However, with all of this being said, long story short for the exam, you need to thank Amazon FSx for OpenZFS when you need support for open‑source ZFS file systems. Now, that's going to do it for this clip, let's go ahead and wrap things up here. And then, coming up next, we're going to have a module summary and some exam tips before we move on.

Module Summary and Exam Tips
Okay, let's wrap things up with a Module Summary and Exam Tips clip before we move on. First up, Amazon EFS exam tips. What should we remember about EFS going into the exam? Remember that this supports NFS version 4.1 for Network File Systems. You pay only for the storage you use, and with this there's no pre‑provisioning required. With that in mind, remember this automatically scales, and it can scale up to petabytes in size. Another key important concept is that it can support thousands of concurrent NFS connections at one time. It's a humongous difference compared to EBS multi‑attached volumes. Also, by default, using Regional settings, the data is stored across multiple AZs within your Region, that means it's super durable and it's highly available. With EFS, you get a read‑after‑write consistency, and we saw that in the demonstration clip where our file was pretty much immediately available for us to read between the two different instances between two different AZs. In your exam, if you have a scenario‑based question around highly‑scalable shared storage, especially using NFS, you should think Elastic File System. Now, two more tips here regarding EFS, you need to know the storage classes, so just do your best to remember that there's Standard, Archive, and Infrequent Access, and you can use lifecycles to move data between the different storage classes. Again, we saw that in the demonstration as well. Moving on to FSx exam tips. FSx for Windows, remember this is centralized storage for Windows‑based applications, so SharePoint, SQL Server, IIS web servers, etc. FSx for Lestre is another one that's meant for high speed, high‑capacity distributed storage requirements, so this is perfect for HPC applications, financial modeling, etc. Another key factor here is you can store data directly on S3 and leverage it. We then had NetApp ONTAP, and this is going to be used only when you need to migrate or use NetApp ONTAP file systems in AWS. This is pretty self‑explanatory. And then finally, we talked about OpenZFS, so when you need support for open‑sourced OpenZFS file systems, this is the one you should use. Now, that's going to do it for this entire module, hopefully you learned a lot about the different network file systems that you can use in AWS. Feel free to review where you think you might need to. But for now, we're going to end this module and clip here, and I will see you in an upcoming module.

Amazon Simple Storage Service (S3) Overview
What Is Amazon Simple Storage Service (S3)?
Up next, in this module, we're going to look at Amazon Simple Storage Service with a quick overview and introduction to this service. This first clip is going to talk about what Amazon Simple Storage Service is, otherwise known as Amazon S3. Amazon S3 is a service that provides us a secure, durable, and very highly‑scalable object‑storage solution. It allows us to store and then retrieve any amount of data from literally anywhere in the world with an internet connection at an extremely low cost. One of the neat things or features about this service is it gives us an easy to use, simple web interface if we want to interact with it in the console. And speaking of that console, here's an example screenshot of what it might look like, at least partially, when you're exploring your different resources within the service. It's pretty simple to use, there's a lot of easy‑to‑click buttons, and other things of that nature. However, with the console out of the way, let's dive into some important concepts for this exam. S3 is a global service that deploys Regional resources. This can really trick people, so be sure you understand this. The service allows you to store an unlimited amount of data and an unlimited number of objects. Now we'll talk about objects in a upcoming clip, but this is essentially just how you reference your data. The service is scalable, which is one of the major features that it offers, so you can start with kilobytes of data and scale up to petabytes or terabytes of data, whatever you need. Now, an object, again which we will cover more in depth, can be up to a maximum of 5 TB in size. This is an important limitation you need to know. So any data you upload as a single object or a single file, essentially can only be up to 5 TB in size at this current time. Also, this service cannot be used to run an operating system and it should not be used to run a database. It will not function properly, you will have a lot of issues, and it probably won't even work in the first place. Lastly, the service is great for things like backups of your data, archiving your backups, hosting media, maybe data lakes, and even static websites, so you can host a static website using the service. One thing I really want to call out once again because this can be a little confusing when you're first leveraging this service, the service itself is global, but remember the resources that it creates in hosts are Regional. This is very, very, very important. Moving on, it's important to know that you can upload literally any file type that you can think of to this Amazon S3 service, so we're talking things like photos, company documents like Word or PDF, you can upload MP4 files, your log files, zipped archives, etc. There are tons and tons of files that you can upload. If you can create a file locally, no matter what the extension is or what type it is, then you can upload it to Amazon S3. Now let's talk about durability and availability. This is one of the big bragging points for this service. From an availability standpoint, this service is built for 99.95 to 99.99% service availability, depending on something called the S3 tier. We'll talk about tiers much more in‑depth later on within this module, however, just understand that this offers up to 99.99% service availability, which is extremely good. And then we have durability. And check this out, S3 is designed for 11 nines of durability for your data and your files that are stored within the service. In other words, it is extremely unlikely that you will ever lose any data that you store in S3. This is one of the biggest, if not the biggest, bragging point about the service. Now, that's going to do it for this introduction into what Amazon S3 actually is. We talked about how it's a global service with regional resources. We talked about how you shouldn't and probably can't use it for a database or an operating system, and we talked about some of the availability and the durability. Let's go and wrap this clip up here. And then, in the upcoming clips, we're going to start exploring the resources that belong to this service.

Amazon S3 Buckets
All right, we just got done covering Amazon S3 at a high level, let's dive into some of the resources, and first up is going to be Amazon S3 Buckets. Amazon S3 Buckets are simply the containers for storing all of your objects and data within the S3 Service. A key concept for a bucket is that all AWS accounts share the same global S3 namespace. What this means is that your bucket names must be globally unique, so you can't share bucket names between accounts because, again, this is a global namespace. And lastly here, even though the bucket names are global, remember that they're going to be regional resources. This is why we called it out in the previous clip that the service is global, but the resources are Regional. This is a very important distinction. Now you might be asking, well, the service sounds great but how safe is my data if I use this service? Well, it's actually an extremely safe place to store your files. Remember, we talked about those availability and durability numbers from earlier and let's talk about them a little bit more in depth now. The S3 Standard storage class, which is the default class, and again we'll dive into classes a lot more in‑depth here in an upcoming clip. But just remember, essentially the default class that stores your objects redundantly stores objects on multiple devices across a minimum of three AZs in a single Region. What this means is your data is stored redundantly across multiple data centers, right, because Availability Zones are logical groupings of separate data centers that are geographically dispersed. This is how they're able to meet that high‑durability number we looked at earlier, the 11 nines. Also, the service handles concurrent device failures by quickly detecting them and then repairing any loss redundancy. What this means is if one of your AZs goes out where your data is stored, well, then S3 is going to recognize that and say, hey, I need to copy this data to another AZ so it can handle any future failures. The awesome thing is this happens all behind the scenes without you even knowing. Next up, let's talk about strong read‑after‑write consistency. First up, read after write, so what this is saying is after a write or an overwrite of your object, so your data, any subsequent read request is going to immediately receive the latest object. So, if I create an object and you try and read it, you will immediately be able to actually get the latest object. The strong consistency comes in where you're saying, hey, I want to list the different objects and files within my bucket where you're going to get all of the changes that have already been made immediately reflected. So again, with that same example, if I update or create a new object and put it in S3, as soon as you list, immediately after I do that, you're going to get the accurate list of objects, so you will see that new object in there. Let's look at very quickly the naming convention requirements for your buckets. Now, I'm not going to read through all of this list because there's a long list, and this is only part one, but I'm just putting this in here for you to reference in the future if you want to come back and review. The big thing I would call out here is that it has to start and end with a letter or number and it can only have lowercase letters. Some of the other requirements are more nuance‑based, but things that you need to know for real world scenarios. So if we move to the second portion on the list, you can see there's a lot of other requirements, these particularly deal with the beginning of your bucket names and the ending of your bucket names. Again, I'm not going to read through all these, that's boring, I just wanted to include them for you to reference later on. Last thing here before we wrap this up, when you upload a file or an object to an S3 bucket, you will receive an HTTP 200 status code if the upload was successful. What this means is it's a RESTful API call to upload objects, that's the big thing to keep in mind. Now, that's going to go ahead and do it, let's wrap this clip up. We just talked about Amazon S3 Buckets and how they are the containers for your data within S3. And coming up next, we're going to talk about Amazon S3 Objects, so the actual data within those buckets.

Amazon S3 Objects
All right, welcome back. Let's talk about the S3 objects that go within your S3 buckets. S3 objects are simply your files, or the data, that you're uploading to your S3 buckets, so your buckets contain your objects. Now, a key thing to note here is that objects are stored in the buckets and offer a flat path, which is similar to, but not the same as, a typical folder structure. So on typical operating systems you would have a folder structure that you would reference, well, S3 offers something similar to make it easier to look at and navigate through, but it's not the same exact thing, and we'll take a look at those here coming up in a second. First up, though, let's talk about some concepts you need to know about objects. Each object or file that you upload is going to have a Key and a Value. The Key will be the name or the reference to the object, and the Value is the actual object data. Within S3, there's a concept known as a prefix. Now, a prefix is meant to help you implement a human‑friendly naming structure for storing your objects, so these are kind of like folders, if you will. When you upload your objects, the object Key is going to be the full path and that's going to include any prefixes, so this is how they make it appear like there are folders to store your objects. And then lastly here, objects can actually have multiple versions and they have metadata that is attached to them, so the metadata can include things like the content‑type or maybe the last‑modified date. Now I've talked a lot about Keys and Prefixes, let's actually break down an example. Here is a real‑world example of a potential object Key within the S3 service. The first thing we'll notice is the S3 URI. So, when you're referencing an object via the S3 API, this will always be included. The next thing we see here is the actual bucket name, so we named our bucket pluralsight‑bucket, and then we start getting into prefixes and the object itself. So the first thing we see here is a /dev. Now this is the first portion of a prefix. It's important to call out that the prefix does include that forward slash as well, so you have to include that. After this, we have a /2024, so this, again, is another prefix after that first prefix. And then the last thing here that we have is the actual object name, so we uploaded a file called test_data.json. In the grand scheme of things, this entire portion right here, /dev all the way to our object name, is going to be the entire Key for the object. So this is how you would reference that object if you were making an API call to try and get it, update it, or delete it. So, hopefully that kind of clears up how prefixes work. Again, they're essentially meant to kind of help serve as a folder‑type structure for us to reference data. Next up, let's talk about multipart uploads, this is an Amazon S3 feature that's going to allow you to upload very large objects in smaller, more‑manageable parts. The object parts are uploaded independently of one another. What this means is that if any part fails, you can actually retry the upload without affecting the other parts getting uploaded. One of the neat things here is that S3 handles putting back those objects together automatically for you once they're in S3. Let's talk about two use cases for this feature, the first is when you need to upload a very large file, especially anything over 5 GB in size. AWS recommends that you use this feature for anything large. And then, the second one here is uploading objects when you have an unstable or maybe a poor network connection. So if you're going to get failures potentially when you're uploading your objects, this is a good feature to use because it can handle automatic retries for you. Let's actually visualize what this would look like with a very simple diagram. We have our user on the left who's going to make a multipart upload call via the CLI or maybe the SDK. They're trying to upload a 200 MB object. And when we make this multipart upload call, it's going to break this object into several smaller chunks that are completely independent from one another, and they're assigned unique identifiers that S3 will track. Once it's broken up into multi‑parts, they actually start getting uploaded independently into the service, and then S3 will do its best to go ahead and put those back together. It gets reassembled, and now we have our 200 MB object living within our S3 bucket, so that's essentially at a high‑level how this feature works. Last things here before we wrap this clip up, object tags and metadata. You can tag your objects using key‑value pairs of any useful information you need. This is generally a best practice to do, so you really should be tagging anything and everything you can within AWS. The last thing here is metadata. Remember, the objects contain metadata and this is basically only data about the object that you're storing, so again, content‑type and last‑modified. I wanted to call this out, again, I know we talked about it earlier on, but it's important to understand you can use metadata on your objects. That's going to do it for this clip. Remember that objects are uploaded into your S3 buckets, you can use prefixes to manage them like folders, and really be sure you understand how you reference your objects themselves using the Key. Equally important, really understand when and why you might use the multipart upload feature of S3. Now that's going to do it. Let's go ahead and end this clip here. And then, coming up next, we're going to start looking at different storage classes and some demonstrations.

Demo: Creating an Amazon S3 Bucket
All righty, let's dive into this demonstration clip where we're going to create our very first S3 bucket. I'm in the Console Home in my sandbox. Let me go ahead and load S3. And once we get here, we're taken to a general dashboard. Now, we already have a bucket created so your screen might look different if you're following along, just keep that in mind. Let's actually work on creating a bucket. For this, what we'll do is we're going to navigate to buckets, which is where we're at now, and I'm going to find Create bucket and I'm going to select it. Now, I'm going to call out really quickly, there are a lot of settings on this demonstration and we're not going to cover all of them because we talk about these other features and these other options and settings later on within this module and in other modules within this course following this one, so we're just going to work on creating a simple bucket for now. The first thing we notice is our Region. Now, I loaded into the North Virginia Region, which is us‑east‑1, so this is where this bucket is getting created. However, remember, this is a global namespace for the service itself, so our bucket is Regional, but the service is not. We then have our bucket type. So we're going to choose General purpose because it's recommended for most use cases, and that's very true. A majority of the time, General purpose will be just fine. They do have a Directory bucket type, which is better for low‑latency use cases, but we're not going to cover those here. The next thing we have to do is create a bucket name, so remember those naming convention rules that were required. So, what I'll do here is I'm actually going to give this a very simple name, and I can tell you this name has already been taken, but I want to show you what happens when you try to create a bucket with a name that's already in place so we'll call it my‑bucket for now, and then the next thing we can do is we can copy settings from an existing bucket. So we have that other bucket in here and if I wanted to, I could choose that bucket and then copy those settings that are set here very quickly with the click of a button. So I'll Cancel out, but that is an option which is really nice to have. Now, what we're going to do here is I'm going to skip over a majority of this other stuff because these are settings that we talk about later on within this course. So I'll skip to the bottom here, and I'm going to create my bucket. However, notice it didn't create because if we go back up, we're going to have an error, Bucket with the same name already exists. So, like I mentioned, remember S3 is a global namespace. We don't have this bucket in this account, but someone has already taken this bucket name, so what I need to do is make this unique somehow, so I'm going to go ahead and add a random string at the end of this, and I'm going to assume that no one's added this to the end of their bucket name. So let's go down, we'll try again, and there we go. We've now created our bucket. Perfect. You can see the AWS region us‑east‑1 here. We see the bucket name. I can go in here and look at the different objects that are in there, and we cover objects in a later clip. But this is now the container for me storing my data in S3. We've created our bucket now we can use it. Now, I did want to show you two more things. I'm going to go back to Buckets, let's create a new one, and I'm going to name this with a capital My‑bucket. Again, remember with buckets, they have to be lowercase. So if I go down and skip to Create, we get another error because, hey, Bucket names must not contain uppercase characters, so remember this constraint. I'll cancel out. Now let's test one more thing real quick. I'm going to go back to my Console here. I'm going to go to us‑west‑2. I'm going to load S3 back up. And check this out, yeah, we're in the Oregon Region, but I can still see all of my buckets, again, this is because S3 is global, but the resources are Regional. So now if I create a bucket, we're going to see it's in us‑west‑2 instead of us‑east‑1. Now, I'm not going to create this bucket, I just wanted to show you how this works and prove to you that S3 is global, and you can see all of your resources from the centralized service. Now that's going to do it for this simple demonstration. We've created our bucket here with our unique name. Let's go ahead and wrap things up and then when you're ready, I'll see you in some upcoming clips.

Amazon S3 Storage Classes
Let's go ahead and pick back up with S3 storage classes. What is an S3 storage class? Well, first things first, each object that we upload to our buckets has a storage class associated with it. The default storage class is a Standard storage class. However, if you want, there are many different storage classes that you can use for your objects, and we're going to dive into those here coming up very shortly. The big thing to keep in mind here is choosing the storage class is going to depend on the use case at hand and different performance access requirements. Regardless though of whatever storage class you choose coming up, all of them offer some type of high durability. Now the numbers might differ slightly, but they're all going to be highly durable. Now let's actually start off with storage classes for frequently‑accessed objects. The first is S3 Standard. Now this is going to be the default class when you're uploading an object and it offers those default service‑level agreements and default pricing, so this is where we're going to get that 99.99% availability and 11 lines of durability. You also have something called Express One Zone. This is a high performance, single‑zone storage class, offering single‑digit millisecond data access. So, S3 is going to distribute your data across those multiple availability zones like we talked about in previous clips, Express One Zone is locked to a specific availability zone within your region, so Express One Zone would be potentially useful for machine learning or high‑performance computing workloads where you're having instances run in a specific AZ and they need to reference data within that bucket. With these both being covered however, S3 standard is going to be suitable for a majority of your workloads, it's best for any general purpose workloads. When we say general purpose, we're talking about use cases like websites, content distribution, mobile, or maybe gaming applications, and even big data analytic storage. So again, this covers a large majority of use cases. Moving on, there's also storage classes for infrequently‑accessed objects, so, in other words, when you don't get your objects often. Now, S3 offers these classes for your long‑lived and infrequently accessed data at a cheaper cost. However, you need to keep in mind that there is a retrieval fee for this data, so it is best used for things like backups and older data that you don't really reference often. So there's a trade off there, you're getting a cheaper storage cost, however you are getting a retrieval fee when you pull that data out. Now there are two common storage classes that you need to know for this type of storage class, the first is Standard Infrequent Access, or Standard IA. This is where it stores your objects redundantly across multiple AZs still, and it's where you could still require some rapid access when you need it, so you would use this if you have infrequently accessed data, but you need immediate access when you do need to get it. This is good for things like backups. There's also S3 One Zone‑IA, or One Zone Infrequent Access, this is where you store objects redundantly only within a single availability zone. Now, this makes it less expensive than standard, but it's also less resilient, so there's some trade off there. In general, this is roughly 20% cheaper than the Standard IA selection. The next storage class we have are storage classes for rarely‑accessed objects, the first one is S3 Glacier Instant Retrieval. This is going to be useful for very long‑term data that's rarely accessed, but when it is accessed, you need millisecond retrieval times. The big thing here is that it allows for real‑time access if you need it. Next is S3 Glacier Flexible Retrieval, this is good where you have archives of data and you might need that data to be retrieved, but you don't need it immediately, so, if it's retrieved in minutes, then that's perfectly fine. The big thing to remember here is that the data is not available for real‑time access. And then, the last one here is S3 Glacier Deep Archive, this is useful for archiving data that is rarely needing to be accessed, if ever. The big thing to keep in mind here is this is never available for real‑time access, and it can actually take up to 48 hours to retrieve it. Now, we'll say, in order from top to bottom on how you see these, that is the order of expensive to least expensive option for these three storage classes. What that means is Instant Retrieval gives you a big cost break compared to standard storage, but it's more expensive than Flexible, and Flexible is more expensive than Deep Archive. When you choose one of these for your archive data, you really need to pay attention on retrieval times for your exam scenarios. Can you get the data within a couple of days or do you need it now, those will be the big difference makers on which one you choose. Another thing here on these three storage classes, there is a minimum duration storage time that you have to meet for your objects, otherwise you're subject to fees, which is what you don't want. For Instant Retrieval, there is a 90‑day minimum storage duration, for Flexible Retrieval, there's 90 days, and then for Deep Archive, there's a 180‑day minimum storage duration time. Now, you probably won't be tested on these exact numbers, but it's just a good thing to call out. There is a minimum duration that you have to store the objects. Now, what if you don't know what kind of access patterns your data is going to have, will it be frequent, maybe it will be infrequent, who knows? Well, S3 offers storage classes for unknown access patterns, they offer something called S3 Intelligent‑Tiering, and this is a feature that's used to optimize your storage costs for you by automatically moving your data to the most cost‑effective access tier and it offers zero‑performance impact. Now, a big thing to call out is there's no retrieval fees when you're using this feature, but you do pay for the monitoring and the autotiering of your data, so there's a trade off there. There are three primary classes with Intelligent‑Tiering, there's Frequent Access, which is the default tier, there's Infrequent Access, so anytime objects have not been accessed in 30 consecutive days, and there's Archive Instant Access. So anytime an object has not been touched for 90 consecutive days, it gets transitioned to Archive Instant Access. A big call out here, regardless of the methods that we just talked about, all of those classes, minus the standard, are valid cost‑savings approaches for your data in S3. It's really going to come down to the specific requirements in the scenario, so really take your time, read through the requirements, and try and find those small nuances or small details that make the big difference. For now, though, that's going to do it for this clip. We covered all of the primary storage classes that you need to know for the exam. Let's wrap up here. And coming up next, we're going to have some demonstrations, as well as some other important feature coverage.

Demo: Create a Bucket with S3 One Zone-IA Storage Class
All right, let's go and dive into this demonstration where we're going to test out different storage classes for our objects. First thing I have to do here under S3 is create a new bucket. I'm going to give my bucket a name. We're going to assume this is a unique name, and if it's not, we'll just fix it. And I just want to show you when you're creating your bucket, you don't set the storage class and that's because you assign storage classes to objects, not to the buckets, so please remember that for the exam. I'll create my bucket. We see it exists now so I'm going to select it, and let's work on uploading an object with a different storage class. What I'm going to do is I'm going to go to Upload and here is where we can upload an object or a file via the console. So I'm going to add a new file here, I have a blue Penguin PNG file, and you can see the type, the name, and the size. The next thing we see here is the destination so our bucket name, of course, and we can set some other stuff for the object. We can set permissions, and the big thing we want to do for this demo is look at the storage class. So if I scroll down under Properties, we can select the storage class we want this object to be assigned to. So, what I'll do here is I'm going to go and I'm going to select One‑Zone Infrequent Access. Now, I'm going to call this out. If you are following along, please don't do this, the reason being is we're going to eat the cost for a minimum storage duration time. So I'm going to select One Zone Infrequent Access, I'm going to scroll down here, skip the other settings, and let's click on Upload. Perfect. So our file succeeded, that means we got an HTTP 200. Let's close this, and now we see our object. So I'm going to select this and we can see the different details about this object. We can see the owner, which is the account name, we see the region that the object is living in, the size of it, the type, the Key of the file, and then we even see the URI and the ARN, etc. There's a lot of stuff here, which is awesome. You even get an object URL, which you can open if it was public we would be able to access. But this is not public, so it's access denied. Now, the next thing I want to show is if I scroll down here, we're going to see the Storage class, so One Zone Infrequent Access. So currently, this is only being stored in a single Availability Zone within S3. Now, if we wanted to, you can edit this and change it so we can select a different storage class if we needed to. I'm going to cancel however, and then I'm going to scroll down and I'm going to leave it as One Zone‑IA. So this is how easy it is to create a new object and then assign it its own storage class. If I go back here to my bucket, let's actually create a folder, which is a prefix, and I'm going to call this images. So we see images/, and I'm going to create this new folder. Now, what I'll do is I'll click on images/. I'm going to upload a duplicate image, so I'll add a file. I'll select another PNG one, and we can see now it has its own prefix, this time let's specify Standard storage class. So notice, three or more AZs for durability, as opposed to One Zone‑IA, which is one. So I'm going to scroll down here, I'm going to click on Upload, and there we go. Now, if I close this, we get a list of objects within the images prefix here, and let's view this. So we see the same details, however, notice the Key, the Key includes the prefix, so images/another_blue_Penguin. So I'm going to scroll down and let's just look at the storage class, it's set to Standard. Perfect. So now we have one object that's Standard and one that is One Zone IA. And again, if we wanted to, we could edit this and set it to something like Intelligent‑Tiering instead. So I can save changes, and there we go. Now, if I close now, go down here, we're going to have a different storage class for our particular object, Intelligent‑Tiering and then some other details specific to the Intelligent‑Tiering storage class. All right, that's going to do it. Let's go ahead and end this demonstration clip here. We created two different objects with two different storage classes and we even edited one. Let's go ahead and end here, and then when you're ready, I will see you in an upcoming clip.

Amazon S3 Versioning
Welcome back. Let's talk about another feature called S3 Versioning. Amazon S3 Versioning is something that you can turn on, or enable, within your S3 buckets so that you can have multiple versions of a single object within S3. Within S3, there are three bucket states specific to versioning, the first is unversioned, and this is the default state that your buckets come in when you create them. You then have versioning‑enabled, so you turn versioning on, and then you have something called versioning‑suspended. Now you might be wondering, well, why is there no versioning disabled or versioning off? Well, that's because once you enable versioning, you actually cannot set that bucket back to an unversioned state, what this means is you cannot turn it off. You can only suspend versioning, so it's always on once you turn it on, but you can suspend the actual versioning process. Let's discuss some concepts that are important for you to know about versioning. First thing, it is enabled at the bucket level, you don't enable it at the object level. This is important. So if you want versioning, remember that it's turned on at the bucket so every object within that bucket will then have versioning enabled. How it works is any writes or any deletes for any single object will be marked as a version, so if you create a new one, that's a version, if you overwrite an object, that's a new version, if you delete an object, that's another version, everything you do regarding writing and deleting will be a new version. Now, this is a general best practice and recommendation for protection against accidental deletions. So if there's any scenario talking about preventing this, you might want to think versioning. A tricky factor here is that when there are objects present before you turn versioning on, so let's say you have objects in the bucket and it's in an unversioned state, once you turn versioning on and you enable it, well, those previous objects will have a null version, so they don't have a real previous version, which makes sense. Also, versioning can be integrated with lifecycle rules, which we'll discuss later on, and it does support a feature called MFA delete, which is also something we'll cover later on. For now, just understand it integrates with other features within S3. And lastly, this is important, this is why I call it out again, you only suspend versioning, you don't disable or turn it off once it has been turned on. Now let's talk about deletion markers. Instead of really deleting objects when versioning is enabled, what happens is Amazon S3 inserts something called a delete marker. This delete marker becomes the current object version, and how it works is it essentially hides your object from being present or visible. To restore the object to a previous version after you delete it, how it works is you simply remove the deletion marker and that's what makes that object visible again. So when you turn versioning on, you're never really deleting an object, and that's why it's good for protection against accidental deletions. I think that's enough with versioning in S3. Remember the three different bucket states, remember you cannot turn it off, and remember some of the use cases and some of the functionalities. Let's go ahead and wrap this clip up here. And then, actually coming up next, we're going to have a demonstration on using versioning.

Demo: Enabling S3 Versioning and Recovering a Deleted File
All right, welcome to this demo clip, where we're going to enable S3 Versioning on our bucket and then recover a deleted file. The primary goal of this demo is to complete a couple steps. We're going to create a bucket, and then we're going to enable versioning on the bucket within the console. After we do that, we're going to show what happens when you upload an original file to that bucket post enablement of versioning. After we do that, we're going to delete the file, and we're going to show that there is a deletion marker present within the version list, so this is essentially going to hide the file from us. After we see that in the version list, we're going to delete that delete marker and that's essentially going to restore that original version of the file for us to use within S3. With that being said, let's go ahead and jump in the console now. All righty, welcome to this demonstration clip, where we're going to turn on versioning within a bucket and then test uploading new objects, deleting objects, and then recovering them. I'm in S3 here, and I'm going to create my new bucket. I'm going to copy and paste in a name here. We're going to hope this is a unique name, and if it's not, we'll just fix it. I'm going to scroll down here, and you'll notice there's a setting for Bucket Versioning during creation. Now, I want demonstrate what happens though when we upload an object before versioning is enabled, so I'm going to leave this disabled, which leaves it in an unversioned state, and I'm going to click on Create. Awesome. We have our bucket, I'm going to select it, and let's upload an object really quickly. I'm going to click on Upload. I'm going to upload a simple PNG file here. I'm going to scroll down and click on Upload. We get a successful message so let's go back, and now what I want to do is I want to upload another PNG file with versioning enabled. So I'm going to go to Properties, I'm going to find Bucket Versioning, and I'm going to go ahead and enable Bucket Versioning. So now, once we enable this, the only thing we can ever do in the future is suspend it. So I'm going to enable, I'll save changes, and that's it. We now have Versioning enabled on this bucket, as you can see right here. Now when I go back, let me upload a duplicate image. I'll add a file, select another PNG, upload it, and there we go. I'll go back to the object list. And now in order to view versions of your objects in the console, they have a little toggle here. So let's click this, and you'll notice two things, our original image here, the penguin_Blue one has a no version ID, and that's because we uploaded it before we enabled versioning. The duplicate one that we uploaded, another_penguin_Blue, was after we enabled versioning so we get a unique version ID for that particular object. Now, one thing I do want to call out here is, if, within this view, you select the object and you try and delete it, this will permanently delete that object version, so keep that in mind. Instead, what we're going to do is uncheck this. I'm going to select another_penguin_Blue, and I'm going to delete it from this view. Now you'll see we just get a delete confirmation. So when I paste and delete and I delete the object, we go back, well, we don't see it anymore. Well, that's because if we show versions, we have a delete marker in place now, so this delete marker is essentially hiding this image that we uploaded, and you'll notice it has its own version ID. So now, to recover this and see it within the list of objects here at the top level, what we need to do is delete this delete marker. So I'm going to select it, delete this, copy and paste in permanently delete, and let's delete the delete marker. We go back, I uncheck this, and there we go, we've recovered our file so that's why versioning is so important to protect from accidental deletions. Now, let's actually test the other way. If I go back in here, let's select this file, let's delete it from here, and let's permanently delete. Now, if we go back, uh‑oh, now we have no object at all, so the version doesn't matter because we permanently deleted it from this version view so it's not showing up at all. So that's how you truly permanently delete an object with versioning in place and that's not recommended, but I wanted to show you how it works. Okay, that's going to do it. I hope you picked up how to enable versioning on your buckets and how it works and some of the functionalities. Let's end this demonstration here, and I will see you in an upcoming clip.

Amazon S3 Object Lifecycles
All right, I promised you we'd talk about lifecycles and let's do that now in this clip, where we're going to have a discussion on S3 object lifecycles. You are able to manually move your objects in your buckets between the different storage classes that we looked at, or you can use lifecycle configurations, which are meant to automate the movement of your objects between the different storage tiers, which is supposed to automatically help maximize cost effectiveness. So you can do it manually or you can do it automatically, and when you do it automatically, you use something called a lifecycle rule. An S3 Lifecycle rule is simply a rule that you define to automatically manage the lifecycle of your objects in your buckets based on your own specified criteria. These lifecycle rules offer two different types of actions, there's a Transition Action and there's an Expiration Action, so let's look at some examples of what each of those could do. For a Transition Action, you could potentially say, hey, I want to transition any objects from S3 Standard to S3 Infrequent Access after 30 days post creation. Or, maybe you want to expire objects, so with an Expiration Action, you can say, hey, I want to actually expire my objects in my buckets after 366 days after creation. The expiration one could be good for potential compliance requirements, maybe you just have to hold on to log data for a year and then after a year you're free to delete it, move it, do whatever you want. What's cool is these lifecycle rules can actually be used for specific object names, they can look for paths and prefixes, and they can even look for object tags so make sure you tag your objects appropriately. Another key feature that you need to know for the exam, these lifecycle rules work for previous, or otherwise known as noncurrent, versions of your objects as well, it doesn't have to be the current version. What this means is that you can actually use these to transition your versions of objects or you can use them to expire versions of objects. We call this out in the versioning clip where we said it works with lifecycle rules and this is what it means. Let's cover three potential scenarios you might see on the exam or at least something close to these on the exam. Lifecycle rules are perfect for transitioning noncurrent versions of objects to S3 Standard Infrequent Access. So maybe you set a transition rule for after 90 days that you want them to go from standard to infrequent access to save on cost, or maybe you want to transition your compliance documents to S3 Glacier Deep Archive after a year so maybe you need immediate access to that data for 365 days. But as soon as they're 366 days old, you can transition them to Deep Archive because you don't need them immediately. And lastly, maybe you want to expire development data 30 days after the object was created because you won't need it anymore, it's just dev data. Now, all three of these, if you noticed, are cost‑savings approaches. You're transitioning to a cheaper storage class or you're actually expiring the object entirely. Now, that's going to do it. Let's go ahead and wrap this clip up on lifecycle rules for Amazon S3. Remember, there are expiration actions and there are transition actions. In addition to that, lifecycle rules do work with versioned objects. Let's go ahead and wrap things up here. And when you're ready, I will see you in a demonstration clip, where we're going to work on setting up a lifecycle rule.

Demo: Transitioning Objects Using Lifecycle Policies
Okay, welcome to this demo, where we're going to go ahead and create a new S3 bucket, and then we're going to work on transitioning objects using lifecycle policies. So in this demo, we're going to do a few things. We'll create our bucket, then we're going to upload a file with a specific prefix in place to test. After this object is uploaded, we're going to work on creating an expiration action. We're going to use it within a lifecycle rule that will be set up to expire our objects matching that prefix after a specific amount of time. Let's go and jump in the console now and get this going. Okay, I'm in my AWS sandbox. I've loaded up my S3 service in my us‑east‑1 Region. Let's get started. I'm going to create a new bucket. I'll choose General purpose and give my bucket a hopefully unique name. I'm going to scroll down here, and I'm going to click on Create bucket. Perfect. So I'm going to go ahead and navigate to my bucket and I'm going to create a new folder. which is essentially just a new prefix, what I'll do is I'll call this dev. Now, once I've created this, I'm going to accept the defaults and create my new prefix here, I'll navigate to it, and let's upload an image to this directory. So I'm going to add a file here, select my image file, and then let's upload. Perfect. So now I'll close this, we see our PNG file here under our dev prefix, and let's work on creating a new lifecycle rule. I'm going to go back to my bucket, I'm going to click on Management, and under here we see Lifecycle rules, so this is where we create a lifecycle rule to expire our object or transition our object. So I'm going to create a new one, we give it a name, we choose a rule scope, so do we want to limit the scope of the rule using a filter or apply it to all objects in the bucket? So this is going to come down to your use case and requirements. For this, we want to only expire the dev content, so what I'm going to do is I'll enter my prefix and I'm going to do an asterisk so any /dev/objects that exist, so this will cover our PNG file, would fall into this prefix match, so we're filtering for only objects that match this. We can also look for object tags. So if we tagged that image, we could look for a Key and Value for a match of a tag instead. So we can combine these filters as needed, but we're not going to do that, we're just looking for a prefix. The next thing we can do is we can say, okay, we want to specify a minimum object size that we want to actually filter for or we can specify a maximum size that we want to go ahead and filter for. Again, we're not going to choose these, but it's good to know that you can actually do this. This is extremely handy if you're going to transition to Glacier like they call out here, and this is because, sometimes, if you transition a ton of small objects to Glacier, you're actually losing money so you want to be sure that you're careful about what type of transition actions you take. Next up, we have our Lifecycle rule actions, so what kind of actions do we want to perform, do we want to move current versions between storage classes, move noncurrent versions, permanently delete noncurrent versions, so old versions, expired object delete markers and incomplete uploads, or expire current versions, which is what we want to do for this demonstration. So I'm going to select Expire current versions of objects, and now we set the number of days that we want to delete after. So I'm going to go ahead and set this to 10 because that is what was in our filter name or our rule name and, of course, unfortunately, we're not going to be able to realistically see this expire because the sandboxes don't last that long, but just trust me that this would work if you tested this on your own account. So I'll set the days after object creation to 10 and now we can review it. So we're saying, hey, Day 0 is when the object is uploaded. And then after day 10, we want to expire the object for anything that matches our filter that we put in. So for this, it's our prefix match. I'll scroll down here, I'll click on Create rule, and there we go, now we have a lifecycle rule in place for this bucket, and we can actually see all of the settings once we choose it. So we see the prefix we're looking for, we see the transition and expiration actions, etc. And that's how easy it is to create a lifecycle rule for your bucket whenever you need to. Feel free to play around with this on your own if you really want to. But for now, we're going to go ahead and end this demonstration here, and we'll move on to the next clip.

Amazon S3 Bucket Replication
In this clip, we're going to look at bucket replication. Within S3, you can enable something called bucket replication, what this does is it allows you to replicate objects from one bucket to a completely different bucket. Now to use replication though, you have to turn versioning on in both the source and the destination buckets, otherwise it won't work. Now, a key thing to call out here that you need to remember, existing objects in a bucket are not replicated automatically. So, if you have objects in the bucket that already exist, and then you turn on replication after that, those previous objects are not going to be replicated automatically by this feature. Once replication is turned on, however, all subsequent updated objects will be replicated. So if you went in and you overwrote all of those existing objects, well, then that new version of those objects will then be replicated to the destination bucket. Another key thing to remember, deleting individual versions and their delete markers are not replicated, so keep that in mind, deletions are not replicated across buckets. Now, let's talk about some concepts that are also important in addition to that overview. With S3 replication, you can actually enable Cross‑Region Replication, so using this would be perfect for true disaster‑recovery methods or maybe you have some type of compliance requirement where you need to store data in a completely different region. You can also do, of course, Same‑Region Replication. So maybe you want to replicate data for testing purposes or maybe just simplified backup strategies, well, then you can do that using Same‑Region Replication. A very important thing to remember, and a neat feature, is that buckets actually don't have to be in the same account, they can be in completely separate accounts and still use this Replication feature. However, no matter what account that these buckets live in and no matter the Regions, there are IAM permissions that are required for the S3 service in order to use replication. Now another key factor here, remember this for real‑world scenarios. S3 replication is asynchronous, it's not synchronous. This means that your objects will not be immediately available in your destination bucket, it will take a little bit of time. So keep that in mind for any exam scenarios and anytime you're implementing this in the real world, it's asynchronous, it's not immediate. Now, I think that's going to do it, that's enough coverage for bucket replication. Remember, you can do Same‑Region and Cross‑Region, it can be a completely different account. You must have versioning enabled in both buckets, and try and do your best to remember that existing objects don't get replicated automatically. With that being said, let's wrap this clip up here. And then coming up next, we're going to have a demonstration on turning this feature on.

Demo: Implement Cross-region Replication in S3
All right, welcome to this demonstration clip, where we are going to work on implementing Cross‑Region Replication in S3. Let's have a real quick architecture overview before we actually jump into the console and get this going. The first thing we're going to have to do is we're going to have to create our buckets, so our source‑bucket in us‑east‑1, and then our destination‑bucket in us‑west‑2. Now while we're doing this, when we're attempting to create our rule, we're going to see that there's a warning that says, hey, you have to enable versioning, so we're going to go ahead and follow those rules and we're going to enable versioning. After we get all of these pieces in place, we're going to create a Cross‑Region Replication rule to replicate the data from our source‑bucket, across regions into our destination‑bucket, and we'll see how that all works. So with that being said, let's end this here. Let's jump into the console, and let's get going. All righty, I'm in my S3 console here in my sandbox environment, let's go ahead and get started. First thing I need to do here is in the us‑east‑1 Region, I'm going to create a new bucket. So, let me go ahead and I'm going to select General purpose, let me copy and paste my naming template in here really quickly. I'm going to add a suffix here. We're going to hope this is unique, and we're going to come down and I'm going to click on Create bucket. Awesome. So we have our source‑bucket, let me change regions to us‑west‑2, and I'm going to create a new bucket here. So, I'll click Create bucket, General purpose, us‑west‑2. Let me paste this in and change this. I'll scroll all the way down now, and we'll create the destination‑bucket. Perfect. So now we have our two buckets here, source and destination. First thing I need to do is go to source‑bucket, and I'm going to find the Management pane. Once in here, you can see we have a Replication rules section, so this is where we're going to create our replication rule to replicate our bucket. So, I'm going to click on Create replication rule. Now, when I do this, hopefully you've been paying attention, we should get an error because, remember, we have to enable versioning on both buckets in order to use replication. So I'm going to click this, and there we go. Hey, replication requires versioning to be enabled, make sure you do that, so let's go and do that really quickly. I'll enable it on the source‑bucket. I'm going to go back to my buckets here and then do the same thing for my destination‑bucket. So, under Properties, I'm going to find Versioning, and I'm going to enable this. Awesome. So now, I'll go back. I'll select my source‑bucket again. I'm going to go to Management, and under Replication rules, let's create a new rule. So, what we can do here is we can give our rule and name, so I'm going to call this replicate‑production. And with this rule, what we're going to do is only replicate production‑tagged items. The next thing we see here is the Status, so do you want to enable or do you want to disable this rule? Well, of course, we want to enable it because we want to test it. But it's good to know you can turn on or off your replication rules whenever you need to. We then have a priority value so this is going to help resolve conflicts when an object might meet multiple rules that you have in place. It's a little out of scope for this, so we're not going to worry about this, plus we don't have more than one rule. So we're going to leave the default at zero. I'm going to go down here to the source‑bucket, so this is where we set up the source‑bucket where we want these source objects to be replicated from. Now, of course, this populates this for us because we're in the console and we selected this bucket to set up the rule. And you'll notice the source name, the region, and we can apply a rule scope. So we can do two things here, we can apply the rule to every object in the bucket or we can limit the scope using filters. And for those filters, we can do prefix matches or we can look for tags. Now to demonstrate this, I'm actually going to go ahead and use a tag filter. So I'm going to go into add tags, I'm going to put environment is production. And what we're saying here is okay, I only want to limit this rule to any objects in this bucket that have an environment Key with a Value of production for their tags, so objects that don't have this tag Key and Value will not get replicated. Next up, we have the destination, so the destination‑bucket that we want to replicate to. So you have two options, you can choose a bucket in this account, which is what we'll do, or you can specify a bucket in another account. Now, there are some settings for another account. You have to give the account ID, you have to give the bucket name, it'll fill out the region for you because S3 is global and it understands, and then you have the option to go ahead and change object ownership to the destination‑bucket owner. So this is going to say, hey, once I replicate these objects into your bucket, I want that account to actually own them, otherwise it's going to remain our object, so these are some of the options to be familiar with. Now, I'm going to choose bucket in the same account, and we're going to browse for our destination‑bucket. I'll choose destination, choose a path, and there we go. It fills out us‑west‑2 because it knows that it's over there, and now we can move down to the next required section, which is IAM rules. Remember, we talked about this in the previous clip, IAM permissions are required for this to work. So if you have a role already created, you can choose it, you can enter it manually, or you can create a new one. Now, we don't have one created so I'm going to create a new role, and we'll explore that while our objects are replicating. So I select Create new role. I'll scroll down here. We're going to skip Encryption, but it's good to know you can replicate encrypted objects with KMS. And the next thing I actually want to touch on is the Destination storage class, so we can change the storage class for replicated objects, so let's actually do that to really test this feature. So, I'll select this, and you can see we can change it to any storage class that's available. So let's say you have in the source‑bucket a bunch of production items and you're just replicating across regions for compliance and disaster recovery, so maybe you just need it in Glacier Deep Archive because it's compliance items or maybe you do need to leave it as Standard because you need it immediately. That's the benefit of this feature is you can choose what replication storage class you want. Now to test this, let's choose Intelligent‑Tiering, and I'll upload objects in the source‑bucket as Standard. So, for any objects replicated, it's going to have this storage class. Now let me go down here, and the last thing I want to touch on, this is likely out of scope for this particular exam, but I'm going to go ahead and enable Replication Time Control. The reason I'm doing this is because, if you remember, this is an asynchronous process, so it can take a lot of time, up to 15 minutes or more. So with the Replication Time Control, this is said to essentially do this within seconds of you actually putting your objects in your bucket and allowing them to replicate so the only reason I'm turning this on is to speed up this demonstration. You don't need to do this, it's just for this particular demo. Now, I'll skip the rest, I'll click on Save, and then we get a pop up here. So remember, like we mentioned in the clip talking about this, existing objects don't replicate automatically only new and updated objects. However, what's nice in the console when you do this is they say, hey, do you want to replicate your existing objects? You can do that now with a one‑time operation here, or you can say, no, don't worry about it. Well, we don't have objects so I'll say no, and I'm going to Submit. Perfect. So now, if I go to my bucket, I'll go to Management to reset the screen. We have our replication rule enabled, we have our destination‑bucket set, the scope is looking for our tag values, and we set the Transition storage class. Perfect. So let's actually test this out. I'm going to go to my source‑bucket and I'm going to upload two different objects. The first object I'm going to upload is a simple PNG file. I'm going to go down here, I'm going to open up Properties, and I'm going to tag this. So let me go down to Tags, I'll add a tag Key of environment and a tag Value of development. Awesome. So, we have environment development, this should not get picked up with our filter rule. So now, I'll scroll back up. Let me make sure it was on Standard, I believe it was, and it is. I'm going to go back down, and let's upload this object. Perfect. I'll close this. And while we wait for a second, what I'm going to do is go to buckets, I'll list the destination‑bucket, and I'm going to scroll over to a timer here, and I'm going to start this. Now I'll go back to my console, and while we're waiting to see if this even does replicate, which it really shouldn't, I'm going to explore IAM really quickly. So, I'm going to load up IAM, I'll navigate to it, and I want to find that role that was just made. So, if I sort by role name here, it actually is at the top, which is really nice, I'll select it, s3crr‑role_for_source‑bucket, so that's what this stands for. Let's look at the policy really quickly, I just want to give you a general idea of what's required for this. We have a lot of List and GetObjects, and it's pointing at both our source‑bucket and our destination‑bucket, as well as their objects within the buckets. Then we also have Replicate permissions, so Replicate, ReplicateDelete, ReplicateTags, and it's the same thing, our objects in our source and our objects in our destination, so this is that policy that was created for us by S3. Perfect. So let's go ahead, I'm going to close this. And if I refresh, we're going to see there's no object still. Let me go to my timer. It's been a little over a minute. Now, I'm going to go back to my buckets. I'm going to go to source‑bucket, and I'm going to upload another object. I'm going to upload production_penguin_Blue, so this is the same file. I'll go down to Properties. We're going to say it's in Standard storage. I'm going to find my tags. I'm going to add my tag of production, so this should be picked up by our rule. I'll go down here. I'm going to click on Upload. I'll close it. I'm going to go to my timer. I'm going to stop this. It has been two minutes. I'll reset it and let's start it again. Now I'm going to go back to my console again. I'm going to go to Buckets. Let me look at my destination, and what we'll do here is I'm going to fast forward until this is in here, and I'll keep refreshing and then we'll see how long it really took. Okay, so it's already in here. Now, I stopped the timer immediately while we were fast forwarding, and I want to show you how fast this really was with that RTC enabled. We see our production_penguin. I'm going to go to my timer, 22 seconds, so it took just under 22 seconds really because it took me a second to stop this, to replicate our object in our buckets between Regions, so that's how fast the process can be with certain features enabled. Now, if I select this, we can open it, and you'll notice it's just a PNG file. So now, let's look at the Properties. We're in our destination‑bucket where it was replicated, us‑west‑2. We see the same Key. Let's look at the storage that this is in, Intelligent‑Tiering. Perfect. So the source‑bucket, if you remember if I go back, was a Standard storage class. So we have our source, we find our production ping one, you can see the storage class is Standard. But in destination, remember with our replication rule, we changed it to Intelligent‑Tiering so it worked. Now, the last thing I want to see here is in our destination‑bucket, did our tags come over as well? So if I select this PNG file and I go all the way to the bottom, we see our tags were also replicated, which was a setting, so environment production. This is perfect. Okay, and that's going to do it. Hopefully, you saw how easy it is to enable Cross‑Region Replication between a source‑bucket and a destination‑bucket. Please remember some of the settings that are required, like versioning and the IAM roles, and also remember this is an asynchronous process. Also remember that we can implement filters so we don't have to apply the rule to every single object in the bucket. For our demo, we actually used key‑value pairs with our tags to filter for objects. Now let's go ahead and wrap this demonstration up, and I will see you in an upcoming clip.

Module Summary and Exam Tips
Okay, way to hang in there once again. You got through this module, so let's have a quick summary and some exam tips. First up, S3. Remember, this is an object‑based storage service that allows you to upload an unlimited amount of files and data. It's built for up to 99.99% service availability, depending on the S3 tier that you choose. It's also designed for 11 nines of durability for standard S3. When you upload files and objects, they can range from 0 bytes to 5 TB for a single object size. Also remember, this is not suitable for an operating system, and you should not be trying to run databases on S3. Next up, please understand how to identify object Keys. Remember these different portions, we have the URI, you have the bucket name, you have your prefixes, and then you have the actual object name. Also remember, for prefixes, the forward slashes matter, they are part of the prefix. And with that, here are some possible prefix matches for the object on the screen. We have an *.json, we have /dev/*, and then we have /dev/2024/*. These all would work to search and find this single test_data.json. Now, I'm including these tables coming up for easy reference just so you can use them in the future, but I'm not going to walk through them all in detail here and you'll see why, they're extremely lengthy. So, we put this together for you just to make it easier to kind of see the differences between the different classes, including things like durability, availability, etc. So go ahead, feel free to review these on your own time. Again, I just put them in here for easy reference in the future. Now let's review tagging and metadata for your objects. Remember that you can easily tag, and you should tag your different objects using key‑value pairs of useful information. With each object, there's also metadata so this is just data about the object that you're storing, so you can have things like content‑type, last‑modified dates, etc. You also need to remember the versioning bucket states. We have unversioned, which is default, there's versioning‑enabled, and then there's versioning‑suspended. Again, remember, you cannot disable versioning once you enable it, you can only suspend it. In addition to that, any objects present before you turn versioning on receive a null version, they don't have a real version. Continuing on the versioning train here, remember, too, that versioning can be integrated with lifecycle rules, so you can move things between classes or expire them even if they're versioned. Also, versioning can be set up to require and support multi‑factor authentication for deletion and use. Speaking of deletion markers, remember, S3 actually inserts a delete marker, which then becomes the object version when you're trying to delete an object. To restore that object to the previous version, you're essentially deleting the delete marker, so you're removing it and making the object visible. The next feature you need to know about multipart uploads. You use these to upload large objects in small, more manageable parts with automatic retries. Remember, if your object is bigger than 5 GB, AWS recommends you use this. It also recommends you use it with any poor network connections. Moving on to the next topic, lifecycle rules. Remember, these are useful to automate the movement of your objects between different storage tiers or classes or you can even expire your objects, that means there is a Transition Action and an Exploration Action, so what this means is they can work on current versions or previous versions. Some exam scenario use cases you might see here, maybe you need to transition noncurrent versions of your objects to Standard Inrequent Access or you need to transition compliance documents and logs to Glacier Deep Archive after a year, or maybe you can just expire development data after 30 days post creation. These are all valid use cases for lifecycle rules. And the last feature we want to review here, replication. Remember, you can replicate objects from one bucket to another bucket, those buckets can be the same or in a completely different Region. Also, any objects that already exist in your buckets before you turn replication on will not be replicated automatically. And speaking of that, also remember you need to turn on versioning for this to work. Lastly, remember that delete markers are not replicated by default, so they're not going to automatically get replicated from the source to the destination. Now, that's going to do it for this module, way to hang in there. We talked about a lot of different S3 features. Let's go ahead and wrap things up here, and then, when you're ready, we can move on to the next module.

S3: Important Features
Performing Batch Operations with Amazon S3
Let's go ahead and get started with this next module, important features of Amazon S3. This first clip within the module is regarding performing batch operations within S3. What is S3 batch operations? Batch operations within S3 is a feature that allows you to perform a single operation on a large scale set of S3 objects at a single time. Batch operations works by executing things that are called jobs. Now these jobs are going to contain lists of objects in the buckets, the different actions you want to actually perform, as well as some other optional parameters that you could pass in. A single job within batch operations can actually perform an operation on up to billions of objects that are in S3, so it's very powerful. When we are discussing batch operations, we need to know what it manages. When you use batch operations, this feature actually tracks your job progress for you. Remember, the job contains all the necessary information for your specified operations. And using batch operations is going to actually track the progress of that job and allow you to view the progress of that job. It also handles retries for you. So it's going to retry up to a certain amount of threshold before it will actually completely fail the job. It also provides notifications of job progress. So if you want, you can actually get notified based on certain points or checkpoints within your actual job and the different tasks that are being performed. And then lastly, it's going to report after your job is actually complete. Even if the job is not completely successful, as long as batch operations can at least successfully complete one task within the job, then you can get a completion report after that task. What this report will contain is things like the job configuration information and then the status and information for the different tasks, as well as the objects that were affected. Now this is all fine and dandy, but what would you use S3 batch operations for? Well, here are four use cases for batch operations. You can use it to copy or move vast amounts of data, we're talking petabytes or exabytes of data, between different S3 buckets or even between different storage classes. You can say, hey, I actually need to update a lot of tags for all of my objects in my buckets, or I need to add them in the first place, well, you can do that with this as well. You can also easily integrate with other analytics services in AWS to transform your data formats of your objects so that it's easier to process for your analysis. And then finally, you can even encrypt thousands or more of unencrypted objects that are currently in your S3 buckets. So, maybe you have a compliance requirement where your objects are not encrypted, you turn on default encryption in the bucket, and then you need to go ahead and make sure you update those existing unencrypted objects, well, this is a perfect use case for batch operations. One last thing here before we move on, a quick pro tip, you can leverage a combination of S3 inventory and S3 select to gather and filter your list of objects that are passed into your jobs. I think that's enough for batch operations. Just try to remember some of the use cases and how it works with your jobs. Let's wrap things up. And then when you're ready, we can move on.

Filter Objects Using S3 Select and S3 Glacier Select
In this clip, we're going to discuss filtering objects using two features called S3 Select and S3 Glacier Select. To start things off, a quick blurb from the AWS announcements. This actually happened just this year. AWS has recently decided that they're going to close new customer access to these two service features. So as long as this is appearing on the exam, we're going to cover it within this prep. Moving on, let's get down to business, S3 Select. This is a feature allowing you to use SQL statements to filter the contents of an S3 object and retrieve only the subset data that you need. Now what does S3 Select offer? The big thing here is that it works via server‑side filtering. What that means is that it works on the Amazon S3 side. So you don't have to spend any compute cycles in order to do the filtering that you're trying to perform. One of the downsides of this feature is that you can only query one object at a time. So it's not necessarily great for large amounts of queries. The feature does however support many different formats. So it supports CSV, it supports JSON, Apache Parquet formatting, which is very popular for analytics. And it will even support GZip and BZIP2 compressed objects as long as the objects that are compressed within those archives are CSV or JSON format. And last thing here, an exam tip. This will help reduce the amount of data that S3 actually transfers whenever you're trying to retrieve information. Because of this, that means it reduces the overall cost and the overall latency in order for you to retrieve your data. That's probably one of the biggest pluses that this offers. The other service is S3 Glacier Select. Now Glacier Select is going to offer the same exact functionality of S3 Select. The only difference is that it's for objects stored within S3 Glacier storage class. That's going to be the biggest difference between the two from a functionality standpoint, they serve the same purpose. Let's actually take a look at an architecture example of using S3 Select. Let's just assume we have a basic employee_data.json file living within our bucket. So in this JSON file, we have the ID numbers of our employees, the names of our employees, and their age. Well, if we wanted to retrieve a subset of that data, let's say we want to look for the name and the age of an employee wherever the age is greater than 25. So based on this select statement, which you can notice is a SQL statement, we're selecting the name and the age from our object, which we've titled s. And we're looking for any data where our age within that object is greater than 25. Once we actually execute this query using S3 Select, we're going to go ahead and get a filtered result as you can see here, which is the highlighted portion. So for this, we're retrieving one portion of a subset of an object compared to the four objects that live within the JSON file. So we don't have to filter through the other three that are irrelevant; our query does that for us. Now one quick thing here before we wrap up, an exam pro tip. S3 Select and Glacier Select are best for small, simple queries. If you need large‑scale data retrieval, that is best done using other AWS services and other AWS tools. Remember that S3 Select can only work on a single object at a time, and that's why it's not very efficient for large‑scale processing. Do your best to really remember that. And with that out of the way, let's go ahead and wrap this clip up, and I'll see you in an upcoming clip.

Demo: Filtering Objects Using S3 Select
Hello and welcome to this demonstration clip where we're going to work on using S3 Select to query an object in our bucket. First thing I want to do here is overview what I've already created. So I'm going to go to my buckets, and you'll see I have a filter‑demo bucket. So this is the bucket we're going to use to host our object. Now what I'll do here is I'm going to go ahead and drag and drop my file in. And you'll notice it's a JSON file, and we called it employees_data. Now we'll look at this file here in a moment. I just want you to know it's a JSON‑formatted object. So I'm going to click on Upload, we're going to go back to our bucket, and we see our object here. Perfect. Now let's jump over into my IDE really quickly, and this is that file. So this is that employees_data.json file. Notice we have an employees list here. And within this list, we have a bunch of objects with different randomized employee names, IDs, ages, and even locations. Now I'm going to skip all the way to the bottom here. And why I did that is I just wanted to show you there are 100 employees within this JSON document. So employee number 100 and it starts at number 1. So we're going to use this to run some SQL commands in order to test S3 Select. So let me go back to my console. We have our document, and now I need to select my document. I'm going to go to Actions, Query with S3 Select. Now remember, for new accounts and new customers, they've decided to pull this feature out. Now this is an old account, so we've had access to this since this account has been created so I have this option. You might not have this option. I just want to call that out. Second thing, remember, this only works on individual objects. You can't run it on multiple objects at one time. And that, again, is why it's not necessarily the best option to parse multiple objects in a lot of data. So with those two things out of the way, I'm going to click on Query with S3 Select, and let's look at these settings. At the top here, we have input settings. You can see the path to the file, the current size, and then we can select what format it's in. So notice the three different formats that are supported, which we called out in an earlier clip. This is JSON, so I'll leave it there. And once we do that, we see the content type. This is important for this demo. There's lines, and then there's document. I'm going to select document because of this callout here. Our document has objects that span multiple lines. If I go back and look, one object here is spanning many different lines. If we had it all on one line as one single input object, we could do this. But since we don't, I had to choose Document. Otherwise, this will break. The next thing we can do is we can select the compression type. So remember the two supported options, GZIP and BZIP2. Ours is not compressed, so I'm not going to worry about that. And then we get down to output settings. So we can change these. We could say, hey, I want a CSV, tab‑delimited or comma‑delimited,etc., or just output it in JSON. So I'll leave JSON for now, and let's move down to the query section. This is the fun part. This is where we get to enter our SQL queries to query this object. So what I'll do here is I'm going to copy and paste several commands in one at a time. Now this file with these commands will be made available as a module asset, so feel free to use these whenever you want, as well as the JSON document itself. So I'm going to go to commands, and let's just get all employees from this file. So I'll copy this, go back, and then let's execute this. So I'm running the query. And on the bottom, you see the query results. It grabbed 100 records, which is correct because we have 100 employees, and it took just over one full second. So now we can look at the return data, and I'm not going to look at all of it. But you can see, it's in JSON format. I'm going to go back to my document here, and let's filter and say, hey, I want to see which employees live in New York. I'm going to copy. I'll replace and then run it. And we get one record. So only one employee out of that entire document is based in New York, and it's employee ID E001. So there you go. That's how you can execute simple SQL statements using S3 Select to query your data within your object. There are tons of different combinations you can play with, and you can even use default SQL notation where you're limiting and selecting etc. For instance, maybe you want to get a count of all employees where their age is over 25, and we say, hey, that results too big. I want to knock down my processing time. Well, maybe you limit it to 10 results instead of 20 or 30 or whatever it may be. And that's going to do it. Again, I'll make these resources available for you so you can play around with this as you want. Feel free to go ahead, copy and paste, manipulate. Do whatever you need. But for now, we're going to end this demonstration. Hopefully you saw how easy it is to use the S3 Select feature. We'll go ahead and wrap up here and then move on when you're ready.

Analyzing Data Using S3 Storage Lens
Okay, let's get talking about analyzing data using S3 storage lines. Amazon S3 Storage Lens is a cloud storage analytics feature in AWS that is meant to provide you organization‑wide visibility into your object's storage and the activity for those objects. But what does the service offer besides that? Well, it offers the ability to use metrics to actually generate summary insights for your entire storage within S3. A cool feature is that it can aggregate the different metrics and then display the information within a centralized dashboard. Even though your buckets are regional and maybe they're spread around the globe, your metrics and all of that information can be aggregated into a central place. In addition to this, you can easily export your metrics in CSV or Parquet formats to another S3 bucket. The reason this is important is because this can easily allow you to perform analytics using other analytic services in AWS or maybe you have your own. CSV and Parquet formats are very popular in the analytics world. And then lastly, it's capable of publishing usage of objects and the different activity metrics to a service called Amazon CloudWatch. Now CloudWatch is covered later on in the prep. But at a high level, this is a logging and metrics service within the AWS cloud. Let's talk about three use cases for Storage Lens. It allows you to put best practices into place for data protection within your organization. So maybe you can run a quick report and you can see, hey, we don't have the correct encryption in place or maybe we don't have the correct bucket policies in place for our objects in our buckets. Due to its simplicity and centralization, it allows you to look for different ways where you can become more cost‑effective with your stored objects. Maybe you realize that, hey, my objects are not really accessed quite a lot, so I'm going to move them to infrequent access so I can get a cost break. And then lastly, you can find and you can report on any potential data anomalies that are occurring within your bucket. So maybe you're getting attacked because you have public data and you need to make sure that those are authentic requests. Moving on, we need to talk about category tiers. So there are different metric category tiers that you need to know or at least be familiar with before you use this service. The first is free. Now these are going to include summaries, cost optimization recommendations, data protections, access management performance, and event metrics. There's also advanced, which is not free. Advanced includes the free metrics in addition to the ones you see here, so advanced data protection and cost optimization, detailed status codes, etc. Let's explore the metrics really quickly. There are eight metrics that are defined for this service. The first is a summary. So this gives you a general insight about your S3 storage. So think things like the total amount of bytes for the storage. You then have cost optimization. This helps you manage and optimize your storage costs. So again, going back to that infrequent access example, maybe you realize that your data is not being accessed as often as you thought, so you can change the storage class. We then have data protection. So this gives you insights for what type of data protection is in place. For instance, are your objects encrypted properly? The fourth is access management. This is going to give you insights for different object ownerships that are in place. In other words, who owns the object and who has access to the object? Let's explore the next four. First here is an event metric. These are insights for something known as an S3 Event Notification. Now we'll discuss Event Notifications in this particular module. So don't worry if you don't know what those are right now. Then we have performance. So this is going to be insights for a feature called S3 Transfer Acceleration. This is another feature that we're going to cover in this module as well. Then there's activity, and this is going to fall under advanced. These are details about how your storage is requested and how the objects are being uploaded to the storage. And the last one here that we wanted to cover are the status codes. Again, this is advanced, and this is going to be a detailed metric that's going to give you very fine‑grained details about the HTTP status codes for your different stored objects. Now that's going to do it for this clip on Storage Lens. Just do your best to remember some of those use cases and really just what this service provides. We'll go ahead and end here. And when you're ready, I will see you in the next one.

Receiving Event Notifications from Amazon S3
Okay, let's talk about receiving event notifications from Amazon S3. S3 Event Notifications is a feature that allows you to receive notifications when certain events happen within your S3 buckets. These are crucial for event‑driven architecture. So you need to know some of the possible events that these offer. Speaking of that, let's look at a curated list. Now these are not going to be the entire list as there are several more than what I'm going to show you. But what I've picked out is some of the more popular ones that I'm very certain you need to know for the exam. The first is a new object created event. So anytime a brand new object is uploaded and put into your bucket, this event is triggered. You can also look for object removal so a deletion of an object. We talked about cross‑region and same‑region replication before, and that also is an event. So if an object is replicated, you can look for that event. You can also look for restored objects. So maybe you have a reduced redundancy storage class selected and then maybe one of your objects fails or goes away and it's later restored, you can go ahead and look for that as well. We can also look for lifecycle events. So remember those lifecycle rules. There's transition actions and expiration actions. You can look for both types of actions, so if your object moves between storage classes or maybe you expire an object. And the last one here, object tagging events. So whenever a tag is added to an object or maybe it's updated on an object. These are six very common scenarios you need to know. Now in addition to the actual events themselves or the triggers of the events, you also need to know the destinations that you can send the events to. The first is known as an SNS Topic. So this is an application integration service in AWS, and it's meant for push‑based messaging. Now if you're not familiar with SNS, don't worry about that. We're going to cover that in a different portion of this exam prep. For now, just understand that it can integrate with this. One thing to keep in mind is that it requires something called a topic access policy with the proper permissions. You can also integrate with SQS queues, which this is a poll‑based or a pull‑based messaging service. It's a messaging queue in AWS. And this also requires queue access policies to be in place. Thirdly, AWS Lambda Functions. This is the serverless platform for serverless functions within AWS. It allows you to execute code on demand or, in other words, event driven, which is very important, and then you can execute workloads based on your events. This, just like the other ones, requires a resource policy to allow invocation. And the last destination here that is important to know, EventBridge Buses. This service is useful for tons and tons of serverless, event‑driven applications and architectures in the cloud. Using this EventBridge service allows for more complex filtering and forwarding of your events. Really do your best to remember these four services as far as event notifications go. Again, we're going to cover them in different portions of this exam prep but you do need to know that they are destinations right now. Let's talk about some use case examples for event notifications. Probably the most popular example that's used today and still comes up on the exam is something like image processing. For example, maybe you have a picture that is uploaded to an S3 bucket from a client. Once that client uploads that picture, it can trigger a Lambda function based on the new object created event in S3, and you can use that lambda function to resize the image to a thumbnail and then upload that thumbnail to a brand new bucket. This is all done in an event‑driven manner, and it's all started or kicked off because of the event notification in S3. You can also use it for compliance monitoring. So let's say you have an object that gets uploaded to your bucket and you need to trigger a workflow to actually monitor that object for any sensitive data and really make sure compliance falls in line with your regulations. Again, using event notifications allows you to do that on demand and in almost real time. And speaking of almost real time, you need to remember, S3 event notifications are not classified as a real‑time feature. This is a major thing to keep in mind. They are not real time. In fact, the events can actually take up to several minutes to actually be delivered to the destination. Now with that being said, I think that's enough on S3 event notifications. Do your best to remember the curated list of events we covered, including things like new object creation, object deletion, and replication. And also remember some of those use cases that we just looked into as well. And with that, let's go ahead and end this clip. And then coming up shortly, we're going to have a demonstration where we use an S3 event notification.

Demo: Trigger a Lambda Function Using Event Notifications
Hello and welcome to this demonstration clip where we're going to trigger a Lambda Function using an S3 Event Notification. In this demo, we're going to perform a common exam scenario. We're going to resize an image that's uploaded to a source bucket, and then we're going to store that resized image in a destination bucket. Let's look at the architecture flow really quickly before jumping into the console. The first thing we'll do is we're going to go into an existing source bucket, and we're going to create a filter or an event notification for ObjectCreatedPut events. Now we're going to look for specifically .png files only within the bucket. So once we configure that event notification and we upload a PNG file, that event notification is going to actually trigger a Lambda Function that we have already set up. I will review this Lambda Function at a high level because it's kind of out of scope for this course. But I will provide you as much documentation and the resources I used. So if you want, you can do your best to follow along. Just please do your best to remember, Lambda Functions are out of scope for this module and I'm not going to cover it too in depth. However, once we trigger this notification, the destination, which will be the function, will receive details contained within that event. What I'll do is I'll print that event in a log file, and we'll also perform our business logic. That logic is going to go ahead, and it's going to get that source object. And then it's going to do its best to resize that image by 50% and store that new resized image in our different target bucket. So without further ado, let's go ahead and jump into the console now. Okay, welcome to the demonstration. I'm logged into my AWS sandbox here in the us‑east‑1 region. Let's go ahead and get started. Before we begin. I want to review some of the existing resources I created to speed this demo up. I've already created our source and destination bucket. So this testing‑demo‑bucket will be our source bucket, and then our testing‑destination‑bucket will be the destination bucket. The next thing I want to show you is my Lambda Function resources. So I have a resizeImageFunction here, and this is the serverless function that's going to be performing our business logic or, in other words, resizing our image. So really quickly, I will review this at a high level, but I want you to understand this is technically out of scope for this course, so I'm not going to cover this too in depth. I will, however, provide as much code and resources and documentation as possible in case you do want to do this on your own. So we have my function here. It's a Python runtime. And if we look at the code here, it's pretty simple. We're importing some libraries, specifically pillow. We're creating a boto3 client, and then we're running our logic here. First thing I'm doing is printing an event so we can see what event was received. And then we're actually getting into the logic. So we're looking at the source bucket. We're looking at the key of the object, so this should be the full path with the object name. We're setting a destination bucket via an environment variable, which is going to be our destination bucket we looked at earlier. And then we're downloading that image from the source bucket. We're resizing it by 50% regarding the width and the height. And then we're going to go ahead, create a new key with a resized in front, and we're going to upload it to our destination bucket. So we're taking that source image based on the event, we're resizing it by 50% and then reuploading it to a destination bucket. Now in order to perform this, I did have to use what is called a Lambda layer, and you can see that down here. And this is just a public layer that supports the pillow library for our function use. Again, I will include as much documentation for this as possible. If I scroll up, the next thing I want to show you under configuration is permissions. So right now I have a service role attached to this Lambda function. Remember, you can use a service role to provide AWS services IAM permissions. And if we look at the resource summary, we're going to see that this is allowing CloudWatch logs permissions. We're allowed to create a log group, and we can put log streams. So this is where our log files will live, and we'll look at these later. There's also permissions for S3. So right now for the testing bucket, we're allowing all S3 actions, and we're allowing it to get objects from any S3 bucket. Now, in real‑world scenarios, you would want to lock this down. But for sake of simplicity, I just said it can get an object from any bucket. Next, I want to show you environment variables. So I set that destination bucket environment variable to our destination bucket. So this is how this all kind of plays a part at a very high level. Now let me go back to code here, and then let me go to Buckets. Let's go ahead and get started. I'm going to go to my testing‑demo‑bucket, I'm going to go to Properties, and I'm going to find Event Notifications. Here we go here. Now real quick, I'll call out Amazon EventBridge is in its own separate pane within this. And remember, this is one of those destinations for event notifications. I say that because this won't show up when we're configuring it within the GUI. So I'm going to click on Create, and then let me scroll up here because the UI is bugged, and let me give my event a name. I'll call it pngFileUploaded, and then we can look for an optional prefix. So you can lock this down to specific prefixes only. We're not going to do that. But I do want to lock it down to a specific suffix. So I'm going to put in .png. And now, what this is saying is hey, for any prefix or any file uploaded to this bucket that ends in .png, we want to trigger this event notification. Now to combine that, we also need to choose the event type. So, what I'll do is I'll scroll through here and just notice all of the different event types that you can choose from. There are tons and tons that are possible. So this allows for a lot of flexibility. There's lifecycle, replication, object tagging, etc. Now for this demo, I'm going to choose put, so we're looking for object created put event types. Next, I'll scroll down here, and we can select our destination. So remember the four destinations. We just covered EventBridge before this. And then we have our other three. We have our Lambda, we can send a message to an SNS topic, and we can send a message to an SQS message queue. For this, I'll choose Lambda. And then in this drop‑down, I'm going to choose my Resize function. I'm going to click on Save changes. And there we go. We now have our event notification created, PNG file uploaded for put event types, all .png files, and we're going to trigger our Lambda Function. Now, if I go back to my Lambda and I refresh this console, we're going to see that S3 trigger is now in place. There we go. So if I select this, it takes us to a shortcut where it's saying, okay, here's the trigger for your function. Let me look at details. It's saying this bucket for any put object created events for any .png suffix file. Now I will say this. What this did, since we're in the console, is create a permission in the background to allow us to do this. So if I go to Permissions and I go all the way down, we're going to see we have a resource‑based policy here. So this was created for us by AWS on the back end. What it's saying is, hey, the S3 principal is allowed to invoke this Lambda Function as long as these conditions are met. So it's in the source account and our source bucket. I point this out because if you do this via Infrastructure as Code, you will have to create this separately. This is its own resource as far as things like Terraform go. So this permission is in place. Let me close it. I'll go back here to Monitor. I'm going to click on View CloudWatch logs. And this is going to allow us to see our log streams once they're generated. So there should be an error right now because there's no log stream yet, which is fine. But right now I'm going to go do my S3 bucket, I'm going to go to my objects list, and let's upload an object and test this out. So I'm putting resize‑test.png. It's a PNG image type, and it's 1.1 MB. So I'm going to go down. Click on Upload. I'll close this, and there we go. We now have our object. So now if I go to CloudWatch here, what I'll do is I'm going to go ahead and refresh until we see our log streams. Let me go to Log groups, I'll go back into this, and there we go. We now have a log entry. So I'm going to select this log stream, and let's look at the messages. After the initial init and start, we see this first line here, which is the event. So this is the event that was printed via that code. You can see a lot of information. The event source is S3, the region, the time the event name, which is our created put event, along with a bunch of other stuff. Feel free to look through this on your own. However, we already did some of the heavy lifting. I look for the bucket and the object. So if I go down here, you can see we successfully processed the .png file and uploaded it as resized‑resize‑test.png to our destination bucket. So let's actually check this out. I'll go to S3, go to Buckets, go to destination bucket, and check it out. Our size is now drastically different because we cut the width and the height in half using a Lambda Function, all triggered off an event notification. Perfect. So this is working. Now to prove this will only look for PNG files, what I can do is go back and let me upload the same file but as a JPG version. So I'll drag and drop this. You see it's a .jpg. I'll upload. I close out. And if I go to CloudWatch here, we're not going to get any other log stream entries because it's not capturing that event because it's not a .png. So this only captured our one PNG event when we created that new object in our bucket. So our filter is working. We're only notifying based on PNG uploads, which is perfect. All right, with that out of the way, hopefully, you can see how you can leverage an SRE event to send an event to different destinations. In this example, we triggered a Lambda Function and resized an image into a much smaller size. Let's go ahead and wrap this demonstration up, and I'll see you in an upcoming clip.

Faster Content Transfer with S3 Transfer Acceleration
All right, welcome back. In this clip, we're going to look at S3 Transfer Acceleration. Amazon S3 Transfer Acceleration is a bucket feature that speeds up data uploads and downloads to and from your S3 buckets. This service works by optimizing transfer speeds around the world by using customer‑facing, globally distributed edge locations. So these are also called point of presence, and it's essentially a network point of presence that is extremely close to the customer. Now you might be wondering, well, why would I even want to use this feature? Well, you might want to upload all objects to a central bucket from customers that are spread around the globe. So maybe you have a bucket in us‑east‑1, but maybe you have customers in eu‑west‑1 and maybe even the a‑pac region. All of those users are not going to have the same performance as someone that was based in the US. So this is a good use case. It's also useful if you need to transfer terabytes of data on a regular schedule from around the world. And then lastly, if you're not able to use all of your available bandwidth over the internet, then this is perfect for you to turn on and test out. However, with that being said, you need to know that Transfer Acceleration might not always result in a performance gain. Now this might not come up on the exam specifically this point. But in the real world, this is very good to know. Now we'll actually demonstrate this coming up next in a demonstration clip where I'll show you what I mean. For now though, let's go ahead and wrap this clip up here. nd then when you're ready, we'll jump into the demo.

Demo: Testing Amazon S3 Transfer Acceleration
All right, welcome to this very short demonstration clip where I'm going to show you the differences Transfer Acceleration can make. I've loaded up this URL, and I'll include this URL in the module assets. And I'm going to click on the shortcut here, a speed comparison tool. So on this, what this is going to do is compare speeds to different regions, and it's going to use both direct uploads and accelerated transfer upload speeds to show you the difference. Now what I'll do here is I'm going to wait until these are all complete. I'll cut forward once it's done, and we will compare the speeds between the different regions using and not using this feature. All righty. So I went ahead and let this finish. We fast‑forwarded into the future. Let's go and review some of these upload speeds. Now, first thing I want to call out this took several minutes. It's not a very fast process. So if you're going to do this, make sure you have time to do it. First region we see here is the primary region, us‑east‑1. You'll see that using Transfer Acceleration was 6% faster. But if we scroll down, notice some of the red marks. Remember, in the clip regarding this feature, I told you that this might not be the best feature for you to use based on your location. You'll notice, even in the US, which is where I'm located, some of these regions are actually significantly slower than if I just did a direct upload. However, sporadically around the globe, like Mumbai and Singapore, my upload speeds were faster using the feature. So this is the perfect example of what I meant when I said it's not the best thing to always use. Sometimes it's faster; sometimes it's slower. Now feel free to go ahead and use this website and test your own results. I'd be curious to see what you get. But for now, we're going to end this short demonstration here, and then we'll move on.

Offloading Costs Using S3 Requester Pays
All right, let's talk about offloading costs using S3 Requester Pays. I think it's safe to assume that you understand by now that the typical default behavior for S3 is that the bucket owner would pay the costs associated with the following, the storage volume of all of the objects, the data transfer out of S3 for any requests. These are commonly paid for by the owner of the bucket who was storing the objects. Requester Pays, however, is a feature that allows you to offload the cost of requests to the originating caller. So we're talking the transfer and the storage of the objects Now, one thing to call out here, the requester cannot be anonymous. So what that means is that they have to be authenticated via IAM. And really what that comes down to in the grand scheme of things is that they have to have an AWS account. It doesn't matter what user puts the objects there or pulls the object out of your bucket, they have to be authenticated via AWS. Another important thing to call out here, you actually configure the entire bucket to be Requester Pays. So you don't do it per object; you do it per bucket. Once you configure your bucket to be a Requester Pays bucket, those requests that are made must either include the x‑amz‑request‑payer header, or they can add the request payer parameter within their REST request. These are the two possible options when they're making a call to a Requester Pays bucket. Now that's going to do it. This is a very short lesson. The key thing to remember from this clip. If you need to offload costs associated with objects in your buckets, you might want to look at Requester Pays. Let's go ahead and end this here. And when you're ready, we can move on.

Serving Websites from S3 with Website Endpoints
Okay, the next feature we need to talk about is serving websites from S3 using S3 Website Endpoints. One of the key features and benefits of using S3 is that you can actually use it to host static websites at a very low cost. Now when we say static websites, you need to know client‑side scripts are okay, so things like JavaScript. But it means that no server side processing is allowed, so no server‑side rendering. So if there's any requirements in a scenario on the exam, talking about server‑side rendering, immediately you have to rule Amazon S3 out. With this in mind, any dynamic websites, especially things that might require things like database connections, cannot be hosted on S3. It has to be a simple static site, so things like HTML and JavaScript. Last thing here, you have to allow public read access for this feature to work. If you want to host a static site, public read access needs to be in place. One of the biggest benefits of using S3 to host your website is that it can automatically scale for you to handle the demand for your website. So you don't need to worry about setting up the proper scaling metrics or provisioning things. S3 does all of this for you moving on. Let's look at some website endpoint examples. When you configure your bucket as a static website in S3, the website is available at the region‑specific website endpoint of the bucket. Now that syntax is going to vary based on the region your bucket is in, and that's only because of how AWS put this into place. It's a little annoying, but you need to know them. The first way is with a dash. So we're going to call this a dash example. We'll use this sample endpoint to break it down. First thing we have here is the bucket name. So in this case, we have our Pluralsight bucket. In addition to the bucket name, you'll always have that s3‑website after that next period. Now with that, we have the region in here with dashes. So please understand how this works. This is obviously in the us‑east‑1 region, and pluralsight is the bucket name. Now if we look at the dot example of the same type of endpoint, the only difference here is that we have a dot instead of that dash. So in theory, this is a completely different subdomain because we have s3website.us‑east‑1. In the previous example, it was s3website‑us‑east‑1. Now, more than likely you won't be tested on the differences here, but you need to know what the syntax looks like in the first place when you're spinning this up. Now with those examples out of the way, let's end this clip here. A few key things to take away. Remember, you can host static websites on your S3 buckets. And what that means is that there's no server‑side rendering being done. So you can do things like client‑side scripts via JavaScript, but you can't do anything on the server side. Two of the big benefits here is that it scales automatically, and it could be potentially very low cost. Now with that out of the way, let's end this clip here. And when you're ready, I will see you in an upcoming clip.

Demo: Host a Website in Amazon S3
All right, in this demonstration clip, we're going to work on hosting our very own static website using Amazon S3. Before we dive into the console, let's have a very quick architecture overview on what this will look like at a high level. What we're going to do is we're going to create a brand new website bucket in S3. After we do that, we're going to work on enabling website hosting. This is going to entail several different steps. We're going to enable it to host a website. We have to allow public access via the bucket policy and then we have to upload some default files. So for this, we're going to upload an index.html file as the primary default root object. We're also going to upload a JavaScript file to demonstrate how client‑side scripts are allowed using this feature. Now after this is all in place, we're going to run a simple GET command to the website endpoint, and this is going to return us a very, very simple static website via that HTML, and we're going to execute some of that JavaScript. Now I am not a web developer, so please bear with me on my very simple HTML skills that we're going to use in this. I just want to show you how this works. So without further ado, let's jump into S3 now. All righty. Let's dive into this console demo. I've loaded up Amazon S3 in my sandbox environment. First thing I have to do, of course, is create a bucket. So I'm going to click on Create bucket. I'm going to give my general purpose bucket a name, and I'm going to scroll down here. The first thing I need to do before I create this is I need to uncheck Block all public access. Remember, it's a requirement to host a website in S3 to allow public access. So I'm going to uncheck this. I'm going to acknowledge that this is a terrible idea usually. And I'm going to scroll down here, and I'm going to click on Create bucket. All righty. So our bucket is created. Let me go ahead and view the details here. And the very next thing I have to do is I need to go to Permissions, and I need to update my bucket policy to allow public access. So what I'll do here is copy and paste a simple bucket policy in. I'm going to copy this ARN here, replace this. And all this is doing is saying, hey, we're allowing anonymous access to get an object or objects within our bucket here. So this is exactly what we need. I'm going to click on Save, and there we go. We've now set up public permissions for our bucket objects. The next step I want to do is I want to upload my objects. So I'm going to click on Upload, and I'm going to drag and drop a couple of files in here. You'll notice two things. We're creating our index.html file, and we will review this here in a moment. And then I'm also uploading a random string generator JavaScript file into a scripts folder or prefix. So once I click on Upload, I'm going to go back to my bucket, and we have our files organized. So I have my index.html file. I have my scripts prefixed with my JavaScript in there. So now, really quickly before we actually do this, let me show you what these files do. When I go over to my IDE, this is the index.html file. So it's a very, very simple HTML file. Please, again, don't make fun of me. I'm not a web developer. However, what we're doing is we're displaying a few different elements where we have a text. We're going to have a button that calls our script here, which is our random string generator. This is just creating a random string based off these characters that I've passed in. Very simple and this will be available for you to use as a module asset. Let me go back here, and let's go ahead and finish setting this up. Next thing I have to do is go to Properties. I'm going to go all the way near the bottom. I'm going to find Static website hosting and click on Edit. So now we can't enable static website hosting because we have the other prerequisites in place. We've allowed public access, and we set up our files. Now for hosting type, we're going to host a static website because redirects are a little out of scope for this course. We specify the index document. Now by default, this will be index.html. But just to be safe, let's go ahead and specify it. Now if you want, you can also specify an error document. Notice the default is error.html. So if for some reason you get an error in your website and you want to return a custom error document, you can do that. Be familiar with this for the exam. And then the last thing here is a redirect rule. Again, this is out of scope for this course and this exam prep, but it's good to know that you can do it. I'll click on Save changes, and there we go. Now, if I go all the way to the bottom again, we see we have enabled static website hosting. It's a bucket hosting type, and we get an endpoint. So this is an URL we can use. Also notice when I zoom in here the format of this website. So because we're in us‑east‑1 region, it uses the hyphen or dash syntax. Remember, we covered the dash and the dot syntax. So for this, we have our bucket name dot s3 hyphen website hyphen us‑east‑1. Remember, instead of the dash before the region, it could be a period or a dot depending on the region. So what I'll do now to test this out is go to this URL. It'll open it up. And boom, there we go. We've now successfully hosted a very simple static website using Amazon S3. And just to demonstrate how JavaScript does work with this, I'll click my button, and we're getting a JavaScript execution on the client side generating a random string every time I click this button. Perfect. So hopefully you saw just how easy it really is to host a static website in S3. Remember, you would use a default root object. You can offer an error.html object. And you have to implement public access for the bucket for this to work. Let's go ahead and wrap things up here, and we can move on.

Optimizing S3 Performance
Let's get started talking about optimizing your performance within your S3 bucket. Amazon S3 has extremely low latency. It allows you to actually get the first byte of your objects out of your buckets within 100 to 200 ms. That is actually a pretty impressive number considering this is not a high‑performance database or some type of NoSQL database. It's just an object store for us to use. Now, when you're making HTTP calls to S3, there are some numbers that you should try to keep in mind when you're designing your architecture using your buckets. You can get up to 3500 PUT, COPY, POST, and DELETE HTTP requests per second per prefix. You can also get 5500 GET or HEAD HTTP requests per second per prefix. You'll notice that this is per second per prefix for both of these numbers. The reason I call that out is because we want to explore maximizing performance in your buckets by using different specific prefixes. In this example here, we have four different prefixes for a file. So each highlighted portion that I've also listed is a separate prefix. By leveraging more specific prefixes, we can maximize our request limit. So let's actually look at that. The big thing to remember is that you get better performance because you can spread your reads across different prefixes. So that's why typically you're going to see in exam scenarios that you'll need to go ahead and be as specific as possible when you're storing your object. Let's just take those four prefixes we just looked at in this example. Theoretically, you could achieve 22,000 GET requests per second by using those four different prefixes. That's because there's a 5500 limit, but we have four different prefixes. So we can actually multiply that limit by 4 because, remember, those quotas and limitations are per prefix per second. A good example of this would be storing your objects for logs, let's say. You would want a year, a month, a date, and likely even an hour. By leveraging that type of format for your storage, you're able to maximize the amount of requests that you can actually make. Shifting gears a little bit, let's talk about something called a byte range fetch. This is a feature in S3 that allows you to parallelize your downloads by specifying specific byte ranges in an object. What that means is that you're looking for a specific portion of an object. One of the key benefits here is that if there's a failure in the download. So let's say you're trying to download a big object and you're using a byte range fetch, well, it's only going to be for the specific byte range. It's not going to be for the entire object. The two big things that it offers is it offers better resiliency for any download failures, and it offers higher aggregate throughput because you're downloading a lot of smaller parts to make up a bigger part or a bigger object. So this is similar to multipart uploads in a way. It's essentially just the opposite workflow. Remember, with multipart uploads, we go ahead and use the SDK to split the object into multiple parts, and then those get uploaded. Well, with this feature, the object's already in S3, and we're just splitting it into smaller chunks to download. Let's actually look at a diagram example really quickly. Let's assume we have our S3 bucket, and we want to go ahead and get a big object from that bucket. When we make a byte range fetch, our objects get split into smaller parallel requests, which is going to offer us a much better delivery performance. If we assume one of these parts fail, well, we just have to download that small part. We don't have to re download the entire object. A pro tip for you. If you use multipart uploads to put an object into S3, then typically it's really a good idea to go ahead and get them in similar part sizes. This is one potential scenario for using a byte range fetch. If you have a very big object and you want to ensure you download that object in the best performance, this is probably the go‑to for you to use. Now let's go ahead. We're going to end this clip here. We reviewed prefix performances. We just talked about byte range fetches. Do your best to remember some of the use cases for those, and let's end this here. And when you're ready, we're going to go ahead and wrap this module up with some module summaries and some exam tips.

Module Summary and Exam Tips
Awesome. Way to hang in there, once again. Let's go ahead and get started with this module summary, and let's have some exam tips that we want to review before we move on. First up, S3 batch operations. Remember, these can be leveraged to perform operations on a large‑scale set of objects in S3 at once. If there's ever a scenario on your exam where you have to perform some type of operation on thousands or even millions of objects in S3, this is probably going to be your best bet. One use case example is to encrypt thousands of unencrypted objects. Next was S3 Select and Glacier Select. Remember, these services or features allow you to use SQL statements to perform server‑side filtering of your objects. That is a huge selling point. It's server‑side filtering. So you don't have to worry about any compute on your end. It's all done on the AWS side. Try to do your best to remember the formats that are supported. There's CSV, JSON, and even Apache Parquet. Now you can also have GZIP and BZIP compression. However, the objects in those archives need to be either CSV or JSON‑formatted. Last the thing here, do your best to remember that these two features are best used for smaller simple selections in order to reduce data retrieval costs and data retrieval latency. These features are not the best solution if you have large amounts of retrievals. Remember, it's best for smaller more succinct selections. Moving on to storage lines. Remember, you can use this to gain organizational‑wide insights about your buckets and the objects within your buckets. Some use cases for using this would be identifying data hotspots and data anomalies. So hotspots could be, okay, well, this data is in infrequent access, but we're getting a lot of retrieval costs so let's just go ahead and move this to standard. Or vice versa. Maybe it's not getting used a lot and you want to go ahead and migrate those objects to a cheaper storage class. The data anomaly side could be, well, it's weird. We're getting a lot of requests for objects that really shouldn't be getting requests. We need to probably investigate this. Next up, S3 Events Notifications. These are very important. Remember, you can use these to trigger workflows based on several events that can occur within your buckets. These are a big driver for event‑driven architectures. Do your best to remember the different popular events for your exams. You can look for objects created. You can look for objects removed. You can look for replication events and even object tagging events. These are four common exam scenario events to keep in mind. Also, you do need to remember the destination types. There are four destination types you have to know. You can trigger a Lambda Function, you can send an event to a message queue in SQS, you can send a push‑based message via SNS, and you can even send your events to something called EventBridge, which is an event‑based bus for event‑based architecture. We also covered Amazon S3 Transfer Acceleration. Remember this could be useful if you need to speed up any data requests both to and from Amazon S3 buckets from around the globe. It works by using closer points of presence to your customers. Next up was S3 websites. You can easily host static websites within your S3 buckets. However, you need to make sure your bucket policy allows that public access, so people can actually reach the website. A very tricky scenario to remember, client‑side scripts, client‑side processing is generally fine, so things like JavaScript functions within your simple sites. However, there is no server‑side processing or server‑side rendering at all. So in your scenarios, if there's any mention of server side rendering or processing, immediately you can know that you cannot host this website in S3. One of the benefits of using S3 for your sites is that it can scale automatically to handle your variable website demands. That's one of the big selling points. It automatically scales, and it's pretty low cost. And then wrapping things up here with S3 performance recommendations. Try to do your best to remember how to use prefixes to optimize your performance, playing your S3 prefixes appropriately so that you can actually maximize your different requests. You really want to try and do your best to use specific prefixes. That's going to be a general best practice. We also talked about byte range fetches. Remember that these allow for more improved download performance, and they offer better resiliency for failures of downloads. This feature allows you to break your object into smaller parts and then download specific portions of that big object. A good way to remember this is this is kind of the opposite of a multipart upload. Instead of breaking a big object into small parts and uploading it, we're taking the big object and downloading it in smaller chunks. That's going to do it for this Module Summary and Exam Tips clip. Let's go ahead and wrap this module up. Thank you for hanging in there. When you're ready, after whatever break you need, let's go ahead and move on to the next module.

S3 Security
Controlling S3 Access with Bucket Policies
All right, welcome to the next module where we're going to talk about Amazon S3 Security. In this first clip, we're going to talk about controlling S3 access with bucket policies. Now, previously, I'm sure you've seen bucket policies in some of our other clips when we had some demonstrations in previous modules, and we probably briefly discussed them. In this module here, we're going to dive into them a lot deeper, as they are very important for securing your buckets. But to start things off, let's review the policy types within IAM. We have two different types. There's identity‑based, which get attached to IAM principles to control the permissions and their different access to AWS resources and services. We also have resource‑based policies, remember. These get attached to the resources in AWS, and they're also meant to help control permission and access, but at the resource level. In this clip, we're going to discuss resource‑based policies known as bucket policies. So remember that a bucket policy is resource‑based. An S3 bucket policy is simply another JSON‑formatted resource‑based policy that gets attached to your buckets and helps you control access to the bucket and its objects. Let's talk about some use cases where you might want to customize a bucket policy. You can use a bucket policy to actually allow public access to your buckets and your objects. We actually saw this in that static website demonstration clip in a previous module. You can also use them to require objects be encrypted when they're being uploaded, and they actually use TLS during their transportation. And lastly, a common scenario is to allow other AWS accounts to have access to the bucket and its objects. So this is very useful for cross‑account logging scenarios. Maybe you have a central logging bucket that you want all of your other company accounts to log to. Well, you can set that up with a bucket policy. For the exam and real world, you need to remember these facts, buckets and objects, by default, when you create them, are private. What that means is you must explicitly allow access to the buckets and the objects using a policy. So, remember that they're private by default and you have to explicitly allow permissions. Now, let's say you want to ensure that you don't want any objects ever made public. Maybe you have a very secure bucket with very important data in it and you want to make sure that it's never accidentally leaked, which, unfortunately, is a very common occurrence. Well, AWS has done its best to make it as simple as possible by putting in a block public access easy setting that you can just click on with a single click of the button in the console. When you're using block public access, it's very useful for easily and quickly denying any public access to your buckets. Even if you already have implemented some type of public access, by enabling this block feature, it will kill all of that access. The neat thing is it can be used both at the bucket level, and you can enforce it at an account level. So you can say, hey, in this account, I want no buckets to have any public access ever. More than likely, you're going to want to turn this on immediately, at minimum at the bucket level. Now, there might be use cases where you have or need public access, like a website, for example. But other than that, typically you're going to want to enable this feature because it overrides any bucket policies. I think that's good for diving into bucket policies. We looked at how these are resource‑based policies that get attached to your buckets. We discussed some of the use cases and we just reviewed blocking public access. Let's wrap things up here and we'll move on and dive a little bit deeper into bucket policies coming up next.

Breaking Down an S3 Bucket Policy
Okay, let's get started breaking down an S3 bucket policy. It is extremely important that you understand how to interpret a bucket policy for both the exam and anytime you're actually leveraging S3 buckets. You'll probably notice that this looks extremely similar to a typical IAM policy, and that's because it follows a similar format. The first section we have here is the statement. So this is the list or the array of the different individual statements that you want to place within the policy. We then have the effect. So, just like a typical IAM policy, are you allowing or are you denying actions for the resources? The next portion is the principal. Now, this is the IAM principle that you want to grant access to or deny access to. The example that we're looking at right now with an asterisk in it actually opens up this bucket to the entire world. That means anonymous user access. So there's no IAM authentication required to access the bucket with this policy. You need to be extremely careful when you do something like this. You should only use this in extremely specific scenarios and use cases. Next, we have the action. So what kind of API calls or actions are you allowing or denying? Now for this, they have to be S3‑specific because this is a resource‑based policy. So, any other namespace like EC2 or anything else of that nature aren't going to work within these bucket policies because it's only going to be specific to the S3 bucket it's attached to. So that's why we see GetObject and ListBucket, which are both S3 namespace actions. Then the last thing here is the resource. These are the S3‑specific resources we are granting permissions for. You need to understand the difference between the two formats that we see here under the resource list. So let's actually review those. These are technically classified as bucket and object resources. They look the same, but you have to know how they actually differ. It's very important. This first one here grants bucket level permissions for the resource. The second line here grants object level permissions for the resource. The key difference here on the bottom is that /*. While this is granting access to all objects within the bucket, you can get much more specific and actually use full prefix matches instead of just doing something like this. Regardless, though, the behavior is the same. This is an object level resource. Speaking of bucket or object resources, let's look at some action examples for each. On the left side, we have bucket actions. So you'll see things like ListBucket, CreateBucket, BucketTagging related things, different actions of those natures. On the right we have object‑specific actions. So this is going to be things like get, put, delete, and restore object. These are pretty self‑explanatory, but it can get confusing when you're on the exam trying to navigate and interpret a complicated bucket policy. So really be sure you take your time and see what's going on. And speaking of difficult scenarios, this is a tricky scenario that could come up both on the exam and when you're implementing bucket policies in the real world. Based on the policy combination below, would you say that the IAM user would have access to the S3 bucket objects? We see on the left we have a bucket policy, and then on the right we have the IAM user's permissions policy. Feel free to go ahead and pause here and look through both of these policies, but I'm going to go ahead and move forward. So, once again, pause if you need to, review it and then resume. However, I'm getting ready to move forward. The answer is yes, the IAM user can access this bucket based on the bucket policy. Let's actually break down why that is. In this scenario, we had an IAM user. Within that user policy, there was no explicit allow statement for S3. It was only EC2‑related. The bucket policy was set to be open for all principals; we used that asterisk, remember. In addition to that, there was no explicit deny in either the bucket policy or the IAM policy. Because of all of this, this results in that user being allowed to access the bucket objects strictly because of how the S3 service operates. I know it seems really odd, but remember, S3 is a public service. So by granting that public access, essentially, it means authentication is not needed. So unless we explicitly denied it somewhere, that means that user can access the objects. Now, that's going to go ahead and do it for this clip. Feel free to review this information. You need to understand how to interpret a bucket policy and you need to really understand the differences in those resources that we showed, so the object and the bucket level resources. Let's go ahead and we'll end this clip here, and coming up next. We're actually going to look at implementing a bucket policy.

Demo: Implement a S3 Bucket Policy
In this demonstration clip, we're going to work on implementing an S3 bucket policy and then modifying it. Within the Sandbox environment, we're already going to have two users created for us. We're going to have a cloud_user, which is the default, and then we're going to have an EC2 user, which is only going to have EC2 permissions. In addition to this, I'm already going to have a bucket created for us. Now, in this bucket is where we're going to go ahead and create a new bucket policy, and we're actually going to copy that policy from an earlier clip, so we're going to use the same thing, but we're going to change the resource ARNs to match our current bucket. Once we do this, we're going to test. So the first thing we'll do is test our CLI, which will be configured with the cloud_user access keys, and we're going to get an object from that bucket. Now, this is going to work because we have permissions with that cloud_user. We're then going to test with the ec2_only_user. Now you're going to be surprised that this also works, and that's because of how this original policy is going to be set up. So even though our ec2_only_user will only have EC2 permissions granted in IAM policies, this resource‑based policy is going to open it up to anonymous access, which means that we can use this user to download the same object. Once we test this, we're going to go back in and we're going to go ahead and lock down this bucket policy to only work for our cloud_user IAM user, and we'll test one more time. So without further ado, let's jump into the console now. Okay, I'm in my Sandbox environment here. I'm logged in as cloud_user. Let's get started. First thing I want to do is review the existing resources. So in IAM here, I've loaded up our two users. First is cloud_user. This is the one that's going to give us all permissions with some permission boundaries in place. So this is going to be the primary user. I've also created this ec2_only_user, and you'll see I gave it AmazonEC2FullAccess, and that is it. So there's no other permissions outside of this managed policy. What I've done here is let me jump to my terminal, and I've actually configured my local CLI. So let me show you this. When I run it with just a basic command, we're going to be cloud_user, as you can see right here. Now, if I break this and I do the same thing, but this time I specify a profile of ec2_user, we get our ec2_only_user. So this is how I'm going to test these CLI calls, so I just wanted to make that clear. I'll break this. Let me go back to my console, and let's begin. I'll close down my IAM, and that brings us to S3. So I've already created a bucket‑policy‑demo bucket here. If I go in here, I left the default settings. So if you go to permissions, you're going to see block public access is on by default, and we have a blank bucket policy. In addition to this, back in objects, I uploaded a PNG file. Now this is a simple Ubuntu PNG file, and there's nothing special about it. It's literally just an image. But this is what we're going to use to test our downloads. So let's get started. What I'm going to do here is under my bucket I'm going to go to Permissions. I'm going to go to Bucket policy and click on Edit. Now, what I'll do is copy in this JSON. And the only thing I have to do here is change the ARN. So I'm going to copy my bucket ARN and I'm going to replace it where it's needed. Remember, these two lines mean two entirely different things. One's for the bucket resource, the other is for object resources. Now, when I go down here and click on Save changes, this is going to break, and I'll tell you why. It's because we didn't allow public access. You wouldn't say, hey, you don't have permissions because it grants a level of public access that's not allowed. So what I had to do is go cancel, go back, and I had to turn Block public access off. So under Edit, I'll hit this, I'll accept the Save changes here and Confirm, and now we can grant that level of public access. So if I go back to Edit, I'll paste in that same JSON here. I'll replace the bucket ARN. And now if I go down and click on Save changes, we're good to go. So our bucket policy in place, it allows everyone these actions on this resource. So let's go back here. I'm going to go to Objects, and let's begin testing. So what I'm going to do to test is I'm going to jump into my terminal session. Now within here, I'm going to run several commands. The first thing I'm going to do is I'm going to list the buckets here as my cloud_user. Perfect. So I can see the bucket in the account. Now let me try that again as my ec2_user. This is working as expected. I get an access denied because I didn't grant permissions to that user to do so. So now I'll clear my screen, and let's move on. The next thing I want to do is I want to test that I can list objects within my bucket. Remember that was part of the policy in the bucket policy. So I'll run this as cloud_user, which is the default, and I get my Ubuntu image. But what happens when I do it as ec2_user? Well, interesting, we can list the images, and that's because that resource‑based policy, the bucket policy, grants permissions to everyone. So even anonymous users can make this call. So that's why this works. Even though we didn't grant it to the IAM principal directly, that policy allows this. So that's why this is so dangerous sometimes. Now let's actually test downloading or getting the objects. So let me copy and paste some commands in here. And before I hit Enter, I'll talk about this. It's running the AWS CLI, the s3api namespace, we're getting an object, and then we pass in information relative to it. So the bucket, the key of the file, and then the local file name. Now I will have all of these commands in a text file as a module asset, so you can play with these if you want. What I'll do is I'll hit Enter, and there we go, this is successful. So now if I quit this and ls my directory, we see our cloud_user_ubuntu.png file. So just to show this, let me open this up and I'll pull it over into the screen. I maximize it over here, and this is all this is. This was pulled from unslash.com, and it's just a simple Ubuntu image. So I'll go back now and let's continue on. Let me clear my terminal, and let's try it now as the ec2_user. So I'll copy and paste a command in. It's the same command. The only difference is we're naming the local file something else. And we're specifying our ec2_user profile. So I'll hit Enter, and there we go, we are able to download that same object. So if I clear lls, we get both files. We have that original one and the new one. And you can take my word for it, this is the same exact file. I promise that this is what it is. So I'll clear my screen again, and let's go back and edit our bucket policy now. So let's jump back into the console and let's edit this bucket policy. I'm going to go to Permissions. I'm going to find Bucket policy, and I'm going to click on Edit. Once I get to this bucket policy screen, I'm going to go ahead and make a change here. So I'm going to paste in this document, and I'll describe what I'm doing. Two big things. I've changed the action to allow all S3 actions. But now I went ahead and I said, hey, lock it down to this. IAM principle only, so only the cloud_user has full access to this bucket. Now, if I go down and I click on Save changes, we have the new bucket policy. So if I go back, we still see our object. Let's test this out. I'll jump back to my terminal, and let's run some of those same commands. So I'm going to paste in the list, the bucket as cloud_user, which is the default profile, and that works. Let me try it as ec2_user. Perfect. So now we get an access denied because now we don't have any permissions allowed anywhere. I'll clear this, and let's try downloading the object. I paste in a new command here, it's that s3api get‑object call, and I'm just calling the object policy_cloud_user_ubuntu., I'll hit Enter, and there we go, that's a good sign. So if I break out, I ls in my directory, there it is. There's our exact same PNG file. Now, however, let's go ahead and test out with the ec2_user. So I'll paste in another command. We'll just call it the same file, which is fine, but I'm using the profile here of ec2_user. Let's see if this works. No, perfect. So our bucket policy is working as anticipated. Now only the cloud_user based on the bucket policy can perform any permissions on this bucket and its different objects. Okay, that's going to do it for this demonstration. Hopefully you saw how incredibly impactful bucket policies are and how to use them. Let's go ahead and wrap this up and we can move on when you're ready.

Bucket and Object Access Control Lists in S3
All right, up next, we have bucket and object access control lists. An access control list in S3 is a list of grants that identify a grantee and the permissions that are granted to them. ACLs are used to grant basic read or write permissions for other AWS accounts. Let's have a quick overview of what these are. S3 access control lists use an Amazon S3‑specific XML schema. It's actually very hard to read and it's super annoying to use, I'll be very honest with you. Now, similar to bucket policies, these are also another type of IAM policy that is specific to S3. There are two different types. There's a bucket ACL, which is used to manage access to the entire bucket, and then you can also implement object ACLs, which are used to manage access to individual objects. Now, you can have one bucket ACL for the entire bucket, but if you use object ACLS, you have to have one object ACL per object. It gets very, very convoluted and confusing because if you have hundreds of objects, this can quickly become a very big mess. When you're using S3 access control lists, you reference something called a canonical user ID. What this is is going to be a unique long string ID that gets assigned to each AWS account. It's meant to be an obfuscated form of that account ID, and it's frequently used within S3 ACLs. I actually don't think I've seen it used anywhere else, only in access control lists. Here's an example of a canonical ID for an account. You can see why this would be a huge pain to have to manage. It's not as easily referenceable as a normal account ID. Now, when you're using ACLs in S3, Amazon S3 actually does support a set of predefined grants, and they call these canned ACLs. So these are essentially premade for you to use very quickly and easily. Here's an example of a list of those canned ACLs. Now, I'm not going to read through all of these. Feel free to review them on your own time for later. I just wanted to include them for reference if you ever need to come back. A lot of the popular ones are private, public‑read, authenticated‑read, and then bucket‑owner‑full‑control. These are some popular examples that could come up on the exam. However, typically, bucket policies are going to be favored over using ACLs due to the ease of use to implement them. In addition to this, AWS even recommends that you leave ACLs turned off completely unless they are 100% absolutely needed. This is because of how confusing and how much overhead they really add. There's a lot of management, they're not easy to read, etc. Bucket policies are preferred over ACLs every time. With that being said, let's wrap this clip up here. Remember, there are bucket ACLs and there are object ACLs, and then really, in the end of it all, bucket policies are preferred over them. Let's wrap things up here and then we can move on.

Demo: Blocking Public Access to Your Amazon S3 Bucket
Okay, welcome to this demonstration clip where we're going to work on blocking public access to an Amazon S3 bucket. Before we jump into the console, let's review what's going to go on with a quick diagram overview. We're going to act as an anonymous user, and we're going to have a Pluralsight bucket already created for us. So this will be in the account, and we'll walk through the settings that are enabled. What we're going to do is we're going to start with an S3 bucket policy allowing public access. So anyone should be able to get the objects within the bucket. Doing that is going to allow us to attempt to download an S3 object in there using the S3 object URL as an anonymous user, and we'll see how that works and how it's successful. You're going to notice some similarities to a previous demonstration where we worked on implementing and modifying a bucket policy. It's going to use the same type of bucket policy to start off, and then we're going to change some other things instead. Now once we test this, we're going to go back in and we're going to enable Block Public Access, but we're going to leave the bucket policy in place because I want to show you how this works. We will then attempt to download the object again as an anonymous user and see if that works. Now, here's a spoiler alert, it won't work. But with that being said, let's go and jump into the console now. Let's go ahead and get started. I'm logged into my Sandbox environment here. I've loaded it up S3, and I've already created an S3 bucket here. Let me navigate to this bucket, and the first thing I want to show you is under Permissions I have Block all public access turned off. This is required for us to start this demo. Now be aware, this is not the default. So if you're trying to do this, you will have to manually turn this off when you're either creating the bucket or after. For this, I've skipped forward and already turned this off. And the next thing I want to do is go down to Bucket policy, and I'm going to copy and paste a simple bucket policy in here. So from here, I'm going to click on Edit. I'm going to go up, I'm going to paste, and then I need to change the bucket on. So I'll select here, I'll replace this, and then replace this. So you've probably seen this. This is the same policy that we used in a previous demonstration. We're allowing everyone or anonymous users to get objects and list the bucket for the resources. So I'll go down here, I'll click on Save changes, and there we go. Now, what I'll do is I'll go back to objects, and let me upload a simple text file. So I'm going to drag and drop this text file in, and this is just a very simple text file. I'm going to go ahead and upload it. I'll close this, and there we go. We now have our text file in our S3 bucket. So now what I wanna do is I'm going to actually select this, and up here there's a Copy URL button. This is going to copy the object URL. Using this, I'm going to go to a different web browser. You'll notice I'm in private mode, so this has got nothing cached, there's no authentication credentials, anything in here, I'll paste it, I'll click on Enter, and we have access. Hello everyone! I am a simple text file, but I mean a lot! So this is working. So now if I go back, we've granted public access to this bucket. Well, let's pretend we didn't mean to do this and I want to actually stop this. So what I'm going to do is go to Permissions. I'm going to go down to Block public access and click on Edit, and I'm going to block all public access. So I select this, I click on Save changes, and I want to confirm. I've confirmed it, and that's how easy it is. So now all public access is now blocked. Now, if I scroll down, you're going to see, we still have this bucket policy that's allowing public access. So how does that work? Well, let's test it out. I'll go back to this. Let me refresh, and boom, access denied. I'll zoom in here, and you can see the message and the code. We're not allowed to do this because anonymous access or public access is now denied. So, even though we have this bucket policy here, this Block public access setting overrides everything that has to do with public access in a bucket policy. In fact, now, if I go in here and I edit this and let's just say I edit it very briefly and get rid of one permission and try to click on Save, well, you can't do that because permissions say you're not allowed to grant any level of public access. And there you go. That's just how easy it is to actually quickly and easily block all public access to your buckets regardless of bucket policy. Let's go ahead and let's wrap this demonstration up.nd then when you're ready, we can move on.

Encrypting Data at Rest in Amazon S3: Overview
All right, welcome back! In this clip, we're going to look at encrypting your data at rest in S3. First thing we need to know, buckets that are brand‑new within AWS now have encryption at rest configured by default. They made this decision quite some time ago, but if you have old accounts that are several years old, well, then it might not be turned on by default. Just keep that in mind. When you're encrypting data at rest in S3, you have several different methods you can use. Let's look at the list of methods now. The first is SSE‑S3. So this is server‑side encryption S3‑managed keys. This uses AES 256‑bit encryption. This is what is used by default for new buckets. There's also server‑side encryption KMS. This uses a service called key management service, and it uses keys within that cryptographic service. We'll explore key management service in a different course within this learning path for the exam prep. However, you need to be familiar that it integrates with S3 right now. You also have server‑side encryption C, which is customer‑provided keys. And then lastly, client‑side encryption. Now, we're going to go ahead and explore these different encryption types in more detail coming up in their own clips, but for now, let's just look at enforcing server‑side encryption. In addition to the defaults, S3 actually makes it possible to enforce that server‑side encryption be used in any of your upload calls for your objects before they're even stored. This is actually completed via a bucket policy with a condition statement in that policy. So this is why it's so important to understand how to actually interpret and create bucket policies. When you're enforcing encryption at rest with S3, the request headers need to include this header, x‑amz‑server‑side‑encryption. With this header, there are two different values you can use. You can provide AES256, which is going to use S3‑managed keys, or you can specify AWS KMS, which will use KMS‑managed keys. And just to repeat it, we're not going to dive into KMS in very great detail during this course, but it does get covered in another course within the same exam prep learning path. Please just keep that in mind. Now, let's actually look at a sample request for an HTTP call for putting an object called myFile into a bucket where we're enforcing server‑side encryption. This is a real example of a potential HTTP request. On the top here, we see the file we're putting, and we see the bucket. Now the key header I want to call out here is right here, this amz‑server‑side‑encryption header. So in this, we pass the value of AES256, which means we're using S3‑managed keys. Now that's going to do it. Let's go ahead and wrap this clip up. I think that's a good introduction and overview to encryption at rest in S3. Coming up next, we're going to start diving into each of those methods in a little bit more detail.

Encrypting Data at Rest in Amazon S3: SSE-S3
All right, as promised, let's begin looking at the different encryption at rest methods. This first one we're going to look at is server‑side encryption using S3‑managed keys. Like we just mentioned, SSE‑S3 is server‑side encryption at rest using AWS S3‑managed keys. That means you don't have any access to them. This is going to leverage AES‑256 encryption types. And right now, like we mentioned in a previous clip, this is the default setting for all new buckets that get created. So if you don't ever customize this, this is the method that is used. Also, like we looked at in a previous clip, remember the header, x‑amz‑server‑side‑encryption, and to use this particular method, you pass in that AES‑256. It is very important to call out. There is no additional cost or performance impact with this type of encryption at rest selected. This is a humongous factor for the exam. Remember this, there's no additional cost and there's no performance impact. That's going to do it for our server‑side encryption S3‑managed keys exploration. Let's go ahead and wrap this up here, and we'll move on to the next encryption type.

Encrypting Data at Rest in Amazon S3: SSE-KMS
Next up, we're going to talk about encryption at rest using SSE‑KMS. SSE‑KMS is the server‑side encryption method using AWS KMS‑managed keys. Remember, KMS is a secure service for cryptographic key management. When you use this method, it's going to offer much greater control and customization for your encryption compared to using the server‑side S3 approach. When you use it, the header will be set to the following, x‑amz‑server‑side‑encryption, and then the value will be aws:kms. This is important for you to remember. This is how you specify that you're going to use a KMS key. Let's look at some concepts specific to this type of encryption at rest. Using KMS keys is very useful for encryption‑related compliance requirements. It allows you to have more customization and control while also leveraging an AWS‑managed service for your cryptographic keys. One of the key notes here, the keys that you use for your encryption have to be in the same region as your bucket. Remember, S3 is global, but the buckets are regional. So if you create an S3 bucket in us‑east1, then your key has to be also existing in us‑east1 in order for it to be used. And a pro tip here, there are additional charges for using this type of encryption. That's because you get charged for making calls to the keys that live within the KMS service. Those calls, when you use this, are going to be primarily for encrypting and decrypting. In addition to that, you have to have permissions to decrypt using that key before you can read an object. Keep that in mind. If you use a KMS key and you want to download and look at an object, you have to be able to decrypt that object using that same key, and you do that via a policy attached to that key. Again, we'll dive into much more detail on KMS in a different course. But for this, remember that if you want to use KMS keys, you have to have permissions to decrypt and encrypt. That's going to go ahead and do it for this clip on SSE KMS. Remember, this allows much more flexibility and control, but there are some costs associated with it. Let's go ahead and end here and we'll move on to the next method.

Encrypting Data at Rest in Amazon S3: SSE-C
Let's go ahead and talk about our next encryption at rest method, SSE‑C. What is server‑side encryption C? Well, this is server‑side encryption where you fully manage and use your own keys. This means that S3 has no access to the key being used. When you use this method, you are absolutely required to leverage HTTPS for any interactions. In addition to that, you have to include your key in every single HTTP request that gets made to your buckets. Now, with all of this in mind, there are three very important headers you have to be aware of if you use this customer‑provided key method. The first is x‑amz‑server‑side‑encryption‑customer‑algorithm. So this is where you actually specify the encryption algorithm that you're going to use. Now, it's funny that they even require this because it has to be AES256. There's no other encryption algorithm that's supported. My guess is they're going to expand on this maybe in the future, but for now understand there is only one value that can be used. The second is x‑amz‑server‑side‑encryption‑customer‑key. So this is the actual 256‑bit, base64‑encoded encryption key used for the encryption and the decryption. This is why it's important that you pass this in over a secure HTTP tunnel. Lastly, we have x‑amz‑server‑side‑encryption‑customer‑key‑MD5. This is the base64‑encoded, 128‑bit MD5 digest of your encryption key, and S3 actually uses this for integrity checks. So they use this value to make sure that, hey, is this key correct, and is it actually valid? Does it match this MD5 value that was passed in? When everything matches up, it's going to go ahead and be able to encrypt and decrypt as needed. There are no additional charges for using this method. This is another big thing to keep in mind. So this offers a ton of control, and you don't get any more additional charges; however, you need to understand the tradeoffs for using this. There's a lot more complexity involved because you have to manage your own keys, you have to do your own thing, and you have to pass in many more headers and information in order to use it. However, with that being understood, I think that's enough for this method of encryption at rest. Do your best to remember the three headers that are used and then some of the nuances that go along with it. Let's wrap things up here and then we'll move on to the next clip.

Encrypting Data at Rest in Amazon S3: Client-side
All righty, let's go ahead and discuss the last encryption at rest method here, client‑side encryption. Using this method means that you encrypt your data locally on your clients or servers before you even transport it to the Amazon S3 bucket. With this methodology, you fully manage all of your keys and the encryption cycles and rotations for those keys. AWS has no insight or control over any part of the cryptographic key process. With that, it means you encrypt before sending to S3, and then you decrypt after you get the object from S3. Everything is done via the client side, thus the name. A really neat feature is that Amazon offers you the Amazon S3 encryption client to actually help simplify this process if you want to use it. It can get very complex and very convoluted, so they offer this tool to kind of simplify it if you really want to go forward with it. The key thing to keep in mind here, when you use this method, S3 has no idea that your object is encrypted. That's because everything is done via client‑side. This means that the service only sees the object as a normal object. With that being said, if someone somehow got a hold of this object, well, in theory, they wouldn't be able to actually do anything with it because it's encrypted client‑side and they wouldn't have access to decrypt it. So really keep that in mind. Everything is done client‑side, thus, the name, so you encrypt locally and you decrypt locally. Let's go ahead and let's wrap this clip up. Hopefully, that is a good overview on using client‑side encryption. Remember, just like the name says, it's all done via the client‑side.

Optimizing S3 Encryption Using Bucket Keys
Okay, up next, let's talk about optimizing your encryption using something called a bucket key. Remember, we talked about SSE‑KMS earlier, and we covered that this incurs additional charges for calls made to that KMS service. So when you need to encrypt and decrypt, those are all calls that are made via an API and they incur charges. This is where a bucket key becomes very useful. A bucket key is a bucket‑level KMS key used for this specific encryption type, so SSE‑KMS. It's a feature that is capable of reducing costs for that service by up to 99%. This is a ton of money you could be saving by enabling a bucket key. How it works is AWS generates a short‑lived, bucket‑level specific key from the original KMS key. So it's essentially a temporary key based off the original key. They use this short‑lived key to store in S3 for a finite period of time. So eventually it will expire and AWS has to generate a new one by using the original key. From this short‑lived key, things called data keys get generated, and those are what are used for encryption and decryption. Now, after your short‑lived key is present and you have a data key present, the requests are now not needed for every single cryptographic call. So now we're not having to make a call to KMS. Instead, S3 can leverage this temporary bucket key to encrypt and decrypt. You can almost think of it like a cached version of the key that expires after a certain amount of time. You 100% want to enable this, as there's really no downside to doing so. The too long didn't read summary version, bucket keys allow you to save on KMS costs when you use SSE‑KMS encryption methods. It does this by decreasing the number of requests that get made to the parent KMS service. Again, you really want to enable these whenever possible because it's going to offer tons of cost savings, and there are really not too many downsides, if any downsides to doing so. Let's go ahead, we'll wrap this clip up and when you're ready, I will see you in the next one.

Demo: Enabling SSE-KMS S3 Bucket Key Encryption
Hello, and welcome to this demonstration! In this clip, we're going to go ahead and leverage server‑side encryption using KMS keys within AWS, and we're going to also enable bucket keys to see just how easy it really is. So I've loaded up my Sandbox environment here. I'm logged in and selected S3. I'm going to create a new bucket. I'm going to give it a name. And then after that, I'm going to scroll down here. So once I get down here, I'm going to find Default encryption. Here it is. So you can see the default encryption type was what we mentioned before. Server‑side encryption S3‑managed keys. Well, we want to change that. We want to actually use KMS. So with KMS, we can select the KMS key or the encryption key that we want to use. So when you do this, you can enter the ARN of a key, we can take a shortcut to create a new one, or we can choose an existing key. Now, I'm going to choose an existing key because creating one is out of scope for this demo. And once I'm here, I'm going to go to drop‑down, and we actually have one that's created for us. So this is a default key within KMS for this S3 service. You can see the alias here that I just highlighted and zoomed in on, aws/s3. So I'm going to select this, and one of the nice things about using the service‑related key, you don't have to do any other specific permissions to use the objects. That's all handled for you, even though it's using an encryption key in KMS. So that's one of the benefits of using these type of keys. So I'm going to select the service key, aws/s3, and then you see we have an option for bucket key. So using this key, they're going to create a bucket key and lower calls to KMS. This is exactly what we talked about. So we can disable or enable. I'm going to do enable. I'll go down and click on Create bucket, and there we go, we now have our demo bucket. I'll click on it, and let's upload a sample object here. I'm going to go ahead and drag and drop a PNG file in here. I'm going to upload it, and there we go. I'll click on Close, and then let's select this object, and I want to actually view the details. So under Properties, we see the Object overview. Now if I scroll down, we're going to see some of the more specific information that we want, specifically, server‑side encryption settings. So this object is now using KMS, so SSE‑KMS, we see the key ARN, which is our service key for S3, and you'll notice it's using a bucket key. So what has happened now is S3 took this original key, it generated a short‑term temporary key within S3 itself, and then using that short‑lived key, it generated what is called a data key. The data key is what's used to actually encrypt this file that we uploaded, and that's exactly how bucket keys worked. In addition to that, that's how easy they are to turn on. There's really, again, no reason to not turn these on. I highly recommend you use them if you're leveraging KMS for your encryption methods. Now, that's going to do it for this short demonstration clip. We enabled default server‑side encryption using KMS keys, and then we enabled a bucket key to offset some of the costs associated with that method. Let's end the clip here, and we'll move on whenever you're ready.

Amazon S3 Encryption in Transit
Okay, up next, let's look at encryption in transit. You can and should leverage TLS during your HTTP requests that are made to S3. I say this because S3 offers both HTTP and HTTPS endpoints. But usually you're going to want to use HTTP S for any of your calls. There's hardly ever a reason to not do so. Now, to enforce the use of it, you can use a condition called aws:SecureTransport. This requires the use of TLS before anything can be done within the bucket. Let's actually look at a bucket policy that requires that condition. You'll notice everything looks pretty normal. In this particular bucket policy, there's one big line here, and that's the condition block. In this condition block, we're looking for a Boolean value where the key is the SecureTransport key. What we're saying is, hey, if the SecureTransport key within the request to this bucket is false, we're going to deny putting objects in the bucket. So this is essentially saying, hey, you have to use TL S or you can't put anything in my bucket. This is a very common exam scenario, so please be familiar with this condition key for a bucket policy, aws:SecureTransport. Now with that out of the way, let's go ahead and wrap this clip up. Big thing to remember here is this condition key, please, please, please take the time to remember it. And when you're ready, we can go ahead and break, and then I will see you in an upcoming clip.

Preventing Accidental Deletions with MFA Delete
All righty. Let's discuss presenting accidental deletions with MFA Delete. MFA Delete is a feature in S3 that enforces that MFA is required to perform different important actions pertaining to the service. In order to even use this feature, you have to enable versioning on your bucket. If you don't do that, you can't even turn this on. And with that being said, the bucket owner is going to be the only one who can actually enable and disable the feature for the bucket. Now, let's actually look at some actions where MFA is actually required. It's going to be required once you turn this on for any type of permanent deletion of objects and even suspending versioning for the bucket. So once this feature is enabled, you can't just permanently delete objects without MFA and you can't suspend versioning without MFA. Where MFA would not be required is for actions like this, where you re‑enable versioning for the bucket, or where you're listing deleted versions of objects. When you're using MFA for this feature, your options can vary. They can be virtual, so things like Google Authenticator or some other type of phone‑based software, or it can be hardware, like a UB key. This is really going to come down to what your company requires or what your organization needs in terms of MFA. Some places require hardware keys, others say that virtual software is perfectly fine. And with that being said, let's go ahead and wrap this clip up here. We just reviewed MFA Delete. Please be sure you remember some of the use cases and some of the things this prevents. Let's go ahead and wrap up, and then we'll move on whenever you're ready.

Logging Interactions Using S3 Access Logs
Up next, let's look at logging interactions using S3 access logs. Amazon S3 access logs provide you detailed records about requests that are actually made to your buckets. Let's look at some important concepts regarding this service. These are going to be very useful for any security and auditing needs for your S3 bucket access. In addition to that, they are extremely helpful for actually analyzing bucket and object usage, as well as analyzing why your bill is what it is. So maybe you're having a really high bill and you want to see why, well, you can dig through access Logs to see what kind of requests are being made. When you enable this feature on your buckets, you specify a target bucket. This target bucket is where all of your access logs are actually going to be stored. Now, when you do this, the target buckets have to reside in the same region as the source bucket, so there's no cross‑region access logging. Please keep that in mind. It has to be in the same region. Now, in that third point there, I say separate target bucket because I want to reinforce that you shouldn't use the same bucket. Technically, you can use the same bucket for access logging that you turn the feature on for, but this is a terrible idea. It could cause an infinite loop, so you never want to use the same bucket for your source and your destination. So that's why I say use a separate target bucket. Now, exam pro tip regarding this feature, you're going to turn this on if there's ever an exam scenario where you have to see any information about requests made to your buckets. This feature allows you the best insights into what's going on regarding any access requests being made. With that being said, really do your best to understand why you would turn on access logs for S3 buckets. Let's go ahead and wrap this clip up, and we'll move on to a demonstration where we're going to turn on this feature in an S3 bucket.

Demo: Turning on Access Logging in S3
Let's get started with this next clip. In this clip, we're going to work on a demonstration where we turn on S3 access logging for one of our buckets. During this demo, we're going to perform a few different things. The first thing we'll do is we're going to turn on S3 access logging on the primary bucket, and then we're going to set the destination bucket to be the destination for those logs. Now, once we enable S3 access logging, we're going to play around with uploading objects to that bucket. Now, I'm going to call out right now before we dive in, and I'll call it out again in the console, log delivery is best effort, and it's not immediate, so it can take hours for our log records to be delivered. And because of this, what we're going to do is instead of waiting around, I'm going to show you an example log file so you can see what kind of information is captured within them. And with the architecture reviewed, let's jump into the console now. All righty, I'm in my AWS Sandbox environment here, and I've loaded up my S3 console window. You'll notice we have two different buckets right now. We have a source bucket and we have a destination bucket. These are the two buckets we're going to use for this demo. And speaking of the demo, let's get started. What I'm going to do here is select my source bucket. I'm going to go to Properties, and I'm going to scroll down until I see Server access logging, which is right here in the middle of the screen. Right now it's disabled, which is expected, but I'm going to turn it on. So I'm going to go to Edit, I'm going to select Enable. Now we fill out a bunch of information. The first thing I'm sure you've noticed is this big warning or information block. Hey, your bucket policy will be updated. What this means is S3 will automatically update our bucket policy on the destination bucket to allow delivery, and we'll see this here in a moment. The next thing we do is we select the destination, so the bucket that we want to actually send the logs to. So what I'll do is I'm going to browse for my destination bucket and select it, and it fills in a lot of the information for us now. Now, one of the cool things here is you can set your own prefix. So, by default, it just uses the root portion or root path within your bucket, but we could give it a custom one like our_logs and it fills it in for us under the format. So now we have our S3 destination. It shows the region, which, remember, it has to be the same region. We see the bucket name, we set our own prefix, and now we can select the key format, so the log formats that are going to be stored. Now, when you select the log format, it's the key format. Please understand it's not the actual format of the log records that are stored, it's the file naming structure of the logs. So you'll notice there are two different options. And this little blip here on the bottom says, hey, to speed up analytics and queries, use this format here. Now we talk about optimizing performance in another module, but I'll just call this out right now. The more specific your prefix is, the more high‑performing queries will be. So you're going to be much more efficient with much more specific prefixes for your data. So that's why they're saying this is a better format. Notice how much more specific it is compared to the first one. It has additional fields, like a SourceAccountId, SourceRegion, etc. So what I'll do is I'll select this, we move down, and now we can set the source of the date that's used in that format. So the two options here are S3 event time and log file delivery time. The difference between the two is that S3 event time means it will use the timestamp in the log format to actually match the S3 event timestamp, so when the event that was captured actually happened. Or, you can do it for the log file delivery time, in other words, when the log file was actually sent. So those are two different formats you need to be aware of. Now, I'll just choose this for the sake of simplicity in this demo because we won't even be able to view these in our Sandbox due to time limitations. But I'm going to click on Save changes, and there we go. We've now enabled server access logging on our bucket. You see the destination bucket. Let's actually go check out that destination bucket now. I'm going to go back to Buckets, select Destination, go to Permissions, and let's check out this Bucket policy. Awesome! So see that S3 automatically did this for us. It created this new statement. Let's break down what it's doing. It's allowing the S3 logging service, which is owned by AWS, to put objects into our bucket here. So notice our bucket arn with the /*. It's also adding a condition, saying, hey, yeah, you can do all of this stuff as long as it's from within the same source account, which is the account we're in right now. So we're allowing the logging service to put objects into this bucket as long as it's the logging service that's being used within our current account. So now we've set this all up. So now when we go back to our buckets, if we uploaded objects, deleted objects, changed different settings within properties and permissions, all of this information is going to be captured within our access logs. Now again, we're not going to be able to demonstrate, unfortunately, within this demo, what this would look like because it could take hours, but instead what I've done is I've went ahead and created an example log for us to look at. Over in my IDE here, we have a log record. Now, this top portion here on line 3 is what it actually looks like if you were to look at a log record. Notice it's very long and it's all on a single line. What I've done is taken this line 3 and broken it down into each field respectively. I've also labeled what each field is. So this first here is the Bucket Owner, which is the canonical ID. We have the Bucket Name, which is the second field, etc. Now, I'm not going to review all these. I'm just including this as an asset for you, so you can look through this and get an example on what to expect with a server access log. It's pretty nifty. There's a lot of information in here, including the object keys, the different HTTP status codes, etc. So feel free on your own time, dig through this file. Again, I will include it as an asset. But for now, we've went ahead and successfully enabled server access logging to be sent to our destination logging bucket here. Let's end this clip and we'll move on whenever you're ready.

Granting Access to Objects with S3 Presigned URLs
In this clip, we're going to explore granting access to objects using something called an S3 presigned URL. Presigned URLs can be used to grant time‑limited access to objects within S3 without ever having to update your bucket policies. Let's explore some concepts around them. How it works is you generate a URL, either via the console, the CLI, or even using a supported SDK. This URL that's generated can then be entered into a browser or some type of application to download or upload objects to the bucket. It works by essentially providing the same credentials as the AWS user that generated the URL. A neat thing with these URLs is that they can be used multiple times. So you can use them infinitely up to a specified expiry date and time. So when you do generate them, you say, hey, I want this to expire at this date and this time, and then up to that particular date window, they can be used over and over again. When you're creating them, there's different things you specify. You specify the bucket, you specify the object keys, you specify the HTTP methods that can be used, and, like we just mentioned, you set an expiration time interval. So these are the important features or portions of the URL when you're creating them. Now, I know it can be a little confusing. Presigned URLs can be tricky. Let's have a quick high‑level architecture diagram on how this might be used. On the left side of the diagram, we have an unauthenticated user. Over on the right side, we have an AWS account where we have an IAM user that has access to our bucket, and we have our S3 bucket in there. First thing we would do is as an IAM user is we would generate that pre‑signed URL with all of that information we just talked about. After you successfully generate it, the temporary URL is returned to you for you to share or use as you see fit. From that point, the user would then share the URL to our anonymous unauthenticated user or software. That unauthenticated user or application could then use that URL in any of its RESTful calls, and it can use it to access objects or put objects until the expiration occurs that we set during the creation. So this is a very high‑level view of how that works. You generate the URL, you get the URL, you share the URL, and then you use the URL. Now, presigned URLs are a popular exam scenario. So let's look at three exam scenarios that could come up that you need to be aware of. Scenario #1, you might have an expired URL resulting in failed downloads when attempting to get objects. So the scenario could mention, hey, you have a user that generated a valid URL and they had permissions to do so, but the URL is not working. We're getting an access denied. Well, that could be because the URL expired. So once it's expired, then it won't actually work, and you'll get access denied messages. Scenario #2, an IAM user who generated the URL does not actually have the correct intended permissions, which could also result in an access denied. Now, I know this sounds impossible. It sounds confusing because you're probably asking, well, how could they even generate the URL if they don't have S3 permissions to do so? Believe it or not, it is possible. I do wish AWS didn't make it possible, it'd be far less confusing, but this is a possible scenario. So if you get an access denied message and the expiration is not expired yet, then this might be one of the potential reasons. And the final scenario here, generating URLs to allow users to access premium content behind a paywall. So maybe you have a subscription service where you're hosting media like videos or photos. Well, you can use these and generate URLs to give them to paid users that have paid for your subscription. This is a perfect use case for pre‑signed URLs. Now, that's going to go ahead and do it. We reviewed pre‑signed URLs during this clip. Please be sure to remember some of these exam scenarios. Remember the overall flow of creating it, as well as some of the required information. Let's go ahead, we'll wrap this up, and then coming up next, we're actually going to have a demonstration where we create and share a pre‑signed URL.

Demo: Sharing an S3 Presigned URL
Let's get started with the next demonstration where we're going to look at creating and sharing and then using an S3 pre‑signed URL. In this demonstration, we're going to basically mimic one of the diagrams we used in a previous clip. We're going to log in as an IAM user, and we're going to have a bucket that's already created for us. Within that bucket, we're going to have an object, and using that object, we're going to generate a pre‑signed URL. Once we generate that URL, we're going to see a temporary URL is there for us to copy. We're going to then use it, or share it, in other words, and we're going to use it within an unauthenticated session within a private window. That's going to do it for an architecture overview. Let's go ahead and jump into the console now and get going. All right, I'm in my AWS Sandbox environment here. I've loaded up my S3 service. Let's get started. I've already created a basic general purpose bucket here called premium‑content‑bucket, and this is meant to represent maybe something like a paywall where you have an application that uses subscription‑based models to provide viewers maybe some videos or other premium content, like pictures. So what we're going to do is we're going to upload a object into this bucket. So let me go ahead and drag and drop an object now. All right, so I'm dragging and dropping in this PNG file here. We can see it's 1.8 MB. Let's click on Upload. Okay, there we go. So it's now uploaded. We have the PNG here. We can see the Properties. Now from here, I'm going to go to Object actions, and I'm going to share with a pre‑signed URL. So when I select this, you're going to see this pop up. Now when you're doing this via code or some type of infrastructure as code, obviously, things are going to look slightly different because it's all API‑based and there's going to be flags and different options. Using the console really simplifies it, which is the point of the console. So let's walk through these options. The first thing we have here is an information block. It's saying, hey, anyone with this URL can access the object it's granted for as long as it's still valid. So, this goes along with some of those use cases and scenarios we talked about. The second portion, or the first portion we can actually change, is the time intervals. So you can specify minutes or you can specify hours for how long this pre‑signed URL lives for. So it can live for 1 to 12 hours, or it can live for 1 to 720 minutes. So 720 minutes is 12 hours. What I'm going to do is demonstrate what this looks like while it's valid and then demonstrate what happens when it expires. So I'm going to say this URL is valid for 1 minute. Now that I've specified the expiration time, I'm going to click on Create presigned URL, and there you go, we now have it. So what I'm going to do is click on Copy presigned URL. I'm going to go to my other private window, and I'm in a private Firefox tab. Now, the other one was Chrome, and I loaded private Firefox just so I can prove that there are no authentication mechanisms in place. So I'll paste this URL, I'll hit Enter, and our content loads. Now, in this URL, we'll see we have the bucket, and then we have some other information in here, including a security token. The security token is what grants us temporary access to this object, as long as it is valid. Now, I'm not going to scroll through all this. You can see when I hover over, it's a very long process. But if I zoom in right here, one key thing I want you to see is the expiry information. You see there's a 60 there. This is the number of seconds that this is valid for. So now it's been over 60 seconds. Let's refresh this. Well, there you go. Access denied because the request has expired. So this temporary pre‑signed URL, which was good for 60 seconds, has already expired. And there you have it. That's how easy it is to create a pre‑signed URL to share with anonymous users. Let's go ahead and wrap this clip up, and we can move on whenever you're ready.

Fine-grained Access Control with S3 Access Points
All right, welcome back! In this clip, we're going to look at S3 access points. An Amazon S3 access point is a method to simplify data access for any AWS services or applications that store their data in S3 buckets. Here are some important concepts regarding access points. When you create an access point, which is an endpoint within your AWS account, they're going to appear as named network endpoints via a DNS name, and they get attached to your buckets. You use these for performing S3 object operations, so things like get and put objects. Each access point is going to have its own permissions policy and its own network controls that get assigned to it. So there's multiple ways to control access. These customized access point policies are going to work in conjunction with the bucket policy that they're attached to. So there's multiple layers of security here. Let's actually look at using them in some scenarios. You can configure the access point to only allow traffic from a VPC to access bucket objects. Now, this requires a VPC endpoint to do so, but this is a common scenario on the exam. If you want to restrict bucket access to a specific VPC, you can do it via access points. You can also configure a custom block public access setting for individual endpoints instead of the entire bucket. So maybe you want to allow public access for a website hosted in a bucket, but there's some other files that are in the same bucket for some reason and you don't want those to be public. Well, you can implement block public access settings and force all traffic to go through that S3 access point. Now, these can be a little confusing, so let's go ahead and let's have a quick architecture diagram. In this diagram, we're going to have two different users that belong to developer and finance user groups. This first one is a finance user. So, using an access point, which is right here in the middle of the diagram, we're going to say, hey, these finance users are only allowed to get objects from their specific prefix. So on this access point, we can grant read‑only access and we can specify that we only want to allow the /finance prefix to be accessed. We can also have a different access point for a group of developers. So in this developer access point, we can say, hey, we want to be able to read and write data to their prefix only. So we grant access to their access point, and then access point has a policy that says, hey, they can only write to the /development prefix. They can't see or do anything else anywhere else within the bucket. So they're using the same bucket, but we're fine‑graining and fine‑tuning the controls using the access points, and then each user group would have their own access point. And the last scenario here is restricting VPC access. Remember, you can say, hey, I only want access to this bucket to come from a VPC. Well, you can do that by implementing a VPC endpoint, pushing traffic to S3 through that endpoint, and then having an access point for S3 that requires traffic come from that VPC endpoint. Again, I know this can be a little confusing. The big thing to remember here is that access points are meant to allow you to simplify and customize access to your objects and data. For a lot of those scenarios you might be asking, well, hey, can't I just use a bucket policy to do that? And yeah, you very well could. However, by having separate access points, it's meant to simplify that process. It's also meant to help you have a better access control. So in summary, if there's ever a scenario on the exam where you need to simplify and customize access to S3, you might want to consider S3 access points. Remember, you create and attach access points to your buckets, and then you customize them via different policies to customize the access that's allowed. Let's go ahead and wrap up here, and then coming up next, we're going to explore some other security features in S3.

Transforming S3 Objects with Object Lambdas
In this clip, we're going to explore S3 Object Lambdas, and Amazon S3 Object Lambda is a feature and function that's meant to easily allow you to add your own code to S3 GET, HEADE, and LIST HTTP requests in order to modify or even process data before it's being returned to an application or user. These all work by leveraging lambda functions, which again are the serverless platform for event‑driven architectures within AWS. Now let's talk about actually using these. Again, these use lambda functions, and this is the serverless offering where you can run simple code functions to execute a workload. They allow you to not worry about managing anything else; you only have to worry about the code itself. In order to use these, you have to create an Object Lambda Access Point. So remember, we just covered S3 access points earlier. Well, to use these, you have to create one for Object Lambdas. You must also, of course, create the lambda function that's getting called. So, the lambda function would store the business logic that's going to be performed during these calls. In addition to these, you also have to create the actual S3 Access Point itself. This is what the lambda will actually make object calls through. Let's look at some use cases for object lambdas. These are very useful for removing sensitive information from data before it gets returned to a client. So maybe you need to remove PII, or personally identifiable information, or maybe some credit card information, etc. Well, you can do that via this feature. You can also create a customized view of objects in S3 that get returned from a LIST request. So maybe you want to perform a filtered list or maybe you want to hide objects from people that make the calls. This is a valid use case. And thirdly, you can use them to convert data formats. So maybe you have objects stored as CSV files, and when they're being returned to your user, maybe you want to convert them to a tab‑separated file instead. You can do things like this using an Object KLambda. Again, these are a little confusing, similar to access points. It took me a while to actually grasp how they work. Let's have a quick architecture diagram on how the flow would look. Let's assume we have an HR user here. This HR user is wanting to get data from our Pluralsight bucket. So what they're going to do is they request an object from the bucket, and that request goes into our S3 Object Lambda Access Point. So this is the first entry point for this request. After that, the redacting function that we have, which is our lambda function performing the business logic, is going to get invoked, it's going to run, and it's going to request the object via the S3 Access Point that we created. That object is then going to be retrieved via that S3 Access Point that we have in place, and it's going to be retrieved, and sensitive data is meant to be redacted by that lambda function, and that transformed object is then delivered to our client without any sensitive information in place. So, this is the general flow. The request starts at the S3 Object Lambda Access Point, it triggers our function, that function makes a request to our object through our S3 Access Point, and then the flow continues where it performs its logic, it transforms or removes information, and then returns the object to the user. The big thing to remember here is if you need to transform S3 objects before returning them to a client or a user, you want to think S3 Object Lambdas. These are perfect for those use cases! Now that's going to do it. Let's go ahead and wrap this clip up here. Again, try to remember some of the use cases specific to S3 Object Lambdas. Let's cut here, and then I will see you in an upcoming clip.

Using S3 Locks to Meet Compliance Requirements
Let's get going with talking about using S3 locks to meet compliance requirements. An S3 Object Lock is something you really need to know regarding security in your S3 buckets. You use this feature to store objects using a write once, read many model. Now the abbreviation for that is a WORM, and you need to know that. What this can do is it can help prevent objects from being deleted or even modified for a fixed amount of time, or even indefinitely. Now let's talk about different modes within Object Locks. There are two different types, there's governance and compliance. So let's look at governance first. Governance mode for Object Locks are where you protect the objects against being deleted by most users. What this means is regular users cannot simply overwrite or delete an object version, and they can't alter the lock settings itself. Those particular actions require specific administrative privileges that are granted to specific users. Now, you can grant some users different permissions to alter the retention settings. And if you need to, you can even allow some to delete objects if it's absolutely necessary. Overall, in a summary, this is a good mode for trial runs of Object Locks. The next mode was compliance mode. Now, this is where a protected object version cannot be overwritten or deleted by any user in the account, including the root user itself. So this is why they call it compliance mode. When an object is locked in this mode, its retention mode cannot be changed, and the retention period cannot be shortened. So once you choose this, once you set the settings, it is in place until it expires. This mode is perfect for ensuring object versions are never overwritten or deleted for the entire duration. This is going to be the unbeatable final boss for locking objects. Remember, governance mode allows you to grant specific permissions and make some changes if you really need to, compliance mode is the final boss, you cannot beat it. Now, for both of these modes, versioning has to be enabled. So if you want to use Object Lock, remember, you have to turn versioning on. Now, versioning is really a recurring theme for being enabled for a lot of different features, so it's pretty easy to remember. A majority of features like this require versioning. Now let's talk about retention periods. A retention period is a period of time that you set to protect your object version. So you're essentially saying, hey, for this fixed amount of time, I want to protect my object versions. When you do this, a timestamp is actually stored in the version's metadata, and that's what indicates when that retention period expires. So it's actually a part of the object. After your retention period does expire, you can go ahead and then overwrite and delete that object unless you put something that is called a legal hold on the object version. Now, let's talk about those legal holds. A legal hold is also something that is meant to prevent versions from being overwritten or deleted. However, these don't have associated retention periods, so they stay there until they are manually removed. So this varies a little bit compared to the governance and compliance modes we just looked at. These stay there and they don't have any retention periods. Now, when you do these, you have to be able to have the PutObjectLegalHold permission for IAM. So any user that is designated to actually put these into place needs this particular API permission. Next up, let's look at Glacier Vault Locks. Now, a vault lock allows you to easily deploy and enforce compliance controls for different S3 Glacier vaults where you put in a vault lock policy. Now, with these policies, you can specify different controls, like that WORM model that we looked at, and you put it in the vault lock policy, and that lock policy is then locked from future edits. So once that vault policy is in place and that vault is locked, that policy is not modifiable. It can no longer be changed by anyone. Really, the big summary for this entire clip, any scenarios on the exam revolving around data retention and compliance against tampering likely involve one of these solutions. And that's going to do it. Do your best to remember the difference between governance and compliance modes. Remember you have to turn on versioning. Also, remember the vault locks for Glacier and the Object Lock legal holds. Now let's go ahead and wrap this clip up, and then coming up next, we will have some summaries and some exam tips.

Module Summary and Exam Tips
Okay, congratulations! Thank you for hanging in there once again. We just got through this module. Let's wrap things up with some summaries and some exam tips. Remember S3 buckets are private by default. This means all objects in it are private by default as well. What this means is you have to explicitly allow people and principals to get access to your objects. Remember Object ACLs. You make individual objects public using Object ACLs, and you have one ACL per object. You also have bucket ACLs. These use S3‑specific XML policies to manage access to the entire bucket. We also covered bucket policies. You can make entire buckets public or private using bucket policies, and you can specify specific prefixes to allow those to be public. In the end, please remember bucket policies are going to be favored over using any ACL type if it's possible. You should always use a bucket policy if you can. Moving on to a bucket policy example. You have to be able to identify what policies are doing for the exam. So please take the time to practice parsing and looking through different bucket policies and really understanding what's going on. A very important thing to know for bucket policies is on the bottom of this one. Remember the difference between the two ARN types and how they operate. Remember there are bucket level and object level ARNs. Continuing on the bucket policy train here. Let's talk about conditions. Two big conditional blocks that you should really be aware of for the exam. The first is the server‑side‑encryption block. This is where you can require the header be present to ensure that the object is encrypted by AWS before it gets stored. So this allows you to specify what type of encryption you want at rest. There's also the SecureTransport block. So this is a conditional key that's going to check to make sure that your requests that are being made to your S3 bucket is using TLS. This is commonly used in bucket policies to deny any requests if they're not using TLS. And the last thing here regarding bucket policies, remember that S3 has a Block Public Access setting that you can easily turn on, and you can use this to override any ACL or bucket policy that might be in place. Next, be familiar with encryption at rest methods. First of all, remember all new buckets have encryption at rest configured by default. In addition to that, you do need to be familiar with the different methods that are offered. You have server‑side encryption S3, where there's S3‑managed keys being used, and this is the default method for new buckets. There's SSE, or server‑side‑encryption KMS, where you specify a specific KMS key to use, and this is going to have cost associated and quota limits associated. There's also SSE customer‑provided keys. So this is where you manage and rotate your own cryptographic keys, and you have to provide three specific headers in order to use this. And lastly, client‑side encryption. This should be the easiest to remember. It's because it's all happening on the client side. This is where AWS has zero idea that your objects are being encrypted and decrypted in the first place. They only see them as normal objects. Now, an exam pro tip here. Remember, you can use bucket keys to save on KMS costs if you're using server‑side encryption via KMS, and it also helps avoid potential quota throttles. Remember, bucket keys essentially generate a temporary short‑lived copy of a key for the S3 service to use. Also, remember S3 Access Points. These are very important for you to understand, to control access to your data. I'm not going to run through all of this diagram again, but please review it and really make sure you understand it. The long story short here is that you use S3 Access Points to simplify and customize access to S3 objects and data. That is the big thing to keep an eye out for. If you need customization and you need to control access at a granular level, you probably want to use an access point. Also, remember, if you need to view details about requests being made to your buckets and your objects, you can use S3 Server Access Logs. Remember when you do this, you specify a destination bucket that has to be in the same region. Following up with that, we have pre‑signed URLs. Please remember some of the scenarios we covered. The first one was with dealing with expired URLs. This can result in failed downloads when attempting to get objects. Remember we looked at an expired URL in that demonstration clip. Number two, there might be a scenario where a user generates the URL, but they don't actually have the correct intended permissions, so this can result in an access denied. Yes, remember, this is possible. It's confusing, but it is possible. And thirdly, generating URLs is a perfect way to allow your users to access premium content that you might have behind a paywall. The next subject was S3 Object Lambdas. The long story short for these is if you need to transform objects before returning them to a client, think using S3 Object Lambdas. These are perfect for transforming data types, or even removing stuff, like personally identifiable information. So, names, addresses, etc. If you need to put extra protection into place for deletions, remember MFA Delete. This enforces that MFA is required to perform different important actions within S3. Like other features, you have to enable versioning on the bucket in order to enable this feature. Also, remember the bucket owner, which is the root account, is the only one who can actually enable and disable this feature for the bucket. This is a tricky thing to remember. It has to be the bucket owner, the root user account, that does it. We just got done looking at Object Locks, so let's actually review those really quickly as well. Remember, you use these to store objects using a write once, read many model, so WORM. Object Locks can be placed on individual objects, or they can be applied across the entire buckets if you need them. Also, remember the two modes, there's governance mode and then there's compliance mode. Within governance mode, users that are normal can't overwrite or delete object versions or alter lock settings unless they have special permissions assigned. These are good for trial runs. Compliance mode is the final level. This is where protected object versions can never be overwritten and never deleted by any user, including the root user. The easiest way to remember this is compliance is for compliance requirements. Governance is where you're really kind of monitoring and testing things out. A quick tip here. If there's any type of scenario about data retention and compliance for your objects, you're going to want to leverage one of the solutions. And then lastly we covered Glacier Vault Locks. This is very specific, so it should be kind of easy to remember. These are meant to allow you to easily deploy and enforce compliance controls for Glacier vaults. You do it via a vault lock policy. Within these vault lock policies, you can specify specific controls, such as that write once, read many model, and you can lock the policy from being edited in the future for a specific amount of time. The biggest thing to remember with these is once they are locked, that policy can never be changed until that time expires. So these are very good for compliance specific to Glacier vaults. Now, that's going to do it for this Module Summary and Exam Tips clip. Let's go ahead and wrap things up. Hopefully you learned a lot. Feel free to go back and review whatever content you think you might need to go ahead and get a better understanding on. But for now, we're going to end this module here. Thanks for hanging in there. Whenever you're ready, we will see you in the next module!

Amazon Route 53: Zones, Records, Policies, and Health Checks
Global DNS with Amazon Route 53
Alright. Welcome to this next module where we're going to start looking at a service called Amazon Route 53. In this first clip, we're going to have a quick review of global DNS and what the Route 53 service is. The very first thing we need to do here is have a review of DNS. If you've used the internet, which if you're watching this, you've used the internet, then you've used DNS. DNS is meant to convert human‑friendly domain names into internet protocol addresses or IP addresses for our computers and networks to use. An example would be us navigating to pluralsight.com and it is getting translated to a public IP address like you see here. Remember, IP addresses are what are used by computers on networks in order to identify each other. These come in two common forms, there's IPv4 and there's IPv6. IPv6 allots for many, many more usable IP addresses compared to IPv4; however, version 4 is the more common format. Moving on to more specific information regarding DNS, top‑level domains and second‑level domains. If we look at common domain names, you're going to notice a string of characters separated by dots or periods. An example of this could be google.com or www.pluralsight.com. These are good examples of domains. Now, you need to understand the difference between a top‑level domain and a second‑level domain. The last word in a domain name represents the top‑level domain, or in other words, for short, the TLD. So in this example, www.pluralsight.com, this .com is the top‑level domain. The second to last word in the domain name is known as the second‑level domain, or the SLD. In our example, this would be pluralsight. Anything in addition to those two is considered a subdomain. So our subdomain in this example is www. Now that we've reviewed DNS, Amazon Route 53 is a highly available, fully‑managed, and fully‑scalable domain name system web service within AWS. One of the biggest benefits of using Route 53 is it offers an SLA of 100% availability. In other words, it is always available for you to use. Also a fun fact, Port 53 is the port number for DNS traffic, thus the name Route 53, it's a play on words or a play on numbers, if you will. So what does Route 53 offer? Well, there are three primary flows or features that Route 53 offers for us to use. We can register new domains and old domains, we can implement routing for our DNS, and we can even leverage it to perform health checks for our resources that are publicly facing for our clients to use. Now, Route 53 is an authoritative DNS service, so what that means is that you control all updates and configurations of your DNS records. It's very important that you remember it is authoritative. You need to be familiar with the term registrar, specifically, a domain registrar. A registrar is an authority that can assign domain names directly under one or more top‑level domains. These domains are then registered with the InterNIC, which is a service of ICANN, and these enforce uniqueness of domain names across the internet. The big thing to take away from this blip here is that you can register domain names with Route 53. Now that's going to do it for this review on DNS and an intro into Route 53. Coming up next, we're going to start looking at managing domains within the service.

Managing Domains with Route 53 Hosted Zones
Alright. Let's talk about managing domains with Route 53 hosted zones. Hosted zones is a very important term you need to be aware of. What these are are containers for your records to specify how you want to route your traffic for your domains and your subdomains. There are two types of hosted zones within Route 53. The first is a public hosted zone. This is meant to contain records for routing traffic on the public internet. We then have private hosted zones. These contain records for routing traffic within private networks like an Amazon VPC. The easiest way to remember these, public hosted zones for the public internet, private hosted zones for private networks or Amazon VPC networks. Now, with a public hosted zone, again, you use these for publicly resolvable domains and you have to either own them or have dedicated control over them. Private hosted zones are for your private domain names. These get associated with your VPCs which allows your VPCs to then query and resolve the different DNS queries. Now, you are charged for using zones, so they're not free to use. It's 50 cents per hosted zone per month for your first 25 zones, and then it's a small charge for additional zones per month. You're also additionally charged for DNS queries. Now, we're not going to cover the exact math on that because it's not something you'll get tested on. Just understand it's not free to use, and you do get charged. Okay. That's going to do it for this clip. Please remember the two different hosted zone types, public and private. Public are for publicly resolvable domains, private are for internal networks. Let's go ahead. We're going to wrap this clip up, and we'll move on whenever you're ready.

Demo: Viewing Public Hosted Zones
Alright. Welcome to this very short demonstration where we're going to view a public hosted zone. I'm in my Route 53 dashboard here, and on the left side in my sandbox account, I'm going to select Hosted zones. Once we're in here, we're going to get a list of hosted zones that belong to this particular AWS account. Currently, we have one, and this comes with each sandbox environment. So we have our hosted zone name, which is the account ID .realhandsonlabs.net. .NET is the top‑level domain. We have real hands‑on labs, which is the second‑level domain, and then this is our subdomain, so our account ID is the subdomain. You'll notice it's a public hosted zone created by Route 53. We currently have two records. There's no description, and then over here is the important part. We have a hosted zone ID. So what I'll do here is I'll select this. Let's navigate in here, and under Hosted zone details, you can see this hosted zone ID. This is the ID you use when you're leveraging some type of Infrastructure as Code and typically any command‑line interface calls. You're very rarely going to recognize or use the hosted zone name. You're going to use the zone ID, so please keep that in mind. After I close the tab, we can see here on one line some important information regarding the public hosted zone. We see name servers, we can see the list of records, etc. Now, if you wanted to, you could create your own public zone here, so let me click on Create hosted zone. I'll give it a name. I can give it a description, and then you select the type. Now, I'll click Public hosted zone, and I'll click on Create, and check it out, I've created a new public hosted zone. Now, here's the catch, this won't work because I don't own this subdomain, and this has not been delegated to me to have control. So do remember this, you can create a public hosted zone for any domain name; however, it doesn't mean it's going to work. You have to own or control the domain that this is hosted under, so I have no access to pluralsight.com and I, for sure, don't have access to this subdomain. So yeah, this exists, but it's not a real working public zone. And with that being said, that's how easy it is to create a new public hosted zone. We looked at some of the information that goes along with it. Let's wrap this up, and in an upcoming clip, we're going to explore private hosted zones.

Demo: Creating a Private Hosted Zone
Okay. Welcome to this demo where we're going to work on creating a private host zone, we're going to associate it with a VPC, and we're going to test DNS resolution. I'm in my Route 53 dashboard. I'm going to go to Hosted zones here and let's create a new private hosted zone. So I'm going to create, we'll give it a name, we'll leave the description blank. I'm going to go down and under Type, I'm going to select private hosted zone. After I do this, you're going to notice I have to associate this in order to use it. Remember, you always associate private hosted zones with VPCs in order to use them. Another thing to realize is remember, Route 53 is a global service, so this can go ahead and be associated in any supported region for any supported VPC, so that's a really big exam tip to remember. Route 53 is global, so this can work around the world. So what I'll do is I'll choose my us‑east‑1 region because that's where my VPC lives. I'm going to choose my custom VPC here VPC‑A. I go down and I click on Create. Awesome. Now, for this, what I do have to do is navigate the VPC, and I have to make sure that my VPC supports DNS names and resolution. So under VPC here, we're going to go into VPC and you'll notice DNS hostnames is enabled, DNS resolution is enabled, so this is supported. This won't work if these were set to disabled, so please keep that in mind as well. So, what I'm going to do now is I'm going to navigate back to pluralsight.internal, and this is a little out of scope. You might not know how to create records, but don't worry about that. We do that here later on where we talk about records much more in depth. For now, just try to follow along the best you can. Under my private hosted zone, I'm going to click on Create record, and let's test this out. What I'm going to do is go to Instances up here. I've loaded my running instances, and I've created a mimic app and a mimic database server. So what I'm going to do is assign one of these a private DNS name and then test it. So let's test with the database. I'm going to go to database, I'm going to find my private IP here, go back to my Route 53, and I'm going to call this database.pluralsight.internal. We'll leave a record, enter my value, I'll set this to 1 minute since it's private, and I'm going to click on Create records. Awesome. So now, I should be able to reference database.pluralsight.internal and resolve this traffic. So what I'll do is go back to Instances, select my App Server. I'm going to connect via Session Manager. Once I select Session Manager, I'm going to click on Connect here, and let's test that I can resolve that database record. So once I'm in here, very simply I'm going to do a quick lookup and a quick ping. So let me go ahead and zoom in a little bit more. I'm going to run an nslookup on database.pluralsight.internal. Perfect. So it's actually got the address. Let's go ahead and try and ping database.pluralsight.internal. Awesome. So this is resolving our private associated hosted zone and that's going to do it. So that's how easy it is to use a private hosted zone for a VPC. Remember, when you create one and you want to use one, you have to associate them to the VPCs where you want to resolve traffic in. Let's go ahead and wrap this demonstration up, and we'll move on to the next one.

Amazon Route 53 Records
Alright, let's get started discussing the all important Route 53 records. A DNS record within Amazon Route 53 is meant to allow you to dictate how and where you actually want your DNS traffic routed. So what makes up a record? Well, there are several pieces of information that you have to configure when you're creating a record. The first is the domain or subdomain name. So an example of this could be pluralsight.com. You then have the record type. Now, we've listed here several common record types that are within Amazon Route 53. Now we're going to cover important record types here coming up shortly, so don't worry about this too much, just understand at a high level, you have to specify the record type. After you pick your record type and your domain or subdomain name, you then have to specify the value. In other words, what resource are you wanting to route traffic to? So this can be things like a simple IP address or even an AWS resource alias, which again we're going to cover here in a moment. You then choose the routing policy, so this is going to be a policy that sets how you want to respond to a query for your DNS traffic. We're going to cover routing policies much more in depth here coming up as well, but understand you have to set a policy to choose how you want to route that traffic. And then lastly, a time to live value, otherwise known as a TTL. This is going to be the number of seconds that you want DNS resolvers to actually cache your record values. Typically, if you want to be a good person on the internet, especially with DNS, you're going to set this number a little bit higher than you would think. You don't want a low TTL because then you're not being responsible on the internet. Moving on, let's actually discuss some of those record types. I told you we would discuss them. Let's do that now. There are four primary important record types you need to know. The first is an A record type. This is meant to route traffic to an IPv4 address. You then have a four A or an AAAA record, and this is meant to route traffic to an IPv6 address. The easiest way to remember these is that the four A's are longer than the single A and IPv6 addresses are significantly longer than IPv4. You then have an NS record, so this is a name server record. This is meant to identify the name servers for your hosted zone. And then lastly, an alias record. Now, this is going to be a special record within Route 53 that allows you to route traffic to other AWS resources with their own DNS names. Now, you might be asking, well, what are some examples of these record type values? Well, let's break that down. For an A type record, you could use this to route traffic to a public IPv4 address of an EC2 instance. So for example, something like this. For a four A record, or an AAAA record, this could be used to set a public IPv6 address for an EC2 instance. Remember, all IPv6 addresses are publicly resolvable. An example of something like that could be what you see here. An example of an NS record is going to be something like this. So this is an actual real‑world example of an NS record within a Route 53 hosted zone. And then lastly, an alias example. You could set this to route traffic to a public DNS name of a CloudFront distribution, so this would be a real world example of something like that. So you can map something like pluralsight.com to this particular alias address and then it will route traffic for you. Moving on to an additional record type, a CNAME record. You have to understand how these work and what they're used for. A CNAME record is meant to map a DNS name to another domain, so a real example could be www.acloud.guru, and you want to remap that to www.pluralsight.com. Some very important information regarding the exam and CNAME records, these cannot be created for the zone apex. The zone apex is the domain that you own, so the second‑level domain, so an example would be pluralsight.com. This is the highest level you can define records for, and because of that, CNAMES are not supported at that level. Now, you really need to understand the difference between an alias record and a CNAME record. Remember, aliases map to other AWS resources. CNAMES map to other domains. Given the choice on the exam, you should likely choose an alias record over a CNAME record. This is a big exam tip. Generally speaking, aliases are going to be the one you want to go with. We reviewed several of the important record types you need to be familiar with. We talked about what makes up a record, and with that out of the way, let's move on to some upcoming clips where we're actually going to go ahead and create our own records in Route 53.

Demo: Creating an “A” Route 53 Record
Alright. Let's get started with this demonstration where we're going to look at creating an A record within Route 53. In this demonstration, we're going to use a pre‑existing public hosted zone. We're going to have an already‑running EC2 instance with a public IP address. What we're going to do is we're going to use that public IP to set as the value for our A record in our hosted zone, so let's go ahead and jump into the console now. Okay. Welcome to the AWS sandbox. I've loaded up my Route 53 dashboard, and before we begin, I want to go to instances and show you our EC2 instance that we're going to use. Here it is. It's a simple web server, and if I go to this public IP address, we're going to get a very simple web page that loads. So once I say yes, that's fine. There we go. It just lists the internal hostname. So with that understood, let's go ahead, go back to Route 53. I'm going to go into my hosted zones. I'll find my public hosted zone that's in the account, and I'm going to select it. Now, once I'm in here, I want to go in and create a new record, so I'm going to click on Create record and let's give our record a name. I'm going to call this test. So test. account number .realhandsonlabs.net, so this is a subdomain of the subdomain. You'll notice we can choose our record type and there are tons of them in here, but we're going to leave it as an A record type because this is an IPv4 address. So for the value, I need to enter the public IP, so I'm going to copy this from EC2, I'll go back, I'll paste it in here, and let's go ahead and create our record. Now, what I'm going to do here is I'm going to open a new tab, and I'm going to navigate to a site. Now, I use this site to check propagation around the world, it's very handy, so feel free to use it if you want, but this is what I'm going to use to test resolution of records. So what I want to do is go back to my realhandsonlabs public hosted zone. I'm going to find my record here, I'm going to copy it, go back to my DNS Checker, paste it in here, and this is an A record, so I'm going to select A, I'm going to click on Search, and there we go. At least for the high level we're getting resolution of that public IP, so 54.173.59.117, that's exactly what we have. So in theory now, what I can do is navigate to that particular site. So if I go back to Route 53, let's say go to, there we go, it's now resolved to the same exact site as if we used the public IP, so this is working. We've now created an A record that resolves to our public IPv4, which is mapped to our EC2 instance. Now, before we move on and wrap this up, I want to show you what happens when you leverage multiple values for the same record, so I'm going to select this record. I'm going to edit it. I'm going to go back to EC2, and I'm going to launch a few more instances like this. So what I'm going to do is click on launch more like this. I'm going to create three total instances in addition to that, and I'm just going to click on Launch instance. Now, what I'm going to do here is I'm going to cut forward until these are all up and running and ready to go, and then I want to show you what happens when we use multiple values for the same record. So I'm going to go back here, I'm going to look for running instances, and I'll cut forward here in a second. Alright. Now, that was very quick. These aren't necessarily reachable yet, however, I can use them. So, what I'm going to do is I'm going to add the other public IPs to that same record. So I'm going to select the third one here, copy this, go back, and then paste it on a new line. Now, I'm going to do the same thing for the other two, so I'll choose the public IP of the second one, I'll go back into that on a new line, and then the final EC2 instance here, public IP, paste that in. So once I save this, we now have, when I select this, four values for the same record. So remember, if you do this, you get a random return with all of the values, so it might be this one that gets returned first or it might be this one. Now, the one thing about this is it's always going to show the same thing here because this is now cached based on my browser. So what I'll do is jump to my terminal and I'm going to show you what happens when we look this up on the command line. So I'm going to run nslookup, I'm going to go back, I'll copy the name, I'll paste it in, and there we go, we get four values. Now, when I run this again, notice that the order has changed. In the first call, the first address that got returned was this one. In the second call, it was this one, so this is what I mean when I'm saying that you get a randomized order of returned values. So in theory, with the second call, it would have used this address, and in theory with the first one, it would have used this address, and that's it. I just wanted to show you what happens when you have multiple values for the same A record. Let's go ahead and wrap this demo up here, and we will move on whenever you're ready.

Demo: Creating an “Alias” Route 53 Record
Alright, let's get started with this next demo. We're going to work on creating an alias record in Route 53. In this demo, we're going to perform something similar to when we created our A record. The only difference is we're going to use an alias record, and that alias record is going to front or resolve to a public load balancer which is in front of a private EC2 instance. Let's go ahead and jump in the console now. Okay. Let's get started. I've loaded up Route 53 in my sandbox environment here, and I've selected my public hosted zone that gets created for us. Now, before we actually create the records, let's look at the underlying architecture. Under EC2 instances, I've created this private web server. Now, it's called this because it doesn't have any publicly resolvable IP addresses, it's only got private IPv4 space. So this only allows HTTP traffic from a load balancer. Now that load balancer, which is here, is a public load balancer, so it's internet facing. This is what is going to listen for HTTP traffic, take that request, and then send it to our back end instance. Now load balancers are out of scope for this course, but we do cover them in great detail in another course within the same learning path. So don't worry about these right now. Just understand this is what listens for traffic on the internet over Port 80 it forwards traffic to our EC2 instance here, which is private. Now to show you this, if I go back to Load Balancers and I go to Details, let me copy this DNS name, I'll go to DNS Checker, and I'm going to go ahead and paste in the DNS name here and click on Search. Now, check this out, it has publicly resolvable IP addresses; however, these are not to be relied upon. These can change at any point in time. AWS tells you you should never rely on the public IPs for your public load balancers. That's where an alias comes in, so let's go ahead and get started. What I'll do is go back to my Route 53. I'm going to click on Create record here. We'll call this test.subdomain. Under record type, I'm going to click on A record. Now with this, I'm actually going to make this an alias record. So under alias, you can see, hey, you can route traffic to an endpoint in a region, so what I'll do is choose my endpoint, we'll say an elastic load balancer which is an application load balancer, and in my region, I'm going to look for us‑east‑1. Now, once I'm in there, I can choose my load balancer. Now, if you're following along, which I haven't walked you through load balancers yet, so you don't need to, but if for some reason you are, you need to get rid of this dual stack here. So I'm going to get rid of dual stack and then just use this. So this is now saying an alias record for test. should resolve to this DNS name. What this does on the back end is it takes all of these changing IP addresses and it manages them for us within our DNS hosted zone, so we don't have to track the everchanging public IPs. This is a perfect use case for an alias record. So we'll leave everything else the same. I'll click on Create record. Now, what we'll do is I'm going to select this test record, I'll copy it, I'm going to go back to my DNS checker and paste it, and let's search. Now, you'll notice we're getting the same IP addresses, and that's because our Route 53 DNS servers are resolving that alias record and showing us that, hey, these are the IPs that that DNS name resolves to for your load balancer, which is your alias value. So now what I can do is I can actually navigate to this and we get our website. So this is the website hosted on our private EC2 instance, which is leaving behind our load balancer. Now, the neat thing is we didn't have to worry about tracking the public IPs for this load balancer. In fact, if you notice, AWS doesn't even give you the option to look up the IPs. You have to manually look them up, and then obviously, since you have to do that, you never want to rely on them. These can and will change without notice, so that's exactly why we create alias records for those AWS resources. Okay. With that being said, you see how easy it is to create an alias record. Remember, these are perfect for use cases where you need to map to changing IPs for AWS resources on the back end like a load balancer or like a CloudFront distribution. Let's go ahead. We'll wrap things up here, and we'll move on.

Demo: Viewing “NS” Route 53 Records
Okay. Welcome to the cloud sandbox here. I've loaded up Route 53, and I've loaded my public hosted zone that goes along with the sandbox environment. Now, within here, under zone details, we can see our name servers. Now, this is going to be a very short clip. These name servers are super important. These are dictating what servers are actually doing the query resolution, so when we add records into our public hosted zone and someone tries to hit our public hosted zone record, these name servers are the ones responsible for returning the actual results of that record. You should make a backup of these immediately when you create a new public hosted zone because if these change and they're not verified and validated, then you won't be able to resolve traffic appropriately for your hosted zone. Now, these are also used for subdomain delegation. Now that's a little out of scope, but understand you'll use name servers to delegate subdomain control as well. Now, like I said, this is a short clip. Let's go and wrap this demo up here. You saw how easy it is to reference your name servers for your hosted zones, so we'll go ahead and wrap things up, and then we'll move on.

Amazon Route 53 Routing Policies
Alright. Welcome back. In this clip, we're going to look at Amazon Route 53 routing policies. A routing policy by definition per AWS documentation is simply a setting for your records that is going to determine how Route 53 actually responds to queries for your DNS records. Let's review some of the important policies you need to be familiar with for this exam. First up is a simple routing policy. This is going to be the simplest version of a routing policy they offer, thus the name. Usually, you're going to use this for single resources or single IP addresses. If you use these in conjunction with alias records, the alias records are going to point to a single AWS resource like an elastic load balancer on the back end. An important fact here is simple routing policies are usable within private hosted zones, so you can use these to route traffic within your private hosted zones to your private EC2 resources. And lastly, this is a tricky scenario here, these actually support multiple values for the same record, so you can have a single record with multiple IP addresses. Now, when you do this, it's important to call out that Route 53 returns all values that are set in a random order. What that means is your client is going to select a random value out of those random ordered values, so really keep that in mind. If you have multiple values with a simple routing policy on your single record, well, you're going to get a random value out of those different records. Moving on to a weighted routed policy. This is where you set multiple resources under a single domain or subdomain name. It works by allowing you to control the percentage or the weight of traffic that is routed to each of those back‑end resources. When you use this type of routing policy, the records have to have the same name and they have to have the same record type. After you implement those multiple records, what happens is you assign each record a relative weight to control the traffic. Now, it's important to call out the overall weight does not need to equal 100, so don't get confused if they bring that up on the exam, it's all specific to a relative weight. Now, a key tip here, you can set weights for your records to 0 if you want to actually stop routing traffic to that resource. So if there's a scenario where you need to stop routing traffic to a weighted resource, you can set the weight to a 0. Now, let's actually look at calculating the weights for this. How that works is this formula here. You take the weight for a single record and you take it over the sum of weights for all of the same records. Now, if that's confusing, let's look at an example calculation here. Let's assume we have four resources that we want to use. We see 1, 2, 3, and 4 here. Notice the record name is the exact same www.pluralsight.com, they're all an A type record, and they all have the same TTL value. Now, what changes here are the weights and the values, so you can see here the weight is different for every record. In addition to that, the values map to different resources, which makes sense. Let's just assume we want to calculate the weight for record number 4. Well, in this instance, we would take 50, which is the weight, over the sum of all weights, so in this case, it's an easy number. It's 50 + 10 + 15 + 25 which gives us 100. So in the end, what this is saying is, hey, for every one out of two queries, I want to send traffic to record number 4, so it would send traffic to that 192.0.2.40 address. Now remember, the overall sum does not need to be an even number like this. It could be any number, and you need to calculate the relative weight over the relative total. Please keep that in mind. Moving on, let's talk about a failover routing policy. This is where you're going to use this to route traffic to a resource that is healthy, and if that resource gets marked unhealthy, then it's going to respond with a secondary healthy resource. In other words, it's failing over. This is also commonly referred to as active‑passive routing, so you're using an active resource, and once that goes away, then that passive resource now becomes the active version. Now when you use this routing policy, it leverages and requires a health check to be put into place. Now, we're going to explore health checks later on, but understand with failover routing, there are health check requirements which really makes sense because you're checking the health to make sure a resource is healthy, and the resource can only be primary if that resource is healthy. In summary, primary resources are meant to be available the majority of the time, and this is perfect if you want a secondary resource or group of resources to be ready on a standby mode. Next up, multi‑value answer routing policies. This is where Route 53 returns multiple values for a single query. These, similar to the last one, also use health checks to return only healthy resources. The difference here is it's not a failover, so it's still returning all of the other values that are still healthy, unlike the active‑passive setup. Now, these are not meant to replace proper load balancing architectures. Please remember that. This is a DNS approach, and it's not meant for load balancing. Now, when you use multi‑values, eight or less healthy records get returned at one time, so it's never going to be more than eight values. It will always be eight or less depending on your setup. Now, that's going to do it for this clip on different routing policies. Please be sure you review this. You do need to understand all of these policies we just covered, and you need to know when you would implement them within your architectures. Let's wrap things up here, and we're going to move on to an upcoming clip.

Additional Amazon Route 53 Routing Policies
Alright. Continuing on the routing policy train here, let's talk about some additional ones you should be familiar with. First up, geolocation. This is where you choose where your traffic is sent based on the actual geographic location of the user making the query. Geographic location, in this case, means where the DNS queries originate from. So this is going to be perfect for a localization. So for instance, maybe you want to display different languages based on the origination of those DNS queries. This is a perfect policy to use. Or maybe you want to restrict access to different content based on location due to compliance laws. A use case would be you want all European queries to go to a web server hosted in eu‑west‑2. This is a perfect use case for geolocation routing. When you use this routing policy, you need to ensure you have a default record in place just in case there is no match to the location set in your policy, so you need a failsafe in place in case there are no matches. Please remember that. Now an exam pro tip here, these are not based on latency. Please remember that. These are strictly based off the location of the origination of the request. For latency, we actually have a latency‑based policy. These allow you to route your traffic based on the lowest network latency for the end user. This means it will be the region in AWS that provides the fastest response times. When you use this routing policy, you create a latency resource record set for the resources in each region where you're hosting your website, so you have to make sure you're still responsibly deploying to the correct regions to actually take advantage of this routing policy. Now, let's talk about some exam tips here. This can be a confusing one and it tricks up a lot of people. You want to consider this routing policy anytime latency is the primary concern for your architecture. Users are always going to be directed to the fastest, lowest latency resource, even if it's across the globe in a completely different region, that's why these get tricky. You do not want to confuse these with geolocation‑based routing policies. Again, if it's anything to do with latency, you want to use this routing policy. If it's something specific to the location of a user, then you might want to use geolocation. Next up, geoproximity routing policies. This is a policy that allows Route 53 to actually route traffic to your resources based on the geographic location of the users and your resources themselves, so it adds a layer of complexity. You can also optionally choose to route more or less traffic to a specific resource by giving it a value. This value is known as a bias. A bias is going to expand or shrink the size of the geographic region from which traffic is routed to a resource. Now, when you're expanding and growing sizes of biases, they have to be specific positive integers from 1 to 99. When you're shrinking the size of a bias, you have to be specific negative numbers from ‑1 to ‑99. So this is kind of self‑explanatory. If you're growing, it's a positive integer. If you're shrinking, it's a negative integer. Now, this is an example of a geoproximity mapping. You can notice there are five different regions we're sending resources to, and in this case, three would be the biggest map, but you grow and shrink these different mappings based on your bias numbers. Now, there's also something known as Route 53 traffic flow for geoproximity routing. You must use this feature to access the maps that we saw in the previous image, so to actually see a map, you have to use this feature. What happens is you use traffic flow to build a routing system that uses a combination of the location geographically, the latency, and availability to actually route traffic. You can build your traffic routing policies from scratch, or you can pick a template from a library and then actually customize it. On the right side here where I just highlighted is an example setup for a traffic flow. You can see it's an easy‑to‑follow flow of a diagram. We have the DNS type, the rules, and then within that, we have mapping, so we have our one region, and you can see there's a bias number, etc. The big thing to remember for traffic flow is that it's used for geoproximity routing, and you have to use it to actually access maps like we saw previously. Now that's going to do it for additional routing policies. Please go through, really review each of these, you have to understand these, and it's really important that you don't confuse them with each other. The one that gets confused the most is the latency based and the geolocation based. Let's go ahead and wrap things up, and we'll move on whenever you're ready.

Demo: Creating a Simple Routing Policy
Hey, welcome to this next demonstration where we're going to work on creating a simple routing policy within our record. I've loaded up my public hosted zone within my sandbox here, and before we dive in, let me show you what we're going to use as the value for our record. I've created an EC2 instance called public web server, and this hosts a publicly accessible simple web page that you can see right here. So with that out of the way, let's go ahead and test. I'm going to go back to Route 53, I'm going to create a record in my public hosted zone, and I'm going to give this a name. I'll call it simple. We're going to leave it as an A record type because it's an IPv4 address. I'll paste in my IP, so let me go ahead and copy that. We'll set it to 300 for TTL, and then under Routing policy is where we can select our different policies, so I'm going to select simple. I'll click on Create record and there we go. That's all we have to do. So now I can copy this, I can go to it, and this is going to navigate to our publicly accessible website here, and there it is. So that's how easy a simple routing policy is. Let's go ahead and wrap this demo up, and we'll move on.

Demo: Creating a Weighted Routing Policy
Okay. Welcome to this next demonstration. In this demo, we're going to work on creating a weighted routing policy. I've loaded up my public hosted zone in Route 53, and before we create the records and the routing policies, I want to go ahead and show you the underlying resources. So I've created four instances here called public web server. These all have the same website as you can see here that they're hosting. It just spins up some basic information. So what we're going to do is use these four instances to create our routing policy. So what I'll do is I'll go back to my Route 53, and I need to create a record for each one of these. So let me go ahead and create four type A records for each one. We'll call them web.subdomain. For this one, I'm going to copy the top public IP here. I'll put that in, and then under Routing policy, I'm going to go and find weighted. Now, what you're going to see here is that we have some more stuff we have to fill out. The first thing we have to do is we have to give the weight. Now, it calls out like you can see right here, it has to be between 0, so it'll route 0 traffic, all the way up to 255. Remember, the overall weight does not have to be an easy number. It doesn't have to be something like 100. For this demo though, I'm going to use 100 for simplicity. What I'm going to do is I'm going to give this particular web server a weight of 50. Now, for the total weight, we're going to have them equal 100. So we're going to mimic our clip where we review this and we assign some other smaller weights to the other ones. So I'll give this a weight of 50 and you have to give it a record ID, so this is a unique identifier for your record so you can identify which ones are which. So I'll go ahead and give this a record ID of primary, and then what we can do here from the same menu is I can add another record, so we'll call this web also. I'm going to copy the second public IP, I'll paste that, it's also weighted. I'm going to give this a weight of 25, we'll call this secondary‑1. I'll add a third record, so this will be our third resource, so let me go back, copy my third IP, paste that value, make it a weighted, we'll give it a weight of 15. We'll call this secondary‑2, and then I add my fourth and final, so web, go back, copy my public IP for my fourth server, paste that in the value, make sure it's a weighted routing policy, and we'll give this a weight of 10. So now what we've had is 10, 15, 25, and 50, so those all add up to 100 as the total weight. So in theory, a weight of 10 here should respond to 10% of the request, 15 should respond to 15%, 25 should be a quarter or one‑fourth, and then 50 should be roughly one out of every two requests. So I'm going to go down here. We'll call this secondary‑3, I'll click on Create records, and there we go. So now what I'll do is I'll navigate here, let me go ahead and go to this, we'll continue and it's bff04, so let's see what IP that maps out to, bff04, it's this primary address, so remember on our weighted values here of 50, this is the first one, so this is the one that should respond roughly to one out of every two. Now, with this, what's going to happen is some of this stuff is going to get cached, so this is not always going to return within the browser, so this is not necessarily the best test after the initial test. Now, since this is caching, the same thing is going to happen locally, but if this was different clients around the world, we would be getting different results. Now, one thing I can do here is let me go back, and I'm going to edit this weight to be 0, so let me select this record, I'm going to edit it. I'm going to set the weight to 0 because I want to say, hey, I don't want to pass any traffic to this primary server, I'll click on Save, and there we go. So now if I go to my CLI and I do an nslookup of that same website here, so let me get rid of the http, we get a different address because now we said, hey, we don't want any of our traffic to get routed to that primary server. In fact, now you're seeing different addresses come in based on the weights, so the more I run this, we're going to cycle through the different IPs, and this is all based off their relative weights that we set over here in Route 53. We also set the primary to 0, which says, hey, we don't want to send any traffic to this primary IP ever again until this is not 0, so we're not seeing that IP in any of these results, which is perfect. Now, that's going to do it for this demonstration. We just created a weighted routing policy. Let's go ahead. We'll end this here, and we'll move on whenever you're ready.

Demo: Creating a Failover Routing Policy
Alright. Welcome to this demo. In the demonstration here, we're going to create a failover routing policy. In this demo, we're already going to have some underlying architecture infrastructure in place. I've already deployed two EC2 instances, one in us‑west‑2 and then one in us‑east‑1. We're going to set up us‑west‑2 as the active and then us‑east‑1 as the passive record. We're then going to simulate a failure in us‑west‑2, and we're going to view how that failover traffic actually happens. So with that out of the way, let's jump into the console now. Alright, I'm in my AWS sandbox here and I've loaded up my Route 53 hosted zone that comes along with the sandbox. Before we begin creating records and policies, I want to review the architecture. I've created two different VPCs, one in us‑east‑1, one in us‑west‑2. I then went ahead and deployed a simple web server in us‑west‑2, as well as us‑east‑1. Here are the respective web servers, so you see us‑west‑2 is alive and then us‑east‑1 is alive, so these are the two websites that we're going to use and we're going to set up failover routing for. So, what I'll do is I'm going to go back to Route 53, I'm going to click on Create record, and let's begin. I'm going to call this app. I'm going to leave the record type the same. I'm going to copy this public IP of us‑west‑2's EC2 instance and paste it in here, and then we can select the routing policy, so in here, I'm going to select failover. Now, before I fill this out, I'm also going to set this to 1 minute to speed up the caching refresh. Again, this is not necessarily the best thing to do. In order to be a responsible DNS user, you should set this a lot higher, but for the sake of this demo, I'm going to set it to 60 seconds. Now, when I go down here, you see some more information. First, we have to select the record type, so is this the primary or the secondary record? Well, we're going to say this is the primary. Once we do that, you're going to notice we have to choose a health check. Remember, failover routing policies require a health check for the primary record. So to get this, I'm going to open up this menu, I'm going to open up health checks in a different tab, and we're going to create a health check. Now, I'm going to go through this pretty quickly because this is out of scope for this particular demo. We cover creating all of the different type of health checks later on within this module, so feel free to watch those coming up next, but for now, I'm going to set this up and skip through a lot of the settings. First thing I'll do is give my health check a name, so I'm going to call it primary. It's an endpoint. I'm going to paste in the IP address that I'm trying to monitor. It's over port 80, and under advanced, to speed this up, I'm going to select Fast. I'm also going to set the failure threshold to 1, so after 10 seconds, once it fails one health check with our health checkers, it's going to be unhealthy. So what I'll do now is go down, I'll click on Next, Create health check. Perfect. So now what will happen here in the next 20 or 30 seconds, this should go into a healthy state. But while it's checking, I'm going to go back to Route 53, I'm going to refresh, and I'm going to choose my primary health check. So now we've said, hey, this value for the app record is the primary record in our failover policy. I'm going to give my record an ID, which is a unique identifier for us, and I'll just call it usw2 abbreviated. I'll then add another record. Now for this, we need the same record name, so I'm going to call this app once again. It's an A record. I'm going to copy and paste the us‑east‑1 public IP in here, so I'll put that in the value. We'll say it's failover. I'm going to set TTL to 1 minute, and then for type, this is going to be secondary. Now, if you notice here, this is optional to have a health check because you can say I don't care if primary fails, just send it to this resource no matter what. Best practice is you're typically going to want a health check, but for this demo, I want to show you how this works. So we don't set a health check here, I give it its own ID, and then I click on Create records. Awesome. So now we have our two records here for app, we have our primary and our secondary. So if I go back to my health checks really quickly and refresh, there we go, it's now healthy. So now if I go to this record here, let me open this up and browse to an http, we should get our primary instance, so I'm going to continue us‑west‑2b. Perfect. Now, I can also check this by copying this record name, go into my DNS Checker here, pasting this in, and clicking on Search. Now, notice it's giving us our primary IP 35.167, which is going to be the IP for our us‑west‑2 instance. Alright, so this is working. Now, let's simulate a failure. What I'm going to do is go to instance state for my primary, and I'm going to click on Stop instance. Now, while this is stopping, what I'm going to go ahead and do is I'm going to cut forward until it reaches a stop state and then we will resume. Alright, so this has been stopping for a while. Now while it's stopping and right before it actually does reach stopped, I'm going to open up my health check here. Let me clear these banners to make it easier to see, and what we're going to do here is I'm going to refresh and eventually this is going to come back as unhealthy. So after it fails enough health checks, it's going to say, hey, this record is no longer healthy, so once it does that, that's going to trigger our failover record. So you can actually see the health checkers are starting to come back as failures, which is good. So eventually, once I refresh long enough, this will come back unhealthy. So what I'll do is I'm actually going to fast forward once this is ready to go and then I will resume. Alright, so just roughly probably 20 seconds later, I would say it's now unhealthy. So in theory, this is unhealthy, and our record for our primary is using the health check saying, hey, if this resource is unhealthy, send it to secondary. Now, we can try refreshing this page, it might not work, but once I do refresh it, if it does work, it should take us to us‑east‑1. Now, what happened here is I might have some caching, so in order to get around this, I'm actually going to go to DNS Checker and I'm going to run this search again for that same record, and you'll notice now it's using the backup IP 44.219, which is our us‑east‑1 instance, 44.219, so this is working. Our failover has now succeeded, and it's saying, hey, the resource you're trying to get to is at this value and it's now populated around the world, which is awesome. Alright, well, that's going to do it for this demo. We've implemented a failover routing policy, and we simulated failure of the primary record value and resource. Let's wrap up here, and we can move on when you're ready.

Demo: Creating a Geolocation Routing Policy
Okay. Welcome to this demonstration where we're going to work on creating a geolocation routing policy. I'm in my public hosted zone once again, and before we continue, I just want to show you the two different resources we're going to use here. I've set up 22 EC2 instances, one in us‑west‑2 and another in us‑east‑1. Now, these both have their own respective sites as you can see here that print out their own information. Now, if I go back to Route 53, let's jump in. First thing I want to do here is create a new record. Now, when I do this, I'm going to give my record a name, so I'm going to call this app, it's an A type, and I'm going to select my us‑west‑2. So I'm going to copy my public IPv4, I'll paste it in here, set my TTL to 1 minute, and under Routing policy, I'm going to choose geolocation. So in here now, notice we can set the location. This is saying, hey, how do you want to route your traffic based on where your DNS queries originate from? If we look at this list, notice just how much information and configuration options there really is. You can choose different countries, you can choose regions, we can choose States within the United States, etc. There's a ton of information, and I'm not going to show you all this, I'm just skipping through really quickly. So what I'm going to do is I'm going to choose default. So for this, what I'm going to say is I want this record value to be the default location catch. Remember, you want a default in place in case you set your location that never gets matched. Now, after this, I need to give it a unique ID, so I will call this default, and now I'm going to click on Add another record. Now I'm going to call this app as well because it needs to be the same record name. We're going to choose type A, and I'm going to paste in the us‑east‑1 IP address, so I'll copy that. I'm going to go here, paste it in, set it to 1 minute TTL, and I'm going to choose geolocation. For geolocation on this particular instance, I'm going to choose the United States of America, so I'm going to choose the United States, and I'm going to give it a name. So what we've set up here is saying, hey, any DNS originating from the United States, I want to send to this particular resource. Any other non‑matching location should be sent to us‑west‑2. So now what I can do is go down, click on Create records, and we have our records in place. So let me go ahead and copy this, and the next thing I want to show you to test this. I've loaded up this DNS Checker website here, which is very handy, and for this, what I've done is I went ahead and selected the United States region, so I'm going to click on this just to show you, but we're in the United States, and when I run this and I search this, it's going to show us that 44 record, which if you remember for our match is our record here, so United States location should be sent there. So this is working. Now, I've also loaded it up over here a Europe‑based DNS check. You can see here in the URL, it's based in Europe, and you can see all the DNS check is coming from Germany, Denmark, etc. I'm going to search this and you're going to notice, hey, this is going to our default geolocation record, so now we've caught traffic outside of the United States, and by default, we're sending it to this other record which is 34.214, so this is working perfectly. Now, that's how easy it is to create a geolocation routing policy. Remember, you can get pretty complex and specify continents, regions, United States states, and even a default record. Let's go ahead and wrap this up here, and let's move on.

Demo: Using Traffic Flow and Geoproximity Routing
Okay. Welcome back. In this demonstration, we're going to work on creating a geoproximity routing policy. I've loaded up my public hosted zone that's given to us in our sandbox here, and to use this, I have two instances that I've created, one in us‑west‑2 and another in us‑east‑1, and these both host their own respective websites as you can see here and here. I'll jump back into Route 53 and let's begin. Now, if you create it from the default UI here, so if I go to Create record, we'll just give it a subdomain name of app, I'll copy in this public IP for us‑west‑2 as the value, I'll set the TTL much lower, and let's go to Routing policy. So I can do GEO proximity. However, notice it's kind of unfriendly if you really think about it. You can choose the endpoint types, but this doesn't give you a visual representation of what's actually going on, so unless you're very good with regions or coordinates like longitude and latitude, I can show you a better way. I'm going to cancel out, and under Traffic policies, I'm going to select that. Now here is where traffic flow comes in, so traffic flow was that visual representation that we talked about in the clip where we discuss these record types and these policies. Traffic policies allow you to input very complex and advanced routing behaviors. So you can do if then type statements, and it's going to map like you see here. Let's go ahead and create one right now. So I'm going to click on Create traffic policy, I'm going to give it a name, I'll click on Next, and now we have our visual representation, so now we can go ahead and say, okay, the start point is going to be an A type or maybe a CNAME, etc. So we'll just say it's going to be an A type IPv4 format. From here, what's it going to connect to? So here we can choose geoproximity. Now, this is where we can actually look at a map, so right now it starts with two regions. Now you can add three, four, five, whatever regions you want, but we'll just use two for this example. Now, to visualize this, we can click this little map button here and it's going to show the two regions around the world here in a second. What I'll do is for endpoint one, which will be the main region, I'm going to say AWS region US East Ohio. So you'll notice right now we have one region on the map highlighted. If I go down to region 2, let's just say I want to do AWS region again, but let's say we'll do Oregon, so us‑west‑2. Now, if you look at the map here, we have two regions on the map that are highlighted when I hover over them. So in theory, what this means is everything within this portion of the map that's highlighted in the middle for region one, if the request comes from there from a DNS perspective, then it's going to use endpoint 1 here at the top. Now, if the endpoint request comes from number 2, region 2, everywhere else on the map that's highlighted in this red color on the outsides basically, will use that particular endpoint. Now remember, you can grow and shrink these based on biases, so for region 1, if I wanted to, let's just say I want to shrink it down to ‑36. Notice how it changes the map. The bias shrunk, which means the amount of map geographically that it takes up has now shrunk. However, I can also do the opposite. Maybe I want to make this 75, so +75, look at the opposite it did. Now, region two is way smaller, so what this is saying is, hey, if any DNS queries come from this tiny little circle here on the geographic map, send it to endpoint 2. Now, what I'll do here is I'll close this and then let's just connect to for just one of these regions to show you what you can do. What we're doing here is saying, hey, if region one is used for our rule, we can connect to another type of rule, so maybe we want weighted, geolocation, latency, or maybe a new endpoint. So I can just paste in a basic value here like an IPv4, or I can create a weighted routing policy as well. Now, what I'll do here is I'll just do another one here, so we'll say endpoint for region 2, and I'm going to use the same IP just because we're not going to use this, and I'll show you why here in a minute. I'll click on Create traffic policy, and there we go. Now, this is the reason why we're not going to finish this step. Check out the pricing per month. Yeah, it's very expensive. So what we would do here in theory is we would add a record for this policy, so what I would do is web. our domain name. What this is doing is then going to say, okay, any DNS queries for web. our subdomain here are going to use this traffic policy, and based on this traffic policy, it's going to go ahead and look up the DNS origin and then use that region map that we defined. Now, I'm going to skip this step because I obviously don't want to go ahead and pay $50 and that's actually going to do it. So since we can't go any further because of the cost, it's just a little prohibitive, we're going to go ahead and in this clip here, but hopefully you saw how easy it is to create a traffic policy for complex routing scenarios and then use the geographic map for geoproximity routing. Let's go and wrap up here, and we'll move on when you're ready.

Demo: Creating a Latency Routing Policy
Okay. Welcome to this demo where we're going to create a latency‑based routing policy. I'm in my public hosted zone once again. Let's go ahead and check out the two instances that we're going to use for this. I have one in us‑west‑2, and I have one in us‑east‑1. Now, these both have the same type of web server displayed, and these are what we're going to use within our routing policy. So under my Route 53 hosted zone, I'm going to click on Create record. We'll go ahead and call this test, we'll leave it as an A, and let's select us‑west‑2's public IP for this first value. I'll set the TTL to a little bit lower here and under Writing policy, I'm going to find and select latency. So now, under here, you can see we select the region, so the Amazon EC2 region where you want this record to resolve to. So for this, this is us‑west‑2. So I'm going to select us‑west‑2, and I'm going to give this a record ID that's unique, so I'll just call it usw2 for us‑west‑2, and I'm going to add another record. Now, this time, I'm going to call it test as well, but I'm going to give it the different value from us‑east‑1, so I'll copy here, go back, paste that, set the same TTL, and then I'll select the same latency routing policy. Now for this, I'm going to choose us‑east‑1 because this is where that EC2 resides. I'll give it another unique ID here use1 and click on Create. Perfect, and that's it. So now we're allowing Route 53 to automatically look up which resource is going to be best based on the latency for those resources. So to test this, I've loaded up this DNS Checker, which I'm sure you've seen several times by now, and for this, we're in the United States. So what I'm going to do is copy this value, let's paste it in here, we'll resolve it, and this sent it to 44.219, so that would be our us‑east‑1 region. Now, I also loaded one up in the Asian continent, so all these requests coming from over here. Let's go ahead and paste this in and see which one this goes to. Check it out. Some of them are a little bit different. So in Pakistan, it's going to go to 34. If we look at Malaysia, it's going to a different resource, etc. So really if you notice the further East on this map, they're using the us‑west‑2 resource because the latency is significantly better, for instance, South Korea. Now South Korea is way over here on this side of the map obviously, and that's going to be a lot shorter of a trip around the world and through the internet from a latency perspective to hit our Oregon based EC2. So this is a perfect example. This was a faster resource for them, which is us‑west‑2, so that's what it resolved to. Now, that's going to do it for a latency‑based record. Just understand you use this to allow Route 53 to say, hey, this is the higher performing result for your DNS query. I'm going to send you to this region and this resource. Let's go ahead and wrap this up here, and let's move on.

Demo: Creating a Multivalue Routing Policy
Alright. Welcome to this demonstration where we are going to work on creating a multi‑value routing policy. Let's have a quick diagram on what to expect because this can be a little hard to visualize when we're just talking about it. We're going to have four web servers already created for us with four different public IP addresses. Once we set up each individual record and add it to our multi‑value routing policy, we're going to go ahead and start querying our DNS entry for those web servers which will be fronted by the routing policy that we set up. Now, each time we query this DNS name, so when we do it over and over again, those record values are going to be returned in a randomized order, so they won't ever be the same each time, unless it's a really random occurrence. Really, really important to call out once again. This is not a suitable replacement for load balancing. Please keep that in mind for the exam. You want to use a load balancer to load balance. This is just meant for any scenarios where you want to randomly return the values on the back end. With that being said, let's jump into the console now. Alright, welcome to my AWS sandbox. Let's get started creating our multi‑value routing policy. A few things I've already done. I've loaded up my public hosted zone within my sandbox here, which is what we're going to use. I've also created four web servers, and you can see those here. Now, you'll also notice they have obviously four different public IPs, so this is what we're going to use to create our different multi‑value answer records. Now, I've also went ahead and created four health checks here, so once I refresh this, these will all be healthy, and you can notice here healthy, healthy, healthy. What I've done is I've set up two different groups if you will, two primary and two backup. Now, these, you don't have to worry about it, but they basically map out in order from top to bottom here. So that's how we're going to use this to demonstrate what happens when a server is unhealthy or a back‑end resource is unhealthy in a multi‑value answer policy. So let's begin. I've got my four instances. I'm going to go over to Route 53 and I'm going to create a new record. Now, once I'm in here, I'm going to create all four records for this, so I'm going to just go ahead and call this app., it's an A record type, and I have the IPs over here in a different document to make this easier, so I'm going to paste in the IP addresses in order from top to bottom. Just remember that please because when I go in here and stop these, I might not call it out, and I want you to understand that. So let me go ahead and create my records here, app. A record type, first value here, I'll set TTL to a minute, and for routing policy, I'm going to choose Multivalue answer. Now with this, you can optionally health check your instances, and it is highly recommended and a best practice to do so, so that's why I went ahead and already created these different health checks. So what I'll do is I'm going to choose primary one because this will be my primary one server, and if we look at the health checks here and I select primary one, it's looking at that IP, so 89.132, 89.132. Perfect. Next, I give it a unique ID, so let's just call this p1 for primary 1 and let's add the rest of our records. So we'll call it app. We'll select the second primary IP here, I'm going to copy it and then paste it in, set my TTL to 60, choose Multivalue, and I'm going to choose primary 2 for this one. I'll call it p2 and let's add the backup instances, if you will. So I'm going to call this app, I'll copy and paste in the other IP address, 1 minute Multivalue. I'm going to select a secondary one for this or backup one, add my last one here, and then I'll copy and paste that fourth IP in there. Set it to a minute, select multi‑value, and then select my last backup to health check. Perfect. So let me go ahead and fill this out, give it a unique ID, and let's create our records. It looks like I forgot a record here. I apologize. Let me go ahead and set this out. This should be s1 and s2, not p1 and p2. I'll create my records and this time for real. There we go. Perfect. So I'm going to close this. We now have our four multi‑answered records here. You could see the routing policies, the four records. Let's actually test this out. What I'll do is I'm going to go ahead and navigate to this DNS name. So once this loads here and I accept that it's not secure, it's going to give us our easy site. Now, if I refresh, it's going to be cached. So what I'm going to try and do here is load up my internal DNS for my browser, clear it, go back, refresh, and I can do this several times. So if I clear this, go back, refresh, it'll change. Now, this is a huge pain, so what I'm going to do instead to visualize this a little bit different is I'm going to go to DNS Checker, I'm going to paste this in, and I'm going to click on Search. Notice the order of IP values in San Francisco. Now, what's going to happen here if I click Search again, eventually these are going to rotate. These are all randomly provided to us, so notice now the first one is 174, and the order is randomized. Now each time I click this, it's going to do the same thing. Now, you might get lucky and get the exact same order, but it's still returned randomly, and there's only so many random operations that can occur with four values. So that's exactly how this is working. We're getting a random order of our different values. Now, let's simulate the failure of two. So I'm going to go to Instances and shut down my two primary instances that are behind our routing policy, I'll stop those, and what I'm going to do here is I'm going to navigate to my health checks, and what I'll do on my side is I'm going to pause and then I'm going to refresh, and once these are unhealthy, I will resume. Awesome. So I'm fast forwarded here to kind of save you some of the boring waiting, and after about 30 seconds to a minute, these have now become unhealthy. You can notice unhealthy and unhealthy because both instances are stopped. So now if I go back to DNS Checker, let's search again. There you go. Now, we're only getting the two healthy resources based on our routing policy configuration where we are evaluating the health for those records and that's it. Let's go and wrap this up. Remember, when you use this routing policy, you get a randomized order of returned values and it is not suitable for load balancing. Let's go and wrap this up and then we'll move on.

Demo: Registering Your Domains in Route 53
Alright. Welcome to this next demo where we're going to look at the process that it takes to register a domain in Route 53. I loaded up Route 53 here and a very quick callout. This is an external sandbox account that's temporary, but it's not bound by the same sandbox rules that you would have within our Pluralsight offering, so if you're trying to follow along, understand you're not going to be able to do this. With that warning out of the way, within Route 53 here, I'm going to go down and go to Domains and click on Registered domains. Once I'm in here, I'm going to have no domains because I haven't registered any within this account. So two things we can do here, you can transfer in multiple or single domains, and when you do this, you basically enter the domain you want to transfer, it'll check if it's possible, and then it will walk you through some of the steps. Now, an important thing to remember here is when you have a domain, there can be a thing called transfer lock in place, that's going to lock it from being transferred, so this check would fail essentially if that's in place. Now, if I go back here, we can also register domains. So what I can do here is search for some type of domain that I want to register. So let me just add in a bunch of random characters and then we'll search, and there we go. So you'll see, hey, an exact match would be a .com here, it's going to cost you $14 a year, and then also we have these other suggested domains. Now, a common practice for a lot of companies is once they get their domain name, they also want to buy up the other ones that are similar to so no one else can use them. The thing to notice here is check out some of the pricing. We have $71 here for .io, $25 for .me, etc. So what I would do is say, okay, well, I want to go ahead and just use .com my exact match for $14 a year. I select it, and it adds it to my cart. From there, what I do is I proceed to checkout. Now, this is where you actually check out like you would any other online ecommerce platform. So you see my domain name, you can buy the duration. So do you want it just for a year? Do you want to buy it for 4 years, 5 years, 10 years, etc? Now notice you don't get any type of discount for buying it for a longer time. This is really just a preference. Do you want to make sure you always have this for 5 years, etc. Now, you can also turn on or off auto renew. So with auto renew, this is exactly what it sounds like. It's going to auto renew your purchase for that current purchase price at the time of renewal, so this is an important thing to remember. If I buy this for a year and set on auto renew and then next year the .com price goes up in money, well, I'm going to pay that new price. So you can lock in your price if you buy it for a certain amount of years. After I do this, I click on Next. It makes you fill out all of your information, and we're not going to fill this out now, but just notice it's pretty standard. You have your address, your general contact information for this domain, and then you can set specific admin contacts, tech contacts, and billing contacts. So sometimes in organizations, the billing email address and organization will be different than the technical or admin, so you can customize this as you see fit for each of these settings. Now, I'm just going to leave them default because we're not going to finish this process, but let's look at this last thing here. Privacy protection. So you can turn on privacy protection for your domain. Now, when you try and turn it off, it's going to warn you like, hey, if someone does a whois lookup, they're going to have all of your information. What that means is if I ran a whois for this domain, everyone's going to be able to see all of this information on the public internet, so general practice is you want to turn on privacy protection. That way they can only look up specific contact information, and typically when they do a whois lookup, it's going to show that hey, Route 53 and Amazon Web Services own this domain and someone's using it through them, so general best practice is to go ahead and turn on privacy protection. Now, we're not going to click Next because this is going to break out and say I have errors like you see here, but we're going to go ahead and wrap this clip up. That's how easy it is to register a domain for your account within Route 53. Let's go ahead and try to remember that you can transfer in domains or you can buy new domains. We'll end this here, and we'll move on when you're ready.

Health Checking Route 53 Resources
Okay. Welcome back. We just got done wrapping up a bunch of different demonstrations. Let's look at health checking our Route 53 resources. Route 53 Health Checks. You can set health checks on individual record sets within Route 53. If a record set fails a particular health check, then it gets removed from Route 53 until it passes the next available health check. With health checks, you can actually set notifications via SNS, which is a push‑based messaging service, and it will alert you about the failed health checks so you can be a little bit more reactive. Now, within Route 53, there are three health checks you need to know for this exam. There's monitoring an endpoint, there's monitoring other health checks, and then there's monitoring something called a CloudWatch alarm. Let's break down each of these now. First up, monitoring an endpoint. This is where Route 53 uses roughly 15 global health checkers to monitor your resources. When you're setting this up, you can set the interval, so it can be set to every 30 seconds or it can be as little as every 10 seconds. You can also choose the regions where you want to actually check from. Now, you want to be careful here. Remember you get charged for DNS queries, and these are essentially DNS queries, so if you set every region for every 10 seconds, well, then you're going to be receiving a ton of traffic that you will have to pay for. When you're using them, the health checks are going to result in either a 200 or 300 status code, so these are going to dictate the health of your resources and then that will dictate whether they pass or fail. A pro tip for the exam and for a real world, ensure that your firewalls, if you're using them, allow the health checkers to actually reach your resources. If your firewalls are not in place, well, then your health check is going to fail, even if your resource is not unhealthy. Moving on to monitoring other health checks. These are also commonly referred to as calculated health checks, so keep that in mind. You use these by combining multiple health checks into a single one. The monitored health checks are called child health checks, and you can have up to 256 at a time, so you can have a ton of child health checks. When you're setting them up, you could configure these checks with AND, OR, or even NOT logic, so you can get pretty complex in how you set these up. You can make sure that child A and child B are healthy, you can say child A or child B, or you can say child A and not child B. There's a lot of different ways you can set this up. This is going to be useful with multi‑layered applications where an application should stay up as long as one endpoint is still healthy. So even though you might have four applications with four different endpoints, maybe you want that to stay up as long as one of the resources is still healthy, so you can use an OR logic there. Last one here monitoring CloudWatch alarms. This works by creating a CloudWatch metric and you associate an alarm to use as a health check endpoint, so you're watching for metrics within a service called CloudWatch. Once that metric is breached or it fails, then the alarm will trigger. Now when that health check does fail and the alarm does trigger, it goes into an alarm state, so it is alarming. Now, an exam pro tip here is that these are perfect for use within private hosted zones where no public access is required. Now, this is because when you're using HTTP health checks or monitoring endpoints, which was that first health check, these are only available for publicly resolvable records, so that's why that last health check type is perfect for private hosted zones because remember, private hosted zones are for internal traffic, so it's going to be non‑publicly resolvable IP addresses. So if there is an exam scenario where you check a private resource, you might want to use that last health check type. With that being said, let's wrap things up here. Make sure you review all three health check types, you need to know them for the exam. And with that being said, let's move on to the next clip.

Demo: Setting up Endpoint Health Checks
Alright. Hello, and welcome to this health check demonstration. I'm in Route 53 here. Let's go and begin. First thing I want to do is we're going to use a single instance here I've created four for upcoming demos that you're going to see, but we're going to use this top instance for this first demonstration. So what we'll do here is I'm going to go to Route 53, I'm going to go to health checks, and I just want to call out this is loading now, by default, the new health check view, so yours might be a little different. However, the process is the same. So if you're following along, try not to get lost. Just understand the view is different because of a default setting within this sandbox account now. Also, if you want to use the old console, you can do that via this banner up here when this does load. With that out of the way, let's create a health check. I'm going to go to Create health check, and under configuration, I give it a name, so I'll call this web. The next thing we do is we choose the resource, so do we want an endpoint, calculate it, or CloudWatch alarm? For this demo, we're doing endpoint so we're going to monitor a single endpoint. After that, we choose, how do we want to specify the endpoint? Do we want to do a domain name, so something we might already own, or do we want to look up an IP address? Now for this, we're going to use an IP address, so I'll go back to my instance, I'll copy this public IP, go back, and I'm going to paste it in. Now, it's also very important to note, with endpoint monitoring, you can choose the protocol, so do you want HTTP, HTTPS, or do you want to just use straight up TCP? So for this, we're going to do an application check essentially, so we're going to look at the application layer on Port 80. Now that we have this configuration set, let's look at the advanced config. This is where you can get very specialized in your health checks. You can set the request interval to be standard, which is 30 seconds, or you can set it to be 10, which is what we covered in our clip. Now for this, I'm going to choose 10, but understand there is a cost associated with it and you have to understand you're getting a lot of queries or incoming traffic for this type of configuration, so be very careful if you want to use this. For the sake of this demo, this is going to make it the fastest, so that's why I want to choose it. We then set the failure threshold, so how many consecutive health check failures do you want it to fail before it's considered unhealthy? So if you have a high tolerance, you can set this number really high, but if you have a low tolerance, like maybe a production application, maybe you want to set it to 1 or 2. So for this, we're going to set it to 1 because I don't want this to ever be down, and I wanted to immediately let me know. Now, we have some other stuff here, string matching and latency graphs, these are a little out of scope, so I'm not going to look for these right now. So I'm going to skip over this, but I do want to come down here to Health checker regions, so this is where you can customize where you want your health checks to actually come from. By default, it uses all available regions, but if you want to, you can say I don't want it to come from Ireland or South America, Asia Pacific, etc. So this will also help cut down on the number of checks that are actually coming into your resource as well. So what i'll do is I'll just leave the current ones I have checked, and then we'll skip down to the bottom. We're going to skip Host name because again, that's a little out of scope for this exam, and I'm going to create my health check. Awesome. So now we have our health check here web, we have a unique ID to reference it, it's enabled, status is unknown. Now, it's unknown because the health checks have not actually come back successful or failed yet. We can also look at the configuration we set, so it's 10 seconds, we set a low failure threshold, etc. So now what I can do here is if I click on health checkers, we're going to see the different regions that we just configured to monitor this endpoint. So you'll notice we have our Asia Pacific Tokyo, we have our US East,, US West, and US West 2. Now these are coming back successful, so if I go back to health checks here, and we're going to notice it's now healthy. We see the details. We didn't create an alarm, which is fine, and we see the state. Now, I want to go ahead and I want to actually simulate this as a failure. So what I'm going to do is I'm going to go to instances, I'm going to select the instance. and let me shut it down. So I'm going to say stop, I'll refresh, and while this is stopping, I'm going to go back to my health checks, refresh, and what I'll do here is I'm going to go ahead and fast forward into the future until this is actually unhealthy. Alright, welcome back. I fast forwarded to the future. It took roughly, I would say a minute or so to update, but now you can see the status is unhealthy, and that's because our resource is down so it's stopped. Now, if I select our health check here and I go down to Health checks, we're going to see which health checker actually failed and you're going to notice all of them, and that makes sense because the instance is actually down. So we get a failure, connection timed out, etc with some of the reasoning. Now, failures can happen for a couple of different reasons. It can happen for how we did it where we stopped the instance or maybe the instance crashes, but also, it's important to note that these health checkers come with their own IP addresses, so if you're not allowing these IPs to perform their health checks, which will be via an HTTP call, then you could get a failure as well, so keep that in mind. You have to make sure you allow these IPs from these health checkers to actually hit your instance. Okay. So this is unhealthy. Last thing I want to show you here, let me go back. We can actually invert this. So let's say, okay, well, if the instance behind this is actually down or not working, I want to invert the action. So what this will do now instead is since these are failing, this in several minutes, so I'm not going to wait for this, will actually be healthy. So eventually, once we refresh enough, this is going to invert and it will say, well, this health check is actually healthy because we inverted the status as you can see here. Now, these have very specific use cases, but you do need to know that you can invert the health check if you really want to. Now, that's going to do it for this demo on creating an endpoint health check. Remember, you can look for domain names, you can look for IP addresses, and really be sure you remember the different advanced configurations we looked at, including interval, failure threshold. health checker regions, etc. Let's go ahead. We'll end this here, and we'll move on to the next demo.

Demo: Setting up Calculated Health Checks
Alright. In this demo, we're going to work on creating a calculated health check. Now, I've already gone through and I've created four endpoint health checks here, which is what we just looked at in a previous demo, and you'll see I have a primary‑1 and 2 and a backup‑1 and 2. Now, these are endpoints. so they're looking at IP addresses, and all they do is they map to our four web servers that I'm sure you've seen in a previous clip. So right now, these are all healthy, and what I want to do is use these to create two different calculated health checks. So let me go ahead, I'm going to click on Create health check. and before I begin, I like to call this out again because this is a brand‑new thing going on. I'm in the new console here, so yours might look a little different, but the configuration options should be the same, so please keep that in mind in case you are following along. So under Create health check here, I'm going to give it a name here, I'll call it primary, and for the resource, I'm going to choose calculated. So this is looking for the status of other health checks and that's what it's going to use for its alarming. So under this, I can choose the health checks I want to monitor. So this is going to be primary, so I'm going to choose primary‑1 and primary‑2. Now, the next thing we fill out is Report healthy when settings, so do you want all health checks to be healthy in order for this to be healthy or can there be a minimum number? So can I have one out of these two be up and consider it healthy? Now, what I'm going to do just for the sake of demonstration is I'm going to say all health checks are healthy, so this requires that both primary‑1 and primary‑2 are reported as healthy. I'll create it, and there we go. We now have our new primary calculated health check which is saying, hey, 2 out of 2 child health checks must be healthy for this to be healthy. Now, I can view the child health checks under this menu, which is awesome. Now, while this is catching up, I'm going to go back and let me create another one. So I'm going to create health check, I'll give this a backup name, I'll make it calculated. I'm going to choose my two backup health checks instead, and for this one, I'm going to set a minimum threshold, so I'm going to say yeah, as long as one of my backups is up and running and healthy, this will be a healthy health check. So I'm going to create this one, and we see our settings here, which is perfect, 1 of 2. Let me go back to health checks, I'll refresh, and eventually these are going to come up as healthy because of where they're at. So what I'll do is I'll pause really fast and then I'll resume once these are both up and running correctly. Alright, so it took about a minute or so for these to come up, but now you see our calculated health checks are both coming back as healthy. Perfect. So let's actually test this out. I'm going to shut an instance down in each of these groups and see how they react. So I'm going to go to EC2 here, and while I was paused, I renamed these so it's a little bit easier to follow along. So I'm going to select one of these primary instances. I'm going to go to Instance state and click on Stop. I'm going to stop it, and then I'm also going to do the same for one of the secondary instances. So I choose secondary, I'm going to go to Instance state, and then Stop this one as well. So now when I refresh, these are going to be stopping, which is perfect. So we stopped one primary and one secondary instance, which is exactly how I want to test. Alright so let me clear these banners here. I'll refresh again, just make sure they're still stopping. Perfect. So now what I'll do here is I'm going to go back to health checks, and I'm going to pause once again. This will take probably a minute or two and I don't want to sit here and have you wait, so I'm going to pause, and then once these are all stopped and our health checks are updated, I will resume. Alright. Welcome to the future. I've fast forwarded here and notice we have some of our health checks coming back as unhealthy, which is expected. So we've had our primary‑1, that's unhealthy, which is an endpoint monitoring, and our backup‑1, which is another endpoint. Now, let's look at our calculated. Notice, our primary calculated is now unhealthy because there's not 2 of 2 coming back that are healthy. Our backup, however, is still healthy because we set an or logic. We said, hey, this one or this one can be healthy, and we'll consider this healthy. So that's what we mean when we say there's and, or, or not logic. For this one, we said there's and because we have primary‑1 and primary‑2 need to be good. This one was an or because we said backup‑1 or backup‑2 can be good. Okay. Now that's going to do it. That's how you create a calculated health check. We demoed the and and the or logic for setting them up. Let's go and wrap this up here and move on.

Demo: Setting up Private Hosted Zone Health Checks
Okay. In this demonstration, we're going to work on creating a new health check that's going to monitor a CloudWatch alarm. Remember, these are perfect for private hosted zone and internal IP checks. To do this, we're going to leverage a private EC2 instance, Private App Server. You see we have no public IP which means we can't use the endpoint monitoring, so this is why we're going to use CloudWatch alarms. So what I'm going to do here is I'm going to go in, I'm going to select my EC2, and under Status and alarms, I'm going to click on Actions, Create status check alarm. Now two things here. First of all, I had to enable something called Detailed monitoring for this demo, that's out of scope for this course, so I'm not going to cover it, and also CloudWatch is also out of scope for this course. Now we cover CloudWatch and detailed monitoring in much greater detail in a different course for the same learning path, but for now, just know at a high level, CloudWatch is used for alarming, metrics, and logs. So we're going to use it for the alarms. So I'm going to go ahead create a new alarm under CloudWatch. I'm going to disable notifications. I'm going to skip down here to thresholds, and I'm going to fill in some of this information. So I'm going to say for CPU utilization for this particular example. Now, honestly, commonly, especially on the exam, you're going to look for status check failures instead. This is one of the better ways to say, hey, my status check has failed, AKA my instance is not available right now, so I need to do something with my routing policy, etc. So typically, you're going to check for status check failures, but for this, I'm going to choose CPU utilization. We're going to say I'm going to alarm whenever it's greater or equal to 25% for a period of 1 minute at a time. So as soon as this is over 25% or equal to for more than 1 minute, then this is going to trigger. So I'm going to scroll down here, I'm going to click on create, and there we go. We now have our CloudWatch alarm. Now let's create our health check. So I'm going to go to Route 53. I'm in Health checks here, I'm going to create a new one, I'm going to give it a name, I'm going to select CloudWatch Alarm, and now we select the region where our alarm lives, so this is in us‑east‑1. We're going to get a list of alarms here, so I'm going to select this, but it's good to call, you can create a new one from this screen if you need to. So you can do something similar to what we did over in the EC2 view, but from here. So we already have ours created. I'm going to go down here, and now we can say, okay, how do you want to treat this health check when there's insufficient data? Now, by default, when you create an alarm like this, the first state is insufficient because it hasn't collected enough data yet to give you a healthy or unhealthy. check. What I'm going to say is I want to treat insufficient state alarms as healthy so we can mark this healthy right away. I'm going to click on Create health check, and there we go. So now if I go back to the Health check menu just because it's a little bit easier to see here, eventually this will go to healthy based on our settings. So what I want to do as well is I'm going to go to CloudWatch and open that up in a brand‑new tab here, and what we're going to do is look for the alarm that we created here for this instance. So what I'll do now is load CloudWatch, I'm going to navigate to All alarms, find our alarm in the menu here once it loads, and then select it. Perfect. So you'll notice it's in an OK state right now because our CPU has not breached our threshold. Alright, well, let's go ahead and test that out. Before we do that though, I want to make sure that our health check has come back healthy, and this will come back healthy here in the next minute or so. So what I'll do is I'm actually going to go ahead and pause and then I will resume once this actually becomes healthy. Okay. That took literally 5 more seconds after I said that, but I went ahead and did what I told you I would do. I paused and resumed, and now we're healthy on our health check. Perfect. So what I want to do now is I want to trigger this, so I'm going to go to Instances, and what I've done here is I've connected via Session Manager. Now, I've also installed a program called Stress. Now to do that, I just ran the command sudo yum install stress, which is fine, and I'm going to go ahead and simulate CPU usage, so I'm going to stress out the one CPU that this entire VM has. So I'm going to run this, and now what we're doing is taking up all of the CPU cycle, so this is going to cause almost 100% CPU utilization, which eventually is going to trigger our alarm. So what I'll do here is I'm going to refresh for a minute, and I'm going to fast forward, and then once this is breached our threshold that we set, I will go ahead and resume and we will pick back up. Okay. So I fast forwarded. Now it's took several minutes, I would say, for this data point to get captured, but notice our CPU is way up. It's at 99%, and we are now in an alarm state. So this is now triggered. So, if I go back to health checks here, if we refresh, eventually, we're going to see this goes to unhealthy. So actually let me go and select this and instead, what I'm going to do is I'm going to refresh on this particular screen instead, and then I will pause, and then I'll resume once the statuses sit unhealthy, and I'll tell you about how much time it took for this to catch. Alright. So I fast forwarded here. I save you some time. I would say it took roughly 2 minutes or so since this alarm triggered until this health check changes its status so keep that in mind as well, it's not real time. But now you'll notice we have an unhealthy status, so this health check is working based off our alarm that we set it up for. Okay. That's going to do it. Remember that you can use this to check for private resources with no publicly resolvable IP addresses, and a common scenario is to use them to check for status checks of your EC2s and then failover your health checks. Let's go ahead and end this here, and we'll move on whenever you're ready.

Module Summary and Exam Tips
Okay. Way to hang in there once again. Welcome to this Module Summary and Exam Tips clip. Let's go over a few things that I think are important for you to remember going into your exam. First up, remember of course, Amazon Route 53 is the go‑to service for any DNS management needs within AWS. Remember that Port 53 is for DNS traffic, thus the name Route 53. The service itself offers many different capabilities, things including domain registration and importing. You can set up DNS routing configurations, and you can set up health checks for your resources that are behind your DNS records. Remember the hosted zones. Public hosted zones are meant to contain records for publicly resolvable resources, so things on the public internet. An example would be a public‑facing website like the one you're using right now, pluralsight.com. Then you have a private hosted zone. These are meant to contain records for internal private networks. An example for this would be something like an internal domain like yourcorporation.internal. The easiest way to remember these is public is for public resolution, private is for private traffic. Also remember, when you use a private hosted zone, you have to associate them with the VPCs. That is the only way you can get your VPCs to resolve host names within a private hosted zone. Also, remember you need to enable DNS host name and resolution support. Moving on to record types that you need to remember, you need to remember the records we covered here in these clips. First to remember is an A type record, this is for IPv4 addresses. They can be public or private. Then you have a four A or an AAAA record which is meant for an IPv6 address. The easiest way to remember the difference here is that single A is shorter than the four A's, and IPv4s are shorter than IPv6. You then have an alias record. Remember that alias records are a special type of record in Route 53 that allow you to front other AWS resources. You can front things like an elastic load balancer or a CloudFront distribution. You also have your NS records, or your name server records. These are the name servers that are responsible for resolution of the queries. Next up was a CNAME. Remember, you need to understand how CNAMES work and what they're used for. You use these to map one domain to another domain. Now, whenever there's a scenario where you would choose an alias or a CNAME for an AWS resource, go with the alias, that's going to be the best bet. In addition to that, remember, you cannot use a CNAME record at the zone apex. Lastly is the SOA, or the start of authority, and these records contain different information regarding the different name servers, etc regarding your zone. Moving on from record types, let's talk about routing policies. Remember these different routing policies. There is simple routing for a very simplistic routing rules. You have failover routing where you have an active passive setup, so you have a primary and a secondary resource, and these require health checks. When the primary fails, it fails over to the healthy secondary. There's weighted routing where you set up weights for your different records to determine where you want traffic flow to go. Remember that the weights are all relative to the total sum of weights between the shared records. We also have geolocation, so this is where you can say, hey, based on where the DNS query originates, I want to send these queries to resolve to a specific resource in my account. We also have geoproximity. Don't get those confused. Geoproximity, remember, allows you to define boundaries on the world map to decide where you want traffic to go. Remember, you can use traffic flow with different bias numbers to influence the size of your regions. Next was multivalue answer. This is where you have multiple values between different records, and what happens is Route 53 will return you a randomized order of all values. This is not meant to be a load balancing concept, please remember that. Lastly, there's latency‑based or latency routing policies. This is going to send users to the most high performant lowest‑latency resource. Now with latency‑based routing, it is important, do not confuse latency‑based routing with geolocation. If there's any scenario specific to latency, you need to use a latency routing policy. Geolocation will not be necessarily the best way to determine which resource is best for those users. Also remember, you can use traffic flow to set up different traffic policies. Traffic policies are there to allow you to set up complex advanced routing policies for Route 53. Remember that GUI that we looked at in the demonstration where you can do a bunch of different branching logic, and it's also the only place where you can view the maps for your regions for geoproximity routing. Lastly here, remember the three different health checks. You can monitor an endpoint, you can monitor other health checks, and you can monitor CloudWatch alarms. Remember however, monitoring endpoints require publicly resolvable IP addresses. Monitoring other health checks are also sometimes referred to as calculated health checks, and then monitoring CloudWatch alarms are perfect for health checking Route 53 private hosted zone records. Now that's going to do it for this entire module. Thank you so much for hanging in there. I know that was a ton of information. For now, we're going to end this here, and we'll move on to the next module whenever you're ready to go.

Amazon Route 53: Resolvers
Hybrid DNS Overview
All right, welcome to the next module in this course. In this module, we're going to look at Route 53 resolvers and hybrid DNS. This first clip is going to go over or review hybrid DNS and what it is and why you use it. So let's go ahead and get started. Hybrid DNS is really becoming increasingly more common within organizations and enterprises. What that means is they have usually both on‑premise resources and cloud‑based resources, which are hosting their workloads because of this mix of on‑prem and in the cloud, proper DNS name resolution is still essential for both types. Well, that's where hybrid DNS configurations come in. Instead of resolving just in the cloud or just on‑prem, we can implement hybrid DNS so we can resolve both ways. Now there are some benefits and use cases you need to be aware of for hybrid DNS for the real world and for this exam. From a benefits standpoint, you're able to manage DNS records for both on‑prem and AWS resources from within the Route 53 service. If you use Route 53, the great thing is their global infrastructure is going to ensure high availability and scalability for DNS. Remember, the service offers an SLA of 100% uptime, which is insanely impressive. Also, it's meant to allow you to easily connect your on‑prem and your AWS environments. So they do their best to make it as simple as possible. Now some use cases for hybrid DNS include things like where an organization has resources split between on‑prem data centers and in the cloud. Or maybe you want to manage DNS during a phased migration to the cloud. So maybe you're having to maintain on‑prem resources for a temporary amount of time, and while you're migrating to AWS, you need to implement hybrid DNS. Well, that's a perfect use case for it. And then lastly, keeping existing on‑prem DNS infrastructure in place while also utilizing AWS for new services. This is also doable. So you can leverage on‑prem DNS setups and then leverage Route 53 for just some particular portions of the hybrid DNS. And we'll review some of those small use cases here coming up within this module. Before we wrap things up, let's have a quick simplified architecture diagram example. In this diagram on the top left, we have our region in ap‑south‑1, and we have some resources. Now those resources have a private hosted zone associated with them, and you can see the domain at the bottom of that region block. On the top right, we have ap‑south‑2, and the resources there have their own private hosted zone associated, and you can see that based on the domain name at the bottom of that region block. On the bottom of the diagram, we have on‑prem. So this is our on‑prem data center hosting our internal resources, and we also have our own internal domain name for that where we can see internal.corp. Now in this example, DNS queries for the aws‑cloud.corp and the aws‑cloud2.corp get sent to their respective cloud‑based regions and their respective resources. All DNS queries for internal.corp get sent to on‑prem. Now this is all doable using specific services and features within Route 53. And with that being said, remember, this is extremely simplified. We're going to cover some of the critical components to actually make this work coming up in upcoming clips. However, for now, remember that you can implement hybrid DNS with Route 53 and on‑prem. Let's go ahead. We'll end this clip here, and we're going to start diving into some of the components that are required to make this work.

Defining Hybrid DNS Rules with Route 53 Endpoints
In this next clip, we're going to look at defining hybrid DNS rules using Route 53 endpoints. The first thing we need to review here is something called the Amazon Route 53 Resolver. What this is is a DNS resolver that's there to respond recursively to different DNS queries that come from AWS resources. Now your VPCs connect two resolver at the VPC+2 IP address. What this is is your base CIDR network address +2. Now it's also available by default in every VC that's created. However, just because it's there doesn't mean you need to use it. Now the VPC +2 IP address connects to a Route 53 Resolver within each availability zone. In fact, this is what takes up one of the reserved IPs within your subnet CIDRs. Remember, in the VPC module, we talked about the five reserved IPs. Well, this is what one of those is used for. This Route 53 Resolver works for the following resources to respond to DNS queries. It works with public records. It'll work for Amazon VPC‑specific DNS names like a EC2 instance's public DNS, for example. And it works with Route 53 private hosted zones, so a private internal.corp network. Now, do you remember the DHCP option sets and the VPC modules? I hope you do. They're very important. Well, when you see Amazon‑provided DNS listed in there for DNS resolution, this is what they mean, the Route 53 Resolver. So hopefully this kind of puts some puzzle pieces into place for you. Amazon‑provided DNS uses the Route 53 Resolver. Now to use this, there are two very important components. There's resolver endpoints and resolver rules. These resolver endpoints and conditional forwarding rules known as a resolver rule work together to allow you to resolve different DNS queries between on‑prem and your VPCs. By using them together, you can create a hybrid cloud DNS setup over a private connection. It's important to call out the connection does have to be private, so a VPN or something called a direct connect. We're going to cover VPNs and direct connects in an upcoming module. The big thing to remember is you need a private connection between on‑prem and your VPC to leverage these for hybrid DNS. The first resource here is an inbound resolver endpoint. This allows DNS queries to your VPC from on‑prem or external networks like another VPC. The next is an outbound resolver endpoint. This allows queries from your VPC to your on‑prem networks or other external VPCs. The easiest way to remember these is you should think about it from the perspective of the VPC you're working in. Inbound endpoints are for traffic or queries coming into your respective VPC; outbound is for outbound request going out of your VPC. Now these two work together with something called a resolver rule, otherwise known as a conditional forwarding rule. This is a forwarding rule for where you want to send your inbound or outbound domain traffic. So essentially, hey, what do I want to send this internal network traffic? Do I want to go to a VPC, or do I want to send it to my on‑prem network? It's very important to know that resolver rules are applied to VPCs. In addition to that, they get shared across multiple accounts if you want using something called AWS Resource Access Manager, otherwise known as RAM. We're going to cover RAM in a completely different course, much more in depth within this learning path and this exam prep. However, just understand you apply rules to VPCs, and you can share them across accounts using RAM. Now before we wrap things up, let's have a quick architecture diagram for both inbound and outbound. First up, we have an inbound architecture diagram. On the left side here, we have our on‑premise network with an on‑prem DNS resolver and a server, and we have an on‑prem resource, server.internal.corp. On the right side, we have our VPC in AWS. We've associated a private hosted zone to that VPC. And then we have two private subnets. The first subnet here is our endpoint subnet. So this is where we host our inbound and outbound endpoints. We also have our private subnet hosting our resources, so in this case an EC2 server located at server.aws‑cloud.corp. So in this example, we're going to assume on‑prem servers want to resolve server.aws‑cloud.corp. And it's going to start by sending this query to the on‑prem DNS resolver. The next step is there is an on‑prem resolver rule that forwards this query to the inbound endpoint in your AWS VPC. So that query gets looked up and it says, hey, I have a rule here that I need to send it to this inbound endpoint over this private connection into AWS. So it's going to send it to an IP address that's assigned to the inbound endpoint. Once this query is in AWS and it hits the inbound endpoint, this endpoint is then going to query the private hosted zone information for the particular resource via the Route 53 Resolver. Once it finds that IP address and it resolves it, it's going to say, hey, I know where that's at. That's in this subnet on this EC2. So I'm going to forward this traffic to the correct private IP all via hybrid DNS. Now we can flip this essentially for outbound. We have the same setup. The only difference is traffic starts from the AWS VPC going to on‑prem. So in this case, we're almost reversing the process essentially. We'll say the server in the VPC wants to resolve server.internal.corp, so it sends the DNS query to the VPC +2 address. That +2 address has a resolver rule associated with that VPC to go ahead and forward any of that domain traffic to the on‑prem outbound endpoints. Once that outbound endpoint in our subnet receives this traffic, that traffic is then sent through the endpoint over the private connection, and it gets sent on‑prem to the on‑premise DNS resolver and DNS server. And from there, the traffic gets resolved and then sent to the internal on‑prem server that we have. Before we wrap things up, two quick exam pro tips for you just to summarize. For inbound endpoints, just simply think traffic coming inbound to a VPC. For outbound endpoints, think the opposite, traffic being sent outbound from your VPC. Now that's going to do it for this lesson on endpoints and rules. Let's go ahead and wrap this up, and we'll move on to talking about DNS firewalls.

Protecting DNS Traffic with Resolver DNS Firewall
All right. In this clip, we're going to look at protecting DNS traffic using something called the Resolver DNS firewall. The Router 53 Resolver DNS firewall is a feature that allows you to actually control access to different sites and actually block DNS‑level threats for different DNS queries. Now these DNS queries do have to be going through the Router 53 resolver. So if you're using custom DNS settings that don't use the Router 53 Amazon‑provided DNS, then you're going to have to think about different ways to have DNS firewalls in place. Let's talk about some concepts that are important for the exam. The first thing you do is you define domain name filter rules that get associated with your VPCs. Now with these rules, you can actually specify lists of different domain names, and you can say if you want to allow them or maybe you want to block them for being used at all. So for instance, maybe you want to block google.com or microsoft.com, etc. Well, you can do that, and then you can customize the responses for those queries that actually get blocked. And in addition to customizing responses, you can actually even fine‑tune domain list to allow certain query types. Now we're not going to get into specific query types for this exam prep because it's way out of scope, but just understand you can customize them and then fine tune these lists as you see fit. Now a very important thing to remember for the exam. This only filters on the domain name. It doesn't actually resolve the domain name. So it's not doing any resolution. But it's going to say, hey, this is trying to query google.com or whatever else you're trying to block. I'm just going to drop it right now. Another important thing to remember for the exam. This is only for DNS traffic. It is not for any other type of layer protocol, so no application layer, no network layer, etc. So things like HTTP, TLS, SSH, those don't get caught with this feature. It's only DNS traffic. And with that being said, let's move on to the next clip where we're going to have a summary and exam tip review.

Module Summary and Exam Tips
Okie dokie, welcome to the Module Summary and Exam Tips clip. Pretty short module, but there are some very important concepts we need to know for the exam. First up, reviewing hybrid DNS. Remember that using hybrid DNS allows you to actually easily connect your on‑prem networks and your AWS environments. Using this, you can centrally manage DNS in Route 53 and resolve traffic both directions. Remember that Route 53 offers amazing uptime, so it's really nice to be able to use and have available whenever you need it. Also remember some of the use cases for hybrid DNS. They include things like managing DNS during a migration to the cloud. And you can even keep existing on‑prem DNS infrastructure in place while you're also utilizing AWS for newer services. So you can have those legacy apps using on‑prem DNS, and you can leverage AWS for new services and set up resolver rules to send traffic to them. The next important concept, the Route 53 Resolver. You have to remember this. This is the DNS resolver that responds recursively to DNS queries from AWS resources. In fact, it actually uses one of the five reserved IP addresses that we covered in a previous module that are reserved within your subnet CIDRs. It uses the +2 address. And speaking of resolvers, let's talk about the endpoints. Remember, inbound endpoints are used for resolving DNS queries coming into your VPC. Outbound endpoints are used for resolving DNS queries going out of your VPCs. These can be confusing for a lot of people, and that's because networking is difficult. So if you need to, just try to think about it from the perspective of the VPC where the queries are trying to resolve. Now you can also use, remember, the Route 53 resolver DNS firewall feature to filter outbound DNS traffic. The big thing to remember is it's only DNS traffic, nothing else. Now that's going to do it for this module summary and exam tips. Again, very short module, but very important information here. So feel free to review it. But for now, we're going to end this module, and I'll see you in the next one.

Advanced VPC: Virtual Private Networks (VPNs)
Protecting VPC Networking with VPNs
Hello and welcome to this module where we're going to look at some advanced VPC concepts. More specifically, we're going to start looking at virtual private networks. In this first clip, we're going to start discussing protecting your VPC networks with VPNs. What is a VPN? Well, a VPN, or a virtual private network, is meant to establish encrypted connections between computer devices over the public internet. It's meant to be a secure method in order to mimic the privacy of using internal or private network connections. Now within AWS and for this exam, there are four primary VPN methods to connect to your AWS VPCs. You need to know these different methods for this exam and really any secure architecture design. The four methods go as follows. There is a site‑to‑site VPN. There's an AWS client VPN. There's VPN CloudHub. And then you have the option for a third‑party VPN solution. Now, before we dive into these individual VPN methods, which we'll cover in their own clips, there are some important components you need to be aware of. The first component is a virtual gateway, otherwise known as a VGW. This is the VPN concentrator that actually gets deployed on the AWS side of your VPN connection. So it's within the AWS cloud. How it works is you create your virtual gateway, and then you actually attach it to a chosen VPC,which you wish to create your site‑to‑site VPN with. Now you do have the ability to customize the autonomous system number, otherwise abbreviated as ASN, if you really want to. They offer a default ASN, and knowing the numbers is not within scope for this exam so we're not going to cover them. However, you do need to know that you can customize them. The next component is a customer gateway, also known as a CGW. This is a software appliance or even a physical networking device that lives on the customer side of the VPN. So this would live within your data centers, in your branch office, etc. With these, you are required to fully configure this device to work with the site‑to‑site VPNs that are in place, and you need to understand that not all devices are supported for creating a customer gateway representation within the cloud. Now the AWS CGW is a resource that essentially provides the required information to AWS about your device or software application. So it's a resource in the cloud, but really it's just a representation of the information that you've configured on your own device. Now let's actually discuss some use cases regarding VPNs that you should be aware of for this exam. They're very useful for securing hybrid cloud network architectures. So when you have an on‑prem data center with workloads and you want to communicate with AWS, well, you can do that over a VPN. And the benefit from that is that it's a secured connection. They're also really helpful for implementing disaster recovery for your networking connections. So you can have multiple VPNs in place that can protect your connections. And then if one goes down, you can have the other still up. You can also use them for interconnecting VPCs. Now we looked at transit VPCs before, and this is a perfect use case for a VPN. With the transit VPC, you have some type of VPN or router solution in the transit VPC that handles all of the routing for you. You can also use them for connecting to third‑party vendors. So maybe they have their own cloud account or some other on‑prem network somewhere, or you can connect to them securely via a VPN that you choose. Now that's going to do it for this overview on VPNs. We looked at some of the use cases just now. We briefly discussed the four different methods. And really the big thing to take away from here is you need to understand what a virtual gateway is and what a customer gateway is. In some of the upcoming clips, what we're going to do here is discuss those different methods in more detail. So let's go ahead and wrap this up here and then move on.

Site-to-Site (S2S) VPNs
All right. In this first clip, we're going to start diving into one of the VPN methods called a site‑to‑site VPN. Now there are some requirements specific to the customer gateway for this type of VPN, so let's review those. First off, you must use a publicly resolvable IP address on your side of the VPN connection. So of course, the customer gateway is meant to represent your hardware or your software appliance. Well, on this, you need to have a publicly resolvable IP address. If you're using NAT Traversal, which is NAT‑T abbreviated and is a special type of NAT in place for network connections like this, then you need to use the public IP of the NATting device that fronts your customer gateway. Now you don't need to know NAT Traversal in very big detail, but understand if it is in place on this exam, you have to have the public IP for the NAT device which is in front of your stuff. So just do your best to remember that. You also must enable route propagation within your route tables in your VPC for this type of VPN to work. So those are three important requirements that you should be aware of for the exam and if you're actually setting this up in real‑world scenarios. Let's actually go take a look at a site‑to‑site VPN diagram now. On the left side here, which I just highlighted, we have an internal data center that we're representing with internal servers. On the right side here, which I highlighted, we have our AWS VPC,and we have a subnet here with some of our EC2 instances, and we've dictated or indicated our route table here on the top‑right portion of that. Now when you do this, what happens is the connection is established to the customer's public IP address on their actual physical customer gateway device, or it could be also a software appliance. But regardless, it's the device on your side that you actually own. Once we do this, this VPN connection is established using the virtual gateway that we've attached to our VPC. Now remember, you can optionally customize the ASN, which in this case is 65000. Now also remember, you don't need to know ASNs in too much detail right now because that's more of a networking specialty. But you do need to understand you can customize them and that they're required on the virtual gateway. This all results in an IPSec VPN tunnel so you can communicate with your private resources securely and encrypted over the public internet. Now this is a very big point right here. These are IPSec VPNs. These are one of the most secure options for VPNs into your AWS Cloud accounts. That's a huge exam scenario to remember. If you ever need IPSec VPNs, think site‑to‑site. Now you also have to remember to enable route propagation. Otherwise, the VPN is not going to function with traffic correctly. So on the VPC side, we need to enable route propagation within the route tables, which is an easy setting. Now pro tip here, just a reminder. I like to stress this again because it's very important. If you need an IPSec VPN solution, you will want to consider a site‑to‑site VPN. Now that's going to do it for site‑to‑site VPNs. We covered just enough for the exam here. Let's move on to another solution coming up next.

AWS Client VPN
All right. Next up, another VPN solution, just like I promised. This time, the AWS Client VPN. The AWS Client VPN is a managed client‑based VPN service that is meant to enable you to securely access your AWS resources and even resources in your on‑prem networks from almost anywhere in the world. Now there are some very important concepts regarding this VPN solution that we need to know for the exam, so let's go ahead and review those. The first is that this is an AWS‑managed version of the well‑known OpenVPN solution. If you have ever worked with any networking, you've probably heard of OpenVPN. Now because of this, you can actually use any OpenVPN‑compatible software to connect to this solution. What that does is it allows you to connect from pretty much anywhere in the world as long as you have an internet connection. Now they're not quite as secure as an IPSec VPN tunnel. However, these do use TLS connections for the VPN so they're still pretty powerful, and they're still protected. And lastly, these do automatically scale or allow you to scale to support the number of users that you need. So you can easily increase and decrease whenever needed. Now big exam pro tip, you'll want to use this solution if you need to access resources from any location outside of an office or a data center and especially if you see anything talking about OpenVPN. If any of those two things are in the exam scenario, you might want to look at choosing an AWS Client VPN. Now that's going to do it for this solution. Let's wrap up here, and we'll move on to another one.

AWS VPN CloudHub
All right. Hopefully you're not getting too much VPN knowledge here. Let's talk about VPN CloudHub. VPN CloudHub allows you to securely communicate from one site to another via site‑to‑site VPN connections. So what that means is it's used in conjunction with your site‑to‑sites. Now let's talk about three very important concepts regarding this service. What it does is it allows you to set up and operate a hub and spoke VPN model for multiple site‑to‑site VPN connections. So please remember that. It's a hub and spoke for site‑to‑site. Now one of the tricky things here is you can actually use this with or without a VPC in place. So you do not have to have VPCs running, being used,etc., in order to use this. You can use it as long as you have site‑to‑site VPNs in place and you have an AWS account. Now they're also perfect for multiple data centers where you need convenient, low‑cost, hub‑and‑spoke VPN connectivity. So let's actually look at why you might use this really quickly with a diagram. In this example, we have three data centers, as you can see here. And then on the top right, we have our single AWS account. Now in each of our data centers, we have their own individual customer gateways with their own public IP. And all of these customer gateways connect to a virtual gateway in our AWS VPC. Now, we want to talk to each other between data centers and not just between the cloud and the data centers. Well then, we have all of these other different connections that need to be set up. So you need a separate VPN connection between each data center, as well as a connection from each data center to the VPC. What this does is, as you can tell, adds a lot of complexity and a ton of overhead. This is an administrative nightmare. Now if we talk about it with CloudHub instead, look how much cleaner this is. Using VPN CloudHub allows us to have all spoke data centers, so Seoul, Singapore, and Mumbai, to set up with site‑to‑site VPNs to communicate with one another and, optionally, a VPC. This is far more efficient, and there's way less overhead. Just look at the difference in the number of connections alone. So big exam pro tip here before we wrap up. You're going to use VPN CloudHub to simplify site‑to‑site VPN connection management. Remember, if you need anything with a hub and spoke VPN model here, this is a perfect choice. Now with that out of the way, let's wrap this one up. And then we're going to talk about our final VPN solution here coming up next.

Implementing a Third-party VPN
All righty. Welcome to our final VPN solution here. I'm sure you're getting tired of hearing the word VPN, so let's get through this. In this clip, we're going to talk about third‑party VPNs. So let's talk about using them in AWS. These are typically going to be obtained from the AWS Marketplace. So that's a place where you can buy and subscribe to different subscriptions and different offerings and different software. On that marketplace, a lot of places that are well known offer VPN solutions. Now when you do this, they run on EC2 instances, and that's what you're paying for. You're paying for EC2 instances, as well as an included software price. Let's talk about some scenarios when you might use a third‑party VPN. The biggest example is when you need to enable transitive routing on the AWS side of things. So if you need a transit VPC, then this is a perfect use case. Also, if you have a desire or a need to enable special capabilities that are not necessarily native to AWS‑managed VPNs. Typically you're going to want to use one of the AWS managed choices. But if there's special capabilities that you need that are outside those, then you might want to use this. And then lastly, any considerations regarding bandwidth. Maybe you need to expand bandwidth or you need to restrict it to a certain amount, etc. Whatever the case may be, if you need more bandwidth control, you might look at using a third‑party VPN. Up next things you need to know for the exam. If you use a third party VPN on an EC2 instance, you have to disable source and destination check on the instance. If you do not do this, your traffic will just be dropped at the EC2 instance because of how the networking works. Also, if you're going to have any failover recovery, well then, automating that and setting that up is entirely your own responsibility. Since this is not AWS‑managed, that means they don't handle any of that for you. In addition to that, you need to plan your EC2 instance sizing carefully. Remember, the size of your instance and the type of your instance directly correlate to the amount of resources like CPU and networking, and you really need to make sure you're picking the right ones. With this type of VPN, vertical scaling is the most common type of scaling. Vertically scaling is when you make a single resource bigger. Essentially, you're giving your EC2 instance more resources by increasing its size. And then lastly, these are generally not going to be recommended for exam scenarios. And with that last point, that goes into our exam pro tip here. Exam scenarios really are going to rarely want you to select this type of VPN solution. Typically, AWS wants you to use one of their own managed ones because of the properties and the benefits they offer. However, if there's any scenario where you have to have something specific that falls outside of the managed options, then this is a perfectly viable choice. Now that's going to do it for third‑party VPNs. We've covered all of the VPN solutions at just enough level for this exam. So let's go and wrap them up, and we're going to move on to our module summary and exam tips.

Module Summary and Exam Tips
All right way to hang in there once again. Another module is in the books. Let's have a quick module summary and talk about some exam tips. First up, remember that VPNs are put into place to offer encrypted, protected networking channels. When you're using VPNs, you need to understand what a virtual gateway does and what a customer gateway does. Remember that VGWs, or virtual private gateways, are the VPN concentrator that gets deployed on the AWS side of a VPN connection, and these get attached to your VPCs. We then have customer gateways, and these are going to be either a software appliance or a physical networking device that lives on the customer side of the VPN connection. In AWS, a customer gateway is essentially representing all of the information that's relevant for the VPN connection. Moving on to which one you should choose. Well, let's have a quick summary on which ones would fit which scenario. Site‑to‑site VPNs. These are the most secure VPNs, and they're going to be used if you need an IPSec connection. For AWS Client VPN, this is useful for anything that requires TLS connections from any device in the world or if you have any OpenVPN requirements. VPN CloudHub is meant for hub‑and‑spoke VPN model management. So if you have a multiple site‑to‑site VPNs that you want to simplify management for, well then this might be a perfect option. Remember, you don't have to have a VPC in place to use this. And lastly, we have third‑party VPNs. These are only ever going to be used for the exam whenever you need very specific customizations that you cannot accomplish with any of the other options. Now with that being said, let's go ahead and wrap this module up. I appreciate you hanging in there. We're going to move on now to another module whenever you're ready.

Advanced VPC: Direct Connections, Direct Connect Gateways, and Transit Gateways
Exploring Direct Connections
All righty, welcome to the next module. We're going to continue with advanced VPC concepts, but we're going to start talking about some other resources and services like direct connections, Direct Connect gateways, and transit gateways. In this first clip, we're going to start exploring direct connections. In AWS Direct Connect, or DX for short, is a cloud service solution that's meant to make it easy for you to establish dedicated connections from your on‑premises systems and data centers to the AWS cloud. Now the big bragging point and the big requirement that these solve for is that direct connects give you private connectivity between AWS and your data centers or your offices. By using these, you can actually reduce network costs, you can increase your bandwidth throughput, and you can provide a much more consistent network experience as opposed to using internet‑based connections. So in your scenarios, there are any of those three use cases or requirements, a direct connect is probably a perfect choice. Now when you use these, they have to be set up between your data center and your hardware and within a Direct Connect location, which is typically either AWS owned or partner owned by someone who partners with AWS. In addition to that, to actually use a direct connect, you have to create a virtual private gateway, and you attach it to your VPC. And then from there, you can begin the establishment of the connection. Now a huge thing here to keep in mind is that these offer private access to both public and private resources. Now, we're going to discuss this in much greater detail coming up shortly in some other clips. But really remember that the use case here is that it's private access to AWS, and you can access both public and private resources. And lastly here, they do support both IPv4 and IPv6 traffic. Now before we move on and wrap this clip up, let's have a quick overview diagram at a very high level. On the left‑hand side here, we have our on‑premises centers or our offices or whatever else it may be. We also have our customer gateway, which is either our software appliance or our physical customer router that we own and maintain. In the middle, we have our Direct Connect location. So this is going to be either partner owned or AWS owned. And within these locations, there are several networking cages. Now how this works is we're going to have either our own customer network cage or we're going to work with a partner who owns their own cage with their own networking hardware like a customer or partner router. There's also going to be AWS‑owned cages. So AWS owns these, and they're going to use this Direct Connect location to manage them. These are going to host the Direct Connect endpoint that we'll use. And then on the right side of the diagram, we have our VPC with our private and public resources, and we have our virtual private gateway attached to our VPC. Now this portion here on the left that we highlighted in blue is called the last mile. This is where your data center connects to the network cage within the Direct Connect location. This portion in the middle here, which is highlighted in green, is called the cross connect, or X connect. This is where your network cage or your partner's network cage actually connects physically to the Direct Connect cage. So this is where the connections actually occur within the Direct Connect location. And then on the right side here, this orange portion that we've highlighted is the actual Direct Connect connection, and this lives entirely on the AWS backbone network. What that does is it provides extreme performance benefits, and it's completely private. Now one thing to call out here, this is private, but it's not encrypted. Now we're going to talk about that much more in depth later on. However, I like to put that out there now because that is a very important thing to remember. These are private, they're not encrypted. Now also remember, Direct Connect connections can be set up to access public or private resources, and we'll dive into that much more in greater detail coming up shortly. For now though, let's go ahead and wrap this clip up, and we're going to move on to talking about choosing different Direct Connect types.

Choosing the Correct Direct Connect Type
Okay, we just got done reviewing Direct Connect connections at a high level. Let's start talking about choosing different connection types. There are two primary connection types for direct connects. The first is dedicated. This is where there is a physical Ethernet connection associated with a single customer. And then we have hosted. This is going to be a physical Ethernet connection that an AWS Direct Connect partner actually provisions to you on behalf of the customer or yourself. So the easiest way to remember this is dedicated is dedicated to you or your customer, hosted is hosted by someone else and they're essentially renting it to you. Now let's talk about dedicated connection types in greater detail. When you're using these, customers can request one through the Direct Connect console, they can use the CLI, or they can even make a RESTful API call. When they make this request, it gets made to AWS, and then it gets completed by a Direct Connect partner. When using dedicated connection types, there are four different primary offerings for bandwidth and capacity. There's 1, 10, 100, and 400 Gbps. So these can get extremely high performant. Now for these, these are usually going to be the best option if you need maximum Direct Connect performance and you need full privacy of the connection. However, there is far less flexibility in choices of capacity compared to hosted. And speaking of hosted, let's talk about that. With hosted connection types, you request these by contacting a partner that's in the Direct Connect partner program. Now when you're doing this, you can also add or remove capacity as required as long as you make a request to those partners each time. With hosted connections, there are many, many, many more available capacities to choose from. Just look at this list. You can choose as small as 50 Mbps all the way up to 25 Gbps. Now I'm not going to read all of these off. You can read it, but you can obviously see that this option provides the most flexibility from a capacity standpoint. However, if you notice, there are some asterisks next to some of those speeds. Well, that's because only specific designated Direct Connect partners can actually create connections with those speeds that are marked. So yes, they offer more flexibility. However, you also have to choose the correct Direct Connect partner in order to take advantage of it. Now very important here, please keep in mind that direct connections are not immediately available to use. They can take weeks on end to actually provision because there's actual hardware and networking being set up for you. Now moving on, we need to discuss what is called a virtual interface. You actually leverage Direct Connect connections by attaching Direct Connect virtual interfaces to your virtual private gateways that are attached to VPCs. Now, we abbreviate virtual interfaces as VIF, so you're going to see that here moving forward. Now your virtual interfaces actually allow you to connect to AWS services over your private DX connection instead of the public internet. And speaking of them, they come in two primary categories. There is a public VIF, which is meant to be used to access public resources. Now when we say public resources, we're talking publicly accessible by default, so Amazon S3, Route 53, DynamoDB, etc. And then we have private VIFs. These are used to access private resources. So the easiest way to remember this is something within a VPC, so privately accessible like an EC2 instance, RDS databases, VPC endpoints, etc. So real quick summary for virtual interfaces because these get very confusing, again, because networking is very difficult, it's very complex, and it can be very hard to set up correctly. A public VIF is meant to access public services. Private VIFs are for private resources like VPC‑bound resources, so EC2, etc. Now before we wrap up, let's have a quick diagram overview of both private and public VIFs. The top connection here, which is orange, is going to be a private VIF VLAN connection. And on the bottom connection, which is blue, is our public VIF. Now when you use a private VIF, again, this is used to connect to AWS resources that are bound to a VPC, so RDS databases or EC2 instances, for example. When you're using a private VIF, the connection goes through the virtual private gateway because remember that's attached to the VPC. Now when you use a public VIF, these are what are used to access public services, so things like an S3 bucket. Notice with public VIFs, we're not traversing through the virtual private gateway because we're not accessing VPC resources. We're accessing public services, but we're doing it via a private connection. That's going to do it for this clip. Do your best to remember the two different connection types, dedicated and hosted. Remember the different reasonings for using one or the other. And also remember the two different VIF types, so there's public and there's private. Let's wrap this clip up here, and then we're going to move on to another service called a Direct Connect gateway.

Centralizing Management with Direct Connect Gateways
All right, let's get started with our next resource, Direct Connect gateways. In AWS, Direct Connect gateway is a regional resource that is globally available, and it allows you to connect multiple VPCs across different regions to your on‑premises networks through a single Direct Connect connection. Let's review concepts that you need to know for the exam regarding Direct Connect gateways. First off, you create a Direct Connect gateway in a specific region, and it's connected to your chosen Direct Connect location that's also within or near that region. Once you do this, it's important to know that AWS has recently launched multi‑account support for this feature. So now you can have up to 10 VPCs to be associated from different accounts with a single Direct Connect gateway. Now one caveat here is they have to belong to the same org under the shared payer account. We talk about organizations in a different course within this learning path, but understand that they have to belong to the same org just for now. Also, these do not allow gateway associations that are on the same gateway for Direct Connect to send traffic to each other. In other words, VPCs can't talk to each other through the gateway. Let's have a quick architecture diagram just to visualize how these kind of work at a high level. We have our typical setup here on‑prem on the left, we have our direct location in the middle, and then we have our two regions with our different VPCs on the right side. Once you do this, you essentially create a private VIF in addition to your Direct Connect gateway, and you associate that private VIF with that DX gateway so it can use your DX connection. Now remember, this is a regional resource, but it's globally available. Once you do this and the Direct Connect gateway is up and ready, you then create and associate different VGWs in different VPCs everywhere that you want to connect to your Direct Connect gateway. So in this case, we created two virtual private gateways in two different regions, and we attach it to the same DX gateway. Now remember, these VPCs can't talk to each other by default through this Direct Connect gateway. That is not allowed. It is not possible. Now before we wrap things up, quick exam pro tip. If you see a scenario on the exam where you need a centralized point for managing connections between on‑prem and AWS resources over a direct connect, you want to think about using a Direct Connect gateway. It's going to simplify the architecture because you have one point of management as opposed to having to deploy several different direct connects for several different VPCs. Now let's go and wrap this up here, and we'll move on whenever you're ready.

Encrypting Network Traffic with VPN over Direct Connect
All righty, let's get talking about encrypting network traffic with a VPN over direct connects. The one thing I want you to pull from this clip, traffic that traverses a Direct Connect connection is private, but it is not encrypted. You absolutely need to know the difference between the two. It's private, not encrypted. Now to visualize this, I put together a diagram. In order to have encrypted traffic over a direct connect, you have to overlay a VPN over the top of it. So in this case, I simplified that existing diagram, and we're just going to use the private VIF VLAN connection to actually show this. What you do is you have your Direct Connect, but you need to lay a VPN over it or VPN over DX, and this is an architecture that provides the best security for your Direct Connect connections. However, it does require a lot of admin overhead because there is added complexity and more resources to manage. When you do this, what you typically do is you set up a site‑to‑site VPN connection over the top of your Direct Connect connection. Now, when you have this in place, your traffic is both private and encrypted. So just remember that. The one thing to pull away from this clip, if you want encrypted traffic on a direct connect, you have to have a VPN in place as well. Now, that's going to do it for this clip. That's enough for the exam. Just do your best to remember the VPN over DX architecture layout. We're going to wrap things up, and we'll move on to another service called a transit gateway.

Centralized Traffic with AWS Transit Gateways
All righty, you're not quite out of the woods yet. We got another gateway for you. Let's talk about AWS transit gateways. Let's take an example architecture that's pretty common as far as connections between different networks go. If we were to assume we connected all of these VPCs via different methods like VPC peering, maybe we want to connect VPNs to the VPCs and we want to connect Direct Connect gateways, etc, just look at how many network resources are required and really how complex even this simplified diagram even looks. There's a lot to manage here. So I think it's fair to say that that previous network architecture is pretty complicated, and there's a lot of different administrative overhead for each networking portion. Well, to answer this, we have a transit gateway. AWS created a transit gateway, which is meant to connect many different VPCs and your on‑premises networks through a central hub. It's here to simplify your network, and it's meant to put an end to those complex peering relationships and requirements. Essentially, a transit gateway is meant to act as a cloud router, if you will. Each new connection is only made once, and then it can leverage the transit gateway, and you control it through other mechanisms like routing tables and security groups. Let's actually look at some concepts that you need to know for TGWs. Again, these are meant to allow you to set up a hub‑and‑spoke network topology for all of your different networks in the cloud and on‑prem. With them, you can connect thousands, yes, thousands of VPCs and on‑prem networks in one simplified manner. How they work is you deploy them to a region, but they are able to work across different regions. So they're not just stuck to working in one region; however, there is some configuration that's required to perform that. Now once you deploy it and you're starting to set it up, you actually control what traffic can go where by using a TGW route table. And these are configured for what are called attachments, which are attached to your VPCs, and we actually cover those in their very own clip after this. Keep in mind and remember they integrate with many different network resources. They can work with Direct Connect connections, they can work with Direct Connect gateways, and they even work with VPN connections. So they're very, very flexible, and they work with a lot of different networking resources. Also, remember this. You can share these between accounts within an organization. You do that via something called AWS RAM, which is Resource Access Manager, and we've discussed these before very briefly. It's just a way to share resources in an organization, but we're going to cover RAM much more in depth in a different course within this learning path. For now though, just remember you can share these between accounts. Now exam pro tip here. Transit gateway supports IP multicast. This is not supported by any other networking service in AWS by default. So if you see a scenario where you need IP multicast within your network connections, think transit gateway. Moving on, let's review the network architecture now that we have this transit gateway knowledge. If you look at this diagram here, this is actually the same amount of networks as before. But you can see it's way, way more simplified than the previous diagram. Look at all of the connections that don't exist. We just have one centralized point, and they all connect to that one central hub. So now since we deployed our single transit gateway, it's attached to all of those different network resources, and we can control traffic in the different flow through that one centralized hub. So that's exactly why these are so beneficial. That's going to do it for this clip on transit gateways. Let's go ahead and wrap things up here. Do your best to remember that these offer a centralized point of management for many different network connections, and they work intra and cross region. Let's wrap this up, and we'll move on to talking about attaching your transit gateways.

Attaching Transit Gateways
Okay, up next let's continue the transit gateway talk. We're going to talk about attaching your transit gateways. A transit gateway, remember, enables you to attach multiple VPCs and VPN connections and route traffic between them. You can also peer transit gateways. This is going to allow you to connect transit gateways across different regions or if needed, between the same region. Now we review all this because these are the common TGW attachments. So you need to know these attachments for the exam. The first attachment is a VPC attachment. This attaches a VPC to a transit gateway, and it works by picking a single subnet from each AZ so that it can be used by that transit gateway to route traffic. Now when you attach these and you use these attachments, this enables traffic to reach resources in every subnet within that AZ that you've attached to. So the big thing here is a VPC attachment is used to attach VPCs to your transit gateway. There's also a VPN attachment. So this is a feature within TGWs that allow you to connect your on‑prem networks via VPNs to multiple VPCs via your transit gateway. So in summary, if you need VPN communication through your transit gateway, you'll use this attachment. And then next, we have a peering attachment. You use these to peer both intra‑region or same region and cross‑region transit gateways. Once you do that, you can start routing traffic between the transit gateways. It's important to call out that peering attachments are not available with transit gateways that get shared with you. So if they get shared via RAM, you can't create a peering attachment. Moving on, now that we know what attachments are, they leverage a transit gateway route table to actually say where the traffic is going. So transit gateway route tables are an essential component of your TGW. You use these to configure routing for your different attachments. So these are just like a typical route table for VPCs and their subnets, except they're specific to transit gateway attachments. Each attachment for a TGW gets associated with exactly one transit gateway route table, so it doesn't have any more than one. Now this is exactly similar, if you remember, to a VPC route table. There's only one that can belong to a subnet. And lastly here, a TGW can have multiple route tables. What that means is you can really set up very complex routing scenarios and really have great control over the network flow. So do remember, the transit gateway itself can have multiple route tables; however, an associated attachment can only have one transit gateway route table attached to it. Let's talk about some use cases for different route table configurations. First would be network segmentation. You can use different route tables to create isolated routing domains within a single transit gateway. So what this means is you can have two VPCs attached to the same transit gateway, and maybe you want to control where they route traffic to and you don't want them to go to the same destination ever. It's also good for hub and spoke, so you can centralize routing for multiple VPCs and on‑premises networks. So you can just create one transit gateway route table for your VPC attachments and then attach it to every single attachment on your VPC, and you're good to go. Similar to that, it's good for shared services. So maybe you have a shared services VPC for an entire organization and you have hundreds of VPCs that are running different workloads. Well, you can create a single route table for your VPC attachments to go ahead and tell those VPCs how to route traffic to the shared VPC through the transit gateway. And then lastly, it's good for any other complex routing scenarios. So if you get advanced routing patterns that are required within your exam scenarios and you need some type of centralized model, well then a transit gateway is perfect because you can use the route tables to set up complex routing. Now that's going to do it for transit gateway attachments and route tables. Please remember some of the use cases. Do your best to remember the different TGW attachments. There was VPC, VPN, and peering, and then remember that you use route tables to control the routing of traffic for each attachment. Let's wrap things up here, and we'll move on.

Module Summary and Exam Tips
All right, thanks for hanging in there. One more module is in the books. Let's have a quick summary and talk about some exam tips. First up, direct connections. Remember, these allow you to establish private connectivity between AWS and your data centers or your offices. You will use this if you need a stable and reliable private connection to the cloud. So if there's any scenarios where you need a reliable connection or a stable connection, this is a good choice. Remember, these are private connections, but they are not encrypted. This is very, very important for you to remember. If you need protection in encryption, well then you're going to have to run a site‑to‑site VPN or some type of VPN over the direct connection. That is the only way to get encrypted traffic over your DX. Now remember also that these offer private access to both public resources via a public virtual interface, and a private virtual interface is used for accessing private resources. Remember, public VIFs would access things like S3, DynamoDB, etc. Private VIFs are used for things that are VPC‑bound or private. Also remember the two connection types and their different use cases. We have dedicated, which is typically going to offer the highest performance, and then there is hosted. This is where a Direct Connect partner essentially rents you a connection and you choose many different capacity options. This is going to have more options for capacity, but you have less control. Moving on to Direct Connect gateways. Remember, these allow you to connect multiple VPCs, even in different regions and different accounts to a single Direct Connect connection. These are regional resources that are globally available. And continuing on the gateway train here, transit gateway. These allow for transitive peering between VPCs and on‑premises data centers. These are meant to work as a hub‑and‑spoke model, and you can share transit gateways via Resource Access Manager. Remember, these are deployed on a regional basis, but again, you can have it work across different regions. For the exam, please remember the different attachment types, VPC, VPN, and peering. Also remember how those are used. VPC attachments are meant to control attachments to VPCs, VPNs are for, obviously, VPN connections, and peering ones are for peering different transit gateways. And lastly, these work with Direct Connect connections, as well as VPN connections, and they support IP multicast. That's a big one. If you see IP multicast requirements, then you want to think transit gateway. Also, remember the architecture that we went over earlier. With connecting all of these different network resources, there's a ton of overhead and a ton of different resources, and it gets really messy really quickly. After implementing a transit gateway, we were able to simplify our network connections. These are the same amount of networks, but now it goes through a centralized hub, which is the transit gateway. Now with this, remember, each of these connections have their own attachments, and these attachments have their own route tables, which define traffic. And with that being said, thanks for hanging in there. We just wrapped up another module for this lengthy course. Let's end here, take a break if you need, and I will see you in the next module.

Advanced VPC: Miscellaneous Features and Scenarios
Demo: Blocking Bad IPs Quickly via NACLs
All right, welcome to this next module, Advanced VPC Concepts. In this particular module, we're going to cover some miscellaneous features and some scenarios that you might run into on the exam. This first clip is going to be a demonstration clip. We're going to go ahead and simulate blocking malicious or bad IP addresses quickly using NACLs or NACLs. Let's have a very quick architecture diagram overview before we jump in the console. We're going to have a VPC created for us, and we're going to have some web servers and some public subnets, and they're all going to have their very own security group. Now within these security groups, they're going to be set up to allow both HTTP and ICMP traffic, and we're going to open it up from anywhere for any IPv4 address. Once we overview that, we're going to act as a malicious user locally on my local client via a command‑line interface. And we're going to start sending fake requests repeatedly to the same EC2 instance. So what I'll do is I'll just send a bunch of pings over and over again. Now obviously, this is just one IP address, but this logic works for several IP addresses or a range of IP addresses. Now, while we're sending these repeated requests to our EC2 instance, we're going to notice our security group is going to remain unchanged. So we're not going to change any rules on our security groups because there's going to be many of them. Instead what we're going to do is we're going to work with the public NACL, and we're going to work with the NACL rules within it to quickly and cost effectively block this bad IP address range. And this will be my specific IP address. Now we're going to continue to send requests to the same instance, but we're going to see how the NACL is going to immediately stop all requests at the subnet level. So this is a very quick way to block bad IP addresses because it's at the subnet level so it never even reaches the security group or the EC2 instance. Let's jump into the console now, and let's get started. Okay, welcome to the sandbox environment here. Real quick architecture overview before we actually begin. I've created three web servers with three different security groups. So Web Server 1 has security_group_1, Web Server 2 has security_group_2, and Web Server 3 has security_group_3. Now these all have similar rules. We're allowing all HTTP and all ICMP traffic. And all these do is they're hosting a simple web server. So, we can look at that real quickly. You've seen it if you've taken any other demos here, but I'll just go ahead and show you real fast. This is what they look like. So let me close this. And the first thing I want to do here is I'm going to copy each public IP, and I'm going to start pinging these. So for Web Server 1, I'm going to copy the IP. I'm going to jump to my terminal. And notice here at the top I have three different terminal sessions going. So we're going to ping each instance in one of these terminal sessions. So I'm going to run ping, paste it in, press Enter. I'm going to go to the second session. I'll go back, copy Web Server 2. Let me copy that. I'm going to ping this one. And then on session 3 here, I'm going to ping Web Server 3. So I'll copy this, I'll ping it, and there we go. So now if I switch between sessions, we have all of the pings going. So let's pretend that, hey, I'm a malicious user, so I need to block this IP address. Now typically, you're going to be investigating some type of log or logging software or monitoring software to see what the IP is. But obviously, I know my IP address, so we're going to use it really quickly. Now these all live within a public subnet. So you can see we have public IPs, and they live in each AZ. So there are different subnets and different security groups. So if I go to VPC and I go down here to our Subnets, we have our public subnets. And you'll notice each one will have a shared NACL. So we have the ACL here. It's in 97f3. This one ends in 97f3 and then 97f3. So what I'm going to do is I'm going to go to this public ACL that's associated with our three subnets, and I'm going to click on Inbound. So right now we're allowing all traffic, and that's why if I go back, we're getting these ping requests from each session. So to quickly stop this, what I'm going to do is go to Edit inbound. I'm going to add a new rule with a lower priority than 100 because remember, the first match wins. So I'm actually going to call this 10. I'm going to say all traffic from my IP, so I'm going to paste this in here. /32 means that this is this single address. This is the most specific a CIDR notation can get. Now after I do this, I'm going to select Deny and then click on Save changes. Now if I go back to my sessions here, look, they're already starting to time out. So I'll switch to 2, I'll switch to 3, boom. Just like that, we stopped all requests from a bad range of IP addresses. And there you go. Now we don't have to go in here, and we don't have to edit all of the security groups one at a time. We don't have to do anything else. We just use the NACL, which is at the subnet layer, and we added in a deny rule for a specific IP range, which was very fast and free to use. Let's wrap this scenario up, and we'll move on to the next clip.

Logging VPC Traffic with VPC Flow Logs
Okay, welcome to the next clip here. We're going to talk about something called a VPC flow log. A VPC flow log is a feature of a VPC that allows you to capture IP traffic information about traffic going to and coming from different network interfaces within your VPC. Now when you're configuring VPC Flow Logs, you have three source options that you can use, and you do have to know these for the exam. You can set up flow logs for the entire VPC, you can set it up for a specific subnet, so all network interfaces in that subnet, and you can even set it up for a specific ENI, so a specific network interface attached to a resource. In addition to this, there are some concepts you need to be familiar with. VPC Flow Logs are collected, and they do not affect your network throughput or your latency, so there's no performance impact. Now with those particular sources, they also support Elastic Load Balancing. You can log RDS traffic, you can capture Redshift traffic, you can log NAT gateways and even transit gateways. So pretty much anything with a network interface in a VPC is supported. Now, let's talk about some use cases here for these. Use case number one, you can use these to diagnose security group rules for denied traffic. For instance, maybe you're trying to reach a web server that you thought was public or open to your IP and you're getting a denied message. While using the flow logs, you can quickly diagnose where the traffic is getting denied and why. Use case number two, you can monitor traffic that's connecting to your instance. So you don't have to do anything with the logs, but you can monitor and alert if you need to based on certain IP traffic information. And then thirdly, you can determine the direction of traffic flow to and from ENIs. So you can see what's allowed and what's denied and which way it's going, as well as the different types of traffic that are getting sent in and out. Now we talked about the sources. Let's talk about the destinations. After you configure VPC Flow Logs and their sources, you have to choose a destination where you want the logs to be sent. There are currently three options that are available to you. You can send it to something called CloudWatch Logs, which is a logging service in AWS. And this is where it's going to send flow logs to a log group to perform operations on or you can just monitor the logs. You can also send them to Amazon S3 directly, so you can store your logs within your secured buckets for cheap, long‑term storage for compliance. Now on the exam, if there's a scenario with VPC Flow Logs and you want long‑term storage, well the big differentiator between CloudWatch Logs and S3 will likely be the cost. S3 is going to be cheaper. And then lastly, you can send it to something called Amazon Data Firehose. Now this is a streaming service that we'll cover in a different course within this learning path. But for now, just understand you can send them to this service for streaming. What this does is it sends your logs to an autoscaling stream to perform analytics, and you can even transform the data before you actually store it. So if there's any exam scenario where you need to stream in near real time or you need to transform the logs before they are stored, think Data Firehose. Let's actually look at a VPC Flow Logs record example. I want to break down each field for you so you understand how to read them, as you do need to know how to interpret these for the exam. First off, remember flow log records will represent a network flow within your VPC. They're going to include a 5‑tuple capture of information by default. And each record actually captures information within what's called an aggregation interval, so this is also known as a capture window. Now, let's go ahead and break down an example. What you see here is an actual real‑world capture of a VPC flow log. The first field here, which is separated by spacing, is the version of the flow log. So number 2 is the default version for Flow Logs. You then have the account ID, so the AWS account ID that the flow log is living in or making a request within. We then have the ENI ID, so the interface ID that's being recorded. You can use this to identify which network resource your traffic is being sent to or the traffic is being sent from. The fourth field here is the actual source IP address that's making the traffic call, so where is the traffic coming from? Use this field to identify and isolate problems related to problematic IP addresses. Following that, you probably guessed is the destination IP address, so where is the traffic going to? Use this to also identify and isolate problems related to IP addresses. Right after the destination IP, we then have the source port of the network call. So, this might be an ephemeral port, etc. You want to use this to identify and isolate any issues related to ports, especially if you're getting denied messages where you shouldn't. Right after this is the destination port. So these are following a similar path, source and destination. Now you'll use this just like you would the source port. You can troubleshoot and identify allowed or denied traffic based on this port. So in this case, this is 443, so that's HTTPS. In addition to knowing how to read these, you need to know some of the more simple or standard protocols. So no 443, no port 80, etc. After this, we have the protocol. So this is the number that represents the protocol of the network call. Now 6 is TCP, and you should know this. You want to use this to identify and isolate problems related to the wrong protocol being used. After the protocol field here, we then have the number of packets being sent within the call. And following that, we have the number of bytes that are getting transferred with this flow log. The next two fields relate to the time. So we have the start time, which is the UNIX time or the epoch, and then we have the end time right after that. So this is also UNIX time. The second to last field is the action associated with your traffic. So they can be ACCEPT or they can be REJECT. Now these are pretty self‑explanatory, but it's hey, is your traffic being accepted or is your traffic being dropped and rejected? And then lastly, we have the log status. So for this it's OK, in other words, that it's being logged. So statuses can be OK, it can be no data, and it can be skipped data. Now, please take the time to review all of these fields. It's very important you know how to read these for the exam. You really need to make sure you understand the syntax. On the bottom here, I have the actual official field names in the same order for you to review. So we have version, account‑id, etc. Understand though, the format itself is actually spread by spaces. I only put commas here for readability. Now moving on, let's talk about analyzing flow logs. Now that we know how to read them and how to use them, you need to know how you can analyze them. You can use AWS‑managed analytics services, especially one called Amazon Athena to query the logs directly. How it works is it works with your logs that are stored in your S3 buckets. It also works on logs that are stored in Amazon CloudWatch Logs. So if there's ever a scenario on the exam where you need to analyze your VPC Flow Logs, especially with a fast, cheap query, you want to think Amazon Athena, and you want to think about storing them in S3 or CloudWatch Logs. Now let's have a couple exam tips before we move on. Number one, if you need to monitor IP traffic, think VPC Flow Logs. Number two, this feature is not the same thing as packet inspection. So if you need packet inspection, this is not going to meet that requirement. Keep that in mind. Now with that all out of the way, it's a pretty lengthy clip, let's go ahead and wrap things up. And we're going to move on to a demo where we set up VPC Flow Logs.

Demo: Setting up VPC Flow Logs
Okay, welcome to this demo. We're going to work on setting up VPC Flow Logs, and we're going to demonstrate what this process looks like. Before we dive in, let's have a quick architecture overview here. We're going to have some resources already set up for us. I've already deployed a web server in a public subnet, and I've set up the security group to only allow inbound HTTP requests. What we're going to do is we're going to repeatedly hit our instance with allowed protocols, and then we're also going to open up a terminal session, and I'm going to try to ping my instance as well, which will be denied. Now before we actually start sending this traffic, we're going to work on enabling VPC Flow Logs at the subnet level to capture all traffic within that subnet. These flow logs will be sent to Amazon S3 for long‑term storage. I'm also going to show you how you can configure this at the ENI level for your VPC Flow Logs. And we're going to send these captured logs to an Amazon CloudWatch log group. So let's go ahead and jump in the console now, and let's get going. Okay, welcome to the AWS sandbox. Let's get started creating some VPC flow logs. Real quick architecture, infrastructure overview here. I've created a web server, and this is hosting a simple static site, as you can see here. I've also created an S3 bucket called pluralsight‑demo‑vpcflowlogs. And this is what we're going to use to store our vpcflowlogs in S3. So with that out of the way, let's go ahead and begin. What I'm going to do is for the first time, I want to show creating a VPC flow log for an entire subnet. So what I'll do here is select my instance. I'm going to go under Networking, which is open, and I see it's in public subnet A within my custom VPC. So what I'll do is I'm going to select this shortcut here and load this subnet. I'm going to go to this tab. And when I select this, you're going to notice there's two different ways we can do this. There's a Flow logs tab here where I can create one, or when I select this, I can go up to Actions, Create flow log. These perform the same actions. So I'm going to click on Create flow log. We see the selected resource is our subnet, and it's available for capture. And then we give some settings here. So I'm going to give this a name of subnet, and then we choose the filter. So do you want to only capture accepted traffic, do you want to only capture rejected traffic, or do you want to capture everything? For this, I'll do All. And that brings us then to our aggregation interval. Now the big thing here is notice its maximum, so it could be a little bit sooner, but it will never be more than this. So we're going to choose 1 minute to speed this up. But realize with 1‑minute intervals, you're getting more log calls, which means more money. So it's really going to depend on your use case. So we'll choose 1 minute, and then we have our destination. So notice the three destinations, CloudWatch Logs, Amazon S3, and then Amazon Data Firehose. Now notice with Data Firehose, you can send to a different or same account. Now you can do the same thing with S3 by specifying the ARN, but the bucket policy must require or allow this to happen. So I'm going to choose S3. I'm going to go back to my S3 buckets here. I'm going to select this and then copy my ARN. And then I'm going to paste this into this ARN field. Now I'm also going to give it a prefix because I want to store these in the same bucket, but I want to know which logs belong to what. So I'm going to call it /subnet. So now these logs will be stored in this prefix under this bucket. Moving down, we then get to some of the formatting. So the first format is the log format. In other words, what format do we want the logs' records to actually be in? Now we explored the default format in the previous clip, and you can see the fields here. But notice under Custom format, if I just Select all, notice all of the different fields that are available within VPC Flow Logs. There is a ton of information. Now this is way too confusing, so I'm going to Clear all and go back to default, and this is what our log format will now look like. This is version 2. The next formatting is the file format, so how do you want to actually store these files with your log records? Default is Text, which is what we will use. However, you can also choose Parquet. Now for exam scenarios, if you're storing log files and then you need to analyze those, especially with a tool like Amazon Athena, you'll want to choose Parquet file formatting. This format is perfect for analytics. For us, however, we just need text because we're going to explore it with a basic text editor. We're going to skip Hive‑compatible S3 prefixes because that's a little out of scope for this, and we're going to move down to the Partition section. So how do you want to partition your logs? Do you want to save them every 24 hours or do you want to partition every 1 hour? Now with S3, remember, the more specific your prefix, the better performance you will have, especially when you're trying to query and search your data. So instead of having one partition per day, we're going to have 24 partitions per day. So I'm going to select Every 1 hour, and I'm going to create my flow log. Perfect. So now this exists. So if I go into Flow logs here, we see our flow log name, we get a unique ID and then the different settings that we set within here. Now one thing I want to call out, this is very important, if I select Actions while this is highlighted, what's one thing that's missing? You'll notice you cannot edit an existing flow log. You can only create new ones or delete existing ones. You cannot modify these. In other words, make sure that you're setting them up correctly. Otherwise, if you need to make changes, well, you're going to have to create a new one and then remove the old one. Please keep that in mind for the exam. You can't modify existing flow logs. Awesome! So this is now set up. And while it's capturing, let's go ahead and set up our network interface example. So what I'm going to do here is under Instance details under Networking, I'm going to go down, and I'm going to select the network interface here and open it in a new tab. Now in this Network interface summary, I'm going to go down to the bottom. And you'll notice we actually have a flow log set up, and that's because we set up this subnet flow log just now, and this instance and ENI live within that same subnet obviously. So what I want to do is I'm going to mimic setting up an ENI‑specific one. So I'm going to go to Create flow log. You'll notice it selected our ENI for the resource, and I'm going to give this a name. So I'm going to call this eni. And then for this filter, I'm going to say Reject because I want to just capture rejected network traffic. And that's because I'm going to try pinging this instance, and that should get rejected. We'll set the same aggregation interval, but this time we're going to send it to CloudWatch Logs. Now notice the two settings here. You have to give it an IAM role, and you have to set the Destination log group. Now I don't have a log group set yet. So what I'm going to do is open up CloudWatch in a new tab. And once I'm in here, I'm going to find logs, and I'm going to create a new log group. So on the left‑hand menu, I'm going to go to Logs, Log groups, and then I'm going to create a new one here. So I'm going to give it a name, and we'll just call this eni. You can set the Retention setting, Log class, etc. This is out of scope for this demo. We cover CloudWatch in a separate course. So I'm going to just leave the defaults here. I'm going to click on Create. Perfect. So now we have our log group here. Let me go back to my creation screen. I'll refresh, and I'm going to choose eni. The next is the IAM role. So we have to choose an IAM role that grants permissions to vpc‑flow‑logs, the service, to actually put logs to our log group. Now I already have one here. So I'm going to search for CloudWatch, and it's right here, VPC_FLOW_LOGS_TO_CLOUDWATCH. Now what I'll do here is once I set this up, we'll review these permissions, and I do make these permissions available to you within the module assets. So feel free to make this if you want to follow along. Now once I set this, I'm going to use the default format. I'm going to scroll down, and I'm going to click on Create flow log. So this is in place. Let me start some traffic capture, and then I will review that IAM role. I'm going to go to my terminal here, and I'm going to ping that public IP. So let me capture my public IP here. I'll paste this in, and there we go. So this is going to be denied, which is perfect, but I'm going to let this run. And then I'm also going to hit refresh on this static site several times. So I'm going to hit this a few times real fast. You can see it refreshing. And then once I do this 10 to 20 times, I'm going to go now into IAM. So I'm going to load up IAM. And then let me view this role really quickly that we just used just so you understand what's being done. I'm going to look for roles. I'm going to search for my CloudWatch role, and let's look at the permissions. So I gave this CloudWatch Logs full access, but I have a more specific policy to use, which is a little bit more secure. Now for the trust relationship, this is important as well. We're allowing the vpc‑flow‑logs service within AWS to assume this role, which is how it's able to push to our CloudWatch log group here, which will eventually have log streams. Now what I'm going to do is while this is generating traffic, I'm going to keep refreshing. I'm going to let my pings run for a while, and then after a couple of minutes, I'm going to resume this clip, and we'll view the logs that were stored. So I'm going to pause here, and I'll fast forward. All right, so our subnet‑captured VPC flow logs are starting to populate. Our CloudWatch is still not there yet. But while this is coming in, we're going to explore these S3 ones. So in my S3 bucket, we set our prefix, so subnet. And then from here, pay attention to the format of these prefixes. We have AWSLogs, the account ID, vpcflowlogs, the region, the year, the month, the day, and then the hour. So this is the 18th hour based on their time system. Now in here, we see our different flow logs. So this is starting to capture these flow logs. Now you'll also notice it's a log.gz, so this is Gzipped. So what I'm going to do is I'm going to go ahead and download this. And once it's downloaded, what I'm going to do here is I'm going to go ahead and unzip this off screen. I'm going to load it in my IDE, and let's actually check this out. So I've just unzipped this. I'm going to scroll over to my IDE here and zoom in, and notice we actually have how the fields are recorded, and then we have some examples here. So these are all flow log records. So this is a record, this is a record, this is a record. So, records are individual lines within a log file. Remember that. So you'll see it's our ENI ID because that's the only ENI we have going on right now. We see the source address, we see the destination address, so 10.0.0.140, and then a bunch of other stuff. So you can see the protocol, etc. Now you'll notice this is not capturing our HTTP traffic because this was going on before we even started hitting our website. So eventually, these logs are going to contain that specific information, but for now they don't, which is perfectly fine. The big thing I wanted to show you is that you store the logs within the S3 bucket using those configuration settings. And I want you to notice the format here. I also want you to know that these were all captured within that max interval time, that aggregation time. So this is the amount of flow records that were generated within our 1‑minute period in between each individual log file. Now what I'll do next is go to CloudWatch. Let's refresh. And now we have our log stream in place. So we're sending our log files to our CloudWatch log group as well. And log groups contain log streams, which contain the actual log records. Again, we cover CloudWatch in great detail in a different module within a different course, so don't worry about this right now. Just understand you can send your logs to these. So I'm going to select this log stream. And if you notice here, the name, if I go back up, it's the ENI ID and then reject. So this is capturing rejected only. And you can see that here, reject, reject, reject, etc. Now you'll notice a lot of these are the same address here. So these are where the pings are coming in at, and we're denying any ICMP traffic. So anything getting rejected is getting captured within these logs. So this is only capturing reject. Now that's going to do it. Hopefully you saw just how easy it is to set up a VPC flow log for different types of resources, as well as different types of filters and the different destinations. Let's wrap this up here, and we'll move on whenever you're ready.

Capturing Traffic with VPC Traffic Mirroring
All righty. Up next, VPC Traffic Mirroring. VPC Traffic Mirroring is a feature within your VPCs that allows you to copy network traffic from an ENI and then send that traffic to an out‑of‑band security or monitoring appliance so that you can inspect the traffic. Here are some important concepts for the exam. When you're setting up traffic mirroring, you set up a source. So this is the ENI that you want to watch. You set up a filter, which is an optional way to filter traffic. You set up a target, which is a destination for these logs in this traffic mirroring, and you set up a session. Now some of this stuff is a little out of scope, so we're only going to cover the important parts. For instance, destinations. Destinations can be several different supported resources. You can send them to a specific ENI. You can send them to something called a Gateway Load Balancer and even a Network Load Balancer. Gateway Load Balancers are specific for security appliances. What that means is if you need traffic mirroring, with a security appliance, you want to use a Gateway Load Balancer as the destination. Now this offers the capability to filter traffic, and you can even truncate packets for better data extraction. So you don't have to capture everything. You can capture specific portions of your packets. In addition to that, it supports same VPC, intra‑region peered VPC or transit gateway‑attached VPC traffic. So these are all valid scenarios and use cases for traffic mirroring. And then the last important thing here, source and targets do not have to be in the same account. That's a very important thing to remember. They don't have to share accounts. They just have to then be peered between VPCs or connected via a transit gateway. Now let's talk about some use cases where you might see this and need to use it. They're very good for content inspection. So when you capture all the VPC traffic, you send it into your inspection software, and you can inspect all that different content going on without even affecting your network traffic. Building off of this, they're also good for threat monitoring. So you can see, hey, I'm getting some calls from a suspicious block of IPs here. I need to make sure this is not a true threat. And then lastly, they're very good for troubleshooting. So you can troubleshoot troubled connections where there's failures or maybe you need to see there's too much access somewhere. Well, you can do that using traffic mirroring, and it's all out of band and it doesn't affect your source traffic. Now let's look at example‑supported architectures here. In this one, we have the same VPC. So we have a private security subnet here on the left, which is going to host our private security appliance. And we have our private app subnet in the middle, which is hosting a private application. And on the right, we have a NAT gateway. So let's say we want to go ahead and set up this private application with traffic mirroring. We want to watch this source EC2 ENI, and we're going to set it up as the source for our traffic mirror. Well then, we can set the security appliance ENI as the target or the destination for that traffic mirroring. Now the nice thing here is that the original traffic, so any calls our private app is making out through the NAT gateway, those still go out the NAT gateway and they go to the destination with no performance impact. We're essentially just duplicating the traffic and sending it to a different location in addition to the actual destination where we want the original traffic to go. In this example, we're going to do intra‑region VPCs. So when we're setting up traffic mirroring for this type of scenario, remember, VPCs can be cross‑account, and they don't have to be in the same account. This is done via VPC peering or maybe you have a transit gateway set up. In this example, we're going to use the private application in a separate VPC, and we're going to set this up as the source for the traffic mirroring. So we're setting up the ENI as the source. And in this particular scenario over the peered VPC connection or over the transit gateway, we can set the target to be set up as a Network Load Balancer or a Gateway Load Balancer in another VPC. You will use load balancers when you need better scaling and better redundancy. Now depending on whichever load balancer we use, we can then forward that load balancer traffic to actually send it to a back‑end security appliance that might be running on a fleet of autoscaling EC2 instances. So this is another valid scenario and architecture for traffic mirroring. Now with that out of the way, do your best to remember that traffic mirroring is good for capturing duplicated traffic info and then troubleshooting, monitoring or inspecting it without affecting the original traffic flow. Do your best to remember the different sources and the different destination types. And with that being said, we're going to go ahead and wrap this up here and move on to our next clip.

IPv6 Egress-only Internet Gateways
Okay, welcome back. In this clip, we're going to discuss egress‑only internet gateways. Here are some important concepts regarding these resources. This is an internet gateway that is only usable by resources with IPv6 addresses. It's meant to work essentially like a NAT gateway, but only for those IPv6 resources that we just talked about. What happens is just like a NAT gateway, it allows outbound for your IPv6 resources to the internet, and it prevents any inbound initiation. So calls can't start from the outside in. They can only respond from the inside out. Now with these, just like an internet gateway, there's no charge. So you don't get charged for using these. But remember, you do get charged for outbound data transfers, so that's a big thing to remember. Outbound data does get charged. Using the resource does not get charged. And in addition to that, in order to use these, you do have to update your VPC route tables because IPv4 traffic is handled separately. Now let's talk about some exam pro tips here. IPv4 and IPv6 routing is handled via different route tables with different route rules. You have to remember that. When you're using dual‑stack resources, they can use either a NAT gateway or an egress‑only internet gateway. So both are supported because they have both types of IP addressing. And then lastly here, you attach your egress‑only internet gateway to a VPC just like you would a normal internet gateway. So you can have both of them attached to the same VPC for different use cases. Now I've talked about routing and routing rules twice now because it's very important. So let's review an example route table where you might leverage an egress‑only internet gateway. You'll notice this is the syntax for all IPv6 traffic, ::/64. That's going to capture all destination traffic for IPv6. Now for this, we set the target to an egress‑only internet gateway, and that's what this resource ID looks like here. And then the last big point here, notice IPv4 traffic has its own route rule because remember, I can't stress this enough, IPv4 and IPv6 have their own routing rules that you have to account for. Now that's going to do it for this clip on egress‑only internet gateways. Big thing to remember here is that it only works for IPv6 resources, you have to have your own routing rules in place for it, and it acts like a NAT gateway for your IPv6 resources. Let's end this here, and we'll move on to our Module Summary and Exam Tips.

Module Summary and Exam Tips
All righty. Let's get started with this Module Summary and Exam Tips clip. First up, remember you can leverage network access control lists to quickly block suspected malicious IP address ranges. On the exam, this is going to be the quickest and the most cost‑effective method most of the time. One network access control list can cover hundreds or thousands of compute resources. That way you don't have to manage the different security groups, etc. Next up, VPC Flow Logs. Be sure to remember, these are meant to capture IP traffic information within your different VPC network flows. In addition to that, really make sure you're familiar with the default version fields. Remember, the default version is 2, and we looked at those fields within our demonstrations. Another key thing here, these are not meant for packet inspection. So if there's any scenario where it talks about deep packet inspection, you cannot use these to perform that. Please make sure you do remember this. In addition to that information, you need to remember the different supported sources. Remember, you can set up a source to be an ENI, so a network interface, you can choose to monitor an entire subnet, and you can actually choose the entire VPC. In addition to the sources, you have to remember the supported destinations. Right now there are three supported destinations. You can send them to an S3 bucket, you can send them to a CloudWatch Logs group, and you can send them to a Kinesis Data Firehose, which is meant for near real‑time streaming. In addition to knowing this information, really make sure you understand some of the common ports that go along with network flows. I have this list here. I'm not going to read it out to you. That's boring, and you really don't want to hear me just read out what's on the screen here. So I just included it so you can reference it on your own time. Really make sure you are familiar with these popular ports. They could very easily come up on your exam. Next up, we talked about VPC Traffic Mirroring. Remember that this is a feature that allows you to basically copy network traffic from an elastic network interface, and then you can send that traffic to an out‑of‑band security or monitoring appliance, and you can then perform inspection on that traffic. For these, you need to remember the different sources. You can source an ENI. You can set up optional filters. You need to remember the targets, so another ENI, a Network Load Balancer or a Gateway Load Balancer, and you have a session. Now I'll tell you the big things here are you need to know the sources and the target more than anything else for this exam. Some use cases for VPC Traffic Mirroring. You can use it for content inspection, so you want to inspect your traffic. You can monitor for different threats and activity that might be malicious, and you can use it for troubleshooting. So maybe you're having some errors in your network flows and you want to inspect your traffic at a deeper level. Well, you can do that with this feature. And then lastly, it supports same VPC traffic, it supports traffic from intra‑region peered VPCs, and it supports transit gateway‑attached VPC traffic. Also remember the Traffic Mirroring target does not have to be in the same AWS account, so it does work cross account. We then talked about egress‑only internet gateways to wrap things up. Remember, these only work for resources that have IP version 6 addresses assigned to them. Also remember, it's outbound only, so it's similar to a NAT gateway for IPv4. Outbound traffic is allowed, and then that response is then allowed as well. However, traffic cannot start from the outside and come in. Now that's going to do it for this Module Summary and Exam Tips clip. We just wrapped up the module. Thanks a lot for hanging in there. Let's end here. And then when you're ready, I will see you in the next module.

Advanced IAM: AWS IAM Identity Center and AWS Directory Services
Using AWS Directory Services
Hello, and welcome to this next module where we're going to look at some advanced IAM concepts specific to services called Identity Center and AWS Directory Services. In this first clip, we're going to explore using AWS Directory Services. AWS Directory Services is a suite of different features and subservices that are meant to provide you several options for you to set up and then run your own Microsoft Active Directory with other AWS services. In simple terms, this is meant to allow you to offload all of the painful parts of keeping your AD online over to the AWS Cloud. It's also going to still allow you to have full control and flexibility of your Active Directory if you need it. Now, there are three service options within Directory Services. There's AWS Managed Microsoft AD, there's AD Connector, and then there's Simple AD. Now, you need to be familiar with these at a high level for this exam, so we're going to go ahead and dive into each of them now. First up, Managed Microsoft AD. This is going to give you the entire Active Directory suite so that you can easily create and manage your own AD within AWS. So we're talking things like create users, security groups, manage MFA, etc. With managed Microsoft AD, it literally is AD in the cloud. It's powered by a pair of highly available Windows Server 2019 domain controllers. What you're gaining by using the service is there's less admin overhead, right? They're managing all of the infrastructure like your domain controllers, and you just have to go in and manage the actual AD portion of it. So, things like group policies, etc. Now, a big thing to note is that this does support GPOs. On the exam, if a solution requires you to implement one way or two‑way trusts, it's going to have to be this service. This is the only one out of the three that supports trusts. In addition to this, AWS Enterprise services actually require two‑way trust to even function properly if you're using AD to tie into them. You might be asking, okay, well, what is an enterprise service? Well, here you go. Some examples you might see on the exam are things like Chime AWS, IAM Identity Center, and Amazon Workspaces. Those are all enterprise service examples to keep in mind within your scenarios. Moving on, let's talk about trusts. Now, you need to know these at a high level. You won't need to know super detailed information about these, but let's cover them really quickly. A one‑way trust is when users in one domain need access to other resources in another domain, but not the other way around. So it's trusted one direction. Two way trusts are where trusts go in both directions. So, each domain trusts the other, and users have access both ways, so it's two ways. Moving on, let's talk about the next option, AD Connector. This is a directory gateway where you actually redirect your request to an on‑premise Microsoft Active Directory. The big thing with this service and feature is that this solution does not cache any information within the cloud. So it's not processing information in the cloud, it's not storing information in the cloud, it's all just a gateway to your on‑prem service. This will be a huge indicator for the exam. If there's any compliance reasons where you can't cache anything in the cloud specific to AD, this will be your go‑to solution. Now with AD Connector, it comes in two sizes, there's small and then there's large. Now if you need to, you can actually spread loads across multiple connectors to scale your performance to meet your demand. With this particular service, your users are managed entirely within your on‑prem Microsoft AD only. Remember, it's just a gateway to your on‑prem AD, and it doesn't cache anything. Let's review some important concepts here regarding this service. Small AD Connectors are going to be good for smaller organizations, and they're going to be able to handle a low number of operations per second. Now, on the other side of that, we have large connectors. These are going to be good for larger organizations, and they're going to handle moderate to high numbers of operations per second. So this is kind of self‑explanatory, small connectors for small orgs, large for medium or large orgs. Another big concept is that this service does support MFA as well. So if you need MFA within your Active Directory and you want to use this, you can. However, it does not support transitive trusts, and there is a strict 1‑to‑1 relationship between your AD and your connectors. So this goes back to that previous portion where we talk about managed Microsoft AD. This service does not support trusts. Moving on to Simple AD, the last option here. This is a standalone managed directory, and it's essentially a Samba 4 Active Directory compatible server hosted and managed by AWS. Like AD Connector, this also comes in two sizes, small and large. With Simple AD, however, you're only getting a subset of features that are provided by AD. So you're not getting all of the possible features. Also, this cannot be joined with your on‑prem AD system. Please remember that it cannot be joined with an existing on‑prem Microsoft AD. And because you're only getting a subset of features, it doesn't support things like MFA requirements, it definitely does not support trusts, it doesn't support PowerShell commands, and it even doesn't support certain service accounts. Now, the big thing here is just understand it's a subset of features for AD, and it's all managed in AWS. Let's explore the sizes for Simple AD. For small AD sizes, this supports up to 500 users or roughly 2000 objects, including everything that's supported, like a user group and computer. For large, it's going to support up to 5000 users or approximately 20,000 objects. So there's a very big difference there. Now moving on, let's have some exam tips before we wrap this up. AWS Managed Microsoft AD, this is going to give you the most amount of features within Active Directory. And it's going to typically be the best choice for anything that has more than 5000 users, and it is the only service that supports trusts. If in this scenario, there is a question or something relevant to trusts, well, then that should answer your question right there, you need to include this service. Next, AD Connector. Remember that this connects to existing on‑prem ADs, and it's going to be the best choice when you're using existing on‑premise AD setups, and, importantly, you want to avoid data retention in Amazon Web Services. That's going to be the key indicator there. And then lastly, Simple AD, we just looked at this, this is going to be the least expensive option, and it's an AD‑compatible service. Remember, it only offers you a subset of features that would typically be available in Active Directory, and this is going to be the best for very simple workloads where you don't need advanced capabilities. And with that, let's go ahead and wrap this clip up. We're going to talk about AWS IAM Identity Center next.

Single-sign On with AWS IAM Identity Center
Okay, just like I promised, let's talk about single‑sign on with AWS IAM Identity Center. Identity Center is going to be the go to AWS service whenever you need to connect your users to your AWS accounts and your cloud AWS‑managed applications. Let's review some very important concepts for this service, as it's very important for this exam. The big thing is that this is a service meant to provide you a single place to manage all logins within an organization. That's probably going to be the biggest key indicator. Essentially, do you need single‑sign on? Now when you use this, you can connect to existing identity providers to manage your different users. So maybe you use Microsoft AD already. Maybe you use Okta or OneLogin. Well, AWS IAM Identity Center can sync with those different identity providers so that you can manage users in one simple place. However, if you want, you can also create and manage users directly in the service. So maybe you don't have an external third‑party identity provider and you just want to leverage Identity Center instead to create IAM users, then you can do that. Now, there are two primary use cases that this service solves for. The first use case here is that you can grant users access to applications that are hosted in AWS or just in the cloud in general. You can also grant users access to AWS accounts that are a part of a multi‑account organization. Now, multi‑account organizations are a best practice, so this is a perfect use case to grant access to those accounts. Now, when you're setting up Identity Center, there are several compatible applications and resources that you can use for this to grant access to your users so that they can access their workloads. Firstly, you can set it up to support things like cloud applications. So we're talking things like maybe you use Salesforce, maybe you have Microsoft 365 set up, or maybe you use Confluence for Wiki pages within your internal organization. Well, Identity Center can be set up to support authentication and access into those services. You can also have it set up to support SAML 2.0 business applications. So maybe you use a third‑party application or maybe you host your own internal application, and its SAML 2.0 authentication‑based. Well, then, Identity Center plays very well with setting up access and authorization into those apps. And lastly, you can actually use this to grant access into your Windows instances running on EC2. So if you use Windows and you want a way to easily manage remote desktop access, you can use Identity Center. Moving on to some more important concepts here, assignments and permissions. How this works is you actually assign users permission sets, and those permission sets are meant to specify one or more IAM policies that grant the level of access that they need. Now, in addition to this, you assign these users to these different accounts in addition to the permission sets, and they work together to actually grant the access that's required. You also have application assignments. So these allow you to assign users within the service to actually log into your apps. So, for example, the Salesforce example we just talked about, or maybe something like Jira, etc. If it's cloud‑based, it plays well usually with Identity Center. And then lastly, you can actually implement something called attribute‑based access control, or ABAC for short. This is going to allow you to implement fine‑grained access controls for different levels of access based on a user's attributes. So for instance, maybe you have a human resource department, and that's a particular attribute on a user. While using those attributes, you can automatically grant the level of access required to different accounts, so it makes things a lot easier. Now, here we've included some other examples, like department, location, and title. So these are all real‑life attributes that you can use to control access to accounts and applications. Moving on, let's look at a high‑level architecture diagram. Now, we don't cover organizations in this course, it's covered in a different course within the learning path, but understand an organization groups multiple accounts under one big resource, which is known as an org. On the left side of the diagram, we have several accounts, dev, security, and management. Now, security and dev belong to their own organizational unit, which is a way to organize and group like accounts. On the right side here, we have Identity Center. Now, right now we have a support admin group and a security admin group. So an example here is we could have two different permission sets that get assigned to their two different groups. So support could have their read‑only dev permission set, and then we can go ahead and add security admins to their group and assign that group full access admin permission sets. Now with these permission sets and these assignments, we can say, hey, support admins, you get read‑only only within the dev accounts, so you don't get access to anything else. Security admins, though, hey, we're going to give you access to all accounts with full administrative rights. So, the security admins can access all of these accounts within the org. The support admins can only read only within the dev OU. Now, this is just an example. The big thing to remember is that it allows for fine‑grained access controls into accounts. Moving on, here's a view of what the console looks like when you use Identity Center. The first thing here is you can view the assigned AWS accounts or cloud application assignments. In this example, we chose accounts. Now with this, we see on the bottom here, the second bulb, the name of the AWS account, so the human‑friendly name that has actually been assigned to you. Thirdly, we have the AWS account ID, as well as the root user address that was used to create the account. And then lastly, the list of assigned permission sets that we get. So, these are the IAM permissions that we have within this account that is listed. Now by clicking on this permission set in the console, it's going to essentially log you in via a role assumption into that particular account. Now real quick before we move on here to big exam tips, you have to have AWS organizations set up to use this service. And number two, if you need single‑sign on and you have multiple AWS accounts, you want to think this service IAM Identity Center is going to be the best choice. For now, though, let's go ahead and wrap this up. We reviewed the different components, we talked about permission sets, and in the upcoming clip, we're going to have a very quick summary and exam tips lesson. Let's go ahead and wrap things up, and we'll move on.

Module Summary and Exam Tips
All right, welcome to the Module Summary and Exam Tips clip. Very short module, but a lot of important information, so let's review. First up, Directory Services. Remember, Managed Microsoft AD is going to offer you the most amount of Active Directory features, and probably the biggest thing is that it is the only service that supports trusts. AD Connector is going to be the best choice when you have an existing on‑prem AD, and this is likely the biggest key indicator here. You want to avoid data retention in AWS. And then lastly, we had Simple AD. This is going to be the least expensive option, and what it does is it offers you an AD‑compatible service. So it's not quite Active Directory, it's compatible with Active Directory. This option is going to be perfect for the simplest workloads where you don't need any advanced capabilities. Next, we talked about Identity Center. Remember, you have to have organizations set up in order to leverage this. Even if you only have one AWS account, then you still have to have organizations set up. You use this to grant access to different AWS accounts, and you set the level of access within the accounts using what are called permission sets. Permission sets are essentially policies that get assigned to a role that is assumed by your users. Now, you can also use Identity Center to grant access to cloud applications via application assignments. So remember, it supports integrations with things like Salesforce, Jira, Confluence, etc. Also, you can use attribute‑based access control, otherwise known as ABAC, to set up fine‑grained access controls into different accounts. So, some examples could be department tags on the user, it could be the title of the user, etc. And then lastly, you can leverage existing Identity providers with Identity Center to simplify user management. So maybe you use things like Okta for single‑sign in already, or you have Microsoft AD where you manage all your users, or you can sync those with Identity Center to make it a lot simpler to log in to your different accounts. The big thing, though, on the exam that I would say is probably going to be the biggest indicator for you is if there is a question about single‑sign on or managing access to multiple AWS accounts in a simplified manner, you're going to want to think AWS IAM Identity Center. Now, that's going to do it for this module. This is a very quick one. Let's wrap things up, and we can move on whenever you're ready.

Advanced IAM: Complex IAM Policies and Conditions
Troubleshooting Overlapping IAM Policies
Hello, and welcome to the very last module in this course. We're going to wrap things up here with some more advanced IAM concepts, specifically scenarios regarding complex IAM policies and conditions. In this first clip within this module, we're going to look at troubleshooting overlapping IAM policies. On this exam and within any real‑world design for IAM, you have to understand the permission evaluation order within AWS Cloud. You need to remember this order. The most powerful statement in any part of this chain of permissions evaluation is an explicit deny. If at any point there is an explicit deny statement, it will always override any allow. If there's no explicit deny explicitly stated, the next check is going to be for a service control policy, which is an organizational policy to see if something is allowed or denied. Now, we cover service control policies in depth within a different course within this learning path. However, you need to understand this order, and you need to understand service control policies are second in line only to explicit denies. We then have a resource‑based policy. So if there's no service control policy in place, the next thing it looks for is a resource‑based policy. A perfect example of this would be a KMS key policy, or more relevant to this course, an S3 bucket policy. A great example for this is if you remember in one of the demonstrations, we said the S3 bucket was allowed to be talked to by every principal, and then we tested with an IAM user that did not have a deny in place or an allow, but that IAM user was allowed to interact with that bucket. The fourth in line is an identity‑based policy. So this is going to be something attached to an IAM user or a role. Fifthly, permission boundaries. Now we're going to cover permissions boundaries here in this particular module, and we're going to talk about what they are and how you use them, but understand right now they're fifth in line. Next is a session policy. Session policies are specific to role assumptions for whenever a session is created and you obtain some temporary token credentials for the role. Now, the biggest thing to remember here, in terms of the order, IAM will always first look to see if there is any explicit deny statements anywhere within this chain. If there is, then everything is denied. Secondly, as it's doing this, it's going to follow a simple Allowed or Denied logic tree. So, basically, yes or no, and it's going to follow through that tree, and then evaluate in this order. Now, in addition to this, it's very important you remember, AWS for the most part is private by default. So within IAM, there's always going to be an implicit deny in place for any requests. In other words, you have to explicitly grant that access somewhere, whether it be a resource‑based policy or an IAM identity‑based policy, but regardless, there's always an implicit deny unless you allow it. Now let's actually explore a conflict example. On the exam, there's going to be some scenarios where you might have to distinguish what's going on within a more complicated IAM policy. On the right here, we have a policy with conflicts in it, or overlapping policies. At the top, we're actually explicitly allowing s3.Get and s3.List actions on all S3 resources. Now it's important to call out this is an IAM identity‑based policy, so this is attached to a user or a role. The second statement block here is where we are explicitly allowing this IAM identity to use the s3:PutObject call within this super‑fancy‑bucket‑name s3 bucket. And then on the bottom here, we have an explicit deny, denying any put object calls, as well as DeleteObject calls within the production prefix of that same bucket. So let's explore the evaluation order here. We have our S3 role here, and we have our users on the left. Now, this IAM role is going to have the same policy we just explored attached to it. So when our users go ahead and assume our role and they get their temporary credentials, they're going to go ahead and use these credentials to begin making some API calls. So in this case, we're going to show three different API calls to the same bucket, and you can see those listed here. So, based off that policy we just reviewed, do you know which one of these calls would be denied and which one of these calls would be allowed? Well, this first call would be allowed, the second call would also be allowed, and this third call is going to be actually denied. So let's review why. If you remember the policy we just reviewed here, at the top here, we're allowing all Get and List API calls for every resource within S3. So we can get bucket and point to our S3 bucket. So this is why this first one's allowed. For the second statement, PutObject with under it /development/file.pdf, well, we're allowing PutObjects for all prefixes as long as it's not production. And speaking of production, this last one is trying to put an object under that production prefix. So really for these last two, the big difference here is the prefixes. Slash development is allowed because we don't deny it, and in fact, we actually allow it. However, for the production prefix, we explicitly deny. So remember, explicit denies always win. Now I told you we would review permissions boundaries real quick. Because we talked about them earlier in this clip, let's review these really fast. A permissions boundary is a feature in IAM where you actually use a managed policy to set the maximum amount of permissions that an identity‑based policy can even grant to its entity that it's attached to. What this means is that this entity can only perform the actions that are allowed by both the identity‑based policy and this permissions boundary. So, essentially where the permissions overlap between the two. An example use case would be delegation of IAM responsibility. Let's say you have an IAM administrator and you want to make sure that you limit the maximum permissions they can grant to the users. In addition to this, you want to prevent the removal of these particular boundaries as well. This would be a perfect scenario for permissions boundaries. You can set the maximum amount of permissions that these IAM administrators can even delegate in the first place. Now, that's going to do it for this first clip within this module. We reviewed the permissions evaluation order, we talked about there always being implicit denies, and we reviewed how to look through an IAM policy that has overlapping or conflicting logic, and we just talked about permissions boundaries. Let's go ahead and wrap this clip up and we'll move on to the next one.

Custom Conditions and Statements in IAM Policies
All right, let's get started with our next clip. During this clip, we're going to look at custom conditions and statements within IAM policies that you need to be familiar with. Let's start things off with some condition statements that you need to know at a high level. For this exam, I'm 100% positive you need to know the following condition statements, ExternalID, MultiFactorAuthPresent, SourceIp, and PrincipalOrgID. Let's explore each of these now. First up is ExternalID. This is going to be where you require a unique identifier be present whenever a role is assumed from another account. A perfect example use case for this is where maybe you have a third‑party service that needs to access resources in your account. Well, you can add this extra layer of security to prevent any "confused deputy" problem. Now, I'll tell you right now, if you see "confused deputy" on the exam, this is going to be a perfect use case for ExternalID conditions. So be sure to add that into the trust relationship policy for the role. If you're not familiar with "confused deputy," it's essentially preventing someone from accidentally obtaining too many permissions within your roles. Now here is an example policy using ExternalID. We see we're allowing the principal, here which is an entire AWS account, to assume the role. On the bottom here is the condition. So what we're saying is, hey, we need to look for this ExternalID key within the call, and it needs to match this value on the right side of this colon. So this condition is a StringEquals condition looking for this unique ID. Next up, we have MultiFactorAuthPresent. Summary of this is that it's going to check if the user has authenticated with multifactor authentication, so the user has that token in addition to its normal credentials. This is perfect for enforcing additional security requirements for sensitive operations, for instance, like MFA Delete, or before deleting terminations of instances. So if you want to prevent an instance from being deleted and you want to make sure that it really needs to be, well, then you can put MFA requirements in place to really add an extra layer of security and checks. Now, here's an example of using MultiFactorAuthPresent. Same type of policy here. We're allowing all S3 actions for all S3 buckets. And then on the condition block, we're checking for a Boolean value that's true. So we're saying, hey, we want to look for this MultiFactorAuthPresent key within this call, and if this Boolean is true, then we will allow it; otherwise, everything is denied. Now, I will call out really quickly on this. You can see this two different ways. Sometimes the condition is instead of Bool, it's saying Bool exists. And with that being said, you need to be able to decipher both ways. Regardless, just take your time and make sure you read through each policy and really do your best to understand what's going on. Next, we have SourceIp. So this is where you can restrict access based on an IP address of the requester making the API call. An example of this would be to limit access to AWS resources to specific network locations. For instance, maybe you want to only allow API calls to come from your office location. And here is an example of what this would look like. Again, another identity policy here. But what we're doing is we're saying, hey, deny all actions on all resources if this IP does not match these SourceIp lists that I have here. So to decipher this, we have a list of source IPs that you can see on the bottom right. So if it's not IP address, aka, it's not coming from this list, we're going to deny. Now, typically, this is going to be used in conjunction with another allow statement, but for sake of simplicity and ease of readability, I wanted to just show you this deny statement. Regardless, just remember, it's used to lock down to a certain set of IPs or prevent actions from a set of IPs. Next, we have PrincipalOrgID. This is going to validate that a request comes from an account that's a member of a specific organization. Now, we've talked about this several times. We talked about organizations much more in depth in another course in this learning path, but at a high level, an organization is a hierarchy of multiple accounts that belong to a single resource known as an org. Now, with this org, you get an organization ID that you can reference. So that's where this comes into play. An example to use this would be to share resources across accounts within your org while preventing access from external accounts. This policy here that I'm showing you now is a real‑world example. I've seen it used all over the world through a ton of different enterprises. Now, obviously, the resource is not accurate, this is a made‑up resource, but the idea of the policy and the design is accurate. This is perfect for cross‑account S3 access. So what we're saying here is we're allowing all principals to put objects into our bucket here with our prefix. Now, typically, remember, you wouldn't want to open up access to all principals. That's a bad practice. But in this case, on the bottom, we're adding this condition where we're saying, hey, we need a StringEquals where the PrincipalOrgID equals our organization ID that we get from AWS organizations. How this works is every member account within your organization has this organization ID included within their AWS API calls. So you can easily reference it and restrict it to only your organization accounts to be able to put objects this way. This is a very real example of real‑world and exam scenarios, so make sure you really understand this. And with that out of the way, let's go ahead and wrap this clip up, and we're going to move on to a couple of demonstrations where we demonstrate some of the stuff we just talked about.

Demo: Using SourceIP in IAM Policies
All right, hello and welcome to this demonstration where we're going to use the SourceIp condition key within a bucket policy. Now, in this demo, I've already created our SourceIp demo bucket here, and all I did was create this with all of the defaults in place. In this demonstration, what I'm going to do is actually use a deny statement to demonstrate how we can use SourceIp list to deny actions as well. Now remember, this can go both ways, you can allow or deny using this same key. For sake of simplicity, it's going to be easier for me to deny. So what I'm going to do here is go to Permissions. I'm going to go to Bucket policy and click on Edit. Now, what I'll do here is I'm going to copy and paste in a bucket policy really quickly. So I'll paste this in here, and then I also have to copy and paste in my bucket ARN here, and there we go. So what we're doing here is I'm denying all principals, the PUT action API call, for our bucket with our condition here. So we're looking for a SourceIp address, and this SourceIp address is my local IP, and remember, from a networking standpoint, a /32 is the most specific you can get as far as addressing goes. So this is my exact single IP address. Now, also remember, you can make this a list and contain multiple IPs, you can change it to a /8, a /20, whatever it may be. For this demo, I just locked it down, or I should say denied it, to my single IP address. So now what I'll do is I'm going to go down here and I'm going to click on Save changes. There we go. So now we have my bucket policy. So let's go and test this out. I'm going to go back to my bucket here and let me try and upload an object. So what I'm going to do here is actually try and upload a PNG file. We're going to get the menu here. I'm going to click on Upload, and boom, failed because access denied. So our bucket policy is working perfectly because remember, we denied any PutObject calls within it from my one IP address. So now what I can do is close this. Let me go back to Permissions. I'm going to delete my bucket policy instead. So let me go ahead and delete it. So now we're back to the basic bucket policy, which was nothing at all. And I am logged in as cloud_user, and within the Sandbox, this has admin rights. So I'm going to go back to Objects. I'll click on Upload, and let me try uploading again. I'll copy in, I'll upload, and there we go. And that's a perfect example. So now my bucket policy is not restricting based on my IP addresses, so now I can upload. And that's how easy it is to leverage the SourceIp condition key within something like a bucket policy, which is a resource‑based policy. Big thing to remember here, you can use it to allow or deny. You don't have to use it to do one or the other. It's flexible both ways. Now, with that being said, let's wrap this demo up, and we'll move on.

Demo: Requiring an ExternalID for Assuming Roles
Okay, welcome to this next demonstration where we're going to require an ExternalID be present in order to assume a role. I'm in my Sandbox environment here, and let's get started. Now, I have two different accounts here. I have my default Sandbox environment here, and then in a Firefox window, I have another separate Sandbox account, and I'm logged in over here as an admin user that has administrator access. So this user can perform pretty much every single possible AWS call as far as API actions go. Now, I'm going to go back to my regular Sandbox here, and this is where we're going to create our role that gets assumed. So, what I'll do here is I'm going to go to Roles, I'm going to click on Create role, and from this, I'm going to select AWS account. Under here, I'm going to select another AWS account. I'm going to go back over and copy this temporary account ID here. I'll paste it in. And then we're going to require an ExternalID. Now, this is where you can put in your randomized ExternalID to make sure that whoever is assuming this role is really meant to. Now a tip here is they have this nice handy dandy online site if you want where you can generate different unique IDs here, so I'll just go and use this one. I'm going to copy it, I'll go back, and I'm going to paste it in. So any time we're trying to assume this role from this account, we need this ExternalID. Now I'll click on Next, and let's just give it admin access. I'm going to scroll down here and click on Next again. I'll give it a name, and we see the trust policy where we're saying, hey, this AWS account can assume our role with this condition. So we're looking for the ExternalID key with this unique value. I'll go down here, I'll click on Create role. Perfect. So now let's test actually assuming this. So I'm going to click on View role. I'm going to go up here and copy the ARN. And offscreen here I actually have an IDE that I'm going to copy and paste information in and out of. So give me one second. So I've copied this role over there. Let me go to my terminal, and let's test this out. So to run this, I'm going to copy and paste this command in. So we're going to run aws sts assume‑role. You then give it the role ARN. So I'll paste that in here, and this is the ARN that we just created. You then provide a role session name. So you can name this literally whatever you want. So we're going to just call it YourSessionName. And then lastly, we're going to copy in our ExternalID. So here I paste in external‑id. I'm going to go back to my browser here. I'm going to copy this unique ID, go back, and paste this in. So now when I hit Enter, we get our temporary credentials. So now what we can do is we can save these into a local temporary file. We could take them, parse them, and then export them into our environment variables. There's a lot of different ways we can use this. However, the big thing here is we've now just allowed ourselves to assume this role. Now, let's try this without the ExternalID. Let me erase this, and I'll run the same command. There you go. AssumeRole denied because we're not authorized to perform it because we don't have the ExternalID. Now, obviously, for security purposes, they don't tell you this within the error message, but this is the reason why. And that's how easy it is. We've now required an ExternalID be passed in with the assumed role call for our role. Now, I want to show you why this is such a big deal. What I'm going to do is go back to my Chrome here and I'm going to create a similar role. So I'm going to go to Roles, I'll create a new one. We'll say AWS account. I'm going to copy in that same account ID from before. So another AWS account, paste it in, and we're not going to require ExternalIDs here. So I'll click on Next, give it admin access, Next, I'll give it a name here, and then I can click on Create role. Now, the reason this is such a bad idea is because this role over here in this other account has admin permissions. So in theory, if they got lucky and they found out the role ARN in that other account, well, this user could assume this test role even if you did not want them to. So that's why an ExternalID is so important. If you require this unique ExternalID, well, then that source roles or source users are not going to be able to actually assume this. So it prevents abuse, even if it's accidental, for privilege escalation. Now, one last thing I do want to show you, it's important to note just for sake of actually using this in the console. When you require an ExternalID, like we did for our other ExternalID role here. So if I load this up here, and we see our trust policy, you can't assume this role within the console. So if I just go up here and click on Switch role, you're going to notice there is no field for an ExternalID. So it can only be done via an API call via an SDK or the CLI. Now again, this won't come up on the exam, but I like to tell people because it can be confusing, as this is the typical way that people like to switch roles. And with that being said, that's going to do it. Remember, using an ExternalID in the trust relationship policy document allows you to implement better security controls and prevent the confused deputy problem that goes along with role assumption between accounts. We're going to go ahead and wrap this demonstration up now, and we're going to move on to our Module Summary and Exam Tips, which will be the final one for this course.

Module Summary and Exam Tips
Okay, thank you so much for hanging in there! This was a pretty quick module, not a very quick course, so I do appreciate you taking the time and sitting through this all. Let's wrap up this very last module with some exam tips. First up, remember, explicit denies will always win no matter what. If there's no explicit deny, the next thing we look for is a service control policy, which is an AWS organization's resource. If there's no SCP, or service control policy in place, we then look at resource‑based policies. So, an S3 bucket, for example. If there's none of those, we move to identity‑based policies, so the permission policies attached to users and roles. We'll then move to permissions boundaries, and then we'll finish things up with a session policy. Now, do your best to remember this permissions evaluation order. The biggest thing I can tell you, though, that you have to remember here, explicit denies. Anywhere within the evaluation order will always win. You cannot override an explicit deny. In addition to this, new I AM entities always start with zero permissions. What this means is that by default, there is an implicit deny present during all action evaluations. So unless you explicitly grant the access to your entities, it's going to be denied by default. Moving on to permission boundaries. Remember, these are meant to restrict the maximum permissions allowed for a user and what can be granted. A big thing to know with permission boundaries, they do not grant permissions; they only limit the permissions. A popular exam scenario for using a permission boundary is IAM delegation. So you have an IAM administrator who creates users and policies, and you want to limit the maximum amount of permissions they can actually assign. Moving on from permission boundaries. You need to understand when and how to use correct condition keys within your IAM policies. Speaking of those keys, here are some popular ones you need to remember for the exam. Now this is just a small list, so make sure you take the time to review others, but these are four of the big popular ones, so that's why we cover them. The first is ExternalID. This is perfect for the confused deputy problem. There is MultiFactorAuthPresent, which checks for a Boolean if there is multifactor tokens present in the call. There's SourceIp, where you can specify a set of IP addresses to restrict or allow access to. And there's PrincipalOrgID, so all member accounts of an organization in AWS contain an organization ID key that you can reference. These are perfect for restricting access cross‑account, and a perfect example of that would be an S3 bucket for centralized logging where you only allow accounts within your organization to actually put objects into it. And with that being said, this is going to wrap up this very last module in this course. Thank you so much for hanging in there! Let's wrap this up, and I will see you in another course hopefully soon.
