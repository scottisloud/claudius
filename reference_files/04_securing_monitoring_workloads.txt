Amazon CloudWatch
Amazon CloudWatch Overview
Okay, hello, and welcome to this course titled Securing and Monitoring Workloads. This is yet another course within the AWS Certified Solutions Architect Associate Prep learning path. This very first module, we're going to talk about monitoring workloads, more specifically with the Amazon CloudWatch service. So let's go ahead and jump in. Amazon CloudWatch at the highest level is a monitoring and observability platform that's designed to give you many different ways to gain insights of your AWS architectures and the applications that they're running. It's made up of several different tool sets and features that allow you to monitor multiple levels of your applications, so if it's multi‑tiered, etc, and it helps you quickly identify potential issues that might be occurring, as well as ways you might be able to optimize your workloads. Some features that we're going to cover throughout this module include the following. We're going to look at different ways and different types of metrics you can collect, including system metrics, application metrics. We're going to look at collecting logs and log files. We're going to look at how we can use the service to trigger alarms, and we're going to learn how we can leverage insights, which are features provided for us to use in CloudWatch. Now first things first, a big thing to keep in mind here, this service, CloudWatch is the service to consider for most things related to monitoring and logging in the AWS cloud. Unless there is a very specific reason not to choose this service, this is likely the one you're going to want. With that being said, let's end here, and we're going to start diving into some of the more specific components you need to know for this exam.

Amazon CloudWatch Logs
All righty, we now know what CloudWatch is at a high level. Let's start looking at how we can use the different services and features within it to better our architecture design. First thing we're going to look at is using it to collect our log files. Picture this. You just deployed your brand‑new shiny application to AWS, and you think you've followed all of the best practices, and everything seems to be running appropriately. Within your applications and your systems, you start generating a lot of logs because, well, your user base has really grown tremendously. So now you have EC2 logs where your apps are hosted. Well, you also have RDS hosting your database, so there's logs coming from RDS as well. And you also integrated Lambda for some event‑driven APIs. And then you have CloudTrail logs for security, and maybe you're even using on‑prem architecture, so you're getting on‑prem logs. Well, that is a lot of different logs to keep track of. So how can we centralize those and be more efficient with how we manage them? Well, that's where Amazon CloudWatch Logs comes in. This is the tool and offering within CloudWatch that allows you to monitor, store, and access your log files from a variety of different sources. Within the service, it gives you the ability to query your logs so you can easily look for potential issues and data that is relevant and specific to your needs. In addition to centralizing the management of your log files, well, they're also encrypted by default. And if you need to, if there's ever a scenario where you need to customize the encryption of specific logs, well, you can use KMS keys to accomplish that. We're going to look at some of the key terms here in a moment, but you can encrypt different log groups containing different logs for different applications however you see fit. In addition to collecting the logs, there are going to be scenarios where you need to send logs to other destinations to analyze, parse, etc. With that being said, you have to understand the destinations that are possible. They include S3, Kinesis Data Streams, Data Firehose, AWS Lambda functions, and OpenSearch clusters and domains. Now we will dive into each of these much deeper later on, but for now just understand they do natively integrate with those services. When you're using CloudWatch Logs to log all of your app data and your service data, there are three important terms you must know. First is a log event. This is exactly what it sounds like. It's the events in the record of what actually happened within your application or the AWS service that is generating the log file. Within your events, it's going to have timestamps and raw event messages, including any custom logging events that you might have coded, so maybe INFO, WARN, ERROR, etc. When you have several log events, they make up a log stream. So a log stream is the collection and sequence of your events from that same source. The easiest way to think of a stream is one continuous set of logs from a single Apache web server instance or maybe the same lambda function after an initial cold start starts up. Again, think of it as a stream of logs, so a stream of events. Thirdly and last here is the log group. This is the collection of log streams with the same retention, monitoring, and access control settings, so IAM permissions, encryption, etc. An example for a log group would be grouping all of your Apache web server logs across all hosts together in the same group. So the high‑level log group is Apache web server logs, and then each stream is specific to an instance ID. That'd be a perfect example. Now when you're configuring your log groups, your log streams, etc, you have to set a log retention setting. CloudWatch Logs allows you to set this customized setting for each log group. You use this to set up what your expiration would be. So you can say, hey, I want logs to expire after 1 day, 7 days, up to 10 years, or maybe you never want them to expire. These are all valid possibilities using log retention. The big thing to remember is you can use this setting. You shouldn't really be tested on minimum and maximums, just more the concept. In addition to understanding the retention, of course, you have to have sources for your logs, otherwise this service does no good. Here are the important services and sources that you have to know for this exam. This is a long list, so take the time you need to really make sure you memorize these and remember them. Of course, the first is our SDKs and then something called the CloudWatch agent. Now we're going to look at the agent in its own clip later on, so don't worry about that for now. Just understand that it is a log source. We also can very easily send lambda function logs there. Now if you've seen demos and other courses with lambda functions, you've seen this already. You can configure VPC Flow Logs to send there, Amazon API Gateway APIs. Your CloudTrail trails can be sent there. You can send your Route 53 DNS query logs there, Elastic Beanstalk application and environment logs, and lastly, ECS containers. Now again, this is not an exhaustive list, but this is an important list. Be familiar with these sources for the exam. Now in addition to your sources, sometimes you're going to have scenarios on the exam where you need to send your logs to another destination, not just CloudWatch, because you want to perform actions on those logs. Well, that's where log subscription filters come in, so let's look at these. This is a feature that's going to give you real‑time feeds of log events from CloudWatch Logs, and you can have them delivered to other services and locations for both actions and even long‑term storage. It natively supports the following, Kinesis Data Streams, AWS Lambda functions, Amazon Data Firehose, and Amazon OpenSearch Service. Do make sure you remember those for the exam. These do come up a lot. When you're using your subscription filters, you'll use a filter pattern, which allows you to set which log events you actually want to have delivered to your AWS downstream resources. This way you're not just sending every bit of data there, you can filter what's getting sent. These log events get sent to the configured downstream resource very soon after being ingested, so keep that in mind. And then last thing here, log subscription filters can be set at an account level or a log group level. If you set it at an account level, then there's one account level subscription filter per region. If it's log group level, you can have two subscription filters per log group. In addition to subscription filters, you can also actually export logs to Amazon S3 buckets. You would do this if you want to export a lot of logs to your bucket for some type of custom processing or maybe long‑term cold archive storage. The buckets that you're using can be in the same account or they can be cross account. Remember, that means if they're cross account, you have to grant IAM permissions to allow the service to put the logs there, and the bucket policy will likely have to trust the service from the other account to allow those objects to be put as well. That's an important thing to remember. Thirdly here, when you're using export, the logs can actually take up to 12 hours to become available before they actually get exported. So what that means is that this is not even close to real time or near real time. So you need to take that into consideration when you're having to use this feature. And to wrap things up, one exam tip before we close out. Log groups are regional resources. Remember that as well. I know there's a lot to remember in this clip, so feel free to review it once again, but do remember they are regional. And with that being said, we're going to end this here, and we're going to move on to custom logging using the agent, which I told you we would talk about earlier. So let's go ahead and move on to the CloudWatch agent now.

Custom Logging with the CloudWatch Logs Agent
Okay, we talked about some of the default built‑in and native log sources that you can use with CloudWatch Logs. Let's now look at custom logging with custom sources using the CloudWatch Logs agent. The default behavior for your custom application logs that are running or generated via your applications running on your EC2 instances are for them not to get sent to CloudWatch Logs. So it's not like lambda functions, built‑in ECS logging, etc. But with that in mind, you can customize this behavior. That's exactly what this agent does. It's an installable agent that you install on your compute that allows you to do the following. You can very easily collect detailed EC2 system‑level metrics. You can collect system‑level metrics from on‑prem servers or servers in another cloud. And you can retrieve custom metrics from StatsD and collectd services. So if this is the service you're using on Linux, you can go ahead and collect metrics using those processes and send them to CloudWatch. And then lastly, in addition to just metrics, you can collect logs. So that's the big thing we were talking about. You can generate logs on your EC2 and on‑prem servers or external servers somewhere in another cloud, run this agent, and it sends your logs to CloudWatch for you to use. Now an exam pro tip here. If you're going to use this, well, you have to configure proper IAM permissions for the agent to push to CloudWatch Logs. Just because the agent is running doesn't mean it will automatically work. Of course, IAM is required for everything in AWS, and this is no different. Now moving on, there are two versions of the agent. So let's go ahead and look at these at a high level really quickly. The original version is called the CloudWatch agent. Again, this is the original version, and it's got more limited support of the logs and the metrics that you can collect, as well as the destinations that you can send to. The unified agent is the updated version of the same agent, and it has many more supported collection options, so many more metrics, log locations, etc, and it supports many more destinations. You don't have to just send to CloudWatch, but it supports CloudWatch and more. Now I'll tell you really quickly. On the exam, the important thing to remember here is that you need to know when you should use the agent. You really shouldn't be tested on the differences between the two versions. It's more so understanding when you would use it and how you would use it. And with that being said, let's compare default metrics for EC2 instances and agent‑supported metrics, so the different metrics you can collect in addition to default. When you're using default metrics and CloudWatch for EC2, it'll collect disk consumption information, high‑level CPU usage, and high‑level network usage. That's pretty much it. When you use the agent, you collect many more, including much more in‑depth disk metrics, in‑depth CPU and RAM, and even process and swap space information. Now I'll tell you, a big one that comes up on the exam is RAM usage and process information. So do your best to remember those. But the big thing to really remember is that the agent supports much more capabilities compared to default. And to wrap things up, let's have a quick exam scenario where you would use the unified agent. Let's assume you're working for a company that audits every Windows and Linux EC2 instance each month to identify potential performance issues, but they have hundreds of instances running in production. Each one of these instances running has to have some type of logging method to collect various system‑level details about those instances. The results that get collected have to be eventually saved to an S3 bucket because the teams are going to use analytics tools to analyze that data. What's an efficient way to collect and analyze logs from the instances with minimal effort? Well, let's look at the solution now. One possible way to solve this, and remember, this is just a possibility, you could automate the installation of the CloudWatch Logs agent on your instances using Systems Manager automation documents. After the agent is installed, you could then configure the agent, whether it be programmatically or manually, to automatically collect and push data to CloudWatch Logs based on the operating system. So for instance, maybe you have a log group for Windows and a log group for Linux, and then within those log groups, you have different log streams based on instance ID, application, etc. And then thirdly, once all this is set up, you can actually automate exporting the logs to S3 using maybe a lambda function, so those teams can leverage those exported logs when they're ready for their analysis operations. Again, this is just one possible solution, but the big thing to take away here is you use the agent to collect more in‑depth information and send it to CloudWatch where you can perform other operations. Let's go and wrap up here, and we're going to move on to metrics next.

Amazon CloudWatch Metrics
Okay, we just got done looking at how we can set up custom logging solutions using the agent within CloudWatch. Let's now look at some other specifics geared around capturing metrics with CloudWatch. Remember, first and foremost, CloudWatch does both logging and observability using metrics. So this is another critical component that you have to know for the exam regarding the service. And with that being said, when we say metrics, metrics are something that are regional, and what it is is it's a time‑ordered set of data points that get published to the CloudWatch service. Now most AWS services are going to provide free metrics for the resources that they create. For instance, this could be CPU usage for your EC2 instances. That is included in a default metric that is captured. Now when you capture these metrics, by default, they come in at a 5‑minute interval; however, you can optionally enable what is called detailed monitoring for some resources that support it. Detailed monitoring is going to be a much faster interval down to 1 minute. In addition to speeding up the monitoring process for your metrics, you can also publish your own custom metrics using the agent and some other services. In addition to creating custom metrics, you can create and share dashboards containing a centralized view of your metrics. This dashboard is an easy to use and view interface, and again you can share it with other IAM users and even other accounts. Moving on to some concepts that you need to know regarding metrics in CloudWatch. You have to be familiar with these important concepts that we're about to show you. One is a namespace. This is going to be essentially a container where you group and isolate common metrics together. Next is a timestamp. This probably seems pretty self‑explanatory, but every metric data point that gets collected will have a timestamp associated with it. That's how this is time series. It captures the metrics in a time‑series fashion, and that's how you monitor and observe your resources and your usage, etc. And then lastly, a dimension. This is going to be a name and value pair that's really meant to identify the metric being used. For instance, a dimension example could be an instance ID, a lambda function name, things of that nature. Again, it's just meant to help you uniquely identify what is being captured and from what resource. Now moving on to monitoring, we briefly talked about basic and detailed monitoring a little bit ago, but let's dive into that a little bit more because you do have to understand when you would use one or the other. For this exam, it's really going to come down to a majority of the time when you would use one or the other for EC2 instance monitoring. Now there might be other use cases, so you do need to pay attention, but a large amount of the scenarios that come up regarding this will deal with EC2 compute. For detailed monitoring, again, it collects intervals in a faster period, and it only works for certain AWS resources and services. And in addition to that, it could incur additional costs when compared to basic monitoring. So it's very specific use cases where you need to turn this on. When we're talking specifically about EC2 basic monitoring, the metrics get delivered to the CloudWatch service in 5‑minute intervals. So for some instances, that might not be good enough. If you have a production system, you want something a little bit more detailed, well, that's where detailed monitoring comes in. By enabling detailed monitoring, you can gather the metrics into CloudWatch in 1‑minute intervals. So it allows you to respond a lot faster, but again, you could be incurring additional costs compared to basic monitoring. One exam scenario that could come up regarding when you should use detailed monitoring. We can assume you need to analyze the performance of an application running on an EC2 instance. With that, you need to have a granularity of no more than 2 minutes. Well, that's where detailed monitoring excels. Remember, basic monitoring is 5 minutes, but detailed monitoring is 1 minute. So this would fall in perfectly for detailed monitoring. Moving on, let's look at metric streams. You need to understand these as well. This is a feature in CloudWatch metrics that allows you to continually stream your CloudWatch metrics to different destinations. So it's very similar to a subscription filter, but this is only specific to metrics. With metric streams, you get near real‑time delivery and extremely low latency. So these are very powerful, and you should use them if you need to monitor an alert based on metrics. Some of the commonly used services that you stream your metrics to include Amazon S3 buckets. This is a very common exam scenario, so you can set up your metric stream, perform your logic, and send it to S3. And lastly, it does also support several common third‑party services, so Datadog, Dynatrace, Splunk, things of that nature typically support metric streams natively. So if there is a scenario where you have to stream metrics to a third‑party solution, this might be a perfect choice. Now breaking things up here with a quick exam tip before we move on. You can use a metric filter to allow you to turn log data into numerical CloudWatch metrics, and then you can alert off of those and take actions. Now, we're going to talk about alarms and all that stuff coming up shortly in an upcoming clip. Let's go ahead and end here, and we're going to move on to a demonstration where we capture logs and metrics in CloudWatch.

Demo: Capture Logs and Metrics in Amazon CloudWatch
In this demonstration, we're going to log into our hands‑on playground, and we're going to spin up a brand‑new EC2 instance, and we're going to install the CloudWatch agent on there. We're then going to configure it, and we're going to test capturing more detailed metrics and custom application logs. So let's go ahead and jump into the console now. All righty, I've jumped into my hands‑on playground here. I've loaded up EC2. Let's go ahead and get started. First thing I'm going to do here is I'm going to launch an instance. So I'm going to select Launch instance, and I'm going to give my instance a name. We'll just call this TestingLogging because that's what it's doing. And then we're going to go down here, and I'm going to choose Amazon Linux 2023. So this is the latest AMI for the Amazon Linux 2023 operating system. So keep that in mind if you're doing this yourself with a different operating system, some of the steps might be different. For now though, I choose the latest. I'll go down. I'm going to choose a t3.small. And then for key pair, I'm going to say none, and I'm going to leave the default network settings. Now one thing to keep in mind here. Since I'm using the default VPC, this has access to the CloudWatch service. That is important to remember for the exam. If you're using private subnets, you need either a NAT gateway or VPC endpoints. But since I'm using the default VPC, I have service connections. Please remember that for the exam. Now I'm not going to allow any traffic. I don't need it. I'm going to skip down here. And under Advanced details, I'm going to do a few things. First thing I'm going to do is select my EC2_SSM_ROLE. Now I'll show this once we launch the instance, but this has some specific permissions for this demo. The next thing I'm going to do is skip all the way down to User data, and I'm going to paste in my script here. So I'm updating Yum, and then I'm installing the Amazon CloudWatch agent, as you can see here on this third line. So once this installs, we'll then configure it. So I'm going to go through here. I'm going to launch my instance. Perfect. So now I'm going to select this. It'll open this up in a new tab. And I want to show you my IAM role. So I'm going to open up EC2_SSM_ROLE in a different tab here. And what I have is the default managed permissions here from AWS, so SSMManagedInstanceCore to allow me to connect via Session Manager. And then I have this CloudWatch Logs permission. Now it says AppSync, not a big deal. It really just allows us to create log groups, log streams, and put log events, which is what we need to allow our CloudWatch agent to push logs. So I was just lazy or some might call it efficient, but I selected a managed policy that has the exact permissions that I need and attached it to this role, so just keep that in mind. This is a common exam scenario. You need permissions to push to CloudWatch Logs. So I'll go ahead and close this down. We'll refresh this, and this should be up and running. So I'm going to select it. I'm going to go to Connect, Session Manager here, and Connect. Perfect. So now we're connected via Session Manager. I'm going to zoom in a little bit more here because we're going to run through a wizard that has very small text. So the first thing I want to do is I'm going to copy and paste this command in. Now these commands that I have that I'm using here will be in a text document for you in the module assets. So feel free to copy and paste if you're doing this on your own. But what we're doing is sudoing, and then I'm running the agent‑config‑wizard that comes with the tool. So I'm going to hit Enter, and it walks us through this wizard. So let's go ahead and do this. First thought, we say, okay, do you have Linux, Windows, or Darwin OS? Well, we're Linux, which is the default choice. And then it does some automatic retry saying, okay, I'm going to try and get the region based on where your EC2 metadata lives, and in this case, it will be us‑east‑1. We're choosing is it EC2 or on‑prem? So I say EC2. And which user are you planning to run the agent with? Now I'm going to use the default. You can use root, it's not recommended, or you can use your own custom user based on your compliance requirements. So we're going to use CloudWatch agent, and then we can turn on the StatsD daemon, and we can automatically set the port for that daemon to listen, and we set the collection interval. So, do we want every 10 seconds, every 30 seconds, every 60? Again, I'm going to choose the default for 10, and then we're going to use the default for the aggregation interval. So we're collecting every 10 seconds, but we're aggregating every minute based on our default choice here. So I say default of 4, which is 60 seconds, and then I'm going to say no to CollectD because I didn't verify this is installed, so I'm going to actually enter no. And then we get to specify what host metrics do we want? So do we want host‑specific metrics like they say CPU, memory, etc. I say yes. Do we want to monitor per core? Yes. So hopefully you can start seeing how in depth you start to get. The next thing is the dimension. Now if you remember, the dimension is a way to identify your metrics or their source. So you'll notice it says ImageId, InstanceId, Type, and AutoScalingGroup if possible. I want to do this because I want to be able to easily identify what metrics we're collecting. In addition to that, it'll aggregate them into one dimension of an instance ID. We're going to say yes here, and you'll see how this works in a moment when we start pushing logs and metrics. Moving on, this is an important thing for the exam, high resolution collection of metrics, so sub‑minute resolution for metrics. You can do this down to the single second, as you can see via option 1. So if this comes up on the exam and you need this, remember the CloudWatch agent can accomplish this. I'm going to say default of 60 seconds. And we're going to say we want standard metrics. Now you could do advanced, basic, none, etc, but we're going to go ahead and say standard. Now what you do here is it gives you the output JSON file that we basically just created using this walkthrough. So you see the agent field, metrics field, etc. I'm going to skip down here, assume this is okay, and say yes, I'm satisfied. The next thing we can do is we can import an existing log agent file if we had it. Now we don't, so I'll say no via the default. And we're going to say, yeah, I do want to monitor for log files here. So I'm going to leave default of 1, and we enter the log file path. Now what I'm going to do is create my own custom application log that's meant to mimic an app log, and we're going to use that. So what I'll do here is I'm going to put in this fake path here. And it's going to be /opt/application.log. So I'm going to write text to this log file to push to CloudWatch. So I'll say yes. Default choice of application.log is fine. We're going to say it's standard log group classing. And notice now the log stream name. So this is going to push to CloudWatch Logs based on this sudo parameter or sudo variable, which is our instance ID. So the log group will be application.log, and then the log stream within that log group will be our instance ID for easy identification. I'll accept this, and then we set the log retention. Remember, you can set your log groups to expire or never expire, which is the first choice there of ‑1. But we're going to go and say just keep these for 1 day, so I'll enter 2. Next, do you want to add any more files? I'm going to say no. I just want to use that one. And I'm also not going to collect X‑ray traces because that's out of depth for this particular demo. And the very next thing, it reviews the JSON with us once again. We'll assume this is correct. You can check this on your own if you want. But this is a very cool feature here. You can store this config as a parameter in Systems Manager Parameter Store. So if you want to use this amongst other instances, etc, you can do that. They just need permission to pull it. I'm going to say no because I don't have permission to create parameters, and I don't need it for later. So I'm going to say no, and now it's gone. So now what we do is we go ahead and run the agent. First, I'll clear my screen and make this readable, and I'm going to copy and paste this command. We're running a sudo, and I'm pointing to the cloudwatch‑agent. And then we're fetching a config for EC2 instances, and we're pointing to our file that we just created. So I'm going to go ahead and click on Enter here. It's going to run. You see it giving a bunch of information. And we're good to go. So this is now running based on our config. Perfect. So the next thing I want to do is I want to generate some log files. So you'll notice under /opt that the application log doesn't exist. Well, that makes sense because I haven't created it. So what I'm going to do is change to my home directory. I'm going to go ahead and create this new random text shell script. I'm going to copy and paste some data in here really quickly. And all this is doing is touching data and echoing data into this log file in a small loop to generate random text. Now this will be available for you as well, so you can use it however you want. For now, I'm going to save it. I'm going to change the mode to allow it to execute, and then I'm going to run it as sudo. So let me go ahead and run this as sudo, and now we can see we appended text. So now if I go ahead and cat that log, we see our text lines here. So I'm going to run this one more time just to ensure we have this going. And there we go. So I ran it a couple more times. Now what I can do is go to my instances. I'm going to zoom out since we're back in the console here. I'm going to go to CloudWatch. I'm going to load that up. And in another tab, I'm going to go ahead and click on Log groups here. And we're going to look for our log group that gets created via this agent once it's up and running. So notice we already have application.log created, which is perfect. So this already collected and aggregated metrics and logs for us. So I can click on this. And within here we'll see our instance ID. So this is already working. That was very, very fast. Remember, the log groups contain log streams, which when I click on this will contain log events. So now what we could do is if we had application.log on an autoscaling set of instances, we would identify them based on the log streams, which map to the instance ID, and then in there we can troubleshoot, etc. So now we see all of that text from our log file, and this is a custom log getting captured by that CloudWatch agent. Perfect. So this is working. We've now captured custom logs. Well, with that being said, let me close this, and I actually want to go over to my metrics. So I'm going to go to All metrics here. And what we're going to do is we're going to go down to namespaces. We're going to select EC2. I'm going to do Per‑Instance. And what we're going to do is search for our instance ID. So I'm going to go back here. I'm going to copy this, and I'm going to paste this in. This will filter all of the metrics. Now you're going to notice, we have a ton of metrics we can choose, and this is just standard. So if we wanted to, we could change it to advanced and get even more, but this agent is allowing us to get even more in depth and capture custom logs. So now what we could do is I could select this instance, it'll graph it. You notice up here, we can change the time period to get a better view, etc. Okay, so that's going to do it. We've now installed the CloudWatch agent on our instance. We went ahead and configured it using the wizard, and we captured some custom logs to push to CloudWatch as well. Let's go and end this demo here, and we'll go ahead and move on to the next clip.

Amazon CloudWatch Alarms
All right, welcome back. We just got done demonstrating using CloudWatch Logs and metrics. Let's build off of that, and let's start looking at alarms. When you're using AWS to host your architecture, you're going to want to set up observability and alarms for when things go wrong. Well, CloudWatch is a perfect service for that, so let's explore how this works. Alarms watch a specific metric over a specified time period that you get to specify, and they change their state based on thresholds that you get to set. So, for example, average CPU usage or maybe disk space, those are all valid thresholds where you can set a value. Once it gets breached, it changes state. By using alarms, it allows you to automatically start actions on your behalf. So, when the alarm goes off, you can perform an action, and it's going to be a lot faster than doing it manually. On the exam, there are going to be two types of alarms you have to know. There's a metric alarm and a composite alarm. Let's actually look at those really quickly and compare each of them. A metric alarm is the most simple version of an alarm where it watches a single CloudWatch metric that you set up. So again, CPU usage, maybe SQS count, things like that. You can also set them up to watch the results of a math expression based on metrics. Now, that's probably a little out of scope for this exam, but you should know it. Typically on the exam, metric alarms will be for watching a single metric. In addition to watching these metrics, you can use them to perform one or more actions based on their breached threshold. So when they change state, you can perform actions. The next type of alarm is a composite alarm. So this is a composite of alarms. You use it to monitor alarm states of multiple other alarms using AND/OR logic. So, for instance, alarm 1 and alarm 2, or maybe you have alarm 1 or alarm 2, that would be an AND/OR setup. By using composite alarms, it's going to allow you to reduce the amount of noise, so essentially false alerts, things like that. In addition to the two alarms, you need to understand the alarm states that are important for the exam. There's insufficient data, which is where it just hasn't collected enough data, thus the name. And you can set your alarm to be not changed, good or bad based on this particular state. Once enough data is captured, you then have an OK alarm state, meaning it's not breached, or an ALARM alarm state, meaning it is breached. Now it's very important you understand you can use these in the opposite manner. So maybe you want to trigger an action based on a state going into OK and not just going into ALARM. Really take your time and read through the scenario on when you would use each alarm state. Moving on to an exam pro tip here, you, again, can use metric filters to create custom CloudWatch alarms. We briefly mentioned metric filters when we talked about CloudWatch metrics. Well, this is exactly how you could use them. By creating a metric filter and getting that log data into a numerical capture, you can customize and create your own CloudWatch alarms based on your applications. Now moving on to alarm actions. Remember, once the alarm changes state or your threshold is breached, you can take actions using them. That's one of the biggest benefits. Here are some common actions you should know. You can send notifications to an Amazon SNS topic, and by doing this, you can both alert an ops team or a customer success center and take an event‑driven action. So maybe you trigger a lambda, things like that. You can also perform an EC2 action. So you can recover an EC2 instance if maybe that instance is failing its system status check. You can go in and say, okay, stop it, start it so we get new hardware, but maintain the same instance. The third big action is you can perform EC2 Auto Scaling actions. So if CPU usage hits a certain threshold and breaches it, well then maybe you want to scale out based on that alarm. This is used with scaling policies very heavy. So if you've done the scaling section of this learning path, you've probably seen this in action already. The fourth and fifth items here are creating both an ops item, so your operations team can go ahead and investigate and mark as resolved, and you can create an incident in Systems Manager. Systems Manager is covered in a different course, but just understand you can create incidents based on alarms going off and an ops item for your team to act on. Moving on to an exam scenario with CloudWatch Alarms. Let's assume your organization has lifted and shifted a legacy system up to an Amazon EC2 instance. The application code that's running cannot be changed, and the application cannot run on more than one instance. It has to run on that single instance, and it can't change very easily. They need you to design a resilient solution that can improve the recovery time for the system in case of instance failure or instance downtime. So for example, maybe something crashes, the underlying hardware or a system status check fails. That would classify as an instance failure in downtime. Well, let's explore a possible solution using alarms. For this, you can easily create a CloudWatch alarm to perform an EC2 instance recovery action for that instance in any case of failure. By using the alarms, they can monitor the instance health and then automatically trigger that recovery action if the instance becomes impaired, so basically unreachable. Now remember, whenever you use this or if you use this, recover actions are going to automatically stop and restart the instance on new hardware. This is one of the benefits of using this action. By stopping and starting, it moves the instance to a new piece of hardware for the underlying host. However, it preserves the instance ID, any associated elastic public IP addresses, and even the metadata for the instance, so this perfectly meets our scenario. By performing this and using this setup, you minimize downtime and you improve recovery time all without requiring any changes being made to the app. By leveraging alarms, once they trigger, you can trigger and kick off an action to recover that EC2, and you're good to go. Now that's going to do it for CloudWatch Alarms. Be sure and go back and really study composite and metric alarms and understand the different actions you can take. For now, we're going to wrap up, and we're going to move on to a demo where we trigger actions using an alarm.

Demo: Trigger Actions Using Amazon CloudWatch Alarms
All righty, I think it's time for another demonstration. In this demo, we're going to jump into our hands‑on playground, and we're going to work with a couple different services to trigger some actions using CloudWatch Alarms. In this demo really quickly, let's go over a high level of what to expect. We're going to have a CPU usage test using a stress command on our Amazon EC2 that we've already created, and we're going to show how we can generate some alarms based on that. We're also going to generate logs with our lambda function where we're going to create a metric filter to also set up an alarm. With that, we're going to send an SNS notification to a fake operations team just to demonstrate how it could work. Now with that being said, let's jump over into the hands‑on playground now. All right, welcome to the hands‑on playground. Let's go and get started. But before we do, I want to go over a few infrastructure pieces I've already created that will be used in this demo. First things first, I've already created this testing logging instance that we're going to use for monitoring some basic CPU utilization metrics and creating an alarm for. I've connected to it via Systems Manager, and I installed stress, which is a package used to stress test the system. I've also, for a later portion in this demo, created this lambda function called TestingAlarms, and all this does is print lines via a logger that we're going to use to create a metric filter to create our own custom metrics, dimensions, and alarms based off of. In addition to that, we've created an SNS topic with an email subscription here with my temp mail email address, which is where we're going to send notifications to. With that being said, what I'm going to do here is I'm going to load up CloudWatch in another tab. And once in here, let's go ahead and work on creating our first alarm. So under Alarms, I'm going to go to All alarms. I'm going to create a new one. First thing we do is we select the metric that we want to monitor and alert off of. So what I'll do is I'm actually going to copy and paste my instance ID because I want to filter off of that. So I'm going to paste this in here. And we'll see the namespace of EC2 Per‑Instance Metrics, or we could use Volume. Now for this, we want Per‑Instance, and you'll notice all of the different ones that we have. Now this is because we have the CloudWatch agent installed from a previous demo. So if you're doing this yourself, this might look different. For me though, I'm going to find CPUUtilization here. I'm going to select it. And I'm going to select the metric here, and now we can review. So we have the metric graph view. You can see I've done some stress testing already earlier. It peaked way up at 97%. And then we see here on the right, we have the metric name, the instance ID, which is essentially the dimension, and then we have the statistics. So I want the average. So whenever the average CPU utilization in a period of 1 minute is going to be greater than our condition, we want to alert. So for conditions, we can select Anomaly, which is if you have any anomalies, it'll do its best to detect it, but we want Static because I want to say, okay, anytime CPU utilization average within 1 minute is greater or equal to 75%, I want to trigger this alarm. Now within this, you have data points, so you can make this more specific. So you can say, hey, 2 out of 2 or 2 out of 4, etc. What we're going to do is say 1 out of 1 because we want to trigger this immediately. For this, we want it to be extremely sensitive. So for the first data point out of the 1 period, if it triggers, we want to go to alarm state. After that, we have Missing data treatment. So, when it's insufficient data state, do we want to just treat it as missing? Do we want to say, hey, the alarm is a good state? Do we want to ignore it and maintain the current alarm state? Or do we want to treat it as bad and say, hey, this is breaching our threshold? I'm just going to go ahead and say treat it as missing, and I'm going to click on Next. After this, we configure our alarm actions. So we're going to do two SNS notifications here. The first will be whenever our alarm state goes into an alarm, we're going to send a message to this SNS topic, which should send us an email. Now I also want to go ahead and say, okay, whenever it's in an OK state, in other words, it's not breaching the threshold, I also want to send a message to that same team because I want to let them know, hey, this alarm is out of an alarm state, it should be okay for now. So we have two SNS actions here based on one alarm. Now, the next four are very important for you to know for the exam. You can perform a lambda action, which means you can trigger a lambda function. Now it could be same account or cross account. You can set an Auto Scaling action to occur, so scale out, scale down. You can scale EC2 services, ECS services. You can perform an EC2 action. These are very common on the exam. So do you want to stop the instance, terminate it, reboot it or recover it? You need to be familiar with these actions, please. We're going to go ahead and remove this, and then the last one you have to know is Systems Manager. So you can create an ops item with severity levels and categories or create an incident for responses from your operation teams. For this demo, because we need it to be simple and quick, we're only going to do SNS notifications, but do be aware of the other actions that are available here. Now for the exam at this point in time, you don't need to know this. It's in preview, so don't worry about it. We're not going to cover it. I'm going to click on Next, and we give our alarm a name. So we'll go and just call this High CPU. We can give it an optional description. I'm not going to. I'll click on Next, and then we review. I go down here, and I create my alarm. Perfect. So now we have our alarm. It's in Insufficient data state, which means it will just ignore any data for now. And eventually, once this is in OK state, we should receive an email saying so. Now while this is collecting enough data, let's say you want to check this alarm state quite often. Well, it's kind of a pain if you have to always go to Alarms, look at All alarms, and let's say you have hundreds of alarms in here. Well, that's where a dashboard comes in. So under Dashboards here, you can create your own custom dashboard for you to quickly view all of your important metrics, data, logs, etc. So for the Dashboard name, we'll just call this Mine. I'll create it, and now we can start adding widgets to this dashboard within CloudWatch so we can quickly view the stuff important to us. So for me, maybe I want to look at the metrics, and I want to look at the line graph. So I'm going to say a line metric, click on Next. I'm going to look for my instance ID again here, and we're going to select that same CPU utilization. So CPUUtilization here. I'll select this. Create my widget, and there we go. So now we can see that same graph, but now it's on our dashboard. Well, maybe I want my alarm as well. I can do that. I'll go to Add widget, select Alarms, Alarm status, find my CPU alarm, and create that widget. We give our widget a name again, if we want to. I'll leave it default, and I'll click Add to dashboard. Perfect. So now we have our alarm, we have our metric graph, etc. I can save this. And there we go. We now have a nice, easy dashboard with exactly what we want to see. So this is now in an OK state, and if I go to my mail, we should receive an email, and we did. We see our alarm state of OK here for our alarm name in our region, and you see all of the alarm details that are relevant to this alarm trigger. Perfect. So this is working. Now let's go ahead, and let's trigger it to be bad. We want to breach our alarm. So what I'll do here is I'm going to go to Systems Manager, and I'm going to kick off my stress command. So this is taking both virtual CPUs and just consuming everything it can out of them. So eventually, our CPU utilization here that we're monitoring is going to be much higher. So right now, what I'll do is I'm going to go ahead, and I'm going to pause, and I'll skip forward once this has triggered our alarm, and I'll show you what that looks like. All right, so I went ahead and cut forward here, and you'll notice the alarm state is now triggered. So it's in an alarm state. And that's because our CPU has crossed our threshold here, and it's averaging 90%, which is above 75%. So now you'll see it's an alarm. We get some of the data, and we should have received an email, and we did. We see ALARM alarm state here in the subject. We have the ALARM name again in the region, and we have details again specific to that alarm that was triggered. So using this information, you could quickly go and say, okay, it's this instance ID. I need to go make sure what's going on and fix it, troubleshoot, whatever you need to do. So perfect. This is working. So what I'll do here is I'm going to kill this stress command. This will go back into an OK state here very shortly in the next couple of minutes, and I will call out, that took a few minutes. It's not one minute. It actually took several minutes for this to capture. So keep that in mind. Now with that out of the way, we've tested EC2. Let's test our lambda function one. So under lambda_function, if you remember here, if I test this, it's going to go ahead and create a very simple response, and we have some logged info. We have an INFO, WARNING, and CRITICAL line as you can see, and we're going to use this to create a metric filter. Remember, metric filters are perfect for creating custom metrics using log files, and I'll show you what that means. So under Monitor here, I'll go to CloudWatch logs, I'll open this up. We have our log group with our log stream. And in here we're going to see where I kicked off that Invoke function. So what we're going to do is we're going to look for any critical lines here. So what I can do is go to my log group. Under Actions, I'm going to create a metric filter. We enter the pattern we want to search for. Now you can use specific regex. I'm just going to look for CRITICAL. So it's going to look for this exact pattern within our log data. So I'm going to say, okay, let's use my log stream to test this. I'm going to test and hey, we get a match on event number 5. Here's the event message. It looks like it works perfectly. So what I'll do is I'll click on Next. We give our filter a name. We're just going to go ahead and call it CRITICAL, and we give metric details. So now we're creating our own custom metric to create alarms with. So for the namespace, what I'll do is I'm just going to call this ApplicationFunction. And then for Metric name, I'm going to say CRITICAL_ERRORS. So we're creating our very own custom metric under our custom namespace here that we're going to use. Next we have the Metric value. Now I'm going to use a simple one. What this is going to do is any time the filter pattern matches, it publishes this value to that metric. So one is very easy, but you can use other values if you want. We'll leave Default value blank. And for Unit, I'm going to select Count. Now notice, there are tons of different specific units you could specify. But for me, I want it to be very easy. I want it to say anytime I catch a critical error in my log, I want to add one count to my metric. I'll click on Next. We review and create. And there we go. We now have our metric filter, and if I select this here, it's going to be a shortcut to take me to CloudWatch. It should load up my metric, and it's going to query specifically for CRITICAL_ERRORS. Perfect. So now what I can do here is eventually I can create an alarm. But to do that, I have to generate this so it catches. So what I'm going to do is go to my lambda. I'm going to go to Test, and I'm going to kick this off three times for now. So this will eventually allow me to generate some metrics here that will be caught within CloudWatch. So what I'll do here is I'm actually going to go ahead and go to All alarms. I'm going to create a new alarm, select my metric, and you're going to notice we now have our custom namespace because this is capturing data. So I select my new custom namespace. We have metrics with no dimensions, which is the one we created, CRITICAL_ERRORS. So I go ahead and select this. I'm going to select it. We'll say Sum instead of Average of a period of 1 minute, and we're going to say any time it catches more than five critical error lines based on our metric filter, we're going to trigger an alarm. So I'll click on Next. We're going to send an SNS notification when it's in alarm. I'll go to Next. We'll give it an alarm name. I'll call it Lambda CRITICAL Errors. I'll click on Next. We review, and I'll create my alarm. So now what I can do is, instead of looking here, I'm going to add this to my dashboard. So under Dashboards, select Mine. I'm going to add my widget here, so Alarms, go to Next, select my new Lambda one. Create my new widget. Go ahead and give it a name. And perfect. So now I can take this, I can drag it, I can move it around, save my dashboard, and I have my view, which is perfect. So what I'm going to do here is I'm going to go to my function. I'm going to trigger this several times. I'm going to make sure this catches more than five criticals, and you'll notice I'm clicking the button over and over again, so we should be pretty good. And now what I'm going to do here is under my Dashboards, again I'm going to pause, and I'll resume once this triggers, and we'll view how this works. Okay, we're resumed here. I refreshed, and now we see we're in an alarm state. Well, that means we should have got an email, and let's go ahead. We did. We see ALARM: Lambda CRITICAL Errors, and we have information on what spawned that issue, and there's not as much information because this is a custom metric and custom filter. So remember that. However, the big thing to remember is we created a metric filter for our lambda function, which allowed us to use log files to look for critical errors or critical syntax that we matched for and generate metrics to create an alarm. That's going to do it for this demonstration. Hopefully you learned how you can use alarms for your own workloads, how you can create dashboards, etc. Let's end here, and we're going to move on to another clip.

Amazon CloudWatch Insights
All right, welcome back. Let's go ahead and dive into CloudWatch Insights. This is an important suite of features you have to be aware of for this exam. Now CloudWatch offers what are called insights to help you gain even more operational visibility into your resources in the cloud. Let's look at the four categories of insights that you have to be familiar with before you take your exam. First, we have Container Insights. There's Application Insights. We have Lambda function Insights, and then the last really big one is Contributor Insights. Let's explore each of these one by one. First up, Container Insights. These are used to collect, aggregate, and then even summarize different metrics and logs from your containerized apps and your containerized microservices. Container Insights work for Amazon ECS, EKS, and AWS Fargate on both of those services, and it also works if you're running Kubernetes on EC2 instances. So any of those scenarios, this can work perfectly for. And speaking of exam scenarios, let's go over one where you might use this. Let's say you're running a critical app on EKS and it's having a microservice architecture design. You need to implement a solution that collects, aggregates, and summarizes metrics and logs from the app all in a centralized location. Well, you probably guessed it. Container Insights is perfect. You log it to CloudWatch Logs and then use these insights to perform even more analysis. Now moving back, let's go ahead and explore Application Insights next. Application Insights are a collection of insights that are meant to help detect problems with your apps by allowing more observability in addition to the native CloudWatch offering. Application Insights are actually powered by Amazon SageMaker, which is a machine learning service, to give you automated dashboards that are going to be showing any potential problems based on the algorithms. The whole point of these insights are to help lower the time it takes for you to troubleshoot issues across entire technology stacks. So for instance, in a tech stack, you might be running Microsoft SQL Server databases, IIS web servers, things of that nature, and this allows you to use all of your technology stacks and leverage Application Insights to make it easier to troubleshoot. You can also use them to generate EventBridge events for event‑driven automation. So if something goes wrong with your IIS server, you can say, hey, well, I want you to restart it, terminate it, etc. This is all easily achievable using these insights. In addition to that, you can also create events in OpsCenter, so an ops item for your operations team to go ahead and troubleshoot, remediate, and resolve. Now a quick reminder with this. These are going to work with many other resources outside of Container Insights, so that's why you might use this. For instance, EC2, RDS, your ELBs, SQS queues, API Gateway APIs, etc. The resource support is far greater than Container Insights, so you really need to be sure you're looking at the scenario requirements if you're going to choose this over another. Now up next, let's move into Lambda Insights and break down how you would use these. This is an insights solution that is specific to monitoring and troubleshooting serverless apps running on your lambda functions. The name probably gives that away, Lambda Insights, but let's talk about what it does a little bit more. What these insights do are collect, aggregate, and summarize system‑level metrics, including your lambda function CPU time, memory usage, disk consumption, and network usage. In addition to the system‑level metrics, you also can collect information about things like cold starts, so the amount of time performance, and Lambda worker shutdowns. Now you're not going to need to know specifically what a worker shutdown is, just understand that Lambda Insights allow you to collect very in‑depth data about your function performance. How they work is they use the CloudWatch Lambda extension. Now this is going to be a Lambda layer that you attach to your functions. Once you do that, it allows you to natively start using Lambda Insights. And then next up, we have Contributor Insights. So let's go and explore these. Contributor Insights are there to help you analyze log data from applications and then create time series that display contributor data. So it's a little bit more specific than Application Insights. These allow you to view metrics about the top‑N contributors. So your top 10, top 100, etc, you can see the total number of unique contributors, and you can view all of their usage. Some examples on when you might use these are to identify bad hosts. Maybe you're getting error codes from specific hosts over and over again. Well, that's viewable. You could also use it to identify the heaviest network users or maybe find URLs that return the most errors. Again, these allow you to view time‑series data from your contributors and identify potential performance issues, problems, etc. To use them, you use rules. Now you can create rules, or you can use AWS‑provided rules, and what these rules do are define the log fields for any incoming data that you want to use to define your contributors. These rules analyze your log data that come into CloudWatch in real time. So that's an important distinction for the exam. In addition to the real‑time aspect, they support a lot of AWS‑native logs, so things like API Gateway logs, Route 53 public DNS query logs, Route 53 resolver query logs, and they even support Container Insights logs, which is handy, and VPC Flow Logs. So there's some more support compared to container logs, but again, this is very specific for time series style of data. Let's go ahead and wrap up the insights section here, and we're going to move on to our module summary and exam tips coming up next so we can review some of the important factors and information you need to remember for your exam.

Module Summary and Exam Tips
Okay, way to hang in there. You finished the first module in this course. Let's have a quick summary, and we're going to review some exam tips that I think are important for you to remember going into your exam. First up, four questions you should ask yourself when trying to answer a question. One, what is the best tool to monitor with? A large majority of the time it will be CloudWatch, but that doesn't mean it's always going to be CloudWatch. In addition to that, if you're capturing metrics, is that metric available by default, or do I need to install the agent to capture a system‑level more in‑depth metric? Third, where can I find logs? For instance, is it a service log that's natively supported by CloudWatch, or do I, again, have to install the agent and push that to CloudWatch? In addition to that, do I need to send those logs somewhere else like a Kinesis Data Firehose or Amazon S3? And then fourthly, do I need to adjust my alarm threshold? Remember, alarms are used to trigger actions and send notifications, so really make sure you're fine tuning your alarms and understanding how you should set them up. With those four questions out of the way, let's go ahead and review CloudWatch, the service, and CloudWatch logs. Remember, first and foremost, CloudWatch is the main tool for anything alarm and observability related. However, not everything should go through CloudWatch, and we're going to cover other services later on within this course because there are some more specific services that could potentially be a better fit. It's all going to come down to the specifics in a scenario. CloudWatch Logs is going to be your primary place to source service and application logs within AWS. For your logs, remember how and when to use the CloudWatch Logs agent. If it's something that's not default, you need to install the agent. If it's an application running on EC2, you need to install the agent. These are common examples on the exam. And then lastly, remember you can use subscription filters and filter patterns to deliver real‑time log events to other services. And speaking of those other services, remember the destinations that are supported for exporting and setting up subscription filters. Remember that you can export to Amazon S3 buckets, but it's not real time. It could take up to 12 hours to be ready. For subscription filters, you can go ahead and push that to Kinesis Data Stream and Amazon Data Firehose delivery stream, you can trigger a lambda function, or you can send the data to Amazon OpenSearch. These last four are the ones you need to know for subscription filters. The first one is only for exports. Moving on to CloudWatch metrics and alarms. Remember, basic monitoring captures in 5‑minute intervals. Detailed monitoring is 1 minute. In addition to that, detailed monitoring is going to incur additional costs. Also remember, you can continuously stream metrics to different destinations using a metric stream. It's sort of like a subscription filter, but it's for metrics. Now with metrics, you can also use metric filters to create custom metrics for CloudWatch alarms. To do this, you can create your own logs that are pushed to CloudWatch, create a metric filter, and then create an alarm on those custom metrics based on those filters. For instance, maybe you're getting a lot of potential server error codes that get pushed to your log files. Well, you can say, okay, if I get 10 error codes in my logs, I want to trigger an alarm. So you create a metric based on those error code counts. And then lastly, remember the two alarm types, metric alarms and composite alarms. Moving on, you also need to remember when to use the different insights features. Go through and review the four insights we looked at, application, container, lambda, and contributor. Typically, the exam scenarios are specific, and that should help you in deciding, but really make sure you know different use cases for each. Moving on to an exam scenario to wrap all of this up. Let's say you're running an application and you're using an application load balancer, Amazon EC2 instances in an Auto Scaling group, and a database running on RDS. All of these resources are running in a custom VPC that you've created and designed. Well, your team wants to capture information about the traffic going to and coming from the different network interfaces in near real time. Your team also wants to send the information to an OpenSearch service domain and cluster for analysis later on. Well, let's look at what this might look like from an architecture standpoint. In this, let's assume we have our three different subnets for our three‑tiered application, hosting an ALB, EC2, and RDS instance. Well, with these, we have network interfaces, of course, in each AZ that they exist. What we can do is we can create a VPC Flow Log for the entire VPC. So now we're capturing information about all those network interfaces, and we can create a log group in CloudWatch Logs and send all of the VPC Flow Log data to that log group. Then within CloudWatch Logs, we can create a subscription filter on the log groups to send the log group data directly to OpenSearch Service. This is a valid solution for that scenario. One of the best aspects is that it's entirely hands off from the monitoring standpoint. We set up the VPC Flow Logs to push to CloudWatch, and we set up a subscription filter that sends to OpenSearch, and we don't have to manage any underlying infrastructure besides that. Now with that being said, let's end this module here. Thanks for hanging in there. Go ahead, take a quick water break or just step away for a little bit, and I'll see you in the upcoming module.

Miscellaneous Logging and Monitoring Services
Amazon Managed Service for Prometheus
All right, welcome to the next module. We just got done reviewing Amazon CloudWatch, which is the primary service for logging, monitoring, observability, etc, but there are some other miscellaneous services that you need to be aware of for this exam. So let's go ahead and jump into the first one, which is Amazon Managed Service for Prometheus. Amazon Managed Service for Prometheus is a serverless Prometheus‑compatible service offered by AWS that's meant to be used for securely monitoring container metrics at scale. Now you can use it for other scenarios, but on the exam, this is going to be the primary use case. When you use this service, you're allowing AWS to manage the automatic scaling based on ingestion, storage, and the amount of queries that are going on for your metrics. In other words, if you're ingesting a ton of metric data and you want to observe that, well, then they're going to handle the scaling for the infrastructure for you. With this service, AWS is going to replicate all of your data containing all of that information across three availability zones within the same region. In other words, it is highly available, and durable, and redundant. On the exam, if you see anything related to PromQL query language for interacting with metrics, that should be an immediate indicator of this service. You use this query language to explore and extract data for your metrics so you can view them, analyze it, all of that good stuff. And speaking of the data that you're interacting with, the data that gets stored is stored in a workspace for 150 days before it is gone. Now this could come up on the exam, and you have to understand you store your data in workspaces and do your best to remember roughly this number. Now an important indicator on the exam, if you see anything related to requiring leveraging the open‑source Prometheus monitoring systems and some type of time series database for metrics, this is a great service to consider. Again, it's probably going to come down to this or CloudWatch, so watch out for those very small, but important indicators in the scenario. And then last thing here before we wrap this clip up, an exam pro tip, this is commonly used to set up advanced monitoring of your EKS clusters or your self‑managed Kubernetes clusters. Again, this is a primary use case, container metrics at scale, and you allow AWS to handle the scaling and the infrastructure for you, and you just worry about the data. Let's go ahead. We're going to end this here, and we're going to move on to another service called Amazon Managed Grafana.

Amazon Managed Grafana
All righty, we talked about Managed Prometheus. Let's move on to another alternative called Amazon Managed Grafana. Amazon Managed Grafana is another fully managed AWS service, but it is specific to the Grafana technology. AWS will handle the scaling, setup, and maintenance of all your workspaces related to this service. So in other words, this is extremely similar to the Prometheus offering, but it's just for a different technology. Now with it, you only worry about your tasks that are within Grafana. These tasks are used to secure your data visualizations so you can instantly query, correlate, and visualize operational metrics, logs, and application traces from all of your different sources that you're ingesting data from. Now a workspace in terms of Amazon Managed Grafana is just a logical Grafana server, and it allows you to separate your data visualizations and your querying tasks. So if you see workspaces on the exam related to this, remember they are logical Grafana servers. Now with this service, it integrates with a ton of different sources for ingesting data, including CloudWatch itself. You can actually ingest from Managed Service for Prometheus. You can work with Amazon OpenSearch Service and Amazon Timestream. Now moving on, let's look at some use cases for this service. Similar to Prometheus, you can connect to data sources like EKS, ECS or your own Kubernetes cluster metrics. There are likely going to be some small important indicators to choose one or the other. You can also use it for Internet of Things, so you can use the vast set of data plug‑ins that are available to make this service a perfect fit for monitoring your Internet of Things and edge device data. To be honest, this is probably one of the biggest use cases for this service, IoT devices, and your monitoring sensors, workloads, etc. And then the third is for troubleshooting. You can use it to centralize dashboards for all of your metrics and analytics, and it allows you to be more efficient at troubleshooting your operational issues. So again, if you're collecting IoT data and you start seeing errors from a certain warehouse where you have sensors sending data in from, well, that helps you isolate it looking at those metrics and those visualizations. Now that's going to do it for Amazon Managed Grafana. Again, remember, this is a managed service specific to using Grafana. Let's go ahead and wrap up, and we're going to move on to a module summary and exam tips clip.

Module Summary and Exam Tips
All right, way to hang in there. That was a very short, but very sweet little module with some other services. Let's review this before we move on. We covered two services that you need to be aware of for this exam, Prometheus and Grafana. Remember, both of these are managed services. That means you offload infrastructure management like high availability and automatic scaling to Amazon Web Services. That is a major benefit to remember for the exam. Amazon Managed Grafana allows you to use several built‑in data sources, including things like CloudWatch, Managed Service for Prometheus, and even AWS X‑Ray. Amazon Managed Service for Prometheus is the serverless Prometheus‑compatible service for securely monitoring primarily container metrics at scale. Again, on the exam, it is primarily used for that specific use case. With both of these, you can leverage VPC endpoints for secure VPC access. This has started to come up on the exam a little bit more. And remember, endpoints are for a majority of services. But remember, this is another managed service, so they have an endpoint that you can spin up in your VPC to restrict network controls. Now the last thing here, a final tip. Keep an eye out for PromQL as an indicator for using Amazon Managed Service for Prometheus. That is a dead giveaway to choose this service. And with that, way to hang in there. We're going to wrap up this module on these other monitoring and logging services, and I'll see you in the next one whenever you are ready.

AWS Organizations and Multi-account Architectures
Multi-account Architectures Introduction
All righty, welcome to the next module in this course. We're going to start looking at a service called Organizations, some other services that play with Organizations, and just overall multi‑account approaches for environment design. As you start getting more hands‑on and working more in depth with AWS, there's going to be scenarios where you're going to have to start looking at deploying multiple accounts for an organization. So before we dive into some services you need to know, let's take a step back and review some of the best practices regarding multi‑account architectures. First up, why do we even want to implement a multi‑account strategy for an enterprise, a company, an organization, etc? Well, multi‑account architectures are considered a critical design strategy for customers in AWS based on the Well‑Architected Framework. If you're not really sure what multi‑account might look like, here is an example account structure. A way to approach it could be having an account per environment, so maybe a dev account, a uat account, and a prod account. You could also separate accounts per department. So maybe you have HR running their own tool sets in AWS. You have a billing team, and maybe you have a security operations team. Well, all of these could have their own account to work in, and if anything happens, it's isolated to that one account. With multi‑account strategies, there are several design benefits that you need to be aware of before you start working with AWS. By implementing this approach, you get stricter controls for security and compliance because if you have different accounts per team and per environment, well, that allows you to get even more granular in an easier way to control what is going on within those accounts. It also allows you to isolate your workloads, as well as reduced what is called a blast radius. For example, let's assume we have a dev and a prod account for an application team. Well, if something goes wrong in the dev account, it's not necessarily going to impact the prod account because they're completely separate. They have different IAM user databases, different access controls, etc. It's not the same as prod because that's a completely different instance as far as accounts go. The third thing here is it allows for billing separation. So this is very important when you have multiple teams under the same management account. Now we're going to look at what that means here coming up shortly when we look at Organizations, but remember for now that multi‑account strategies allow you to easily separate bills for your accounts to see exactly what is being spent by whom. And lastly here, it allows for optimized Well‑Architected Framework adherence. Remember, there are several pillars you have to meet to be a well‑architected architecture, and this falls into one of those best practice strategies. You want to have multi‑account whenever possible. Now that's all nice, but you might be asking, okay, how can we manage multiple accounts at scale? I have hundreds of teams that could potentially have many different accounts. Well, how can I do that very easily? Well, the go‑to solution for this is AWS Organizations. So let's go ahead. We're going to break here, and we're going to start looking into Organizations at a high level coming up next.

AWS Organizations Overview
All righty, remember, as you start scaling in AWS, you're going to want to maintain that multi‑account strategy. AWS Organizations is a service to help us accomplish this. What it is is a free governance tool and service that is going to allow you to create and manage multiple AWS accounts, all from a single location. When you spin up a new organization, you get one main account called the management account. So this is where you manage everything within your org. It's also sometimes referred to as the payer account, and you'll see why that is here coming up shortly. When you have this, other accounts can either join the organization via an invitation, or they can actually get created using the service. So you can do either/or. These accounts that are part of the org and are not the management account are known as member accounts. These member accounts can only belong to a single organization. They cannot be part of more than one. Remember that for the exam. Now to organize your different member accounts, especially when you start getting hundreds of different accounts with teams all around the world, you can logically group and manage them using what is called an organizational unit, or an OU for short. Now let's go ahead and look at a very high‑level architecture of what Organizations kind of looks like in a multi‑account environment. At the top here, we have the root organizational unit. This is going to be the default OU that is always immediately available when you create a brand‑new org. This is where all new accounts will start, unless you start moving them around, creating them in different OUs, etc. The big thing to remember here is the root OU encompasses everything. What that means is it has all other OUs nested in it, and it has all accounts fall underneath it. Now once you start growing within Organizations and you start getting multiple accounts, for instance, you can see we have Billing, we have Team A, and we have a Security organizational unit. Well, these separate OUs allow you to isolate accounts for better access control via IAM and networking access. They allow better billing isolation, and they even allow better resource management. One of the key benefits here is that you can reset quotas for services because you have multiple accounts, and each new account allows you to reset a quota limit. So in this instance, we see Billing. We have two accounts that fall underneath this organizational unit, and we base those accounts based off regions. Team A in the middle has three accounts that are separated via environment, dev, uat, and prod. And then our security OU has a SecOps account, but within it we have a nested OU. So this is important for you to remember. You can nest organizational units within other organizational units for easier management. Now we're going to dive into using these a lot more in depth coming up next. But for now, just understand at a high level how this works. We have the root OU, we have our management account, and then we have our member accounts within their other separate OUs underneath those. Now a real quick exam pro tip. This is kind of coming up on the exam every so often, but a common integration is to have IAM Identity Center integrated with Organizations. It really allows you to easily assign different permission sets to different accounts based off the org structures. Moving on to another important concept here, PrincipalOrgID. The aws:PrincipalOrgID is a global condition key. That is a critical concept for you to understand for the exam and really more importantly, any real‑world scenarios involving Organizations. This particular key becomes available for you to use correctly once you create an organization. When you create an org and you have your accounts within that org, you can start checking for it and using that key in different policies. So you can use it in a service control policy, which we'll cover later on. You can require it in IAM identity policies and even resource policies, so an S3 bucket policy, for example. Now let's actually look at an exam scenario regarding this condition key. Let's assume your company has just began using AWS orgs to manage their multiple accounts that belong to different application teams. The management account contains an S3 bucket, and it needs to contain analytics reports that are generated by the different application team accounts. You need to find a way to limit access to the bucket to allow users of accounts within your organization to only have access. Well, let's explore a solution that meets the requirements, and it has very little operational overhead. An easy way to accomplish this is to set up your S3 bucket and then use a bucket policy to go ahead and restrict access. So in this, using the PrincipalOrgID condition key, we have our bucket policy example here. We're allowing all AWS principals, which generally is not a very good idea, but if you have hundreds or thousands of accounts, well, you can't list those in the principal list. It'll be too big of a policy. So in this case, we're saying, okay, all principals are allowed to put objects within our bucket path here, so our central‑log‑bucket with our *. The important thing here is the Condition field. We are checking for the PrincipalOrgID within the key fields. So we're saying, hey, does the PrincipalOrgID string equal this particular string? So this is an organization ID example. What this does is it allows all principals in the organization to upload to that bucket. Since we allowed all of the different principals, as you can see here, we're only restricting based on this condition at the bottom. This is an important thing to remember for the exam and when you're doing this within Organizations in real life. Now an important thing to note here is because we are allowing all principals, you're essentially allowing any potential user, role or service to put objects in that bucket if they belong to your org. So keep that in mind. This resource‑based policy gives pretty much everyone in the org permissions to put an object. So with that being said, maybe you want to add some other type of condition, but the big thing to remember here is this allows you to do this or meet this requirement. Now that's going to do it for this example in this clip. Let's go ahead and wrap up, and we're going to move on to some other important features you need to be aware of for the exam within Organizations.

Important AWS Organizations Features
All righty, we just got done looking at using Organizations for multiple accounts. We looked at a high level, and we explored PrincipalOrgID conditions. Let's move on to some other important feature sets you need to know for the exam. For the exam and for Organizations, there are two essentially categories of feature sets you need to be aware of. The first is all features, and then we have consolidated billing. Let's explore consolidated billing first. Consolidated billing is a free feature within Organizations that allows all member account bills to roll up to a single payer account, which would be the management account, and it allows you to have a single payment method for all of those bills. So this is why the management account is typically called the payer account. With consolidated billing, you also get discount sharing. This is where Organizations allow you to share your reserved instance purchases and your savings plans purchases and discounts across multiple member accounts. So for example, maybe you buy one big savings plan at the management account level and you want to share it with all of the member accounts within your org, which could be hundreds of accounts, so you get discounted savings. And lastly, this is one of the biggest benefits of the cloud in general is a usage discount. In other words, the more you use, the more you typically save or the cheaper things become. In the terms of Organizations, it's the ability to combine usage across all member accounts to share volume pricing discounts. So again, maybe you're running a ton of EC2 compute. Well, the more you use, the cheaper per hour the price is typically. So you can combine all that and get an aggregated usage discount across all of your accounts. Now, let's review. Remember this particular diagram here. We had Organizations with our several organizational units and accounts. Well, for this, in this particular case, our management account would be the payer account. So all of these member accounts that we see at the bottom of the diagram, their bills would roll up into a single consolidated bill that gets paid for by this management account. Now that's going to do it for consolidated billing. But you also need to know the other features that go along with the all features feature set that we looked at earlier, so let's explore those now. The first is a stack set. Now we cover this in a different course where we cover CloudFormation, but Organizations allows you to easily deploy stack sets across multiple accounts in multiple regions. We also have delegated admin use. So what you can do with this is you can select different accounts within your organization, so your member accounts, and you can delegate policy and service management to those accounts. So, for example, maybe you want one singular account to be able to deploy stack sets to your organization. Well, you can make that account a delegated admin for that service specifically, and they can deploy from there. Thirdly is the OrganizationAccountAccessRole. Now this is the default name, and it can be named whatever you want, but the big thing to understand here is Organizations can create a single IAM role that gets created in each member account for easy role assumption. So what you could have is a sys admin or an engineer that has access to the management account, and there's a trust policy set up that allows them to assume this org access role in the member accounts to help with troubleshooting, deployments, etc. Fourthly is cost allocation tagging. These allow you to enforce tag compliance for better cost tracking. So maybe you tag based on operational unit, maybe you tag on department, application, things of that nature, but this allows you to enable them at the org level and enforce compliance. And lastly, Service Control Policies, or SCP. At a high level, this is a policy to restrict or allow account actions. We're going to dive into Service Control Policies in their own clip, so don't worry about those for now at an in‑depth level, but understand these are an important feature to remember for the exam. Now moving on, some common best practices and strategies for using Organizations. Remember, multi‑account strategies allow you to avoid running into account service limits. So if you have a hard service limit, maybe you have too many VPCs in a single account in a region, well, you can create a new account, and you just reset that quota. Secondly, you can enable an organizational CloudTrail trail for centralized logging of API actions. Now we're going to cover CloudTrail in its own course later on, but do your best to recall this when we're in that section. Thirdly, you can implement a logging or security account for hosting centralized CloudWatch logs in a centralized S3 bucket. This is important to remember for the exam. In addition to that, go back to that AWS Amazon S3 bucket policy that we looked at where we use the condition for the PrincipalOrgID. That is common as well. Now that's going to do it for this introduction to some of the feature sets that are important for you to be aware of. Coming up next, we're going to start diving into Service Control Policies as they are very important for you to understand when working with an organization.

AWS Organizations Service Control Policies (SCPs)
All righty, we just got done looking at some important feature sets to be aware of. One of those was an SCP, so let's dive into those really quickly because these are critical for you to understand how to use. An SCP is simply a JSON policy that gets applied to different member accounts and organizational units to restrict actions for users, roles, and services. An important thing to know with these is they do not grant permissions. They only dictate or say what permissions can be granted via IAM policies, so identity policies or resource‑based policies. With that in mind, you have to explicitly allow permissions to be granted when you are using these. The default behavior is to deny all actions, which is just like IAM. Now the default policy that you would use to explicitly allow all permissions is the full AWS access policy, and that's recommended to apply immediately to your accounts and then start denying things as you need to. It's really just going to come down on your compliance requirements, but that's generally the easiest and quickest way to get started. An important thing to know here is just like IAM policies, you can have overlapping or stacked SCPs within your organization, and you'll have to understand how to interpret what actions would be allowed or denied based on the overlap. Moving on, let's look at two important concepts to remember for the exam in the real world. First off, SCPs do not impact the management account. That is critical to understand. Because of this, you typically don't want to do anything non‑related to Organizations within the management account. They do, however, impact the root user and all other users within all member accounts. So with that being said, if you need to restrict root user actions in member accounts, think SCPs. That is the easiest and quickest way to restrict root actions. Now moving on, let's look at an example of a service control policy. This one is specific to denying CloudTrail changes throughout the org. So you'll notice it's pretty much just like an IAM policy. We have our deny effect, so we're denying actions. And to deny, we're selecting these CloudTrail‑specific API calls, so DeleteTrail, CreateTrail, RemoveTags, etc. for all resources that fall under that API namespace. Again, this is just like an IAM policy, but there are some other things you can accomplish using them. With that being said, you are going to need to know how SCPs get evaluated when they are applied at different levels of an organization. So let's explore that now. In this simplified org, we have a few different OUs with different accounts. So we have our Root OU with our management account here at the top. We have our Billing OU with our two regional accounts here on the bottom left, and then we have a Team‑specific OU with their different application environment accounts, so dev, uat, and prod over here on the bottom right. Typically speaking, you would apply the full AWS access SCP to the root level, which means it would get applied to all member accounts and allow all actions to be granted. Remember, it's not granting those actions, it's just allowing the actions to be granted. Now let's assume we go ahead and apply a DenyEC2 actions SCP at the management account. Well remember, it doesn't matter. These have no effect on the management account, so this is really doing nothing. The third policy here is our DenyCloudTrail policy that we just looked at. Let's assume we apply this to all member accounts within the Team OU. So what this is doing is combining the full AWS access policy here at the top with our DenyCloudTrail policy. So with this, even though everything is allowed, we're explicitly denying CloudTrail actions within these three accounts. Now let's also assume we go in and we attach AllowCloudTrail to the prod account only. Well, it doesn't matter because we have a deny statement somewhere in this evaluation chain. Remember, denies will override any allows anywhere within the chain of permissions. So we have this DenyCloudTrail that's denying it to the OU. And even though we're trying to explicitly allow it in prod, it doesn't matter, it's overridden. Again, this can get very complex. So really look, first of all, if there's any deny statements anywhere in the flow of the permissions evaluation, and then start breaking it down per OU and per account. For now though, we're going to end here, and we're going to move on to a demonstration where we're going to create an organization.

Demo: Creating an Organization
All righty, I think that's enough talk for now. Let's go ahead and break, and we're going to jump into a demonstration. We're going to work on creating our very own organization within our AWS account. In this demonstration, we're going to do a few things. We're going to first, of course, have our management account. So, what I've done is I've already created this account that we're going to use to host our organization service. From within this account, we're going to create a new AWS account, and then we're also going to invite an existing account to join the org, so this way we can see how both ways function. And then we're also going to work on creating a different OU within another OU so we can see how nesting organizational units work. So let's go ahead and jump into the console now. All righty, I've loaded up my console window here. The very first thing I want to call out, you will not be able to follow along with this demonstration or any of the demonstrations in this module using the hands‑on playground. It's just not supported. So to accomplish this, I have a separate set of escalated privileged accounts that I'm using for this that are going to be destroyed immediately after this course is released. So with that being said, I've loaded up my console here. I'm in us‑east‑1, which is really not a big deal because I'm going to go to Organizations, which is technically a global service. So when I'm in Organizations, you'll see it lands us on the screen, and I'm going to click Create an org. And just like that, that is exactly how easy it is to create a brand‑new organization, and now the account that we're in currently is now the management account. You can see at the bottom, it's listed it as the management account, and it fell under the Root OU, which is the default organizational unit for all accounts within the organization. Remember, the root is the top, and everything underneath that falls into the Root OU. So with that being said, on the left side here, you'll see we have our Organization ID. This was created for us by AWS when we created our new org, and this is what we could use with that PrincipalOrgID condition key that we looked at in previous clips. And with that understood, let's go ahead, and let's start working on creating accounts, putting in place policies, that kind of thing. So the first thing I want to do here is I'm going to click on Add an AWS account. Now there's two different things we can do here. We can create a new one, or we can invite an existing one. We will do both, but for now, let's work on creating an account from within Organizations. The first thing we have to do is give our new account a name. So what I'll do is I'll give this account a name. We'll call it Development Team, and then I have to give the email address for the new account. So this is going to be the root user for this new account that we're creating from Organizations. So in here, what I'm going to do is paste in a new email address that's temporary that I'm going to use for this. So what this is doing is setting this as the root user for our new development team account. And then the third thing we see here is the IAM role. This is that cross‑account org role that we looked at in a previous clip. So what will happen is it will create this OrganizationAccountAccessRole in that account and allow this account and any of the principals with valid permissions to assume this. Now you can name this whatever you want. This is just the default and what most people use. So feel free to change this if you're doing this on your own. I'm going to leave it as the default. I'm going to go down here, and I'm going to create my new account. At the top, you can see, hey, you've submitted a request for a new account in your org. So what's happening is AWS is working to create this brand‑new account for us via the AWS Organizations service. So eventually, once I refresh, we're going to see that new account listed in our org, and there we go, Development Team at the bottom with our new account ID and the account email. Two things to note here. One is that this now falls under consolidated billing within this org. Remember, we added this account from within the org. And typically when you create a new account, you have to set up billing. Well, we have that enabled, and now this bill for the Development account will actually roll up to our Demo Management account. The second thing is there's no root password here, so this is actually one of the most secure ways to create an account because there's no password to compromise. To log in as root, you have to reset the password via notifications. And ideally someone's monitoring the email address and can say, oh hey, someone's trying to be an unauthorized user here or gain unauthorized access. I need to shut this down. So just keep that in mind for a best practice outside of the exam. Now with that being said, let's actually go back in, add an account, and I'm going to invite an existing account here. So what I've done is I've created a separate account that we're going to invite into this org. So let me go ahead, and we can use the email address or the account ID. I'm going to put in the account ID. So let me go ahead and grab it really quickly, and I'll paste it in here. So I'll go ahead, and I'm going to copy it. I'm going to paste it in here, and this is the account ID of an existing account that I've already created that's outside the org. Now if you want, you could add a message as well so they can see this when they're accepting the invite, which we'll go ahead and do. So now when this invite goes out to join the org, they'll get this message at that account. I'll send my invite. And there we go. It now is sent. We can see it was open right here. To accept this, I'm going to go ahead, and I'm going to navigate to my multi‑session that I have logged in already. By doing this, I'm going to log into that other account because I've already authenticated, and you're going to see under Organizations, I have one invitation. So let's view that, and we see the details. We see the Management account inviting us, the email address, the organization ID that we would join, and the note that we sent. So from here, the account owner or someone with permissions can decline or accept, so we're going to accept it. And now we belong to that organization. That's how easy it is. Now typically, we could by default leave this org whenever we want, but we're going to put a policy in place to show how you can deny accounts from leaving the org. So for now, what I'll do is go back to my multi‑session here. I'm going to go back to the Management account, close these other tabs, and now we're back into our main management account, and we'll see that external account right there at the bottom of the screen, which is the account we just accepted the invite in. Perfect. So now we have accounts in here. This is awesome. Now what I want to do is I'm going to create some organizational units and then move some accounts around. So I'm going to select Root here. And under Actions, I'm going to create a new organizational unit. So just like our intro clip here, let me go ahead and call this North America. And we're going to base our accounts to pretend that they're all based in North America, but you could do regions, environments, etc. I'll create it, and then in there, we can start moving accounts. So what I'll do is I'm going to move development and external into that new OU. So under accounts here, I'll go to Move. We can select the OU, and I'll move them. And now, we have them in our North America OU under our Root OU, and then our Demo Management account is only in Root. Now in addition to this, if we wanted to, I could select this organizational unit and create another nested one. So just like our intro, let's just call this Security. I'll create it. And now we have a nested OU within there, and let's just say I want to move this account into that particular OU because it's a security account, so I'll select it and then move it. There we go. We now have nested OUs within our organization. We have the management account, etc. It's all working perfectly. So now what we could do is we could apply SCPs. However, to do that, I have to go to Policies. I'm going to find Service control policies, but notice all the other ones that you can put into place, backup policies, you can use tagging policies, etc. I'm going to select Service control, and I'm going to enable these. There we go. It now enables it. We can use SCPs throughout our org, and you'll see right here, the first one created by default via AWS is FullAWSAccess. This is the one we talked about in the previous clips. What this does is it allows us to grant permissions for all actions to be taken. Notice the content of the policy here. So with that understood, we know the contents, and you'll notice the targets. It's every single account within the organization because it's applied to the Root OU, as you can see here. So remember, the root contains everything, and all of the accounts are in Root, so this applies to all accounts, which allows us to easily grant the proper access controls in each account. If we were to detach this from some of these accounts, it would immediately prevent all actions from being taken, and you will break things, so please don't do that. But with that being said, let's go ahead and test out our very own policy. I'm going to go back to Policies here. Let me clear this banner. And under Policies, let's go back to Service control. I'm going to create a new policy. We're going to call this policy DenyLeaveOrg. We'll give it a description, and then we get down to the policy itself. Now I have this already copied, so I'm going to paste it, and this will be available if you want to use it in the module assets. But real quickly, it's saying, okay, the LeaveOrganization API call for all resources, I want to deny that. So when we apply this, it's going to say, hey, you cannot leave the org. This is a good security measure to ensure that you're meeting compliance in your member accounts. So what I'll do is I'll go ahead and select it. We see the content, there's no targets. Well, I want to attach this. So let's go ahead, and I'm going to attach it, and I'm going to attach this to Root. What this does is it attaches the policy again to every single account within Root, which means that our accounts in our org will have this policy applied. So if I select Security, which is under North America, this account will have this policy applied, as you can see at the bottom of the applied policy list. Perfect. So now if I jump back into our external account that we invited here, I'll go ahead and switch really quickly. Let me refresh this or click on Go to Console Home. Sometimes this breaks, of course. I'll go to orgs. And let me try to leave the org, Leave organization. Oh, I don't have permission. So this is working already. The SCP is immediately denying me the access and permission to leave the org. So this is a good security measure to have put into place for your member accounts. Now with that being said, that's going to go ahead and do it for this demonstration. We created some accounts, we created some organizational units, we created a nested unit, and we even created and applied our very own service control policy. Feel free if you have access to your own organization service. Check out all of the services that you can have, be a managed service via delegated admins throughout this list. Check out the different policies. You do need to know SCPs, tag policies, and backup policies at a high level, and even feel free playing around with the org ID as a condition key. But for now, we're going to go ahead, we're going to end this demonstration. I hope you learned a lot, and let's move on to another service called AWS Control Tower.

AWS Control Tower Overview
All right, let's move on to our next service that you have to be aware of regarding multi‑account governance and management, and it's called AWS Control Tower. Control Tower is an AWS service that gives you a simplified, quick way to set up and govern an AWS multi‑account environment, and it sets all of your accounts up based on best practices. In other words, it gives you an abstracted way to easily provision new accounts quickly using established compliance policies. Now these can be your own policies that you put in place, or they could be standardized industry best practices. The service itself builds off of and extends off of AWS Organizations, and it uses Organizations in the background to essentially do the following. It allows you to easily automate account creation, so you can spin up new accounts. And when you create these new accounts via Control Tower and AWS Organizations, you can easily and quickly implement security controls within those accounts. Now, in addition to security controls, which we're going to dive into here in a moment, it also helps you prevent and detect governance drift within the accounts managed by Control Tower. In other words, if you create an account via Control Tower and it belongs to your org and you specify that you need it to meet a certain compliance via some policies, well if that ever drifts, you can easily see that drift within the service. In addition to that, there are means of preventing drift in the first place, and we'll dive into those much deeper here coming up. Now it's important to note, this requires AWS IAM Identity Center in order to work properly. With that being said, however, you don't have to use it if you don't want to, but it does require that it's enabled for its own particular principles that it uses to control account creation, automation, drift detection, etc. Really do your best to remember that for the exam. Moving on, let's talk about Control Tower concepts that can pop up on the exam that you need to be aware of. First, we have a landing zone. This is a portion within the service that gives you a well‑architected multi‑count environment for all of your AWS resources. In other words, it allows you to very easily enforce your compliance regulations on all of the member accounts within your organization, and you can do so in an easily repeatable manner. Next, we have shared accounts. Remember, you have member accounts and the management account within an org. Well, these shared accounts are considered member accounts. When you enable and start using Control Tower, it's going to automatically create a log archive account and an audit account. These are the shared accounts. Both of these accounts get created within a security organizational unit, and they're meant to serve as a central logging account for CloudTrail and auditing configurations, respectively. The third concept here is a control, which was also originally referred to as a guardrail. You might see either/or. On your exam, they mean the same thing. What these controls are at a high level are a way for Control Tower to automatically apply mandatory preventative controls and mandatory detective controls for securing the organization. Now building off of the controls, let's look at the two that they cover on the exam currently. One of them is called a preventative control. These are implemented using service control policies within the organization, and they actually prevent actions. If you recall when we covered SCPs, these allow you to define which permissions can be granted within the member accounts. The other type you have to know about is a detective control. These are implemented using AWS Config rules. Now we're going to cover Config in its own module here coming up shortly, but for now, remember that Config is the background service for detective controls. The big thing to remember here is they are not preventative. They are detective, thus the name. Now let's actually look at how a detective control could be used. How it works is you create these Config rules, which again, we will cover much more in depth coming up in an upcoming module. But you use them to determine if resources are considered COMPLIANT or NON_COMPLIANT based on your configurations. If a service or resource is determined to be NON_COMPLIANT, you can actually trigger what is called a remediation, and you can trigger a notification. Now notifications can go through Amazon SNS, and you should keep that in mind for the exam, as any real‑time notification typically uses that service, but remediations use either lambda functions or SSM automation documents to go ahead and remediate that particular resource to make it COMPLIANT. Now the big thing here is don't focus on the Config‑specific stuff. Just remember you can use these detective controls to trigger remediations. Exam pro tip here, just a review. Preventative prevents, detective detects. Remember that for the exam. It seems like it's pretty easy now, but when you're reading through all the scenarios, you might confuse the two. Now let's really quickly look at examples for each of the type of controls we just covered. First is a preventative. This is a real AWS‑provided example. Disallow changes to bucket policy for S3 buckets. So what this would do is put an SCP in place throughout the org and apply it to all of the accounts that are member accounts. This policy would say, hey, you cannot change bucket policies for any S3 resource. In other words, it's preventing a change. Detective could be detect whether encryption is enabled for EBS volumes attached to your EC2 instances. Again, this doesn't prevent encryption from not being used, but it detects if there is such a thing in place, and if it gets detected, you can work on remediating that issue or that non‑compliant resource using an automation document or a lambda function. Again, Detective detects and can notify, but it does not prevent. That's what preventative are for. Now the last thing here that we need to cover is AWS Control Tower Account Factory. This is a feature in the service that enables cloud admins and users that are granted permission in Identity Center to provision accounts in your landing zone, which we covered earlier. If there's a scenario where you have to create repeatable deployments of new accounts, think Account Factory. With that being said, let's wrap this up, and we're going to move on to sharing resources using AWS RAM.

AWS Resource Access Manager (RAM)
All right, we just got done looking at accountant governance using Control Tower. Well, there's going to be use cases and scenarios where you need to share resources across the org, and that's where RAM comes in. So let's go and explore this now. AWS Resource Access Manager, or RAM, is a free service that gives you the ability to share resources with other accounts without the need to create duplicates. Now these accounts that you share resources with can be member accounts within an organization, or you can even share resources with supported external accounts. You share with external accounts by sending invitations for them to use and accept the resources. For the exam, you're going to need to know some of the commonly shared resources. Some common ones include Transit Gateways, VPC subnets and VPC security groups, AWS Network Firewall policies and rules, which we'll cover in a later module, Route 53 resolver rules so you can say for hybrid cloud computing and hybrid DNS, which type of network traffic you want to resolve and where you want to send it to, and then lastly, you can share a VPC customer‑managed prefix list. Now it's important to note, if you share resources using RAM with an organization, it allows member accounts to automatically access that shared resource without having to accept an invitation. Remember, if you share it with an external account outside your org, you send them an invite, and they have to either accept or reject it. If you share in an org, you skip that step entirely. Now let's move on to an exam scenario where you might use RAM. Let's assume your network security teams want to isolate workloads by creating an AWS account for each workload. So in other words, they're following a multi‑account strategy. The team needs help to create a solution that's going to centrally manage networking components for the different workloads in those different accounts. They need this solution to also be able to create accounts with automatic security controls. Well, let's go ahead and explore a possible solution that will meet the requirements and have little operational overhead. First, you're going to be creating a multi‑account environment to isolate your workloads. And in addition to that, they want to be able to centrally manage security controls. Well, this seems like it would be a perfect use case for leveraging Control Tower. Second, the networking security team wants a way to centrally manage their networking components for workloads across different accounts. This would fall perfectly in line with the AWS RAM service. By putting both of these together, you can create a new networking account that has a VPC with the required subnets, and then you can share those required subnets using RAM with your organization, which hosts those different workload member accounts. This is a great example of using RAM with Organizations and Control Tower. Now using RAM, remember, is free. However, if you're sharing resources, you might be paying for those resources that got shared. Please remember that. The service is free, but services and resources you share might not be free. Let's go ahead. We're going to end this clip here, and we're going to jump into a demonstration where we share resources using RAM.

Demo: Sharing Organizational Resources with AWS RAM
All righty, let's get into a demonstration using AWS Resource Access Manager. Before we jump into the console, let's have a high‑level look at what we plan on accomplishing. What we're going to do is we're going to have an AWS organization already created for us, hosting several accounts in a different organizational unit nested within root. This org was created in a previous demo, so if you need to see how that was created, feel free to go check out that demonstration earlier in this module. But for now, what we're going to do is in the Management account, we're going to treat this like a centralized resource account, which is typically not the best idea. However, for simplicity, it's easy to do this and follow along. So in the Management, we're going to use AWS RAM to share an IP prefix list that we can share for VPCs to use in security groups, NACLs, etc. We're going to create this customer‑managed prefix list in the account. We're going to go to RAM, and we're going to share it with the organization to show those settings. Once it is verified to be shared from that account, we're going to go ahead and log into one of the member accounts and verify it exists and we can use it. So, let's go ahead and without further ado, let's jump over into my console now. All righty, I've loaded up my AWS organization that we created in an earlier demonstration. One thing I need to call out here, this service will not be usable in the typical hands‑on playground, and neither will RAM. This is because this group of accounts that I'm using for this demonstration are temporary escalated accounts that I can only use for this particular course, and these will be blown up and deleted shortly after this is released. So, that's why I can do this. Understand you cannot do this in the hands‑on playground. But with that being said, let's get started. I'm in the management account here, and what I'm going to do is I'm going to go to VPC. So let me load up the VPC service here. And another thing to call out is this is regional. So I'm going to create this prefix list in us‑east‑2, but remember, these are regional. So if you need it in other regions, you need to create this in different regions as well. With that being understood, I'm going to go to Managed prefix lists over here on the left, and let's create a brand‑new one. So what I'm going to do is create a prefix list. I'm going to give it a name. And what we'll pretend is this is a group of customer IPs that they've given us to allow list or add to our allowed list or whitelists for our firewalls. So I'm going to call it Customer‑IPs. We're going to say the max number of entries will be 10. So they can never be more than 10, they're IPv4 addresses, and let's add these new entries. Now what I've done here is I have a set of IP ranges that I'm going to copy in here, and all these are are simply S3 IP ranges at this point in time. So these could change. It's really not a big deal. We're not going to use them. I'm just adding them, so if you wonder what they are, that's all they are. So what I'll do here is let's just pretend this is our set of west IPs, our set of east IPs, and our set of south IPs. For now, that's all where our customer is at, and that's all we need. So we're adding three ranges. I create my prefix list. Okay, so it created our prefix list. We see our entries. I'm going to go ahead, and I'm going to go to Resource Access Manager now. So let me load up this service here, Resource Access Manager. All right, so this loads up RAM. We're in the Shared by me section. There's also a Shared with me. But before we do this, there is a setting that is tricky. So under Settings here, I need to enable sharing with an org because I'm going to share this resource with an organization ID, and this has to be enabled. So I'm going to save this setting. We get confirmation that it is shared. Sometimes this disables it, so if that happens, just go back in and save it as enabled. And I'm going to go back to Resource shares. I'm going to create a new one, and let's begin. For the name, I'm going to give it a name that's easy to identify. The next thing we do is we add our resources. Now you're going to notice, I'm going to zoom in here, there are so many different resource categories. I'm not going to cover them all. Again, just be familiar with some of the common ones that we covered in a previous clip, including Subnets, Transit Gateways, VPC Security Groups, and Resolver Rules, etc. For this, I'm going to choose Prefix Lists. We're in us‑east‑2, remember. I'm going to select our new one that we created, Customer‑IPs. It shows the resource, and now I can go to Next. From here, you can assign the managed permissions to share this list. Now I don't have to do this because it's not required for this resource type, but understand this might be required, depending on what you're sharing. This is not common on the exam. You don't really need to know this. It's more so understanding what the service does. With that being said, we're going to share this prefix list, so I'll go to Next. And now we can choose who we want to share with. Do we want to allow it to be shared with anyone, or do we want to only share within the organization? So I'm going to select the second box, and this is why we enabled that setting earlier. From in here, I'm going to go ahead and select this little check box here to display my org ID. And I actually want to share this with the entire org. So I'm going to select the organization as a whole. So this selects basically every account and every OU within the org itself. So now you'll notice that the Principal type is an Organization. I'll click on Next. And we get a review, and I'll click on Create resource share. Perfect. So now we've shared this resource with our organization. So if I go back to Shared by me, we see this listed. So let's test this out. What I'm going to do is under here, I have multi‑session support on. I'm going to go into my shared account that I've created earlier. So I'm going to select this. It's going to log me into that external account in the org. And in this member account, I can see resources shared with me right here. So this is working. Let's test out using it. I'm going to go to VPC here. Again, I'm in the us‑east‑2 region for this. It has to be the same region. I'm going to go to Managed prefix lists, and here we go. We see our prefix lists with our entries. Perfect. So now we can centrally manage these IPs. So what I can do now if I go to Security groups, let's just create a new simple security group here. We'll just call it demo, demo. We'll do inbound rule, allow HTTP from our prefix list here. So we'll do pl Customer‑IPs. I'll click on Create, and there we go. So now we're allowing inbound HTTP from our managed prefix lists, which was shared with us by the owner account that you can see here. Well, let's see how fast we can manage these. Let's say we want to add or remove an IP entry from here. Let's say we want to remove the south entry. So what I'm going to do is I'm going to go back into that other account. So I'm going to jump back into the management account where we shared this resource from. I'm going to edit this prefix list. Let me modify. Let me just remove south. We'll just say they're only east and west. I'll save it. And now we see once I refresh here, it should only be two IP ranges, and there we go. Now let's go and test. I'm going to jump back into that member account that we were just in here. I'm going to load up my VPC. And just like that, it's already shared the updated info. So it really is super simple and super quick to centrally manage and share a shared resource within an org. It's pretty much near real time. We edited it, and by the time I swapped to the new account, it was already updated. So now our security group just has to reference that one particular prefix list, and this is managed for centrally by a completely other account. Perfect. So with that being said, let's go ahead. We're going to end this demonstration. We worked on creating and sharing a valid resource across an organization, and we made sure it actually worked. Let's end here, and I'll see you in an upcoming clip.

Module Summary and Exam Tips
All right, it's that time again. Way to hang in there through another module. What we're going to do here, have a quick summary, go over some exam tips I think are important for you to take with you in the exam, and really just reinforce some important concepts before we wrap things up. First things first, you must remember the benefits of implementing a multi‑account environment strategy within AWS. Some of the biggest benefits include billing isolation and blast radius reduction. Now that we are masters of understanding multi‑accounts, let's go in to review AWS Organizations. Remember, this is the free governance tool that you should go to to create and manage multiple AWS accounts from a single location. You can use it to create new accounts, or you can invite existing accounts to join your organization. Remember, there's a management account, and then there are member accounts. Those are the two classifications within an organization. The management account is also considered the payer account for consolidated billing. In addition to understanding when to use Organizations, you also need to know and understand when and how to leverage organizational units to organize your accounts, especially with regards to SCPs. And speaking of SCPs, let's review important benefits and important features you need to remember for the exam. First, consolidated billing. Remember, you can turn this optional feature on to centralize your billing through your organization, so you have one payer account paying one single bill. By using consolidated billing, you can also use discount sharing. This is where you can buy reserved instances and savings plans and share those throughout your member accounts in an org. In addition to that, you also get usage discounts. Remember, one of the benefits of using the cloud is economy at scale. So, in other words, the more you use, typically the less you pay per hour or per second because they give you a discount. So by enabling this in your organization, you're combining all of the usage across all of your member accounts and in turn effectively getting a discounted rate. The fourth thing to remember here is a delegated admin. You can use delegated administrator accounts for specific services and features within your organization. An example would be a stack set or CloudFormation delegated admin account. So maybe you have a platform engineering account where they deploy shared stack sets, or maybe you have a security account and you want to enable Amazon GuardDuty, and that is the delegated GuardDuty account. We're going to cover GuardDuty later on when we get into more specific security features and services. The fifth thing is cost allocation tags. Remember, by enabling Organizations, you can also enforce cost allocation tag compliance throughout your member accounts, so you can easily identify who is getting billed for what. And lastly, remember the cross‑account role that gets create. At account creation time or when an account joins an org, it is typically known as the OrganizationAccountAccessRole, but you can name it whatever you want. This is an easy‑to‑use cross account IAM role that gets put into every member account for centralized access. Now moving on to Service Control Policies, you have to, and I do mean you have to understand how to use these. Remember, they're a JSON policy that get applied to accounts and organizational units to restrict actions for users and roles. These do not affect the management account. That is a common tricky scenario that comes up. However, they do impact member accounts and the member account root users. This, again, is a common scenario. If you have to restrict root actions in a member account, think SCPs. Be sure to remember that the default behavior is to deny all actions from being granted. Because of this, you have to explicitly allow permissions to be granted when using an SCP. Typically, you'll use the full AWS access AWS managed policy to do so. And lastly, understand how to interpret the syntax of a policy, as well as how to implement the overlapping of policies. You'll need to be able to distinguish what permissions are being granted or not granted based on overlapping policies in an organization. And an exam pro tip regarding SCPs. Remember, actions must be explicitly allowed by an SCP and an IAM policy for a principal to use it. SCPs override everything within the chain, unless there's a deny statement somewhere down the permission evaluation line. Big thing to remember though for the exam, actions have to be allowed by an SCP and a policy in order to be granted. Moving on to Control Tower and Resource Access Manager. First up, AWS Control Tower. For the exam, remember, this is an AWS service providing a simplified, abstracted, and quick way to provision and govern an AWS multi‑account environment based on industry best practices. For it, remember the different controls and how those different controls work. Preventative prevent actions and detective detect compliancy for resources and services. The preventative controls use SCPs, detective controls use AWS Config rules. Then we dive into RAM. Resource Access Manager allows you to easily share AWS resources with other accounts without needing to duplicate those resources. Keep in mind that this works with member accounts in an org, and it also works with external accounts. For external accounts, you must invite them to accept or reject the shared resource. And lastly here, try to remember the commonly shared resources that we covered earlier in this module. Some common ones are Transit Gateway attachments, VPC Subnets and Security Groups, and Resolver Rules. In the end of all of this, really just do your best to understand how all of these services that we covered and their features work together to implement a best practice architecture. And with that being said, let's wrap up this module, and we're going to move on to a more specific module where we start diving into other tool sets and services for compliance and governance within our accounts.

Account Security and Governance
AWS CloudTrail Overview
Okay, in this module, we're going to go over some security stuff specific to securing your accounts and governing your accounts regarding compliance frameworks, regulations, etc. The very first service we're going to look at here is CloudTrail. This is a very critical service for auditing actions that occur in your account, so let's go and get started with it. AWS CloudTrail is a service within AWS that increases the visibility and the ability to govern your AWS accounts, and it does this by recording user and resource activity. Now when we say user, we also mean roles, service principals, etc. It's not just an IAM user. It expands to anything that can take an action within your account. It works by recording management console actions and service API calls. So anything that occurs within your account is technically able to be recorded by CloudTrail for future investigation. It's also important to note, this is enabled by default for all accounts once they get created, and it cannot be turned off. Keep that in mind for the exam if it comes up. Remember it is on by default when you create an account. Speaking of the exam, you also need to know what CloudTrail allows you to accomplish. At a high level, it allows you to perform after‑the‑fact incident investigation. So if an action occurs, you can go back and see who did it, when it occurred, etc. It also allows for near real‑time detections of API calls. This allows you to run event‑driven automations once the detections occur. And lastly, because of how it works by auditing your account, it helps you maintain regulatory compliance. So you can ensure that services are not being configured improperly, people are not taking actions that are not allowed, etc. Now to accomplish all this, it records sources of data, logs, actions, things like that. For the exam, you need to know what it does record. It works with any action taken within the management console. So if you can click it within the console, it gets recorded within CloudTrail. This includes sign‑in events. So if a root user signs in, if an IAM user signs in or someone assumes a role, well, that's all recorded. It also works with any AWS CLI call. Again, remember, the CLI is just a way to essentially call the API actions, so that's recordable. Thirdly, if an AWS service makes any actions for you, for instance, maybe you have EventBridge rules that trigger on a schedule and they call a lambda function, those are recorded within CloudTrail as well. And then lastly, any SDK API call. Again, it's an API call, so it's recordable, and it doesn't matter where you're making the call from. It could be just a RESTful API call, it could be a Python Boto3 call. Things like that all get recorded within the CloudTrail within the account. Now to understand CloudTrail at a deeper level, you have to understand what an event is. An event is the record of an activity within an AWS account, and it provides you the historical trails of what happened. Remember that these events can be taken by IAM identities or AWS services, so users and roles or the actual services themselves. Again, for our example, EventBridge may be triggering a lambda function. Building off of the events, you need to know three specific event types that are recorded in CloudTrail. The first is a management event. This is information about management operations that get performed on resources. In other words, they're called control plane events. These are going to capture separate read and write events. So if you're just getting, listing, describing or maybe you're creating, destroying, etc, those are all different types of events within Management Events. A perfect example of a management event, which is a write event, is a TerminateInstances EC2 API call. The second category is a data event. This is information about the resource operations performed on or in a resource. This is the data plane. These tend to be very high volume because typically you're interacting with a lot of data all the time within your AWS accounts. For example, maybe you're making a GetObject or DeleteObject API call. Well, those are classified as data events because it's performing operations on the resource or on the objects. And then lastly, an insight event. This is where you can capture unusual API calls or unusual amounts of error rates in your accounts. How it works is the service is going to analyze a normal set of management events over a certain period of time, and it creates a baseline. Once it has that baseline, it uses that to compare against future events. An example of an in‑site anomaly might be a sudden increase in API calls for delete buckets. For instance, maybe there's 30 delete bucket API calls in a single minute, and that's way out of the normal activity that normally or typically occurs in an account. That could become an insight event. Now moving on, let's look at a high level what the details look like for a management event. We're going to dive into a demonstration later on where we'll explore the JSON version. However, this is the high‑level details that are visible in the console for the service. The first thing we have here is the Event time with the timestamp. So this is when this event occurred. Second, we have the username or the principal that made the call. So in this case, it's cloud_user within a sandbox environment. Now the username could be a role principal, it could be a service, etc. Again, remember, this doesn't have to just be an IAM user. The third group here is the Event name and the Event source. So this was from SQS, and cloud_user performed the CreateQueue event, which is a write event. The fourth group here are more source information. So we have the actor and the source IP address. You can see where this API or action occurred from and the access key that was used. So, this is specific to cloud_user. It could come up as something else if it's a different principal, but this is pretty easy because, again, it maps to an IAM user. And then the fifth important section here that we have is the region. So, one thing to keep in mind is CloudTrail is a regional service. So this is capturing data in the us‑east‑1 region, and that's where this CreateQueue event occurred. And then 6, it is a read‑only event or is it not a read‑only event? In this case, it's not. It's a write event because we created a queue. So this field would flag if it's read‑only or not. Now that we know what an event looks like at a high level, you also need to know about event retention. By default, events that occur in your account are stored in the history pane, which is what we just looked at really quickly, for up to 90 days before they are removed. Remember that number for the exam. If you need something longer, you can customize them to be stored long term within CloudWatch or Amazon S3 using what is called a trail, which we'll look at here coming up next within the module. For now, just remember if you need more than 90 days of storage of your events, you're going to have to customize them to be stored somewhere. Now real quick before we wrap this up, two exam tips to end this clip on. By default, management events are stored. However, you have to manually enable the capture and storage of data and insights events. Remember that. Also, if you need to audit activity within your accounts, think CloudTrail. That is the immediate thing you should look for in the scenario. Do you have a reason that you need to audit API activity, malicious user activity, etc? You should immediately be thinking, hey, I need to go look at CloudTrail and look up these logs. With that being said, let's end this clip here, and we're going to move on diving into CloudTrail trails much more in depth now.

CloudTrail Trails
All righty, we just talked about AWS CloudTrail at a high level, and we looked at events. Well, CloudTrail trails are what you use to customize capturing your events. And you need to know this for the exam, so let's dive in. A CloudTrail trail is simply a configuration and a resource in your account that enables you to customize the different events that you want to capture, you can customize the storage destinations, the encryption options and much more. Now, since we understand what a trail is, let's look at some important concepts that you need to carry with you into your exam. First things first, these are regional resources that capture events in their respective regions. Remember that, please, for the exam. With that being said, it does support multi‑region deployments, or you can do single region if you want. Now multi‑region is recommended because then you're capturing all events with the same customization options across all regions that are enabled within your account. In addition to the region support, you use these to customize the delivery of your logs and your event storage for long‑term storage. You can send them to Amazon CloudWatch log groups, and you can send them to Amazon S3 buckets. It's just going to depend on the use case that's in the scenario. For instance, maybe in CloudWatch, you have to perform some type of subscription filter, or in S3, maybe you want to run some type of Amazon Athena query on that data. Again, it's just going to come down to the scenario. Fourthly here, you can actually trigger rules based on events that get captured using Amazon EventBridge. So if an event comes in, for instance, let's say a sign‑in to the console, well you can trigger an Amazon EventBridge rule, which maybe sends a message to Amazon SNS, triggers a lambda function, things like that. Trails also support customized encryption using KMS keys. So while the data is captured, you can encrypt the trail in all of that data to meet different encryption requirements. This is perfect if you want to lock down a trail to make sure no one can see what's going on or access the data. We're going to talk about KMS in a separate module in this exact course, but for now just understand you can customize encryption using different encryption keys. And lastly, you can set up Amazon SNS notifications for log file delivery updates. What this means is let's say you configure your trail to store your logs in Amazon S3. Well, what you can do is set up an SNS notification anytime those logs get put into your bucket successfully. This comes up a lot for scenarios where you're integrating with a third‑party vendor or third‑party SIEM where you want to go ahead and take actions on those logs stored in S3 after they are delivered in near real time. Moving on to some more concepts that are important to know. You also can enable what is called an organizational trail. So this is a config that enables delivery of CloudTrail events in the management account and all member accounts in your organization. It works by allowing you to set the same settings up, and you can send all of those trail events throughout your entire org to a single S3 bucket that's centralized, a centralized CloudWatch Logs group or even the same Amazon EventBridge bus. Again, the thing to remember here is an organizational trail allows you to deploy the same multi‑region trail across your entire org. And then you also need to know about log file validation. This is a very, very handy security feature that allows you to determine whether a log file was modified, deleted or unchanged after it was delivered by the service to the destination. This typically occurs within S3, and in one of our demonstrations, we're going to see how it works. However, for now, just remember you enable this to ensure that the delivered log files were not modified. This uses SHA‑256 for hashing and then SHA‑256 with RSA for digital signing to ensure and validate that things are as they say they are. Now another important thing to remember here. This is less for the exam and more for real world, but you get one free trail per region. So there's really no reason to not turn this on, even if you're not storing the data long term. It's a general best practice to immediately turn on a trail, especially since it's free. Now, to wrap things up, let's end with an exam scenario where you might use CloudTrail. Let's say recently you've noticed an IAM user has been making configuration changes to some important resources in your company's account. The most recent change that happened happened during a production deployment. Well, that's an issue, and you've learned that several security group rules are no longer configured as they should be, which is affecting that production deployment. Your security team has come in and they asked you to identify who made the changes to the security groups and potentially other resources. Well, this is all achievable using CloudTrail. Remember, if you need to audit actions, think CloudTrail. Let's just say over here on the left‑hand side we have our user that's making the different updates. So in this case, let's say it's adding ingress rules that are not allowed to our security group. Well, that event data is an API call, and it gets captured and sent to CloudTrail, which can be configured to send data to CloudWatch or Amazon S3. It's up to however you want to configure your trail. So in other words, by implementing and using CloudTrail, you can quickly identify what user has been making those changes because remember, it captures the principal that's making the changes. In addition to that, it's also including things like the event name, the timestamp of when it occurred, and the affected resource, which in this case would be our security group. This makes this service a great tool to audit historical actions that have occurred. Now with that being said, let's go ahead, we're going to wrap up here, and we're going to move on to a demonstration where we create our very own CloudTrail trail.

Demo: Creating an AWS CloudTrail Trail
Okay, I think this is a perfect time to jump into a demonstration. In this demo, we're going to go ahead, and I've created an IAM user that's going to have limited permissions that we're going to use in the CLI. What we're going to do is we're going to turn on a CloudTrail trail with our own customized settings, and we'll explore all of those settings that are important for the exam. And we're going to set it up to store our logs in S3. What we're going to do is we're going to use those limited credentials to attempt to terminate some EC2 instances that are in us‑east‑1. Hopefully, with our permissions, we should get an Access Denied. Now this event data, which is not necessarily real time, but it's near real time, will eventually be sent to our CloudTrail trail, which will then be aggregated and delivered to Amazon S3. From within S3, we're going to use Amazon Athena to perform an ad hoc query of the CloudTrail logs to view that information and really just view the different types of queries that are possible using this setup. So with that being said, let's end here. We're going to jump into our console, and let's begin. All righty, welcome to the hands‑on playground. Now, just real quick, let's review what I've already created, and then we'll get going with CloudTrail itself. I've created a simple EC2 instance called Production Server, and then I created a limited user in IAM with EC2 read‑only access. This user is what we'll use in our CLI. So if I jump over to my terminal window here and I run get‑caller‑identity, I've set up limited_user as the default user for our future commands. So this is how that's going to work. Now with that being said, let me jump back into the console. I'm going to go ahead and close this down for now, and I'm going to go to CloudTrail. Let's begin. This is the dashboard, and one thing to note on the dashboard here is Event history. Event history is the default setting. So this is the 90 day of storage for your events. So if I click on this, we could in theory go back 90 days and look at every single event that was captured in this account. For instance, you can actually see not too long ago, I actually created those access keys that I am using as the limited_user. So if I select this event name, we get that detail pane we looked at earlier in the theory clip, we see the resources that are referenced with this event, and then we get the actual JSON here at the bottom. So this is a lot more details that you can use for automation, workflows, things like that. Now with that being said, this is just one of many events. You'll notice at the top here, we have Read‑only as a lookup attribute and it's set to false. So if I set this to true, it's going to filter down, and you can actually filter on a lot of things. You can look at the Resource type, you can look at the Event source, so IAM, EC2, things like that. So if you need to filter, you can do that in here. However, this is only default. You can't edit what gets captured or how it's captured. Let's actually go ahead and do that now and create our very own CloudTrail trail. To do that, I'm going to go over here to Dashboard. I'm going to find Create trail. Now you can also do it from Trails on the left over here, but I'm going to select Create trail from the dashboard, and let's create our first trail. Let's just call this demo‑events. After this, we select the storage location. So do you want to create a new S3 bucket, or do you want to use an existing S3 bucket? What I'm going to do is select Create new S3 bucket, and I want to show you the permissions that get created on that bucket for us via this process. So this will be the name of the bucket. You'll see we have the account ID in here, and then we have a random ending to this string generated for us. So we have our bucket name. The next thing we can do is encryption. We do want to always encrypt our logs. There's really no reason to avoid doing this unless cost is absolutely an issue, but from a security standpoint, you'll likely want to always turn on encryption. So for this, if we disabled it, obviously then they're just plain text, but we're going to enable encryption using KMS, and we cover KMS in its own module in this same course. But at a high level for a reminder, it's a cryptographic key service in AWS for encryption and decryption. When we select Enabled, we can then say, okay, do I want to do a new key, or do we have an existing one that we need to use? For this, we're going to select New, and then we give our key an alias. So I'm going to go ahead and call it demo‑events‑key. So what this is doing is having the CloudTrail service create a brand‑new KMS key for this specific trail to encrypt the logs as they go into S3. The next settings we have here are very important for the exam as well. First, Log file validation by default it is enabled, and I would recommend you always leave this enabled. Remember, this is the way to ensure that files were never modified, changed, erased, etc. It uses hashing to verify that everything is exactly what it says it is. So we're going to leave this enabled, and then we have SNS notification delivery. This is very useful if you're using some type of SIEM third‑party provider, or if you just want to notify your teams that CloudTrail trails and the logs are actually being delivered to your destinations. So, for this demo, let's go and turn that on. I'm going to say create a new SNS topic, and what I'll do is I'll subscribe a temporary email address to this topic after we create this. So we'll leave the default name, and I'm going to scroll down here. The next thing we see is CloudWatch Logs as an optional setting. If you need to, you can push your CloudTrail trails to CloudWatch Logs. This is extremely useful if you want to create some type of subscription filter or a metric filter based on logs and API calls. With that being said, let's go ahead and test it out. I'm going to say, yes, I want to send my CloudTrail logs to CloudWatch. I'm going to have you create a brand‑new log group, and we'll leave the default log group name. And for this, I'm going to have you create a new IAM role. Now if you had an IAM role existing already, you could select it. However, I'm going to go ahead and create a new one specific to this CloudTrail trail, so let me go ahead and name it. So I enter my IAM name. If I expand Policy document, you can see the permissions that are getting created. We're allowing it to create log streams and put log events for a specific ARN within that CloudTrail log group based on this CloudTrail trail. So let me minimize. I'll go to Next. And now we get to choose our log events. So let me go ahead and clear this banner, and let's explore this. You have to know this for the exam. By default, remember, management events are selected on every new trail. Again, this is the operations plane for your resources, so things like create access key, terminate instance. Do your best to remember the type of calls that fall under those. We then have data events. Now remember, data events can really rack up very quickly because they're capturing actual data‑specific operations. So for instance, we can look at S3 objects like GetObject, DeleteObject. We can look at DynamoDB tables like PutItem, GetItem, things like that. So you'll notice also there are tons of resource types that fall underneath this. Now, you won't need to know all of these for the exam. There's obviously just way too many. However, I would say be familiar with S3 and DynamoDB at minimum. Those are very common to come up as data events on this exam. For now though, I'm going to go ahead, and I'm going to say I don't want to capture data events. Thirdly, we have Insights events. Remember, these are perfect for finding anomalies in your API calls based on your trails. It builds a baseline and then says, oh, I think there's some unusual activity, I better alert you. So we can look at the number of calls or the rate of errors that are coming in. For me, again, we don't have time to really use this, so I'm going to say I don't want it. And we're going to skip over Network activity. This is not really prevalent on the exam right now, so it's out of scope. With that being said, let's look at the Management events details. Remember, there are read events and there are write events, and they are treated separately. So depending on use cases, maybe you just want to capture write events, or maybe you just want to capture read events, or maybe you want to capture both. Again, you can configure your trail however you want. Another handy thing is you can exclude high volume events as well, like KMS encryption decryption calls, RDS data events. So for this, I'm going to go ahead and select that, and then I'm going to click on Next. We get our review screen, so we see our trail attributes, all of the different things that we're creating like the CloudWatch Logs group, and I'm going to create my trail. All right, so that took roughly 30 seconds or so. I skipped forward just to save you the boring details. And we now have our custom trail, demo‑events. Also remember before I dive into this, these are regional trails. So if you operate in other regions, you need to create a trail in that region. Now you'll notice this is set to Multi‑region, which means it was created throughout all of the regions for us by default, which is perfect. But for now, what we'll do here is I'm going to open up demo‑events. We see all of our settings. Let's go ahead, and I'm going to select the Trail log location here and open it in a new tab, and this brings us to our S3 bucket where we are storing our logs. So eventually we're going to start getting logs within this CloudTrail folder, and this is where we're going to go ahead and parse from using Athena. Now, while this is getting set up, I'm going to do a few things. The first thing I want to do is I'm going to go to my Trail details, and I'm going to verify SNS. So this is in place. What I want to do is load up SNS in a new tab. And I'm going to subscribe my temporary mail address so we can see how that notification delivery works. So I'm going to select my topic, verify which one that was again. So we have the 6cf72, which is this one here. I'm going to go and add a subscription. We'll say it's an email. I'll go to my temp email here, copy it, and then I'll create my subscription. Now what I'll do is I'm going to go ahead and confirm this really quickly, and now we should be good to go. So I'll just refresh, and we are confirmed. So this is set up now. I can close this window. Next thing I want to do is I want to test terminating this instance. So what I'm going to do is jump to my terminal. I'm going to go over to my IDE off screen. And I'm going to go ahead and paste in a command here. So we are terminating instances, and I need to copy the instance ID in. So I'll go back, I'll copy it, jump into my terminal, and I'll go ahead and paste this. So we're trying to terminate this instance with those limited user credentials, and we should get some errors, and we do. So let me run it a few times just to generate a few different events. Perfect. So what I'm going to do here is I'm going to jump back into my console, and what I'll do is I'm going to wait, and then I'll fast forward to when we start seeing logs generated, and then we will resume. Okay, so I resumed really quickly. We're starting to get some logs delivered. We're not getting them delivered for our region just yet, which is fine, but I want to show you a few things. First, the notification deliveries. Notice that we are getting a ton of notifications every time that a log file gets delivered. So this can be very noisy, but it's very useful if you're using a third‑party provider that needs to react to these files getting delivered. You'll notice it gives us the S3 bucket ARN and then the prefix and the object path for that file. So in theory, you could parse this with a SIEM provider and then perform some actions. Now with that being said, If I go back to my S3 bucket here, we can actually start seeing our region. So what I'm going to do is I'm going to select a region. It goes by year, month, day. You do need to be familiar with the syntax on the path as well. Notice the bucket name, the default is AWSLogs. We have the account ID, CloudTrail, and then that region breakdown. So in here we have an object. If I select this, we can start seeing some information. The biggest thing here is our encryption settings. It's using our encryption key that we specified when we created this trail. So if I go here, we see our alias, demo‑events‑key. So this is using custom KMS encryption. If I close this down, the other thing I want to show you under the bucket is under Permissions, the bucket policy that was created for us here. This allows CloudTrail to put events into the bucket. So this is created for us also. Perfect. So with that out of the way, what I'm going to do now is I'm going to go ahead, I'm going to go into my region again, and I'm going to pause one more time until we get more events so we can start parsing this with Athena. So let me load this, and then I'm going to resume here in a moment. All right, so I did cut forward. We're getting more logs delivered now, which is perfect. One thing I want to show you really quickly is if I jump back to under the account prefix here, the digest structure is how we can verify that log file validation. So in here, we would start getting these different files here that validate what is going on. So we would get a digest file with hashing to match our CloudTrail log files that were delivered under CloudTrail. You don't need to know the specifics, just understand that is how it's used. Now with that being said, let's go ahead. I'm going to load up Athena, and let's start working with querying these CloudTrail logs. So under Athena, I'm going to start copying and pasting in some of these settings and these queries, and I'll explain them as we do them. So the first thing I'm going to do here is copy in this CREATE TABLE one. Now this CREATE TABLE query is going to do exactly that. It's going to create a table for us in a default database. But before I do this, I do need to set up a saved query setting. So under Settings, I want to confirm it's already set. Make sure you set this if you're following along. This is going to be required before you can run queries. So, I'm going to leave this here. This is the default bucket I used in previous tests, but again, make sure you do set this up if you don't have it. Perfect. So let me go back to my editor. And one thing I have to change here is I need to change the location. So right now, it's just a default. I need to change it to the bucket. So I'll go here, copy the bucket name from the URL. I'll paste that into my query. And then for the account ID, I need to change that as well too. So for this, I'm going to go up here. I'm going to copy the account ID, and then I'll paste it in and replace this. Now this code will be available for you in a module asset if you want to do this as well. I'm going to run it, and there we go. We now have on the left‑hand side a default database with our cloudtrail_logs table with all of the event fields that we can now parse. So with this, I'm going to create a new query. And let's just go ahead, and let's get all records from this table. Now this is generally not a great idea because it's going to get a lot of records, but I'm going to run it. And it actually was pretty quick. You'll notice here we have 500 records or events that have been captured in our logs. So Athena just went through, scanned, and looked at all of the data in those logs, and you'll notice here we have each line or each row is an event. So we have the eventversion, useridentity, and we have a bunch of other key information. So if I go over, we have the time, eventsource, name, region, all of the important information to go ahead and diagnose what's going on in the account. Now with this, we could start running more important queries. Maybe you want to look for specific users, for example, or maybe you want to look for console logins. Well, that's all achievable using this service and CloudTrail Logs. So for our test, let's take an example here. I'm going to create a new query window. I'm going to copy and paste in a command here that I've had already created on the side. And what we're going to do is we're selecting all records from our table where the ARN matches our limited user. Now we could do it for the Access Denied error message. There's a lot of ways we can do this. But I just want to see wherever limited_user has been called, I'm going to run this. We should get some results. We do. We get six results into all of those CloudTrail logs. We see the relevant information, access keys, etc. If I go over here, well, there we go, eventsource, ec2, eventname, Terminate. Now if I look further in here, we're going to see that it was Access Denied. You can see right here, UnauthorizedOperation. Perfect. So we were able to diagnose what was going on. You are not authorized to perform this operation. Limited_user does not have the proper permissions. Perfect. So that's going to do it for this demonstration on using CloudTrail trails. We customized one, we created one, and then we parsed it using Amazon Athena. Let's go ahead. We're going to wrap this demo up, and I'll see you in an upcoming clip.

AWS Config Overview
All right, we just got done reviewing CloudTrail for auditing historical actions that occurred in our account. Well now we're going to use a service called AWS Config to detect configuration issues within our accounts. AWS Config is an inventory management and control tool and service that allows you to view the configuration history of your infrastructure over a period of time. It offers the ability for you to create rules to essentially make sure that your resources in your accounts and your architectures conform to the requirements that you specify. It's important to note that Config is a regional service, so you configure Config rules, which we'll cover in its own clip on a per‑region basis. When using this service, it is capable of setting up to have you receive alerts via Amazon SNS whenever it detects changes in compliance status for your resources that you've configured. Now with all of this added up, it works really well with AWS Organizations. You can perform aggregations of your results across multiple regions in multiple accounts so that they all group up into a single centralized account for easy access and easy reading. And then lastly here, it also integrates with Amazon S3. With S3, you can store your historical config data in your buckets so you can maintain that and store that long term. Now moving on, within Config, you have statuses. Now we're going to cover these more in depth here later in the module, but I want to cover them at a high level because you're going to see these more and more as we continue on in this clip and in the next clip. When you are scanning resources for their status, you're going to either get a COMPLIANT status or a NON_COMPLIANT status. Now there are some other statuses, but they're not important for the exam. Just understand for now COMPLIANT and NON_COMPLIANT. I'm sure these are pretty self‑explanatory, but we're going to dive into them again much more in depth later on. Now to break things up, an exam pro tip here. Config is a reactive service. It is not proactive. You need to remember that for the exam. It does not prevent changes from occurring. It just records them. This also, if you remember, are how the detective controls within Control Tower work. They use Config as the underlying service. Now let's look at a couple of use cases for Config that you might run into on the exam. Restricted‑ssh. So this is a rule, and again, we're going to cover rules much more in depth, but for now, just understand you can use Config to accomplish this. But restricted‑ssh is going to allow you to make sure that incoming SSH traffic for your security groups are restricted to a specific CIDR and not just open to the internet. You can also check if EBS encryption is enabled by default for an account. So if it's not, then that account and those resources are going to come back noncompliant. Third popular one, checking that an S3 bucket has default encryption enabled or it denies PutObject calls that don't have encryption specified. And then the last common use case here, checking that there's at least one AWS CloudTrail trail that's created in the account in the different regions. These are all popular to come up on the exam, so understand these use cases. And then to wrap things up real quickly, one last exam pro tip, you can use AWS Config to evaluate tagging compliance for resources as well. So this is kind of another use case, but it's very common to come up on the exam. So do your best to remember this scenario right here that you can accomplish and solve using AWS Config. With that being said, let's end here. We're going to go to the next clip, which we're going to start diving into rules and remediations.

AWS Config Rules and Remediations
All right, we just got done reviewing what AWS Config is and what it allows you to do. Now we need to start diving into the resources that make the service work. An AWS Config rule is the heart of the service. This is a compliance check that helps you manage your ideal configuration settings for different resources in your accounts. Once your rule is in place, the service is going to evaluate compliance status using that rule as the framework. Now there are two offerings of rule types within Config. There's an AWS Managed Rules, or you can make your own custom rules. Both of these rule types can be triggered based on configuration changes that get detected, or they can be scheduled on a regular basis. Do your best to remember how you can actually evaluate your rules. Now, when you use custom rules, they leverage AWS Lambda functions or Guard, which is a GuardDuty‑specific language, to perform evaluation. For the exam, I would be more familiar with Lambda than anything, as that's typically how this gets used. Now with the rules, you can also perform a remediation. Remediations are going to go through and attempt to put your resources into a compliant status. So in other words, you remediate non‑compliant resources, and in addition to that, you can retry. So for example, maybe you have a security group that allows too much traffic over port 22. Well, maybe your remediation can go through and automatically try to remove that rule, and if it fails, it'll try again up to a certain amount of retries to make it compliant. Now these remediations are completed and ran via Systems Manager Automation documents. Really do your best to remember that as well. That's very important. And another important note, Config is never free. This is important to understand. You pay for rules, evaluations, etc. you never have any free resources within the Config service. Now building off of rules, you can also set up notifications for your rules when they're evaluating compliance status, so you need to know the different destinations that you can send notifications to. The first is EventBridge, so you can trigger automated workloads using EventBridge based on compliance status detection. Again, maybe your security group is too open, it goes into noncompliant. And Config sends that information to EventBridge, and maybe you trigger a lambda function, you send an SNS message, etc. The second is you can send updated configuration details to S3. So again, you can store historical configuration details and data long term in an S3 bucket. And then thirdly, you can just send straight‑up messages to SNS. So you can publish notifications whenever a change is detected or whenever a compliance status changes, and you can send these to maybe an operations team, an admin, things of that nature. That's going to go and do it. Let's end here. We just covered Config rules, destinations, and remediations, and we're actually going to jump into a demonstration where we use AWS Config.

Demo: Recording Resource Compliance with AWS Config
All righty, time for another demonstration. In this demo, we're going to work with AWS Config to record resource compliance. Before we jump in, let's go over what we can expect to accomplish in this demonstration. What we'll do is we'll log in as cloud_user in the hands‑on playground, and we're going to disable some VPC Flow Logs on an existing VPC. Now, to counter this, we're going to work on creating an AWS Config rule that ensures that VPC Flow Logs are enabled. Now when we configure the managed rule, it's only going to work for periodic scans, but we're going to simulate that or speed that up with a manual evaluation, and I'll show you what we mean when we get there. However, the result will be the same. It's going to scan VPCs, it's going to find that we disabled it, and it's going to say, hey, this VPC is NON_COMPLIANT, and we're going to work on triggering Systems Manager Automation documents to go ahead and remediate that issue and turn on our VPC Flow Logs. With that being said, let's jump into the console now. All right, I'm in my hands‑on playground here. I'm logged in as cloud_user. Before we get going, let me review what we've already created very quickly. We have our default VPC, and I created a demo‑flow flow log here that gets sent to our S3 bucket, which is a simple bucket I created for this demonstration, which is here. On this, you'll see under Permissions, we have a bucket policy, allowing the flow logs to get put in here. So really make sure you understand this as well. Now when I go back, I'm going to go ahead and leave this open. Last thing is I created a role that we're going to use later on, and the policies for this will be available for you in the module assets, and we'll talk about this more here in a moment. So, let's get started. I'm going to go to AWS Config. Now, if you load this for the first time, there might be a section where you have to go ahead and set it up, and if you want to and you're following along in the hands‑on playground, go ahead and do so. For me, I've already done this before or it's been done in this account. So over here, I'm going to go to Rules, and I'm going to add a new rule. Now under rule types, we can create a custom Lambda rule. We can create a custom rule using Guard, which is a custom policy that really isn't that prevalent on the exam. The big thing to know right now is Lambdas or managed rules. What we're going to do is we're just going to use a managed rule. So for this, notice there are 630 AWS managed rules. There are a ton. Lucky for you, I already know which one we're going to do. So what I'm going to do here is type in flow‑log. And we're going to select vpc‑flow‑logs‑enabled. It's EC2 label, and notice it's DETECTIVE. It gives us a description over here on the right‑hand side. I'm going to click Next, and now we can give it a name. Now we'll just leave the name the same, vpc‑flow‑logs‑enabled. We'll leave the description the same, and notice under here, Evaluation mode. Now, depending if it's a custom rule, you can change these. For this, we can't because it's a managed rule with specific settings. For instance, notice Trigger type right here. It's only set to Periodic, but if it was your own rule, you could do when configuration changes occur. So if we disable them, well, then it's going to go ahead and say, oh, I saw that, go ahead, evaluate this rule again, make sure that resources are compliant, and then do whatever you need to do. So for us, frequency. We can do 1 hour, 3, all the way up to 24, so once a day or once an hour. Now it doesn't matter for us. I'm going to select 1 hour because we're going to manually evaluate this to kind of speed up this process. However, the big thing to know is the trigger types, periodic or when configuration changes occur. Remember that for the exam. Then next, we have the parameters here. So this is specific to the rule. Now I'm not going to put anything in here. It's not a big deal. We're going to leave this blank, and then we're also going to leave Rule tags blank. Now obviously, tag things please if you're doing this in production. I'm not going to because it's a sandbox that's going to get blown away. We review it, and then I save it. Perfect. So now we have a rule. I can select it, and eventually, we're going to see all of the resources that fall under this rule resource type. It's looking for VPCs specifically, so that's what's going to show. Now what we can do here is under Actions, I'm going to click on Re‑evaluate. So this is going to manually trigger an evaluation of this rule for this account in this region. Remember, Config is a regional service. With that being said, what I can do here is refresh, and eventually we're going to see our VPC down here, and it should be compliant. And there you go. Notice ac3bf. That is the default VPC here, and it's compliant because we have Flow Logs enabled. So if I select this, we're going to see all the details, we can see the JSON and the rules that were applied to this resource. All right, so we're in a good state. That's awesome. Let me go back to my rule here. And what I want to do is I want to make it noncompliant. So to do that, what I'll do is go to my VPC. I'm going to select this flow log here, and, oh, I'm going to delete it. Let me go ahead and delete. And eventually, since it's periodic, in 1 hour, this should scan again and say, oh, hey, this VPC is not compliant anymore. What do you want to do about it? For us, however, I'm going to go ahead and reevaluate once again. So what I'll do here is I'll refresh, and eventually we should see updated results. And what I'll do is I'll refresh on the back end until this shows up. So let me go ahead, I'm going to pause and then cut forward when it's ready. All right, so I cut forward. I need to tell you really quickly, that took a few minutes, so it's not immediate, which is a good thing to understand for the exam. If you didn't configure the trigger type to be a change notification when you manually reevaluate, it can take some time. So that took a few minutes, but you'll notice here at the bottom, we are now noncompliant for our VPC. And if I select this, well it's because this rule here, VPC Flow Logs not enabled, which makes sense. We disabled or deleted it. Perfect. So this is working. We're now saying, okay, we have a non‑compliant resource based on our rules. Well in here also, we can select the resource timeline, so we can see the historical tracking of when it became noncompliant. So you'll notice, hey, configuration changes. It was in compliance when we scanned here. We can look at the changes, so when we went ahead and turned things on, when we turned things off, when it got evaluated, and when it was noncompliant when it got evaluated again. So again, Config gives you a historical trail of when events occur specific to your resources that are getting tracked. So with that being said, let me jump back here to Dashboard. We can see a nice little handy metric system with the different items that are being recorded, non‑compliant resources here. So I can go here, look at the resource directly, I can click on it, see what's going on, etc. What I want to do is go to Rules. I'm going to click on my rule, and what I want to do is I want to work on remediating the non‑compliant resource. So for this rule under Actions, I can manage remediation. Now with Config, you can perform manual or automatic remediation. So when it detected it as noncompliant, if we had automatic remediation set up, it would automatically try this remediation step for us with retries and backoffs. What we'll do is we'll just do manual remediation. So for remediation action, notice all of the different types of actions that are possible. Now I'm not going to show you all these. You don't need to memorize them. The big thing to remember is you can remediate non‑compliant resources using Config and automation documents. So what I'm going to do is I'm going to look for VPC, and I'm going to find AWS‑EnableVPCFlowLogs. This is a managed document that we can use to turn back on our flow logs. Now in here, you're going to notice it gives you all of the parameters and the information you need to fill those out. So let's go ahead and fill this document out. The first thing we have is the VPC IDs. So what we're going to do, we'll skip over Rate Limits, that's fine. We'll accept the defaults. But for the ID parameter, we're going to pass in the VPCId. So this is going to take the resource ID, which is the VPC ID based on our rules, and pass it into this document for us automatically. Now that's not in scope for the exam. That's more of a just something you need to know when you're doing this in the real world. So the next thing we have to do is LogDestinationType. So LogDestinationType, if I go up, can be cloud‑watch‑logs or S3. So I'm going to go ahead, I'm going to put S3 down here because we want to put it into our bucket, and then we have LogDestinationArn. Well let me scroll back up and see what that is. Well, that's the destination where we are sending these logs to. So it's the S3 bucket in our case because that's the type of ARN we need for an S3 bucket. So what I'll do here is go to my buckets. I'm going to list, select it, and I'm going to copy my ARN and go back. And I'm going to go down and paste that ARN in. There we go. Next, DeliverLogsPermissionArn. So, probably what it sounds like, IAM role with permissions to go ahead and do what we need. However, if you notice for this specific document, and it's not always the case, it says if you're using S3, you do not need to specify this. So we're going to leave ours blank. The next was TrafficType, so we're going to go ahead and do all. We want to set all traffic data captured. And then the last thing that is important is the automation assumed role for this. So this is exactly what it sounds like. The automation document is assuming a role that it can use to enable the flow logs. So this is that role that we talked about earlier here. So I'm going to copy this ARN. I'm going to go back, and I'm going to paste this ARN in. Perfect. Now I'll save my changes, and we've set up a remediation step for this rule. So now what we can do here is if I go back to Rules, let's go ahead and select it. I'll scroll down to the bottom here, and I'm going to remediate this resource. So I'm going to click on Remediate. And what this is going to do is it's going to execute our automation document, you can see it's queued up, and eventually it's going to turn our VPC Flow Logs back on. Now what I'll do here is I'll pause again, and I'll cut forward to once this is complete. Perfect. Action executed successfully. All right, well, what we can do now is I can go up, I can reevaluate this rule, and eventually this is going to come back compliant because we executed an automation for remediation. Now that's going to do it for this demonstration on AWS Config. We recorded historical changes with our resources. We saw if they were COMPLIANT and NON_COMPLIANT, and then we automated a remediation effort using an automation document. Let's go ahead and wrap things up here, and then I'll see you in an upcoming clip.

AWS Trusted Advisor
All right, let's move on to another important service called Trusted Advisor. AWS Trusted Advisor is a fully managed, best practice auditing tool for your AWS accounts. It's important to remember it works at the account level. The reason I say it's important is because another very important thing is that it requires no installation. So you don't have to install and configure agents, there's no tools, software, etc. You just use the service at the account level. Now it operates by using industry and customer‑established best practices, so it uses years of data from security experts, AWS, etc. And it builds all of those and then inspects your environments, so your AWS accounts, and it looks to make recommendations for several things. Some of the common ones that come up on the exam are recommendations on where you can save money. So maybe you're underutilizing compute, maybe you can get rid of things, etc. It's also going to make recommendations or advise to improve system availability and system performance, so using Auto Scaling, using Multi‑AZ, things like that. And then it also helps you close potential security gaps because it's looking for things like overly permissive security groups or NACLs. By leveraging Advisor, you can see, oh, this is an issue. Let me go through, and I want to close these gaps and close these security vulnerabilities and then check everything out again. Now with the service, there are check categories. So when it's making these checks, they fall under one of six categories. The first is cost optimization. This is where it's going to make recommendations on where you can save money in your accounts. So again, unused resources or maybe other ways to downsize and reduce your bill. We then have performance. So how can you improve speed and responsiveness of your applications? Do you need to scale up vertically? Do you need to use different types of services? Things like that. The third is security. So this is self‑explanatory, but what kind of security settings can be changed or made to make your solutions and environments more secure? Are you rotating access keys? Are you denying overly permissive traffic? All of those would fall under a security check category. Fourthly is fault tolerance. What this is is a recommendation to help increase, of course, resiliency of your environments. So are you using proper Multi‑AZ deployments? Are you implementing redundant solutions? Those would fall under a fault tolerance category check. Next is a service limit category. So this is going to check the usage for your entire account and make sure that you're not approaching or exceeding known quotas for services in AWS. If one of your checks comes back on this, well, maybe you need to request an increase in the quota for that service. And then lastly, operational excellence. These are recommendations to help you operate much more effectively and do so at scale. So, things like using Auto Scaling groups, using Global Accelerator, those are all going to fall under operational excellence. Now it is very important to note here, you don't get access to all of these checks by default. It's going to depend on the support plan you've signed up for on your accounts. With that being said, if you have Business, Enterprise On‑Ramp or Enterprise Support plans, you can use all checks in all categories. And even more importantly, you can use the API and the CLI to reference your checks. So in other words, you can automate a lot easier using Advisor. On the other side, if you signed up for Basic or Developer Support plans, which are much cheaper, well, you have access to all checks in the service limits category. But other categories are limited, especially security checks. So again, the support plan will dictate what kind of checks you have access to. Now with that being said, let's wrap up Trusted Advisor. Just remember, it makes advice or advises you on how to improve your accounts. We're going to end here and move on to another very important service called Amazon Inspector.

Amazon Inspector
All right, let's move on to our next service called Amazon Inspector. When you're deploying to AWS, it is critical that you're able to assess your security posture for your architectures. That is part of the Well‑Architected Framework. Well, this tool offers you a way to automate security assessments that are going to help improve your security and your compliance for your applications and your resources that you've deployed into AWS. It works by automatically assessing your applications for specific vulnerabilities like CVEs or deviations from industry best practices. So for example, maybe you're using a library package for your application that's outdated, or maybe you're allowing too much traffic to your EC2 instances. That would be a deviation. After this service runs through and it performs these assessments, Inspector goes in, and it produces a detailed list of security findings that get prioritized by the level of severity, and it gets an assigned risk score. So each of these assessments and these reports are going to be catered directly to your specific account and your specific resources. When you get these findings, they can actually be sent directly to Amazon EventBridge. So if you want to trigger automations based on a finding, you can do that. For instance, maybe you want to go ahead and run Patch Manager to patch some of your instances using Systems Manager based on a finding saying that you have a vulnerability. Again, this is all possible with Inspector. Now for the exam, you're going to need to be familiar with the two scan types that this service offers you. The first is agent‑based. This is going to be the most powerful way to scan and assess your posture. It uses the SSM agent to continuously scan instances in your account. By using this agent, it's able to collect inventory information, package installations, and things of that nature, and then it can use that to compile that detailed score. You can also use an agentless scan. How this works is it's going to scan EBS snapshots taken from your EBS volumes attached to your instances. And it runs scans on those to collect its software inventory. So it's not as invasive as agent‑based, but it is a little bit more limited on the information it can collect. In addition to the scan types, you'll need to know for sure the different types of scans that are available and some of the findings that they can actually give you. The first scan type is, of course, EC2. This uses primarily the SSM agent to collect software inventory from your different instances, and then it's going to go through and look for package vulnerabilities. The neat thing about this service is it will also look for potential network reachability vulnerabilities as well. In other words, are you too exposed to the internet? Are you allowing unrestricted traffic for SSH, Remote Desktop Protocol, HTTP, things like that? It also scans your ECR images. So, they are phasing out the scan on push method, which is still on the exam, and you need to know that, but this is the thing that's taking over for scanning your different images to look for your different software vulnerabilities that are stored in those images in your ECR repo. What this does is it offers enhanced scanning for finding operating system and language package vulnerabilities for your custom images. And then thirdly, you can scan your lambda functions. So when you turn this on, it enables automated security vulnerability assessments for your functions in your accounts. Now there are two types within this type of scan. There's a standard scan, which covers your application dependency vulnerabilities, so in other words, your libraries that you're importing, whether they be custom or standard. And then you also have code scanning, and this is going to scan your actual application code to look for any issues. I will say for this exam, you more than anything need to understand you can scan your lambda functions at all using this service. And then lastly here, exam pro tip. Just think, Amazon Inspector if you ever have a scenario where you need regular security scans across numerous instances or lambda functions. Also do your best to remember that you can trigger EventBridge automations using the findings that are generated from this service. And with that being said, let's go ahead and wrap up Inspector, and we're going to move on to another important service called GuardDuty.

Amazon GuardDuty
All righty, continuing on the account governance and security train, we're going to talk about Amazon GuardDuty. This is a very important service for you to understand how it's used in your accounts. Amazon GuardDuty is a managed threat detection service that uses machine learning to continuously monitor your accounts for potentially malicious behavior. It is very important you understand this is an IDS, an intrusion detection service, and not a prevention service. You will get some scenarios on the exam that are going to try and trick you to think that this is an IPS, intrusion prevention. It is not, it is an IDS, intrusion detection. Some example threats that it looks for and it can alert on include things like unusual API calls coming from a known malicious IP. So for instance, maybe they're trying to delete instances, list buckets, things like that. It also can alert you if there's potentially compromised instances. So if you have an application or maybe you have some engineers that log into an instance and they perform their work there, and maybe they accidentally download a malicious piece of software and it gets installed, well, this can help you find that. Thirdly, port scanning. If someone is performing port scanning on your public‑facing applications, it can alert you as well. This is a very common threat that comes up all the time within the cloud because a lot of things are publicly accessible, and you will get a ton of port scans looking for potential vulnerabilities. It also very commonly will look for unauthorized cryptomining activity. This is a very common exam scenario. If you have a potentially compromised instance and there is a cryptomining application installed on it, Amazon GuardDuty does a good job of alerting you and catching that. Now with that being said, we understand this is an intrusion detection service. You need to understand some concepts that are important for this service for this exam. First and foremost, it does not require the installation of any software or any agents. So you don't install it on compute, you don't install it at the network entry point, you don't do anything installation wise, it just gets enabled on the account. With it, it can be pretty expensive, but you do get a free trial for 30 days before they start charging you. When you get alerts after you enable this, they appear in the GuardDuty console, and they will appear in EventBridge. So you can, just like other services, trigger event‑driven workloads and automations with EventBridge. Now how it works is it actually leverages many different databases, and a lot of these are third party or external feeds. It updates its databases using the known malicious domains, IP addresses, etc, and it uses it with its machine learning to say, hey, this looks like a bad API call from my malicious IP. I'm going to alert the account owner. This is a common scenario on the exam as well that it uses external feeds to update its own database that it uses. Now moving on from concepts, you need to understand some data sources that this service can use. The first group here is going to be foundational, so these are built in, very easy to use, and very common on the exam. The first foundational source is CloudTrail logs, so you can look for bad or malicious API calls, anomalies, things like that. You can also send your VPC Flow Logs to GuardDuty. So with these, it can look for potentially harmful network calls or port scanning for your architectures. And then thirdly, another very, very common source is a Route 53 Resolver DNS query log. So you can see, are people doing a ton of lookups? Is it out of the norm? If it is, well, it can report that for you. We also have extended sources. So these are possible, and you need to know them, but they're not the most common. The first here is RDS and Aurora, so you can see if something's going on with your managed database systems. You can look for S3, so are buckets open? Are people making bad calls to your buckets? And you can also send Lambda and EKS information to them, so you can see if your lambda functions might be compromised, if your EKS clusters might be compromised, and if people are trying to abuse those. I will say the top three here are the most common, but you do need to be familiar with the bottom three as well. Please do your best to remember these six categories and data sources. Now another exam pro tip to break things up. It is common to set up a delegated admin account in your organization to act as the Amazon GuardDuty centralized account for reporting. This is very common on the exam as well, and it works very well with Organizations, obviously. Now moving on to an exam scenario. Let's go ahead and assume you're hosting a web app, and it's running on EC2 instances that you've decided to put behind an internet‑facing application load balancer. Your application is using RDS for MySQL for its database for storage of the data. Your company needs the ability to automatically detect and respond to suspicious or unexpected behaviors in the environment within the account. Now, of course, you're a great solutions architect, and you've already worked on implementing AWS WAF to the architecture to prevent some malicious calls and potential web application attacks. But you need something else. So what is a possible solution to add additional protection to meet these requirements? Well, this is a perfect scenario and use case for Amazon GuardDuty. So on the left side here, we're going to assume we have a bad or malicious actor, and they're trying to perform a complex attack that you didn't catch with AWS WAF, and we're going to assume the attack is successful and they bring down your application. Well, what you can use is GuardDuty to easily implement protection here. So you can turn on GuardDuty in your account and start performing threat detection services. Based on GuardDuty, it's going to catch and have a finding on these attacks that are being performed. With it, you can send them to Amazon EventBridge, which can filter for those findings, and then it can trigger an event‑driven workload. So a good example is, hey, okay, we see we're having some attack, we trigger EventBridge. Well, maybe I want to trigger an AWS Lambda function and invoke it, and then they can go ahead and adjust and add AWS WAF rules to protect against that new threat. So now we've updated our WAF in an automated event‑driven fashion using GuardDuty, EventBridge, and Lambda. And if the bad person tries to perform the same complex attack, well, this time it gets blocked at our WAF that we've implemented. So this is a very, very good solution for automating and using managed services to update protections of your account specifically geared around GuardDuty. With that being said, let's end here. Do your best to remember this exam scenario and really when you would use GuardDuty, and we're going to move on to another service to look for sensitive data in your S3 buckets called Amazon Macie.

Amazon Macie
All right, we just got done looking at protecting our accounts using the intrusion detection service known as GuardDuty. We're going to move into something more specific and look at protecting sensitive data in your buckets using Macie. Before we dive into this service though, let's quickly review PII, or Personally Identifiable Information. Now this is going to be based on how AWS classifies it for this service, so just keep that in mind. Let's go ahead and jump in. PII at the most simple level is personal data that is used to establish someone's identity. With this data, if it was ever exposed and stolen, it could very easily be exploited by criminals, so things like identity theft, financial fraud, adding them to spam caller lists, things like that. Some of this information includes things like home address, email address, Social Security numbers, passport numbers, and it even includes things like your date of birth, phone number, bank and credit card numbers. These are all pieces of information that could potentially leak personal data and be used to establish someone's identity. Again, it's personally identifiable. Because of this, this is all classified as sensitive information that needs to be protected. I think we can all agree if this data lives in someone's application, well, yes, I want you to treat it as very sensitive. I don't ever want it stolen. Well, that's where Amazon Macie comes in. Macie is a fully managed data security and privacy service that's going to leverage machine learning and pattern matching to discover potentially sensitive data that is stored in your Amazon S3 buckets. By using it, it recognizes if the objects in your buckets contain sensitive information, so Personally Identifiable Information, Personal Health Information, and financial data. Again, anything sensitive in S3, you can scan for it using this service. In addition to just scanning and finding, if it does find sensitive data, you can easily configure it to alert you via an SNS topic to send you notifications and potentially trigger workloads. Remember, using SNS, you can put in Pub/Sub architectures. And with Pub/Sub, you can have several different consumers from the one producer. In addition to Amazon SNS, it also works with Amazon EventBridge. I hope you're starting to notice a pattern here that EventBridge integrates very easily with a lot of these security services so you can kick off your own automations. On the exam, this service is going to be great for you trying to meet compliance frameworks like HIPAA, PCI, and GDPR. If you see any of those specific to S3, I would immediately at least consider including Amazon Macie in my solution. However, with that being said, this only works with S3. It doesn't do anything else anywhere else. So, EBS, EFS, those things don't work with this service. It's only looking in your S3 buckets. That is very important. And an exam pro tip here to wrap things up. Again, this is going to be a perfect tool choice for discovering PII or PCI within Amazon S3 buckets. I repeat that because it's important. That is a perfect scenario for using Macie in your solution. It is a managed service. You turn it on, and you can generate findings. With that being said, let's wrap this up, and we're going to move on to another important service called Security Hub.

AWS Security Hub
All right. Picture this. You've got your multi‑account environment. You started spinning up all of your protective services like GuardDuty, Macie, things like that. Well, what happens when you get so many accounts that it starts getting confusing? You want a centralized place to manage these notifications and these different findings. Well, that's where AWS Security Hub comes in. This is meant to be a single place to view all security alerts from several different services, including the following. You can centralize GuardDuty findings, Inspector assessments and reports. You can centralize Amazon Macie's sensitive data classification in the event for findings of that data. You can centralize firewall management. So when you have AWS Firewall Manager, if there's any type of updates or notifications, they can also centrally manage those. The big thing to remember for this service is it aggregates across multiple accounts in an organization, typically using a delegated admin member account. In this, more than likely you'll delegate some type of security team account to be the delegated admin, and that's where all of the findings will be reported to. It's going to be perfect if you need centralized findings for the following, the NIST framework, so National Institute of Standards and Technologies, and the Payment Card Industry Data Security Standard, or PCI DSS. If you need to centralize findings from many accounts for these particular compliance frameworks, I would immediately consider this service. With that being said, it's important to understand it's perfect for having a single place to view security alerts across accounts, but it does not do anything about them. You still have to take action using those services or some other service. This is simply meant to be a hub for your security alerts and your findings. Please remember that for the exam. With that being said, we're going to wrap this clip up, and we're going to move on to another similar service coming up next.

AWS Audit Manager
All righty, we just got done discussing Security Hub. In this clip, we're going to look at Audit Manager. Again, let's take a step back and picture this environment and this scenario. You're deploying your applications and your resources to AWS, and you fall within one of the many different frameworks that require you to be compliant, so maybe PCI, SOCs, different stringent requirements of those natures. Well, Audit Manager is a managed service that allows you to continually audit your usage in AWS and ensure that you're staying compliant with those standards and those regulations. It is an automated service that's going to help produce reports specific to auditors for things like PCI compliance, GDPR. This service is perfect to help you gather the information to justify that you're falling in line with those complicated frameworks. It also works with your own custom assessments and custom frameworks. So for instance, maybe you fall under PCI, but in your industry, maybe there's a smaller, less well‑known framework and compliance regulation that you need to follow. Well, you can create your own, and you can use Audit Manager to generate custom reports for that as well. On the exam, a great indicator for this service is anything regarding HIPAA or things like GDPR where you have compliance and it's asking you about continuous auditing and automating audit reports. If I had to pick one of those, automating audit reports is going to be perfect for this service. Really just try and do your best to remember, Audit Manager, well it manages your audit reports. That's the way I like to think of it. With that being said, we're going to wrap up this short clip here, and we're going to move on to another security service similar to Audit Manager coming up next.

AWS Artifact
All righty, let's dive into our next service here called AWS Artifact. For the exam, there's very few instances of this popping up, but you do need to be aware of it in case they start adding more questions regarding it. So let's very quickly review what this is. At the highest level, this is a service in AWS that provides on‑demand downloads of AWS security and compliance documents. It is going to be perfect for any scenario where you need to provide documents that show that your services that you're using in AWS meet compliance standards. So if you need to download documents to prove that the AWS service is going to meet ISO standards, PCI standards or SOC standards, that should be an immediate indicator to leverage AWS Artifact. You can also use it to download compliance documents for independent software vendors, otherwise also called ISVs. Again, the biggest indicator on the exam is if you ever need to download compliance documents specific to the services that you're using, this does not help you prove compliance for your applications that you're hosting. It's only the AWS services. And with that being said, we're going to end here, and I'll see you in the upcoming clip.

Module Summary and Exam Tips
All right, another day, another module completed. Way to hang in there. Let's go over our typical summary and then go over some exam tips that I think are important for you to remember when you're taking your exam. First up, let's review CloudTrail. Remember, this is the go‑to service to audit any and all API calls and actions that occur within your AWS accounts. It is extremely useful for after‑the‑fact incident investigation. It's not a preventative service, but it can alert you and provide historical trails on actions that have been taken. For the exam, it is critical that you remember how and why you turn on log file validation and remember the different encryption options. You can specify a custom KMS key per CloudTrail trail. And also, you have to remember the different event type categories. Management Events, which have read and write events. There are Data Events, which are more specific to actions occurring in or on the objects or the data, so an S3 object, a DynamoDB item, things of that nature, and Insights Events, which are meant to give you insights about anomalies and unusual activity. Now in addition to the review, some more important things to remember about the service and its use cases. Remember why and when you would use an organizational trail. Again, this is perfect for implementing the same exact trail with the same exact settings throughout an entire organization. You also need to understand the destinations for storing your trail logs, CloudWatch or Amazon S3. You need to understand when and why you would enable SNS notifications of log file delivery. Remember, this can get very noisy, but it's good for SIEM or third‑party vendor integration. And lastly, trails are regional resources, but you can have multi‑region trails, so you can duplicate the exact same trail across all of your active regions. Shifting gears to AWS Config. Anytime there's a scenario where you need some type of a rule that needs to be set up for an account, you want to think about using Config to check for compliance status on those resources. With it, you can automatically remediate issues using automation documents and checking via lambda functions. If you recall in our demonstration, we leveraged an AWS managed automation document to trigger a remediation effort for our rule. You really need to think of this as the one‑stop shop to see what has changed, because remember that it provides you a historical trail of all of your resources that are being recorded. Again, remember in our demo we viewed that historical timeline when our rule was compliant and then when our resource fell out of compliant. A perfect example for using AWS Config on the exam is identifying untagged resources in an org. You can use it to check if volumes are encrypted or not by default or maybe if a security group has too permissive of a rule set. Up next, Trusted Advisor. Remember that this service overall is free to use, but you'll need better support plans in order to get full access and make the most use out of automating via the API. It's strictly an auditing tool, and it will not solve the problem for you. This is a common trap that the exam will try and set for you. If you need to take actions, this service will not do that for you. With the higher support plans, remember, you also get access to EventBridge to kick off things like lambda functions or other automated workflows to solve the problems for you. Because you get access to the API and the CLI interaction, that's how you can use these other services to automate remediations. Now, a quick example on when you might use Trusted Advisor on the exam. Let's assume you have an organization and you have several member accounts, and you need to find a way that best optimizes costs for you. Well, you can very easily use Trusted Advisor to get cost optimization recommendations. Remember, it will go above and beyond the compute optimizer solution that AWS offers because it looks at everything in the account. Up next, Inspector and Artifact. Amazon Inspector will automatically assess applications and environments for vulnerabilities or even deviations from standard best practices. These findings that get generated can be sent directly to EventBridge to trigger workflows. In addition to the destinations, understand the sources for the scannings. It supports EC2 instances, it supports ECR images, and it even supports lambda functions. Next, we have AWS Artifact. This is going to give you the ability to download on‑demand documents that are important for security and compliance verification. If you need to prove that a service in AWS falls in line with a compliance framework and standard and regulation, think Artifact. Then we have Security Hub and Audit Manager. First up, Security Hub. Remember, this is the single place or a hub, if you will, to view all of your security alerts for many different services. You need to remember that it supports GuardDuty, Inspector, Macie, and Firewall Manager. You can aggregate and centralize all results and findings within Security Hub for these services. We then have Audit Manager. This is an automated service that helps you generate reports specific to auditors for different compliance, so PCI compliance, PII, GDPR, and many more compliance frameworks, including your own custom ones. The difference between this and Artifact is this generates reports specific to your environment itself and your different applications. Artifact is only for AWS services. Next up, GuardDuty. This is a service that's an intrusion detection service that uses AI and machine learning to learn what normal behavior looks like in your accounts and then alert you if any abnormal or malicious behavior is detected. You need to understand the three common sources here, CloudTrail logs, VPC Flow Logs, and DNS logs. These commonly appear on the exam as data sources for this service. And in addition to the sources, remember that it updates a database that it uses of known malicious domains and IPs using external feeds from third parties. So it uses this database to trigger alerts or findings. And speaking of triggering, it can trigger EventBridge rules. So if you have a finding generated, you can kick off an EventBridge rule that maybe either notifies someone, performs remediation actions or something like that. Next up, Amazon Macie. Remember, this helps identify sensitive data in your S3 buckets. When we say sensitive data, we mean things like PII, PHI, and financial data, so, credit card numbers, it could be email addresses, home addresses, things like that. This is great for making sure you're falling in line with HIPAA and GDPR compliance. In addition to falling in line with compliance, the alerts that get generated via Macie, when it finds sensitive data, it can create an alert that can trigger EventBridge. So you can send that alert to an EventBridge rule, and that can take an action. So maybe it kicks off a lambda function, maybe it sends an SNS notification. Whatever you need, you can do it via Automation and EventBridge. And then lastly here, very, very important for the exam. This service only works with S3 buckets. Do not fall for any tricks asking you to use Macie for other services in AWS. It only works with S3. With that being said, let's end this summary clip here. We'll wrap up, and I'll see you in the next module whenever you are ready.

Infrastructure and Application Security
AWS Certificate Manager (ACM) Overview
All right, we just got done with the module reviewing different ways we can govern and secure our accounts at an account level. And now we're going to dive into more about the infrastructure and the applications running on that infrastructure and how you can best secure your workloads. This first clip is going to be about a very, very important service regarding TLS certs called AWS Certificate Manager, also abbreviated as ACM. ACM is the go‑to service in AWS that is going to allow you to create, to manage, and deploy both public and private TLS certificates with AWS services. It works by allowing you to remove a lot of the manual processes that go along with issuing certs, and it enables you to issue your own certs with literally just a few clicks of a button or a few automated API calls. One thing to keep in mind at a high level is it also offers a private CA, or certificate authority, option as well. Now you're not going to need to know a lot about that on this exam, as it's way out of scope, but understand it does offer it at a high level. When you're using ACM, it is going to commonly integrate with the following services, especially on exam scenarios. It very easily integrates with Elastic Load Balancing, so classic load balancers, network load balancers, and application load balancers. It also commonly integrates with CloudFront distributions. So when you're securing your distribution with your own custom domain name, well, ACM is the way to do so. And then the third popular integration is Amazon API Gateway. It integrates with both regional gateways and edge‑optimized gateways. Now each of these integrations have their own nuances and specific configuration options that you have to know for the exam, and we will look at those here coming up shortly. The last thing I want to touch on here at a high level, this service cannot directly associate certs with EC2 instances. Now there is a small exclusion for Nitro Enclaves, but for the most part on this exam, you need to understand they do not directly get associated with instances. That is a common trick that comes up in the scenario. Please remember that. And then in addition to remembering that, again, I'm going to stress this, you need to remember those commonly integrated services, API Gateway, ELBs, and CloudFront. Those are going to be the services to remember for exam scenarios. With that being said, let's wrap up this overview, and we're going to start diving into more specifics regarding ACM.

ACM Public TLS Certs
Okay, we now know what ACM is at a high level. Let's start looking at public TLS certs within it, as these are probably the most important thing to take away for this service for this exam. When you're using ACM for public TLS certs, you can request and use these on your AWS resources and services for free. That is one of the biggest benefits of using this certificate service. As long as you're using them in the service with the services, they are free. However, keep in mind you're still paying for the resources that are using them. So if you're using an ELB for instance, and you're assigning a certificate to your Elastic Load Balancer, well, the cert is free, but the load balancer obviously is not. You're still paying for that. In addition to the cost benefit, they also allow you to automatically use managed renewal of certs that are issued by the service. Again, this is important to remember for the exam. They have to be issued by ACM. Now I say that because we're going to talk about importing here coming up next, so that's a very important distinguishing feature. Fourthly here, ACM will automatically update your certificates on your integrated resources. So, for example, you issue a public cert using ACM, you attach it to your CloudFront distribution using a custom domain name. Well, when it needs to be updated, it will automatically do so with the service, and the service is going to automatically update the attached certificate. Again, this is one of the biggest benefits for the exam to remember. Now when they get renewed, ACM‑issued certs are going to auto renew roughly 60 days before expiration, assuming they are still valid. When we say valid, we're going to talk about validating domain ownership coming up next, but just keep that in mind. As long as they're still valid and you can prove that you own the domain, they will auto renew about 60 days before they expire. And lastly, of course, if you're going to issue a cert, well, you need to validate that you own the domain. You can't just issue a cert for google.com because, well, I'm guessing you don't own that domain. And if you do, well, congratulations! I'd love to hang out with you sometime soon. Now moving on to types of domains to know. For the exams specific to ACM, there are two categories of domains you need to know for when we're validating ownership or assigning certs. First is a FQDN, or a Fully Qualified Domain Name. This is going to be the complete address of a host on the internet to specify the exact location. So for example, you would request a cert for only matching www.pluralsight.com. That would be an exact FQDN that would point to a specific host or load balancer on the internet. The second type is a wildcard domain. This is going to use an asterisk as a subdomain to match all possible subdomains using a single certificate. So an example for this is you request one cert with a wildcard subdomain, and it works both for blog. and app.pluralsight.com. If we use the FQDN version, we would need two separate certs for both of those domains, as opposed to a wildcard cert, which matches both automatically using one cert. Moving on to validating that you own your domains. There are two methods you have to know for the exam for ACM. The first and the preferred recommended method is using DNS. DNS validation works by having you create a CNAME record in your DNS zone to validate that you do in fact own that domain. This is important to know because this is required for enabling automatic renewal within this service. Remember that for the exam. You have to use DNS validation to enable that managed automatic renewal. The second type of validation is email validation. Now this is not recommended, but it is a possibility if you must use it. How it works is ACM is going to send validation emails to the different system email contact addresses for the domain listed. Now you don't need to know those emails. You just need to know how this works. It's going to send an email to some specific system email contacts in the domain, and you have to validate using those emails. For the exam, if you use Route 53 for your DNS management, DNS validation is going to be super easy and quick because it allows you to very easily automate creating that CNAME record, all with the click of a button or a single API call. Now shifting gears, we talked about public certs and ACM‑issued certs, as well as some of the benefits that go along with those like managed automatic renewal and the free cost. But you also need to know about how to import TLS certs. First and foremost, you can import public certificates that you have attained outside of AWS. So if you use something like GoDaddy, DigiCert, things of that nature, you can import those and manage them within ACM to a certain extent. One important thing to remember for the exam is there is absolutely no automatic managed renewal for your imported certs. You lose that benefit when you import. That benefit only works for ACM‑issued certs. Thirdly, imported certs work the same as ACM‑issued certs. So when you import your external cert, it creates an ACM resource for you, and you can attach those resources to your integrated services. So your ELBs, distributions, API Gateways, all of those still work with these certs because they're in ACM. Now another thing to keep in mind. To renew these certs within ACM themselves, what you do is you obtain a new cert outside using a third party, and then you would have to reimport that into ACM. There's no automatic management for you, again, because you're importing it. And the last thing to remember here, ACM as a service will send expiration events for certificates 45 days prior to expiration. Now in addition to that exploration event, AWS Config, which we looked at in a previous module, does have a rule called acm‑certificate‑expiration‑check that is a managed rule that can evaluate if your certs are expiring within a specified configured number of days. Now speaking of that rule, let's look at an exam scenario that could leverage it. In this, we're going to assume we have an ELB using a certificate that we've imported into ACM. Your security team is saying, hey, we need to be notified 30 days before the expiration of each certificate that's in use. Well, using a few services, this is easier than it sounds. First off, we have ACM, and we've imported our public certificate that we're going to use on our resources. In addition to that, we can use that managed AWS Config rule to schedule an evaluation to say, okay, when does this cert expire? Well, that Config rule, acm‑certificate‑expiration‑check, can be configured to check for that expiring cert based on our 30‑day setting. So in this example, hey, we're expiring in 29 days. I better notify Config. Using Config, if you remember, you can integrate with other services. So for this example, we can integrate with Amazon EventBridge. With Amazon EventBridge, we can do a few things. We can send a notification to SNS to then notify our admin and security team users, and maybe we want to invoke something like a lambda function to perform some type of remediation effort or other type of workload. This is possible using Config, ACM, and EventBridge. Now moving on to regional and global certificate requirements. There are going to be scenarios on the exam where you have to know each of these requirements, so really pay attention here. First, regional resources require that ACM certs be deployed in that same region. An example would be, let's say you have an ALB in us‑west‑2, and you want to assign a public custom domain name with TLS access. Well, if you're using ACM, which you should be, the cert has to also exist in us‑west‑2, so they are regional resources. This is especially important for multi‑region architecture scenarios. So if you have a primary and secondary region, and you're using ACM certs for ALBs, remember they need to exist in each region where your ALBs exist. Moving on to global resources. If they are global, they need an ACM cert to be deployed in us‑east‑1 region only. This is required. It doesn't matter if your cert exists in every single other region in AWS, it has to be in us‑east‑1 to work. An example of a global resource would be an edge‑optimized API gateway. When you create this, you're going to need an ACM cert to exist in us‑east‑1 only in order to use it. Now this is going to be the same for CloudFront as well. That's a very common scenario. Edge‑optimized gateways and CloudFront distributions both need us‑east‑1 certificates. Now with that being said, let's go ahead and wrap up here, and we're going to move on to a demonstration where we're going to start issuing public certs using this service.

Demo: Issuing a Public TLS Cert with ACM
All right, let's go ahead and take a break from the theory and jump into the real‑world application. In this demonstration, we will issue our very own public TLS certs using ACM. Before we jump in, of course, let's have a high‑level overview of the architecture we're going to look at here. What we'll do is we'll use ACM to create multiple public TLS certs, one in each region where we have an architecture set up. What I've done and I will quickly overview is I've set up the exact same architecture in us‑west‑2 and us‑east‑1. Both of them have an ELB fronting a single EC2 instance. What we'll do is we're going to go ahead, and we're going to create our TLS certs in each region. Remember, certs are regional. And for this regional service, which is an ELB, we need to create a cert for each of those. What we'll do is we'll then work on applying those certs to our HTTPS listener on our ELB, and we'll test out that everything works appropriately after we assign those certs. With that being said, let's jump over into the hands‑on playground now. All righty, I'm in my hands‑on playground. Before we jump in and start creating certs, I want to do a very, very brief overview of what I've already created. In two different regions here, I've created two different tab groups in my Chrome browser to represent the regional resources. This first tab group is us‑east‑1. I've created my application load balancer that's fronting a single instance, and it's only listening currently on port 80. So if I go to this and I open this record here, we get our very simplistic web page, which makes sense. This is probably something you've seen very often in previous demos. I'll close this. And then over in my us‑west‑2 tab group here at the top, I've done the exact same thing. I've created an ALB, and it has its own, of course, DNS record here. And if I go here, it should take us to our us‑west‑2 instance, and there we go. So these are what we're going to use to test our regional certs. Now in addition to this, I've also in Route 53 in my public‑hosted zone that comes with this hands‑on playground, created two weighted records here, app. hosted zone name. These are the records we're going to use to test the regional traffic. Now the only reason I did this is because I have to manipulate the weights to force my traffic to go to the us‑west‑2 region based on my location. It's really only important for testing on my end. So if I go to app., it should take us to us‑east‑1. And no matter what I do, it's not going to send me to us‑west‑2, which is perfectly fine. With that being said, let's go ahead and begin. The first thing I want to do is I'm going to load up us‑east‑1. Now I want to create an HTTPS listener with a custom domain name. So what I'm going to do is I'm going to go and find listeners. We're going to add a new listener. And I'm going to select HTTPS. Now I'm not going to walk through this in detail. This is available in a different course within this learning path, so please feel free to check out that course if you want to walk through setting up an ELB. What I'm going to do is I'm going to forward to my existing target group containing my instance. And we see our security policy, and then I want to use a TLS cert. Now, of course, I'm using a custom domain name. So, I need to create one. So what I'm going to do here, as I already have this opened up, I'm going to go to ACM in us‑east‑1. Remember, these are regional resources. This is even more important because ALB is a regional resource, so we need to create the cert in that region. So what I'm going to do is under List certificates, I'm going to request a new cert. Now for this, we're going to request a public certificate. I'm going to click on Next. Now we can enter the FQDN. So remember, fully qualified, or you can even do a wildcard. We're going to test out both here in this demonstration. For this, however, I have app dot. So what I'm going to do is copy this. I'm going to go back, paste it in, and now we have our fully qualified domain name. Now if you want to, you can add other names to the same certs. You can have multiple domain names, you can use it for CNAMES, all of that fun stuff regarding TLS certs. However, the important thing to remember here is you can set up fully qualified or wildcard for this exam. The next thing we have here is our validation method. Remember, you can do DNS or email. If you choose the recommended DNS validation, it's very easy to do, and it enables you to automatically renew your certs, which is very important and one of the things to look out for on the exam. So, since we're managing this DNS, in Route 53, this is going to be super easy. So I select DNS validation. And then we can select the key algorithm. Now this is out of scope for this exam, but if you need to customize it, it's important to know you can. I'm going to click on Request. And now it's trying to issue our cert, but you're going to notice here, it's pending validation. Now this is saying, hey, I want to give you this cert, but you have to validate that you own this domain. I'm not just going to give it to you, and it's not just going to work. So to do that, since we chose DNS validation, it's very easy for us because I can click Create records in Route 53. Now if you're using DNS in a different system, maybe in Google Domains or Squarespace, you can export these values and these records to CSV and then set them up in your external DNS to do the same thing. However, since we're using Route 53, I click this little button here. I say yes, create my records, and it's that easy. So now if I go back to Route 53, I refresh my zone. We'll see that ACM created this record for us. So it's putting in this unique value here for the record name. It's a CNAME, and then it has a unique value as well to say, okay, if this pulls back when I do a check and a lookup, if I see this value, then you do own this domain, so we're good to go. So now if I go back to my cert, I can refresh this page. And eventually this is going to be issued, and there we go. So it's now validated that we own that domain. We can start using this. Now the next thing I want to show you here is under Details, notice right here, it says In use, No. On the right side, it says Renewal eligibility, Ineligible. You might be asking, well, Andrew, I thought you said if you use ACM‑issued certs that are validated, they can be renewed automatically. That's true. However, they have to be attached to resources. In other words, they have to be in use to be eligible for automatic renewals. So let's go ahead and test that out. I'm going to go back to my listener here in us‑east‑1, refresh, select my ACM cert that I've just created. I'm going to go down here, and I'm going to click on Add. Okay, so now we have our HTTPS listener. You're going to notice we have our cert here listed over in the Attributes section. And now if I go back to ACM in us‑east‑1, if I refresh this page, eventually this is going to show up as In use and Eligible, and there you go, In use, Yes, Renewal eligibility, Eligible. So now, ACM will handle renewing this for us as long as we own and validate this domain and this cert is attached to our resource, which is listed here. Now, in theory, I can go to my site, my app dot. I can enter HTTPS here. And it should take me to our validated and secure domain, and it is. Notice HTTPS. We have a valid certificate here. Perfect. However, if I were to continue refreshing or hit this site from somewhere else closer to us‑west‑2, I would receive an error because it's not using a cert. So let's set that up in the other region. What I'm going to do is I'm going to go to us‑west‑2, and I'm going to do the exact same process here. So I'm going to fly through this. I'm going to go ahead and add a new listener on 443, but this time on us‑west‑2, so 443 on HTTPS. We'll select the same target group. We're going to create a new ACM cert. So under ACM here in us‑west‑2, notice we don't have a cert listed because again, these are regional. So for this though, what I'm going to do is request. And instead of doing a fully qualified domain, I'm going to do a wildcard because maybe I want to add a different domain or subdomain on this particular load balancer at some point. So what I'll do is I'll do *. and then copy in my public‑hosted zone. So now I'm creating a wildcard ACM cert. I'm going to do DNS. We're going to go ahead and click on Request. And now it's waiting or pending validation. So I'm going to do the exact same thing here. Once these values load, I'm going to go ahead and create those records in Route 53. I'm going to click on Create Record. And it's now in our Route 53‑hosted zone. So if I refresh, there we go. We have our other CNAME record that was created for us. This should come up as issued here very soon, so I'll just keep refreshing until it is, and then we will continue. All right, so it took a few more seconds. It's now issued. We can now use this. So what I'm going to do is go back to my EC2 Load Balancer. I'll refresh, and there we go. So now we're using a wildcard cert to use this on our ALB, which will work for, of course, the same website name because that asterisk is a wildcard and matches this particular site, app dot. So when I go back, I'll click on Add. And there we go. So now to force me to test this, what I'm going to do here is I'm going to go in, and I'm going to change the weight on this record in us‑east‑1 to be 0 because I want to disable traffic and test that traffic goes to us‑west‑2 and is secure. So let me edit, set the weight to 0. I'll save it. And then what I have to do here is I'm going to wait a few minutes, so I'm going to pause because I need this to be updated in the back end. And then once I validate that traffic is going to us‑west‑2, well, there it is actually, so I don't even need to pause. I apologize. That worked a lot faster than it's worked in previous testing, and now we see our TLS cert is also working for us‑west‑2. So we have our different cert that also matches our DNS and domain name, and it's secured our connection. And that's it. That's how easy it is to create your own public ACM‑issued certificates to attach to your resources. Big thing to remember here is when they're issued via ACM and you use DNS validation, they become eligible for renewal automatically when they are in use. So in use means you have to attach it to a resource. Now one last thing, this is just a fun thing to show you here. If I go back to certs in us‑west‑2, I create a new cert. Let's say I want to create the exact same cert that I have over here in us‑east‑1. So we're going to do the exact same fully qualified domain name. I'm going to do Request, add this in, DNS validation. I want you to see something. This is a pretty neat little feature that they've created for this in this service. Once these CNAME records load, there we go. Notice the CNAME name and then the value. Now you don't have to memorize this. I'm going to tell you that these are the exact same as the ones in the other region. So notice, this is for Oregon, us‑west‑2, and we just went ahead and created this, and it's already issued. Well, that's because those CNAME records match the existing one that we created earlier for our other region. This is just a neat thing to keep in mind. You don't necessarily need to know it for the exam. I just like to point it out as a really cool feature that was put into place. Now that's going to do it. We've just set up two different regional ACM certs for our regional ALBs. Let's end here, and I'll see you in an upcoming clip.

AWS Key Management Service (AWS KMS) Overview
All righty, we just looked at encryption in transit using ACM for TLS certificates. Let's now move on to encryption at rest using a service called AWS KMS, or Key Management Service. AWS KMS is a managed service that makes it easy for you to create and control encryption keys used to encrypt and decrypt your data in the cloud. Keys that get created in the service are protected by FIPS 140‑3 Security Level 3 validated hardware security modules, or HSMs for short. The big thing to remember here is that it leverages HSMs on the back end that are managed by AWS. When you're using the service, it very easily integrates with several AWS services in the cloud, like Amazon S3 buckets, Amazon EBS volumes, and even your RDS databases to protect your data at rest. In addition to using it with services, one of the most important ones arguably on the exam is that you can track usage of your keys in the service using CloudTrail trails. So if there's a scenario where you need to audit decryption and encryption using your keys, think CloudTrail. In addition to auditing and access control, you control access to the actual keys and the use of the keys via IAM and key policies, and we're going to explore those in their own clip coming up shortly. And then lastly here, you can leverage VPC endpoints for secure access. So if you want to use a managed key and keep that internet traffic within the backbone of AWS, you can do that securely using a VPC endpoint. Big exam pro tip here before we move on. If you need to encrypt data at rest in AWS, you will more than likely want to use KMS in your solution. This is the service to use for this exact type of scenario. And with that being said, we're going to go ahead and wrap up this overview clip, and we're going to start diving into more specific portions and resources of using KMS.

AWS KMS Keys
All right, we now know we can use KMS for cryptographic operations in AWS. Well, to do that, we have to use keys, so let's explore keys now. An AWS KMS key is a regional resource that is a regional key that you get to create and you get to manage for your own use in your own applications. Now they did used to call these customer master keys, or in other words, CMKs. However, AWS is transitioning to calling them KMS keys. So just keep that in mind if you see one or the other. They are the same thing. Now when you're creating a key using this service, you create it using key material. The key material origin is going to identify the source of that material for your keys. Once you choose an origin, it cannot be changed, and you do need to know at a higher level the different sources. There are three of them. First, you can use KMS, the service. When you do this, AWS creates the key for you using material generated within those HSMs that we talked about that are managed by AWS in the back end. This is by far the easiest and quickest way to create a secure key. You also have a custom key store. This is where the key material gets generated and used within an AWS CloudHSM cluster. We're going to look at CloudHSM in its own clip after this, but just remember you can use a custom key store with CloudHSM. And thirdly, you can import the material. So you can import your key material from your very own key management infrastructure, and then you associate it with the key, which is the resource. This is good for very specific requirements for compliance and security. Moving on, let's look at the three high‑level key types that are categorized. First is customer managed. With customer‑managed KMS keys, you create them, you own them, and you manage them. That means you have pretty much full control of them in terms of how many capabilities are offered by the service itself. It works by allowing you to create and maintain key policies and IAM policies to control access. So again, you have full control over these keys. Now we're going to look again at key policies in their own clip coming up in this module. So don't worry about that right now. Just understand they are used to grant access. Thirdly, they allow for the ability to enable and disable your keys if ever needed. So if you need to quickly disable temporarily, you can do that, and then once you need to, you can re‑enable them again for use. In addition to enabling and disabling, you can enable automatic rotation of key material. This is a very important exam scenario that can come up. Using these keys, you can enable automatic rotation of the material that they were generated from. In addition to automatic rotation, you can also manually rotate it if you need to as well. So again, you can enable automatic or enable manual rotation for your keys. Remember that for the exam. And then lastly, you can schedule deletion. Now when you delete a KMS key, there is a waiting period. You can't just delete it immediately. Instead, you have to wait for at least 7 days, or you can schedule it for up to 30 days for your waiting period. This serves as a protection window, if you will, for deleting your keys. Once you delete your key, if you have stuff that's encrypted using that key, well, you have no access to that data because the key is completely gone. So instead, how it works is you schedule deletion, let's say for 7 days, then your key is disabled for 7 days, and you can cancel that deletion if you need to. Now it's important to understand customer‑managed keys are never free. You pay per use, and you pay a monthly fee per key. So if cost optimization is the key focus for the scenario, a customer managed key might not be the way to go. Moving on to the second category, AWS owned. These are keys that an AWS service owns and manages for use in multiple accounts. That's a very important distinguishing feature to understand. These are going to be perfect if you do not need to audit or control the encryption key that is protecting your data and your resources. They live in an AWS‑owned account that you cannot access and you cannot view. However, the services you use in your account are allowed to use these keys to protect your resources. So it's a cross‑account share of the key. The big thing to know here is they are free of charge. That is a humongous benefit of using these type of keys. However, you lose flexibility and customization. You can't handle rotations and deletions, as they are completely managed and dictated by the service that's leveraging the keys. So keep that in mind when you're on your exam. The third type of key here is an AWS‑managed key. This is considered now a legacy key type that's getting replaced by the AWS‑owned keys that we just looked at; however, these still can appear on the exam, so we need to cover them. What they are are KMS keys in your account that get created, managed, and used on your behalf via services in your account. With these, you can view the managed keys in their account, and you can view the key policies, and you can audit key usage. So that's a key difference between AWS owned and AWS managed. You can audit usage, you can view the policy, even though you can't change it, and you can view the key itself. However, you cannot change any properties for the keys. So just like AWS‑owned keys, this is no different. You can't do anything regarding rotation config. You can't update or change key policies, and you definitely cannot schedule deletions. So really do your best to remember the key differences and the benefits and the cons of using each type of different key. Now one last thing to end on here, very important. KMS keys never leave the KMS service. So if that comes up on the exam, remember, keys do not ever leave AWS. You leverage the service to leverage the keys. With that being said, we're going to end here, and we're going to move on to talking about access controls using key policies within the service.

AWS KMS Key Policies
All right, we are now masters of KMS keys. Well, we also need to master the ability of granting and controlling access, and to do that, you primarily use key policies. A key policy is a resource policy specific to controlling access to KMS keys. Remember, there are identity‑based policies and there are resource‑based policies. This is a resource‑based policy. It's important to understand these are the primary method for controlling access to your keys. In addition to that, every key that's created in KMS must have exactly one policy attached to it. You cannot have more, you cannot have less. It has to have exactly one policy where you edit your access control. Now it's important to understand these do work together with IAM policies and what are called key grants to grant access as well. So even though these are the primary method, you can combine their usage with policies and key grants to further customize access. We're going to talk about some of these uses later on. A very important thing to remember, no principal, including the root user, has any permissions to a KMS key unless it is explicitly allowed. This can be a tricky scenario that comes up, so do remember this. You have to explicitly grant permissions for the key using a key policy. And speaking of policies, just like IAM policies, if there is any sort of deny statement in the permission evaluation chain, that is going to result in a denial of use. Remember, any explicit deny will win over any explicit allow in AWS. Now a quick reminder here, key policies have to explicitly allow access to keys. You cannot only use IAM policies to grant access to a key. It will not work. This does come up on the exam, so make sure you understand that. Key policies are the way to explicitly grant access. Now let's move on to looking at how you can control permissions, specifically different ways scenarios can be presented on the exam. First, you can just use a key policy. So you can say, hey, I want the full scope of access to be controlled in a single document, which is my key policy. I don't want to worry about it anywhere else. You can also use a combination, so you can use IAM policies in combination with a key policy. This really enables you to manage all permissions for your IAM identities in IAM. So in other words, maybe you allow everyone in the account to use a key and you want to rely on only IAM policies to further control that. So you want to maybe deny explicitly, etc. You can also use what is called a grant. Now you only need to know this at a high level for this exam, but using a grant is where you use a key policy for the primary permissions, and then grants are used to temporarily allow permissions to be delegated from authorized principals. Now that might be confusing, but just understand, they allow a grantee or someone that's trusted to call a grant operation on the KMS and then they expire. So they're temporary access that is granted for key use. Now these are very commonly used by AWS services to provide grants to decrypt, encrypt very quickly, and then expire. Now let's actually look at a key policy breakdown. This is a very, very simple key policy for a key that I created in a demo account that allows default permissions. So we see here the first statement block. We're enabling IAM user permissions. So what we've done is we've allowed the entire account. You can see here under principal we have the root, which is the entire account, to have all KMS actions for all resources that this policy applies to. The second part of the same policy continued. So we're saying, okay, user cloud_user in this AWS account here can perform all of these actions for any resource that this key policy applies to, which in this case would be a specific key. Again, it almost looks identical to an IAM policy because it is pretty close in how it works. It's a JSON document, you use the same syntax, etc. Now that's going to go and do it for this AWS KMS Key Policies clip. Remember, key policies are the primary method of access for granting permissions to your keys. Let's end here, and we're going to move on to a more complicated solution called a multi‑region key coming up next.

AWS KMS Multi-region Keys
All right, we now know how to control access to our keys using key policies, but there's going to be some scenarios where you have multi‑region architectures and you want to use KMS encryption. Well, that's where multi‑region KMS keys come in, so let's explore these. First things first, remember, KMS keys are regional resources. In other words, if you need encryption in us‑east‑1 and eu‑west‑1, well, then you need KMS keys in both of those regions. However, KMS does allow you to use what is called a multi‑region key. How this works is KMS creates the keys in each region, and each of those keys uses the same key material and the same key ID. In other words, it acts like you have the exact same key in every single region where you want it. So with that in place, it allows you to encrypt data in one region, and then you can decrypt it in a second region without having to reencrypt or requiring any type of cross‑region KMS calls, etc. With that in mind, multi‑region keys are still regional resources. They are not global. You need to remember how they work. It creates basically an identical key in each region that you specify using the same material and the same ID. Now with that in mind, let's look at a quick exam scenario where you might use a multi‑region KMS key. Let's assume you work for a startup and you're building an application in AWS and you're going to be storing data in S3 buckets across two different regions. You have to use a KMS customer‑managed key to encrypt all data that is stored in the buckets. So because of this, you can't use default encryption in your S3 bucket. The data in both buckets must be encrypted and decrypted using the same KMS key, and the data and the key needs to be stored in each of the two regions that are chosen. Let's go and explore a possible solution here. Right away, this is a perfect solution to use KMS to create a customer managed multi‑region KMS key. In addition to that, you can create an S3 bucket in each required region and then configure cross‑region replication between the buckets so that data is in sync. Now when you use each multi‑region key in their respective region, you're getting the same encryption across multiple regions. You can then configure your application to use the KMS key in client‑side encryption to make sure that you're meeting a standardized encryption method and approach. By using this method, the multi‑region KMS key allows for seamless encryption and decryption across your multiple regions, and you don't need different separate keys for each region. Remember, it uses the same key in the respective region. What this architecture does is it simplifies key management, and it's going to ensure that the same key material is available in both of your regions you've selected so client‑side encryption can leverage consistent methods for encryption. Really, a key takeaway from all of this, pun intended, is that a multi‑region key is perfect for consistency and redundancy for encryption across regions. If you need the same encryption across multiple regions, I would immediately consider multi‑region keys. Let's go ahead, and we're going to end this here, and we're going to move on to our next service called CloudHSM.

AWS CloudHSM
All right, on your exam, there's going to be some instances where KMS encryption is not going to be enough from a compliance requirement standpoint. So that's where CloudHSM can come in, so let's dive into this now. Before we jump into the service, we need to review what an HSM is. An HSM, or a hardware security module, is a computing device that processes cryptographic operations and provides secure storage for cryptographic keys. Based on this, AWS CloudHSM is an AWS‑based HSM that enables you to easily generate and use your own encryption keys in AWS. Now there are a lot of key differences that determine when you would use this over KMS. One of the biggest differences is that this is an actual physical device entirely dedicated to you. What that means is you get complete control over high availability modules that you get to use whenever you want. AWS goes in, they set it up for you to use, and then they are entirely hands off until you need to decommission those modules. One of the big benefits of using this service is it provides extreme low latency access and super secure HSM management. This is far more secure than KMS in terms of controls. On the exam, there are going to be a few specific indicators to look out for regarding CloudHSM. First, if there's any mention of needing full control of the underlying cryptographic hardware, that's an immediate indicator. If there's anything saying, hey, you need full control of users, groups, and keys inside the modules, then this is a perfect choice. The third one is if you need KMS key material backed by something else like your own HSM cluster, this could be a perfect option. And then lastly, if you ever need cloud‑based services for cryptographic needs and KMS is not enough, you might consider CloudHSM. Now exam pro tip to wrap this clip up. This is going to be perfect for when you have the strictest of requirements for cryptographic keys while also wanting to leverage a cloud service. That is a perfect scenario for using CloudHSM. Otherwise, for the most part, KMS should do the job just fine. Let's go ahead. We're going to end here, and we're going to move on to a demonstration coming up next.

Demo: Encrypting Data with AWS KMS Keys
All right. Welcome back. Let's jump into a demonstration. I think that's enough theory for now. In this demo, what we're going to do is we're going to use AWS KMS, and we're going to create a multi‑region KMS customer‑managed key with a customized key policy. After we do that, we're going to make sure that the replication occurred, we're going to test out the key policy, and then we're going to explore some AWS‑managed keys that exist in the account for our services to use. So without further ado, let's jump over into the hands‑on playground now. All right, welcome back. I've jumped into my hands‑on playground. I'm in us‑east‑1. Before we get started, I want to show you one thing I've done previous to recording this. In IAM, I created this another_admin user, and I've copied the exact permissions that cloud_user have. I'm saying this because we're going to log in as another admin and test our key policy. With that said, let me go back to KMS here, and let's begin creating our first key. I'm going to find and click Create a key, and that brings us to Customer managed keys where you can create from there. Now the first thing we do is we select the key type. You do need to know, you can have symmetric or asymmetric keys. Symmetric is when a single key is generated and used. Asymmetric is when you have a public and private key pair. So asymmetric, you can think of something like an SSH key pair. You have a private key that works with the public key. For this, we're going to choose symmetric. After that, we have key usage. On the exam, you're really only going to need to know that it can be used to encrypt and decrypt. This other option here is for the specialty security exams. I wouldn't worry about it for this one. So we're going to choose Encrypt and decrypt. And under Advanced options here, we can start getting into some more stuff. First up is Key material origin. Remember, we covered this in a previous clip. The recommended and easiest way is to use KMS to create and manage the material for you. You can also import your own key material, use CloudHSM as a key store, or you can use an external key store. On the exam, these three here that I've just chosen are going to be the big ones to remember. So do your best to remember these three options for the exam. Next, we have the regionality. Remember, KMS keys are regional resources. So we can create a different key in each region, or we can have KMS create a multi‑region key for us to go ahead and create a duplicate or a replica of each key so we can have the exact same encryption. So for this, I'll choose Multi‑Region. I'll click Next, and now we can give our KMS key an alias. So what I'll do is I'll call this demo‑key. The alias will be an easy way to reference our key, and I'll show you why that is coming up after we create it. We can then give it a description. And then you should tag it if it's in production. I'm not going to because I'm going to delete this, and then we get to defining our key policy. Now, first thing we do here is we can define key admin permissions. So for this, I'm going to say, hey, cloud_user, which is me, is a key admin. Underneath that, we can say, do you want them to delete this key? We'll say yes, that's fine, we trust them. We'll go to Next, and now we define key usage permissions. So you'll notice this is optional. This is defining what users or roles can use the key. Now we're not going to select anyone. And in addition to that, at the bottom, you can also share this key with another account. So if you need to do cross‑account KMS encryption, you can do that by sharing a key using the key policy. We won't select anything for now. I'll click on Next. We see our policy. I'll click on Next again, and then we can go ahead and finish. Now when I select our demo‑key here, first things first, you'll notice the key ID right here at the top. This is why we create an alias. If you're doing anything in the console, it's a lot easier to type and read demo‑key than it is mrk‑ee, a bunch of different characters. Now, of course, if you're using Infrastructure as Code, which you should be, then this is not necessarily a big deal, but for the sake of readability, we want to use the alias. Now under this, we see the Key policy, Cryptographic configuration, Key rotation, is it enabled, disabled? So let's go and turn this on. Under Automatic key rotation, I'm going to go to Edit, and I'm going to Enable. Now, notice you can set it for 90 days all the way up to 2,560 days. For you, just some quick math, 2,560 days is roughly 7 years. So that's something to keep in mind for the flexibility on the numbers you can set. What we'll do is we'll say, okay, I want to rotate this once a year. I'll click on Save, and now KMS will automatically rotate our key for us. If we wanted to, we could also rotate it now. Notice number of on‑demand rotations remaining, 10. So let's say, oh, I think the material might be compromised. I want to rotate this. So I'm going to click on Rotate, and now we have different key material on the back end once this completes. Now I'm not going to wait for it, not a big deal. Next thing I want to look at is regionality. So notice here, we've created a multi‑region key, however, we need to create replica keys. So, since this is the primary key, we can create replicas. So what I can do here is create a new replica. We'll go ahead and choose us‑west‑2. I'll click on Next. It gives us the exact same information. I'm going to go ahead and skip to review because everything should be the same. And then I'm going to go down here and click on Create new replica keys after I confirm. And there we go. So now we have a replica getting created in us‑west‑2, and notice it's the exact same key ID. The only big difference, of course, in the ARN is the region, as you can see here. The alias, key policy ID are all the same. Next up is our aliases. So you can have more than one alias. A little out of scope for this exam, but more than anything, just understand you can use a key alias for easy reference. Now, with that being said, we went through a lot of these settings. I want to test this key policy. Now first things first, I will show you this. Let me switch. One thing in here is, even though we didn't grant another admin permissions here, this top block where it says Enable IAM User Permissions is basically saying, okay, we're offloading everything to IAM. So since another admin here has the exact same permissions as cloud_user, it's going to be able to use this key, but I don't want that. So I'm going to edit this key policy. I'm going to remove this block here. And what this is doing is only using the key policy to grant access. So I'll go down, I'll save my changes, and there we go. So now let's test this out really quickly. What I'm going to do here is I'm going to log out, and I'm going to sign in with that another_admin credential set. So let me go ahead and do that really quickly. What I've done is I've copied this information over on the side, so let me go ahead and paste this in. We've logged in as another_admin. I'm going to go to KMS in us‑east‑1. And you're going to see right away our demo‑key Not authorized. That's because we removed those permissions. So even though I'm an admin user, we're using the KMS key policy to define who can and cannot use this. So our key policy is working appropriately. Now with that being said, what I'll do here is I'll log right back into cloud_user. So let me do that really fast. I'm going to jump forward. And perfect. So now I'm logged back in as cloud_user. We go back into KMS here, and we're going to see our keys, and perfect. We can see all the information, it's working. Now, the next thing I want to show you is an AWS managed keys. So this account has already had some created for us. Remember, these are created and managed by AWS for services for them to use. So for instance, someone's already created an S3 version. So this is a KMS key for KMS encryption at rest in an S3 bucket. Notice it's allowing all principals within this account and S3 to use this. So if I go to S3 here, let me just select one of these buckets, and let's just check out the encryption settings. So under Properties, I'm going to go down until I get to Default encryption here. I'm going to select it, and I'm going to say SSE‑KMS. We can choose from our existing keys, and you're going to see we have S3. I'll select that. We'll save changes. And now we just leveraged an AWS‑managed key for encryption. Now remember though, you have no way to edit or configure this key. You can only use it, so you can't change rotation, you can't change the key policy, etc. If you need any customization at all, you're going to need to use a customer‑managed key. With that being said, one more thing I want to do here. What I want to do is let's jump over to us‑west‑2. If you remember, we created a replica key over here. Well, there it is. This shortcut worked because it's the exact same key ID, exact same key alias, and it should be the exact same backing key material. So the cryptographic info should be the same, the key policy should be the same, but the key thing to keep in mind here is this has the old policy. So that's something to remember. Remember, we edited and removed this portion of the policy because we wanted to only use the key policy to define permissions. So just keep that in mind. Probably won't come up on the exam. It's more so the idea that you can use the exact same encryption across regions. Now, that's going to do it. We just created a multi‑region key in KMS. We also viewed AWS‑managed keys for managed services like S3. Let's end here, and we're going to jump into another clip coming up next.

AWS Secrets Manager
Okay, moving on to the next service here, Secrets Manager. There's going to be scenarios where you have to manage secrets in the cloud. Well, this is a perfect service to do so, so let's dive in. What Secrets Manager is is a service in AWS that will securely store, encrypt, and even rotate database credentials and other secrets. So there's a lot of things you can store in this service. It offers encryption and transit via TLS, of course, just like other AWS services, and it also offers and basically requires encryption at rest using KMS keys. So that is an immediate difference between Parameter Store, which we cover in a different course, is that these are always encrypted all the time. Now, an important key indicator to choose this service on the exam. You can use it to enable automatic credential rotation for supported services. Those supported services include RDS and Aurora. Those are the two most popular ones on the exam. In addition to automatic credential rotation, it allows you to apply fine‑grained access control using IAM policies, which really shouldn't come as a surprise because IAM is used to control access to everything pretty much in the cloud. And then lastly here, another very important thing to keep in mind, this is never free. This is going to be something to keep an eye out for if you're deciding between Parameter Store and this. If cost is the only concern and the only differentiator, you might consider Parameter Store instead. If you need anything else like automatic rotation, secure management for a secret, I would think this service. Now let's cover some concepts to keep in mind going into your exam. This service enables your applications to make an API call to the Secrets Manager service and retrieve secrets programmatically. Of course, you have to have IAM permissions to do so, but it allows you to do it very easily in a programmatic manner. It is very popular on the exam for completely managing the lifecycle of your Amazon RDS and Amazon Aurora admin credentials for your databases. Again, this is a very popular scenario, so keep an eye out for it. The service also allows you to store pretty much any other type of secret you can think of, provided you can store it as a key value pair. So maybe SSH keys, API keys, things of that nature. If you can store it in a key value pair system, you can put it in Secrets Manager and retrieve it. And lastly here, we talked about automatic rotation and management of credentials. Well, it leverages lambda functions to generate new secrets when performing rotations. So not only can it rotate it for you, it will generate the new secret and then update whatever resources are integrated with it. It's very, very handy and a very nice feature within this service. Moving on to something known as a multi‑region secret. On the exam, multi‑region designs are huge, so you need to understand when you can use specific features of services to design an optimized architecture. Using Secrets Manager, you can create multi‑region secrets, which allow you to replicate your secrets across multiple regions. When Secrets Manager does this, it replicates the entire encrypted secret data and the metadata for your secret as well. So if you have any tags, things like that, they also get replicated. When you use this, you can actually promote one of the replica secrets to a standalone secret and start managing it completely separate than the others. This is perfect for multi‑region database implementations and potential failovers and testing. Again, remember you can create multi‑region secrets using Secrets Manager. Now let's go over a quick exam scenario where you might use Secrets Manager. Let's assume you have a team, and it's running an application on Amazon EC2 instances. The application uses Aurora for its database, and the EC2 instances currently connect to the database by using usernames and passwords that are currently stored locally in a file on the compute. Well, you've decided, hey, that's not secure, that's not operationally efficient. I want to minimize operational overhead of my credential management and lock it down. Well, let's go and explore a great solution. In this solution, AWS Secrets Manager is designed specifically for this type of workload. You can use it for secure credential management, and it integrates seamlessly with Amazon Aurora. This integration allows you to use the service to enable automatic rotation and management of those credentials. So now the application running on your EC2 instances can leverage the IAM role and instance profile, and it can then retrieve the managed credentials at runtime via the Secrets Manager API or SDK call. Again, this is a perfect scenario for Secrets Manager. With that in mind, one exam pro tip before we wrap up. Any scenarios on your exam looking to reduce credential management overhead should immediately consider Secrets Manager. Again, that is a key indicator to look out for. Let's go ahead. We're going to wrap up here, and we're going to move on to a demonstration of using Secrets Manager next.

Demo: Managing Secrets Using AWS Secrets Manager
All right, let's go ahead. We're going to jump into another demonstration. This time we're going to look at using AWS Secrets Manager to accomplish a scenario that is commonly appearing on the exam. During this demonstration, really quickly, let's review what we plan on doing. We are going to log in using admin credentials in our hands‑on playground. And then we're going to create an Amazon Aurora or RDS database. And when we do that, we're going to select the feature or capability of managing credentials using Secrets Manager, and we'll see exactly how that works. So let's go ahead and jump into the playground now. All righty, I've loaded up Amazon RDS in us‑east‑1 in my hands‑on playground. Let's go ahead and get started. First thing I want to do here is I'm going to click on Create database. Now in here, I'm going to click on Easy create. I'm going to select PostgreSQL. And I'm not going to cover a lot of these settings because that's covered in a completely different course, so feel free to check that course out if you want. For this, I'm going to skip through, select these settings, and the only thing I really want to talk about here is the username and credentials. So we're going to use the default username, postgres. But for Credentials management, I'm going to select Managed in Secrets Manager. This is going to leverage Secrets Manager for all credentials management. That means it's going to create the password for us. It's then going to handle rotation for us on a set schedule, and we can easily automate grabbing those credentials using an API call, an SDK, an application, etc. So what we'll do is I'm going to select this, and then you can see we can select the encryption key. So the default and the one we're going to use is the AWS‑managed Secrets Manager key. So this key is only for this particular service, but if we wanted to, we could create a customer‑managed key if we needed to customize that encryption. Now we covered creating a key in a different demonstration, so feel free to go back and view that if you need to. But for this, we're going to use this AWS managed key that we have no control or customization over. The only thing we can do is audit the usage. So I'm going to close this. We're going to use that key. I'm going to go down here, and one thing I'm going to have to change just to call out is I'm going to need to make this publicly accessible after we create this. Now with that being said, I'm going to create my database. And what I'm going to do is select my database here, database‑2. And I'm going to go ahead, and I'm going to wait for this to become available, and then we will resume. So I'm going to skip forward. And once this is ready, I'll resume, and we'll start looking at the credentials and how to connect. All right, so I went ahead, I jumped forward. It has successfully created my new RDS database instance. Let's go ahead and review the important stuff for this demonstration. Now it's just configuring some enhanced monitoring, which is perfectly fine. What I want to do is I want to come down to Configuration here. And right here in the center middle of the screen, we have our Master credentials ARN. Notice, it's giving us the ARN of a Secrets Manager resource. So if I select this shortcut here, it loads up Secrets Manager. And it brings us to the secret, which is created and managed by RDS. So you'll notice you can't modify the values; however, we will show you how you can rotate them. Now in here at the top, we have the details. So we see the key that was used to encrypt it. We see the secret name for reference in the ARN, as well as a description. Another key thing to call out here is, remember, these are regional resources. So if we wanted to, we could replicate this, and I'll do that here in a moment, but for now this secret only exists in us‑east‑1. Now at the bottom of this page here, we get some of the good stuff. We see the secret value here at the top. So by clicking this button, it's going to decrypt the value using that KMS key, and it gives us a key value format to get the username and password. Now if you use the API call or the CLI, you will likely get a plain text, which looks like this, and you'll need to parse these values, which is pretty simple. It's basically just parsing JSON. But in the console, of course, they make it simple for us so we can see it in a nice, easy‑to‑read format. So we'll use this here in a moment. We'll skip down here, and then we see Resource permissions. So you can add a resource policy for cross‑account access. So if you need to grant permissions across accounts to different roles and users, you can do that in this service. And finally at the bottom, Sample code. This is a really cool feature. It gives you code that you can use for your chosen language to go ahead and automate getting the secret from this particular resource. You can see get_secret_value_response, and it's getting the secret name. So this is how we could use code and automation to pull that value without having to hard code it in a local file. Now with that being said, the next thing I want to show you here is rotation. You'll notice by default, since we used RDS to create this and manage this, rotation is enabled, and it's on an automatic schedule of every seven days. So this is rotating the credentials or the password for our user every seven days. Now that might be a bit excessive, so maybe you want to edit the rotation and say, you know what? I'd rather it rotate every 90 days, so let's go ahead and do that instead of every 7. Now the big thing here really is it really shouldn't matter because you're automating retrieving your secrets, as opposed to having to grab them manually or encoding them in a file, etc. The beautiful thing is you just automate the grabbing of this secret via an API call, and it really doesn't matter what you set as a credential because you never really reference it directly. Now the next thing we have here is the versions list. So this is going to show every single version with a version ID that was ever created for your secret. So we'll look at this here in a moment when we rotate the password just to view how this works. Next is replication. Remember, you can have multi‑region secrets with AWS Secrets Manager. So what I can do is click Replicate. And let's say I want to replicate to us‑west‑2, so I'm going to select us‑west‑2. We'll use the AWS‑managed key, you could create another one if you want, and I replicate it, and it's that simple. So now in a moment, this is going to be replicated to another region with the similar ID as we have here, and then it uses the same encryption key, but in a different region. So this is a different encryption key on the back end, but that's really irrelevant. The big thing to realize is we've just replicated this secret over to our other regions. So if I look here and I look at the value, let's go to Secret value, we see SFyW. If I go back here, SFyW. So the secret is identical, but in a different region. Now I'll close that, and let's test this out. I'm going to copy this password here. I'm going to jump into pgAdmin. I'm going to edit my connection here, so let me go through, go to Connection. We see postgres Username. I need to edit the endpoint, so I'll jump back to RDS. I'm actually going to go ahead and copy this first, so let me go to Connectivity. And before I do this, I'll paste this in and save it. I need to go back, and I need to edit public accessibility because this is not publicly accessible right now. So I'm going to go up. I'm going to go to Modify. I'm going to find where I can go ahead and enable public accessibility. So under Additional configuration, Publicly accessible. I'm going to go down here, and I'm going to click on Continue, and we've successfully modified. So now this is going to be publicly accessible here after this is done modifying. So what I'm going to do is I'm going to pause once again. I'm going to cut forward once this is ready to go, and then we will resume. All right, so the status is available. I've jumped forward. We now see it's publicly accessible. So I'm going to make sure that this endpoint is set up. I'm going to jump back into my program here. I'm going to go to Properties, Connection. This looks good, so let me just make sure. I'm going to close out, jump back, go to Secrets Manager, and then copy my password. So now I can jump back into pgAdmin. I'm going to connect to this. I'm going to paste in my password, and there we go. We've now connected to our database using the Secrets Manager's credentials. Now obviously in theory, you would automate this or do it via some type of API call in your application, but I just wanted to show you how easy it is to use. So now we can see our databases, create stuff, etc. Now with that being said, what I can do here is I'm going to disconnect. I'll say Yes. I'm going to reconnect, and let me save my password and paste it back in. And now when I try this again in the future, it should not work once we rotate. So let me jump back to RDS. We're going to make sure that this is still good to go. We are. And now I'm going to rotate their credentials. So under Secrets Manager, Rotation, I'm going to go ahead and click on Rotate secret immediately. I'll rotate it, and there we go. It's now going to start trying to rotate. Now one thing I did want to call out while this is doing this is under Edit rotation, if you're doing this custom, so you're making your own secret and you want to use automatic rotation, as you can see at the bottom, it would use a rotation function, which is a lambda function to perform that rotation. Since this is managed using RDS, it handles that for you and you don't need to do this. However, keep that in mind on the exam. If it's your own custom secret and you want to rotate, you need a lambda function. So I'm going to cancel out of here. I'm going to go to Versions. And you can see here on the bottom we have a new pending version. So the top one is the current and the original, and it's trying to rotate the credentials to this new updated version. So once this is all said and done, this bottom portion is going to be the current version, and it's going to update our RDS database to use that for the master credentials. So eventually over here when I retrieve this, this password will change. So what I'll do, once again I'll pause, jump forward, and once this is changed, I'll go ahead and show you. Okay, so that took maybe another 30 seconds. I skipped forward, but you see now we have an AWSPREVIOUS label for our first one, and now we have a current and pending version. So the pending is still there because it's still trying to change it on the RDS instance. The current is the one saying, hey, this will be the current version. So now if I go back, I retrieve, we have a different password value, which is perfect. So because we have this other version here, it changed the credentials for us, and it's trying to update our RDS database for us as well. So it's going through, trying to modify this on the back end. And in theory, again, all we would have to do is make an API call here, parse this value, and then use it to log in, so we don't have to store it anywhere. So what I'll do is I copied it. I'm going to jump back to pgAdmin, and let's just see if this is working yet. I'm going to connect. It should fail because it tried to save the old password. I'll paste this one, and eventually this is going to let us connect, and there we go. We now just used the updated password that was created for us. Over here in Secrets Manager, it was rotated for us using managed rotation, and it's just that easy. So let's go ahead. We're going to wrap this demonstration up. I hope you learned a lot on how to use this service with a managed service within AWS, and I'll see you in an upcoming clip.

Module Summary and Exam Tips
All right. Way to hang in there. Once again, let's go over a quick module summary, and then I'm going to review some specific exam tips for the services we covered before you go into your exam. First up, ACM. Remember, this is the go‑to service when you need to create, manage, and deploy public and private TLS certs with AWS services. Using this service is going to be perfect for scenarios whenever you need TLS certs deployed to your AWS resources and services in your applications. With that being said though, remember it only allows integration with specific supported services. The three common ones are Elastic Load Balancers, CloudFront distributions, and Amazon API Gateway. Really do your best to remember it does not allow you to directly associate certs with EC2 instances. That is not supported with ACM. And then lastly here, this is free to use. That is one of the biggest benefits of ACM. You get free certs to attach to your services. It also offers managed automatic rotation for certs issued from the service if you're leveraging DNS validation. Remember, they also offer email validation, but then you're losing out on that automatic managed rotation for your cert, so keep that in mind. Next up, key management service. AWS KMS is a managed service that you can use to create and control your encryption keys that you can use to encrypt your data at rest. It's important to remember your keys never leave KMS. They never leave the cloud. You only reference them using metadata, which are your keys. We also covered multi‑region keys. This is perfect for multi‑region consistent encryption. It creates the same key across regions using the exact same key material and the exact same key ID. In addition to multi‑region keys, remember you primarily control access via key policies. And in addition to that, you have to have exactly one key policy for every key. Next up, do your best to remember the key types that we covered and the attributes on when you might use them. We have customer managed where you control everything like rotation, deletion, disabling, etc. There's AWS owned where you use them and you have no insights into the usage or the policies, and there's AWS managed, which is the legacy version, and they're being replaced by AWS owned. To be honest, the big differences here are if you need more control or if you need something super simple with less overhead. That would be how you kind of decide between using customer managed or one of the AWS owned key types. If there is any indicator about needing to rotate, delete, edit the key policy, you want to go with customer managed. Now, exam pro tip here. If you need encryption at rest with AWS services, you likely will want to use KMS. Again, this is good for a majority of use cases in the cloud, and also remember it's only for data at rest. It's not in transit. However, if there's instances where KMS is not enough, we have CloudHSM. Remember, with this, you get a dedicated HSM in the AWS cloud where you get full control of the underlying hardware. AWS sets it up for you, and then they hand it off, and you control it all the way until you want to decommission it. With this, however, yes, you get more control, but that means more management. One of those things is you don't get automatic key rotations. You're responsible for handling that. And to be quite honest, if it's going to be on the exam, more than likely you're going to be using it to generate key material and store key material for a KMS key. Again, it's perfect for anything that KMS can't solve if you need a cloud‑based service. We then talked about Secrets Manager. Remember, this is a service meant to be used to securely store your application secrets, including database credentials, API keys, SSH keys, etc. If you can store it as a key value pair in a document, you can store it in Secrets Manager. In your scenarios, your applications are going to use the Secrets Manager API call to get secrets, and it also allows you to leverage the service for automatic rotations of managed secrets. On the exam from an indicator standpoint, anything related to needing to offload credential or secrets management, or if there's something wanting automatic password rotations for managed services like RDS or Aurora, this is a perfect use case for Secrets Manager. Also, in addition to this, if you're having to choose between Secrets Manager or Parameter Store to store your secret data, cost and automatic rotations are likely going to be the primary factors. If cost is the issue, you want to probably go with Parameter Store. If you need automatic rotations, well, that's not supported in Parameter Store, so you would immediately want to think Secrets Manager. Now that's going to do it for this module. Thanks for hanging in there. Let's take a break, grab some water, take a nap, and I'll see you in the next module whenever you're ready.

Network Security
AWS Shield and Shield Advanced
All right. Welcome to the next module in this course. During this module, we're going to look at several different ways to implement the best network security practices for your architectures in AWS. To start things off, let's look at Shield, which has a Shield Standard and Shield Advanced offering. Before we dive into the specifics of this service, let's go ahead and visualize what a distributed denial of service kind of looks like from an infrastructure standpoint. On the right side here, we have our lovely, well designed, beautifully architected AWS resources that make up our application in the AWS cloud. Well, these are publicly accessible, so unfortunately, that means some bad actors are going to try and take advantage of that. And let's just assume they make a DDoS attack, which is going to flood the network with illegitimate traffic and disrupt service for our actual good users. What this would result in, of course, is disaster. It's going to prevent users from accessing our online services and our sites and our platforms, which is no good. Well, that's where AWS Shield comes in handy. By using Shield, we can implement a way to essentially block and absorb these DDoS attacks without affecting the back‑end infrastructure. So now all of the bad actors are now, in fact, sad actors. First up, let's look at AWS Shield Standard. What this is is a free DDoS detection and mitigation service that's meant to protect all AWS customers at the perimeter of their application networks. Perimeter is a key word here. It's not internal. It's wherever you set the perimeter of your network. Please remember that. So it could be an internet gateway, could be something else internal, etc. Now how it works is it's going to help work against the most common network and transport layer DDoS attacks. So some of the most common ones that are used today are caught or supposed to be caught by this service. Some attacks include things like SYN or UDP floods, reflection attacks, and other common Layer 3 and Layer 4 attacks. Keep that in mind, Layer 3 and Layer 4. Now this is commonly used with some of these common resources on the exam, so be familiar with these. It works with Route 53‑hosted zones for DNS. It automatically protects CloudFront distributions, and it works with Global Accelerator standard accelerators by default. All of these are leveraging Shield Standard by default for DDoS protection. Moving on, there's also an offering called Shield Advanced. So let's look at how that differs from Standard. Shield Advanced provides enhanced protections with always‑on monitoring for applications that are running on even more AWS resources and services, and it includes Layer 7 traffic as well. Some of the supported resources you need to know for the exam are all of the Shield Standard resources that we covered. But it also covers Elastic Load Balancers, Amazon EC2 instances, and any resource that you can essentially assign an Elastic IP to. All of these are covered within Shield Advanced. In addition to the expanded supported resources, it does also give you 24/7 access to a proactive Shield response team that helps manage and mitigate application‑layer DDoS attacks. It also offers cost protection for your bill. So if for some reason a DDoS attack is successful and your bill gets a very, very high fee, well, they're going to protect you against that cost with a type of insurance, if you will. However, all of this good stuff is not free. In fact, it's actually quite expensive. You have to pay $3,000 per month per organization to enable this. So that is a humongous cost and something you have to keep in mind on the exam. To wrap things up here, let's have a quick exam scenario on where you might use Shield Advanced. Let's assume you're about to deploy a public‑facing web app within AWS. The architecture is going to consist of EC2 instances in a VPC that are fronted by an Application Load Balancer. With this, you're using an external service for DNS, so not Route 53. You also need a recommended solution, or you need to recommend a solution to detect and protect against large‑scale DDoS attacks. Let's explore a possible way to solve this for an exam solution. Well, let's assume with the high‑level architecture here we have on the right side our simplified version of our architecture with an ALB and our EC2 and our third‑party DNS. Well, first things first, Shield Advanced is perfect for this. It's designed to provide enhanced protection against large‑scale DDoS attacks, remember. In addition to that, remember you can only directly integrate ELBs with Shield Advanced. Standard does not cover these resources, so that is a key indicator immediately. And then finally, you also cannot protect the DNS system because it's managed by a third‑party infrastructure. If it was within Route 53, it would fall under Shield Standard, and then you'd be good to go. So just keep that in mind as well. Now that's going to do it for this clip. Let's wrap up here, and we're going to move on to another important network security service called Web Application Firewall.

AWS Web Application Firewall (WAF)
All right, we just got done reviewing AWS Shield and Shield Advanced. Let's now look at protecting our Layer 7 traffic, specifically using Web Application Firewall. AWS WAF, Web Application Firewall, is a very important service you have to understand how to use for your architectures and on this exam. What it is is a service that allows you to monitor the HTTP and HTTPS requests that are forwarded to your supported resources. What that means is it is meant for Layer 7 specifically. So that's application layer traffic, thus the reason it supports HTTP. For the exam, you have to know the following resources that are supported to be protected by this service. You can place it in front of Application Load Balancers, Amazon API Gateway. You can place it in front of your Amazon CloudFront distributions, and you can use it with AppSync and your Cognito user pools. Pretty much anything that could leverage Layer 7 traffic can be protected with this service. Again, that is very important for you to know for the exam. Moving on to some important concepts that we need to be familiar with. When you're working with WAF, you define what is called a web ACL, or a web access control list, that defines all of the request criteria that you're actually looking for. When you do that, you can create rules, and these rules are going to be matching or looking for matches based on conditions that you get to set. Now we're going to talk about some of these conditions coming up shortly, and you do need to know those as well. When you're creating a web ACL, understand these are regional resources, so you'll create them in each region that they are required. And in addition, to make things easier, they have a feature called rule groups. So if you have multiple rules that you're creating over and over again and you want to apply them to multiple resources, well then you can add them to a rule group for a lot easier reuse. Now when you're working with these particular web ACLs, you have rule actions. So this is going to be the action that is taken by the service when there is a match. The first is an allow action. This is probably pretty self‑explanatory, but what this is going to do is allow any requests that are matched to be forwarded. You then have a block action. Again, opposite of allow pretty much, so pretty self‑explanatory, but it's going to block all requests that match. So if it sees something that matches, it blocks that request. Thirdly, we have count. This is going to count the request, so it doesn't block, it doesn't allow, it doesn't do anything except for count, and then it works with other rules if you have them in place or it by default will follow the default action that you set. And then lastly, you can implement captchas and silent challenges to prevent bots from accessing your resources or performing some type of attack. This is a good one to know. It's starting to pop up on the exam more and more, but really be sure you know all four of these actions and when you might use them. Now, of course, these rule actions only actually occur when you have a rule statement that actually matches the traffic that you're looking for. So let's look at six rule statements you need to be familiar with for this exam for implementing in front of your resources. The first is an IP set match statement. This is going to inspect and look for the IP address of the web request, and it's going to say, hey, does this match our set that we have in our list or within our range, and if so, what action should I take? Remember, you can block, you can allow, you can count, or you can enforce captchas. The next is a geographic rule statement. This is going to look for geo matches to manage your web requests based on countries and regions of origins. So in a way it's kind of similar to CloudFront geo restrictions, except for you can't allow, block, etc. using this within WAF. Thirdly is regex. So this is where you specify a regex match statement, and it's going to tell WAF to match the request component against a single regex that you put into place. The fourth one is SQL injection. This is a very popular exam scenario. This looks for exactly what it says, a SQL injection attack within the web requests. So any malicious SQL code in your web requests will trigger and match this rule statement and perform your action. Fifthly is a string match, so this is where you need more customization. It's going to look for custom strings in your request and then perform an action. And then lastly, another extremely popular example on the exam, cross‑site scripting, or XSS. This is where it's going to inspect any requests for malicious scripts within the component or the request itself. What happens here is attackers are going to use vulnerabilities in a website, and they use them as a vehicle to inject malicious scripts into legitimate web browsers. To be honest, for this exam, I would be familiar with these four rule statements the most. If you're going to really focus on some, I would choose these four. They are the most common. Exam pro tip. You can also use a rate‑based rule statement to allow you to limit requests when they're coming in at too fast of a rate. So for instance, if we go back to our DDoS attack, if you're starting to get a ton of requests from the same source, you can implement rate‑based rules to say, hey, you need to slow down. We only want to allow 10 requests per second, something of that nature. Moving on, let's look at a quick exam scenario before we wrap this clip up. Let's go ahead and assume your company that you're working for has users all around the world that are accessing an HTTP application, which is running on EC2 instances in different regions. What you want to do is you want to improve availability and performance of the app, and you also want to protect it against common web exploits. As soon as you see web exploits, that should be a key indicator. There's also going to be a requirement for static IP addresses. So let's go and explore a possible solution. Well, in this exam scenario solution, there are several components that are important for you to know. We will assume, of course, we have our global users over here on the left. And the first component for this architecture that we can put into place is Global Accelerator. Remember that Global Accelerator improves global app performance by routing traffic to the optimal endpoint based on health, geography location, and latency. The bigger thing here for this scenario is that it provides static IP addresses for reference. Well, with Global Accelerator, you have endpoints, and the endpoints can be your ALBs, etc. But in front of that, you can place AWS WAF. WAF is going to protect your ALBs and your applications against common web exploits. So again, SQL injection, cross‑site scripting, etc. These are all protected via WAF. And then lastly, on the back end, of course, we have our ALBs, which are used for our HTTP‑based application because they provide advanced routing, SSL termination, and easy integration with our WAF service. So this is a perfect and common scenario that can come up on the exam. Really understand how you can use WAF and where it belongs in an architecture. With that being said, let's end here, and we're actually going to jump into a demonstration on setting up AWS WAF.

Demo: Protecting Resources with AWS WAF
All right. Let's go ahead, and we're going to take a quick break from the theory. Let's jump into our hands‑on playground and have a quick demonstration on using AWS WAF. In this demonstration, we're going to perform a very simple test. We're going to have some pre‑existing infrastructure already set up, and we're actually going to leverage our Web Application Firewall service to block inbound traffic to an ALB for any IPs that fall into an IP set that we'll create, which will match my external proxied IP. With that being said, let's jump over into the console now and get started. All right, I've loaded up my hands‑on playground, and I've loaded up the AWS WAF service, as you can see. Before we dive in, I want to show you the pre‑existing infrastructure I've already created for this. Under the EC2 console here in EC2 service, I've already created an Application Load Balancer, as you can see here. And what this is doing is it's split between three AZs in us‑east‑1. And it forwards traffic to a back‑end target group, which has three EC2 instances that belong to an Auto Scaling group. These instances, when I close this, host a very simple web server, which I'm sure you've seen if you've followed other demonstrations that I've done, and all they do is show some information about each instance. So when I refresh, we get our round robin, we're all good to go. Now with that being said, let's go back to WAF. And the first thing I want to do here is I need to create my IP set. So under WAF on the left‑hand side, I'm going to select IP sets. And the reason I'm doing this is because right now at the point of recording, you can't create this during the creation of your ACL rules. So just keep that in mind. That's not something you need to know for the exam, but it's just more of a gotcha so you don't do it later on. So under here, I'm going to create my IP set, and I want you to notice this is regional. Again, these resources are regional resources. They're not global. So even though we're in the global dashboard here, these are regional. Remember that for the exam. With that being said, I'm going to create a new one. We're going to call it malicious_andru. We can give it a description. And then again, we choose the region. So we can make this global for CloudFront specifically. But if we're attaching it to regional resources like a load balancer, well then we need to select the region that the load balancer is in. So for me, it's us‑east‑1, and then you can select what type of IP version you want. For me, I'm going to choose IPv4. So let me go ahead, go off screen here, and I'm going to copy and paste in my IP address for my proxy. Okay, so I'm going to go and paste this in, and this is my proxy right now that I have. And notice it is an IP range. So, because this is a specific IP address, it's a /32, which is the exact address. It's the most specific prefix you can get. You do need to know how to use prefixes and CIDR ranges for this exam, so get comfortable knowing how ranges work. With that being said, I'm going to enter this, create my IP set, and there we go. We have my malicious_andru IP with its ID. Now let's continue on. I'm going to go over to Web ACLs, and let's create a new one. Again, this is going to get annoying probably, but this is regional. Please remember that. I'm going to create my web ACL, and the first thing we can do here is give it a resource type. So let me go ahead and zoom in really quickly just to make it a little bit easier to read. And you're going to notice we can choose Global, which is only going to be CloudFront distributions, or we can choose Regional, which is going to be, of course, everything else. So we're going to choose Regional because we want to use an Application Load Balancer. And then we choose the region, which is the default here. We give this a name. We'll just call it blocking_ips. You can give it a description, which I'll give it the same. And then we can give it a CloudWatch metric name for CloudWatch metric collection. So I'm going to leave all of these the same, and then we can now associate our resources. So down here in the bottom, I'm going to add a resource. I'm going to find Application Load Balancer and then select it, and I'll add it. Now notice all of the different resources that are available here, API Gateway, Cognito user pool, etc. Please be familiar with the different resources that WAF can protect. You do need to know those for the exam. With my Application Load Balancer selected, I'll click on Next, and now we get to our rules. So this is where we specify our conditions, our matches, etc. that we want to action on. And for this example, we are going to use our IP set. So I'm going to add a rule. And just to show you, there are a ton of managed rule groups here. Now you don't need to know this for the exam in depth, but understand that AWS does actually offer these. So there's some that you can just choose, add it to your web ACL, etc. and let them manage everything for you. You'll notice some cost money, others are free. Now feel free to poke around in these. They are pretty expansive. And another thing to keep in mind is they do partner with some of the leaders in the cybersecurity world like F5, Fortinet, etc. to work with partnerships for managed security groups. Again, these are not free. As you can see, you have to subscribe in the Marketplace. Now these aren't going to work in the sandbox, so I'm not going to do these, but I'm going to go ahead. I'm going to go back here. Now I have to go ahead and reset this, so I'm going to choose Regional, us‑east‑1. I'll paste in my name again in my description, readd my Application Load Balancer here. Click on Add, go to Next. And this time under Add rules instead, I'm going to choose my own. So I'm going to select Add my own. And now we can either build a rule from scratch using their editors. We can select rule groups that we've had or create a rule group, or what we're going to do for this simple demo is choose IP set because we have that IP set we want to block. So what I can do here is I'll give my rule a name. I'm going to call it blocking_andru because I am blocking myself, and then we get down to the IP set selection. Under here, I can choose the IP set that I created earlier containing my address. Again, this is typically going to contain malicious addresses or blocks of addresses that you either need to block or you need to allow specifically. In other words, you can use IP sets to blacklist or whitelist your different applications. Just keep that in mind. For this, we choose it. I'm going to say Source IP address, but if there was some type of forward proxy or anything else in between like another load balancer, you could look for IP addresses in headers, more specifically the X‑Forwarded‑For header. This is common with clients that use proxy IPs. Now I don't have to worry about that with how mine works, so I'll select Source IP address. The next thing we have is the action. Be familiar with the actions you see here. We covered four of the five, and really these top four are going to be the most important ones to know for this exam. We can allow only these IPs, so we can essentially whitelist or allow list this IP set. We can count, we can force a captcha, a challenge, or what we're going to do is we're just going to block it. So anything that matches this IP set in the source IP header is going to get blocked. In addition to this, you can actually customize responses. So let's go ahead and do this. I'm going to enable my custom response. I'm going to give it a custom response code, and I will tell you there are limits. This is as high as you can go for a response code. So if you try anything more, understand it's going to error out. I'm just trying to save you a headache. So I'll say 599 for the response, and let's add a custom header. I'm going to copy and paste these values in. So for the key, I'm going to say X‑Reason‑For‑Deny. And then for the value, I'm just going to give a very honest reason. I do not like you. So now anytime there's a block matching this IP set, this should get returned. The next thing we can do is customize the response body. I'm not going to do that. That's out of scope. I'm going to click on Add rule. Perfect. So now we have our rule up here. And if we had more, you could add more, and then you could edit them and prioritize them, but we only need the one for this particular demonstration. You see below the amount of capacity units that are going to be consumed. This is out of scope for this, but it's just 1 out of 5,000. And then we get down to default actions. So remember, if you have your actions and your statements, you also have a default. So by default, I'm going to say, hey, allow everyone else that doesn't match this rule to go ahead and use my Application Load Balancer. If it were on the other side of things and we wanted to only allow these IPs instead, you could block all of the others and say for the rule that you only want to allow via an action instead of block. Just understand you can do either/or for the scenarios on the exam. For this, I'll choose Allow. We could do the same thing where we customize the request. I'm not going to do that, and I'm going to click on Next. We have our priority here. We only have one, so no big deal. I'll click on Next. We can set up the CloudWatch metrics, which I'll leave as default, and then I'm going to click on Next. We review, we see all of our settings, and I create my web ACL. Now I'll call this out. If you are doing this in your own environment in the hands‑on playground, you might run into the same issue I'm getting right now. This is just spinning endlessly. So what I'm going to do is I'm going to go up here, open WAF in a new tab, select it, go to my Web ACLs, and you're going to see it is there. The only problem is underneath this, if I look at Associated resources, there's no associated resource. Unfortunately, there is a bug sometimes, and it really, really is annoying within WAF that will not find your resource. So what I'll do here is I'm actually going to close this, open up this other one. And a quick way to bypass this is if I go to my load balancer and I go down to Integrations here under the config, I can actually configure this to work with WAF. So I'm going to associate this, find existing, and then select my web ACL here. I'll click on Confirm. And what I'll do is I'm going to pause here, let this run. As you can see, it can take several minutes. And once it's done, I will resume. All right, so I went ahead and cut forward. First of all, I want to warn you, 2 to 3 minutes was a super lie. That took almost 5 or 6 minutes. So please keep that in mind. I know AWS is perfect and never has any issues, but just remember that. Now with this integration in place, we'll see it down here integrated. And if I load up my Shield or my WAF, we're going to see our WebServerTg Application Load Balancer, and it is associated, so this is perfect. So now if I go to my site, let me go ahead and refresh, well, it's not working. And in fact, notice the 599 error code in the middle of the screen. So now, if I go ahead and I open up my tab here, I'm going to go ahead and refresh. We're going to select the actual response. I'll close this. I'll zoom in a little bit more, and you can see our custom response header, X‑Reason‑For‑Deny. I do not like you. Perfect. So this is working. Now everyone else except for my IP address will be able to hit our Application Load Balancer, all based off of our rule and IP set that we customize over here, and that's going to do it. Now, feel free to play around with AWS WAF. It is very powerful, and there are a ton of different settings and configuration options that we just can't realistically cover in a demonstration. But for now, that's going to do it. Let's go ahead and wrap up this demonstration, and we're going to move on to the next clip.

AWS Network Firewall
All righty, we've talked about Web Application Firewalls. Let's move in a little bit more internally and look at Network Firewall. AWS Network Firewall is a stateful, managed network firewall and intrusion detection and prevention service for your VPCs. Within it, it includes a firewall rules engine that's going to give you complete control over your network traffic. For example, you might have a scenario where you need to block outbound Server Message Block, or SMB, requests to stop the spread of malicious activity. This is doable using the service. When you're using Network Firewall, you have to understand the resources that it protects, as well as where it's actually implemented in your architecture. You can use Network Firewall for Layer 3, 4, and 7 inspection and protection, and it is important to understand it works for any direction of traffic. What this means is it could be egress, so from internal to external to the internet, it could be ingress, so from the internet to your VPC, or even traffic coming from or to a direct connect, and it also works for VPC‑to‑VPC connections. So east to west is what's commonly referred to in that scenario. The big thing to remember is it's any direction of traffic. Moving on to important concepts and rules. Using this service, it runs both stateless and stateful traffic inspection rule engines, so it is a very powerful service. You can actually use it to inspect and filter your traffic based on IP, based on port, based on protocol, domain names, and even regex patterns. Within it, you can use filtering, and these filtering rules allow you to allow, drop or alert for your matching traffic, so you have some flexibility there. In addition to the filtering and the rules and inspections, you can also very easily integrate with things like Amazon CloudWatch, Amazon S3, and Amazon Firehose for easy logging. So if you need a logging solution with the service, remember those three options. And then lastly, when you're using the service, what happens is you deploy the firewall endpoints within their own subnets, and you route your traffic through them both ways. And speaking of that, let's actually look at an architecture on what this would look like. In the middle here, we have a very, very simplified VPC setup with an internet gateway, a public subnet, and a private subnet. When you're using Network Firewall, remember that you deploy the endpoints into their own subnets, so there should be nothing else living within these subnets in order to follow best practice. You set up your VPC route tables to direct all traffic to and from your endpoints. Within those endpoints, they're going to leverage the Network Firewall rules that you've put into place, and they're going to evaluate that traffic and then perform any actions that you've set. So in this example, if traffic starts here at the internet gateway and it's trying to hit our EC2, well it goes through our firewall endpoint. The endpoint looks up the firewall rules and evaluates the traffic, and then it references its route table that we've configured to go ahead and send the traffic into our private subnet. Now you might be saying, okay, well, what happens if there's a response? Well, it works the same way. The only difference is you set your route tables to route traffic outwardly through that endpoint. So now our traffic from our EC2 knows to send all outbound traffic to our firewall endpoint subnet instead of our internet gateway. So once that traffic is responding, it goes to our endpoint, it goes ahead and performs inspection again, and if it needs to, it allows that traffic out. Again, the big thing to remember here is your endpoints belong in their own subnets, and you route traffic to and from those subnets. Now to wrap things up, quick exam pro tip. This service is perfect if you have any scenario where you need to replace an existing traffic inspection and traffic filtering solution with some type of managed solution in AWS. That is the exact use case this is there to fill. With that being said, let's go ahead and wrap up this clip, and we're going to move on to another service that we can use to easily manage all of our network security components.

AWS Firewall Manager
Okay, we just got done looking at protecting our networks and our VPCs using Network Firewall. And before that, we looked at Web Application Firewall. Well, if you want to centrally manage all of these together, you can use this service, Firewall Manager. So let's go ahead and take a look at what this offers. AWS Firewall Manager is a security management service that essentially allows you to manage all of these components in a single pane of glass or a single view. You use it to centrally manage multiple WAF web ACL rules across multiple accounts in an org, and in addition to that, you can leverage it to manage Network Firewall rules. You use what are called security policies, which are where you set a common set of rules that get reused throughout an organization and across regions. Now speaking of security policies, let's look at the resources they support and some important concepts you should know. First up, AWS WAF rules. This is probably the most common scenario on the exam. You can easily create and manage AWS WAF rules for multiple accounts in multiple regions with this service. It also allows you to implement Shield Advanced throughout an org. Thirdly, our VPC security groups. So if you have a standardized set of security groups and security group rules that you need your organizational accounts to use, you can create these within the service and update them within the service. Fourthly is Network Firewall. So, if you have your Network Firewall rules similar to your WAF rules, you can create those and manage those here as well. An example for this would be if you have maybe an egress inspection VPC that you want all organizational traffic to go through, you could set that up via this service. The fifth resource that it protects are Resolver DNS firewalls. So if you enable this service, go ahead and use this within an organization to simplify it. If you're not familiar with this feature, please look at our other course where we dive into this a little bit more. And then lastly, it is important to know these are regional resources, which makes sense because WAF rules are regional, network firewall rules are regional. This just allows you to manage them in a simplified centralized place. Now two exam pro tips to wrap this up. Existing rules automatically will be applied to new resources that are being used and managed. So for instance, if you have a common set of WAF rules that are already created and in place and you have a new account that gets added to your org and you're enabling AWS WAF in it, well it's going to go ahead and say, hey, this is managed. You have to have these rules in place. I will apply them for you. So in other words, it allows you to very easily maintain compliance and regulation requirements. The big thing to keep an eye out on the exam for using this is anytime they mention the need for multi‑account organizational WAF rules or Network Firewall rules, this is a perfect scenario to use this particular service, so do remember that. With that being said, we're going to end here, and we're actually going to wrap this entire module up with a summary and exam tips clip coming up next.

Module Summary and Exam Tips
Okay, way to hang in there. We've reached the end of this module on network security. Let's have a quick summary and then go over some exam tips I think are important for you to remember going into the exam. First up, let's review Shield Standard and Shield Advanced. Remember, Shield itself is a service that's used for anything related to mitigating DDoS attacks in AWS. The Standard offering is free for all accounts and users, but it does offer less protection compared to the Advanced offering. With Advanced, you need to be familiar with some of the key differences. Honestly, the first and biggest one is that it is expensive. It is $3,000 per month per organization. However, with that big cost price point, you do get all Standard resources plus more. It protects in addition to the Standard resources, Elastic Load Balancers, EC2 instances, and anything with Elastic IP addresses. You also get access to a 24/7 Shield Response Team to help mitigate and protect. And fourthly, another huge thing to remember is you get cost insurance for any successful DDoS attacks. So they will cover or insure any cost that's occurred. Moving on to Web Application Firewall. It's important you remember this is a Layer 7 security feature, so it works at the application layer. What that means is it's specifically for HTTP requests to any of your back‑end resources and apps. With that, it supports Layer 7 resources, so Application Load Balancers, API Gateway, CloudFront distributions, and commonly Cognito user pools. Now we have AppSync in there as well. It's not as common on the exam, but you should probably be familiar with it. Thirdly, remember the rule configs listed here. You need to understand you can filter and match based on IP sets, geographic regions. You can look for and protect based on regex statements and SQL injections, string matches, and cross‑site scripting attacks. Those are some of the more common attacks for matching. However, you also might see scenarios where you need rate‑based rules. So if you need to limit any requests based on region or source, you might look at implementing a rate‑based rule as well. And fourthly, WAF is commonly used in conjunction with Shield Advanced and AWS Firewall Manager for organizational network security protection. It allows you to very easily simplify meeting compliance restrictions throughout an org. Moving on to Network Firewall, you need to understand and remember this is a stateful managed network firewall in IDS and IPS. It works at the perimeter of your networks, so perimeters could be where VPCs talk to VPCs. It could be an internet gateway, or it can be a virtual gateway for VPNs. The big thing to remember is that it's at the perimeter of your network. With this service, you can inspect and filter traffic based on IPs and ports, protocols, domain names, and even regex patterns. So it's a very powerful service. Again, also remember, since it's the perimeter of your network, that means it can work for any type of flow. It can work for egress, ingress, VPC‑to‑VPC, etc. And then lastly, how it works is you have your endpoints in your subnets, and you route traffic to those subnets containing those firewall endpoints. And in there, the traffic gets evaluated using your configured rules. Once it's evaluated, it can block, it can allow, it can count, etc. And then lastly, remember, if you need to centrally manage WAF rules, Network Firewall rules or even security groups for an org, AWS Firewall Manager is the choice you should be considering. With that being said, let's end this module here. Thanks for hanging in there. We'll take a quick break, grab some water, take a rest, and then I'll see you in the next and final module in this course.

Cost and Budgeting Services
AWS Cost Explorer
All right, let's start our final, but one of the most important modules in this course relating to cost and budgeting services. When you're using the cloud, cost optimization is one of the most important pillars in the Well‑Architected Framework. So let's talk about how we can meet some of the best practices using services in AWS. First up, let's go ahead and begin talking about AWS Cost Explorer. This is a very important service for you to understand going into this exam. Cost Explorer is a service that's offered to you that's meant to be an easy‑to‑use tool, which is going to allow you to visualize, analyze, and even manage some of your cloud costs. Within the service, you're going to be able to generate a lot of different custom reports based on a variety of different factors. Some of those different factors and customizations include things like generating reports based on custom resource tags. You can look for specific resource types themselves, so EC2 instances, RDS, network endpoints, etc, or you can even look at service categories, so broader categories for specific resources that fall under a specific service. And then lastly here, you can also get very granular. So you can look at your entire account, and you can start looking at hourly, daily, and even monthly times for your different reports to really analyze at a deeper level how you're actually spending your money within your accounts. One of the big things to remember for the exam with this service is that it offers built‑in forecasting for up to 12 months. How this works is it uses your historical data that it's collected over a certain amount of time, and then it uses some machine learning on the back end to go ahead and generate forecasted amounts to say, hey, we think you're going to spend roughly this amount of money based on the current and previous spend. In addition to the forecasting that it offers, it also can recommend you the best suggested savings plans for your accounts. So even though AWS would love for you to spend your money with them, they do want you to have an optimized architecture, meaning they want you to use the best cost‑savings approach that you could get for your workload. So they will recommend the best savings plans based on your data. Now moving on, let's look at a snippet of a Cost Explorer example from a previous account that I used to work in. Here on the left, we have the actual dashboard graph that you see when you load it up. Now we'll look at this in another demo later on within this module where we start looking at some other services as well, but for now this is the general high‑level view that you get when you load into the service. You'll notice here on the top of this graph, we see the cost and usage. So this is saying, hey, here's your total cost, here's your average monthly cost, and this is the amount of services that you have running. Below that we have the actual cost amount. So you can see based on the way we've configured this graph, it's based on the different services. So you can see blue, or the bottom is Route 53. We have KMS, Config, Glue, etc. It breaks it down in a nice, easy‑to‑read fashion. And then on the bottom of the graph, we have the breakdown based on that graph and the numbers up top. You can see it's giving us the exact total per service, again based on how we set this particular report up. And speaking of setting this report up, let's look at how we did that. On the right here, we have our report parameters. So let's break down some of the important ones you need to know about. At the top here, we have the date range and the granularity. So in this case, we said 2023‑03‑01 ‑ 2023‑08‑31. This is the North American way of representing dates. So if you are in Europe or somewhere else overseas, I apologize if it looks a little confusing, but that's the date frame that we set here. We then have the granularity. So we said, hey, go ahead and group this into monthly reporting. So that's why we have March, April, May, June, etc. And then right below that, in addition to the granularity, we have the dimension here. So we grouped it by service. So that's why we're getting the cost and usage breakdown based on the services themselves. Moving on to the bottom, we get down to the more specific filters. So here, some of the important ones you need to be aware of is you can look for specific services within the service groups. You can break it down per region so you can see which region is costing you what. And if you're in an AWS organization, you can use this within the payer account to look at linked accounts, so in other words, your member accounts within your org. This is a great way to view which accounts are costing you what within your consolidated billing invoices that you receive. Now after those three, we then have other filters. Now you probably don't need to know these for the exam necessarily. The other ones are at the top that are more important. But it is good to be familiar with this. So if you have access to the service, please play around with it. It's pretty neat what you can get it to do and show you, and it's very important. One filter I did want to call out here on the bottom, which I just highlighted, is tags. Again, tags are going to be the most important way to track your spend within an organization. Please make sure you are tagging resources within your accounts. Now speaking of tags, let's talk about cost allocation tags. These are very critical for you to know on the exam and honestly whenever you're deploying to the cloud. Within AWS, there are two types of cost allocation tags. There's AWS‑generated, which are the first ones, and these are going to be applied for you by AWS, and they're always going to follow the same prefix syntax. So an example is aws:createdBy. This will say who created the service, the resource, etc. The big thing to understand here is that this is a category of cost allocation tags, and it will follow this prefix. The other type of cost allocation tag is a user‑defined tag. This is exactly what it sounds like. You define, you create, and you apply your own tags, and they have a user: prefix. So a real‑world example could be user:Department because maybe you want to break down costs based on department. The big thing with both of these is that they have to be activated separately. So you have to say yes, I want AWS generated, and yes, I want user‑defined cost allocation tags. Remember that for any real‑world application. And with that being said, let's go ahead and wrap Cost Explorer up, and we're going to move on to another service that works very closely with it called AWS Budgets, coming up next.

AWS Budgets
All righty, we looked at exploring costs using AWS Cost Explorer previously, and let's build off of that now, and let's learn about using AWS Budgets to actually implement and control budgets for your accounts. AWS Budgets is a service in the cloud that allows organizations to easily plan and set expectations around their cloud costs within their accounts. Now I will say it works also for single accounts that are not part of an org, but as far as the exam goes, it's commonly going to be used with AWS Organizations. Within the service, you can use it to easily track any ongoing spend, and you can use it to create alerts to let specific users know when they are getting close to exceeding any of that allotted spend. So in other words, if they're getting close to exceeding their budget, you can alert them, and you can alert other teams to go ahead and let them know, hey, you're getting close here, let's go and cool it down. Now with Budgets, it is very commonly used with Cost Explorer, so you use these two in conjunction to create more fine‑grained budgets. Remember, Cost Explorer allows you to get very granular with your details and your reports, and again, you use it in conjunction with this service to create a better budget. And then lastly here, one reminder, you're going to see this a lot through this module because it is important, make sure you tag all of your resources properly. This, again, is going to make it a lot easier to create a better budget and to have better reports regarding your costing and spending. Moving on, let's talk about budget actions. These are an important aspect for the exam for you to be aware of for this service. A budget action is a feature where you can run actions whenever a budget exceeds a certain threshold, which can be a certain dollar amount or a percentage of a dollar amount. Now these can run either automatically by themselves, or you can run them after requiring manual approval. So maybe you want human intervention before they actually get triggered. Now let's actually look at some examples of where you might use a budget action specifically on the exam. First example here, you can actually use them to apply deny IAM policies in order to restrict actions. So, for example, maybe your users or roles are spinning up EC2 instances too often and the budget is about to be exceeded. Well, using an action, you can apply a deny IAM policy to those IAM entities and deny them from creating any more EC2 instances. The second example is similar. You can also apply Service Control Policies to an org, including an entire org, an OU or a specific member account to prevent actions at a broader base. So if we use that same EC2 example, maybe you want to deny the entire organizational unit the ability. Well, you can apply an SCP that denies EC2 actions to the OU using a budget action. And then the third example is you can use actions to send emails or publish messages to an Amazon SNS topic. So this is important to know. You can have a static list of emails that get sent, or you can use an SNS topic to alert. It's really going to come down to the use case. For instance, maybe you need an SMS notification, etc. In that case, you would use SNS. Now moving on, let's look at an exam scenario that is possible with AWS Budgets. Let's assume your company uses AWS Organizations to manage multiple accounts. They're wanting you to set up different budgets for the multiple accounts, so they want to track it using different budgets. Now one of the requirements is you need to receive alerts and automatically prevent creation of any new resources in those accounts whenever the allocated budget threshold is met during the specified time period. Well, let's explore a way that's possible to solve this. First, you can use Budgets to create a budget for each account. So you can set the budget amount under the billing dashboards of the required accounts using this service. Secondly, you can create an IAM role for your budget's service to run your budget actions with the required permissions. So, Budgets can assume the cross‑account role, it can run the budget action, and it can use those actions to shut stuff down. So with this, you have to allow the service to assume the role. Keep that in mind. The third thing here is you can add an alert via SNS or email notification to notify you and your company when each account meets its budget threshold. So again, it allows for alerting. And then fourthly here, you can use your budget actions that assume that IAM role that you created to go ahead and apply a service control policy to prevent creation of any new resources within that specific account. Again, this is a possible exam scenario and just a way to solve it using Budgets and budget actions. With that being said, let's go and wrap up, and we're actually going to look at a demonstration in the console of using and creating a budget.

Demo: Creating an AWS Budget
All right, let's go ahead and jump into a demonstration. In this demo, we're going to jump into the console, and we're going to work on creating our very own AWS budget. Before we jump in though, let's have a very high‑level look at what we can expect to create within AWS. Within our sandbox account here, we're going to set up an AWS budget to look for a static threshold breach of a certain dollar amount. Once that breach occurs, which we're not going to be able to really demonstrate, but I'll show you what it looks like, we're going to configure our budget to do a few things. We're going to configure it to send several alerts. So one of those alerts will be to an Amazon SNS topic. Now we'll have a subscriber to that topic using a sample email address. However, the idea of it is to show how you can trigger a Pub/Sub event‑driven architecture. In addition to using SNS, we're also going to set up a static email recipient. And then lastly, we're going to show how you can give your budget actions, an IAM role that it can assume to perform actions within your accounts. So with that being said, let's go ahead and jump over into the console now. Okay, I've jumped into my sandbox here. Now, one thing I do want to call out is this is a separate sandbox hands‑on playground, so you will not have access to the billing console if you use our built‑in playground. I just want to call that out. This is a special privileged version. With that being said, let's go ahead and review really quickly two things that I've already got set up for us. Firstly, I have an EC2 instance, and I'm going to demonstrate how we can use this budget action to shut this down. I've also used my temp mail. Now if you've watched other demonstrations that I've created, well, I like to use this service. It just allows me to have a temporary email address for demonstrations. So with that being said, I'm going to go back to Billing and Cost Management. So this is the overall service. And on the left here, we have all of the different built‑in services and features. So to start things off, first thing I'm going to do, I'm going to find Cost Explorer here under Cost and Usage Analysis. Remember, Cost Explorer is one of the easiest ways to generate reports and views of graphs of the different costs and usage within your accounts. This is typically used in conjunction with AWS Budgets to get a more accurate budget set up. So what I'm going to do here is zoom in just a little bit, just for readability's sake, and let's walk through some of the parameters. In the middle here, we have our graph with our monthly cost, and right now we're grouping it by dimension of service, which you can see right here, which I highlighted. So just like in the theory clip, you can see the graph with the color schemes, and then on the bottom you see the breakdown. Since this is a new account, I don't really have a lot going on in it, so we don't have a lot of cost, and a lot of stuff falls under Free Tier. So that's why there's not a lot of charges, but the overall idea is the same. Remember, with the date range, you could set a specific date range. You could auto select based on past data, so let's say the last three months and then I can apply, or you can even do forecasting. So let's say, hey, I want you to look plus three months in the future using your forecasting algorithms. So when I apply this, it says, hey, I can do this, but you have to disable the Group By selection, which is grouping by service. So I'm going to say, okay, go ahead and do that, and there we go. So what this is doing is using our historical data. So in this instance, December, and January, and February. And it's saying, okay, this is what I think you're going to spend, or I'm predicting what you're going to spend based on that historical data. So this is what that forecasting looks like. Now in addition to that, another thing to realize here is you can, again, filter based off linked accounts. So if you have an organization with member accounts, you can filter those here. So you'll notice I have several accounts within this organization. And what I could do is select them or deselect as I want and view the cost for that specific account within the main account. Cool. So with that out of the way, go ahead, feel free to play with this if you have access to Cost Explorer in your own accounts. But I'm going to go ahead, and I'm going to find Budgets over here on the left, and we're going to begin. So let me select Budgets. And let's go ahead and create one. So I'm going to find Create a budget. We're going to use Customize because I want to walk through this step by step so we really understand what's going on. First thing we do here is we select the budget type. So there are four types you can use, Cost budget, which is what we'll use, you can choose a Usage budget, which monitors the usage of specific usage types or groups, you can choose Savings Plans budget to monitor your savings plans consumption or Reservation budget, so this is like savings plans, but it's more specific to reservation alerts for supported resources. So with that said, let's just go ahead. We're going to do Cost budget. I'm going to click on Next, and the first thing we do is give it a name. So let me go ahead and name my budget. And for this, I'm going to call it Sample‑Budget‑Monthly. I'm going to set this to be a monthly budget. The next thing we see here is the budget amount. So the first thing we can set is the period. Do you want this to be a daily budget, a monthly budget, a quarterly budget or an annual budget? Obviously, depending on what you select here will change your options at the bottom. But for this, what we're going to do is select Monthly. After we select the period, we choose the renewal type. So do you want this to expire at some point in the future, or do you want it to constantly recur until you cancel it? I'm going to choose Recurring because I want to constantly monitor this account's budget. We'll go ahead and choose the start month to be this month, but you can do it in the future if you want. And then we get down to the budgeting method. So this is where you set the actual budget essentially. Do you want it to be a fixed amount, so a fixed dollar? Do you want to plan each month in the future because maybe you're accounting for some growth in the future as more customers onboard to your application, or do you want it to use auto adjusting? So this will use their own built‑in algorithms, machine learning to go ahead and build your own budget based on patterns. So what we'll do is I'm going to choose Fixed, and I'm going to set this dollar amount to $10 US. In other words, what this is saying in review, every month on a recurring budget basis, I want you to look for a $10 fixed budgeted amount. Moving down, we have our Budget scope. So what do you want to include in this budget? Now we're going to choose All AWS services because I want to monitor the entire account no matter what is being used. However, you could filter on specific cost dimensions as you see. So within this, you can get very complex. You can choose a service, a linked account, a specific region, etc. So maybe we just want to look at service, and I want to say, hey, EC2‑Other, I only want this budget to apply to that. You can do that using a filter. Again, this is very closely resembling Cost Explorer because it uses Cost Explorer data on the back end essentially. For me, I'm going to remove these. I'm going to choose All services. And we're going to skip over this because it's kind of out of scope for this particular exam, but this is going to look for all unblended costs in every category. So we're going to allow the default. I'm going to skip down and click on Next. Perfect. So now we get to configure our alerts. We see our budget amount, and now we can set an alert threshold. Remember, budgets get triggered or budget alerts get triggered when they cross your threshold that you set. So let's set one. So, you can see we can choose percentage or an absolute value. Now I'm going to choose % because it's an easy value. It's 10, and I'm going to say 75%. So what we're saying here is when the actual cost is 75% of the budget, which is going to be $7.50, I want to go ahead and trigger this alarm. Now you could do actual costs, so you're actually incurring this, or it could be forecasted. So, an estimated forecasted amount will be greater than your budget. I'm going to choose Actual. And then below that we can set our notification preferences. So remember, you can do three primary things here. You can set static emails, you can configure an SNS topic, or you can actually set up a chatbot alert for Chime, Slack, and Microsoft Teams. So what we're going to do here is I'm going to enter an email first, so I'm going to go to my temp email. I'll copy, go back, paste this in, and there we go. So now anytime this alert is breached or this alert is triggered, it will send a static email to this email address. This is good if you have a static set of emails you want to always alert to. We can also do an SNS, so let's actually do that. So what I'm going to do is I'm going to create a new topic here. I'm going to click on this shortcut. Go to Topics, let's create a new one. I'm going to make it Standard. We're going to call it Budget‑Alerts. We're going to skip over everything else for now, and I'm going to click on Create topic. Now in this, I'm going to very quickly create a new subscription. I'm going to choose Email, and I'm going to enter my email address, Create, and then I'm going to confirm this on the back end. So I'm going to cut forward and confirm this, and then I will make sure that it is confirmed. All right, so I did that offscreen. I cut forward. Our email is now confirmed. If you need to know how to do that, please check out our decoupling and scaling course. We do cover how to go ahead and set up subscriptions. But for now, this is in place. So now on my Topics, we have our Budget‑Alerts here. We're going to go back. I'm going to copy my ARN. I'm going to go back to Budgets. I'm going to paste in my ARN here. And you're going to notice, hey, you don't have permissions, which makes sense because we don't allow Budgets to use that topic based on the access policy. So if I go back here, click on Access policy, we're not allowing the service itself to use this. So what we're going to do is I'm going to go ahead, go back. I'm going to click on this little permissions button, and it's going to give me this little statement I can copy. So I'm going to copy this, go back to my Access policy. I'm going to edit that access policy. So let me go ahead, go here. I'm going to go down, make a new block here with a new statement, paste it in, and there we go. Now the one thing I do have to do here is make the ARN. So what I'm going to do here is copy this. And we're going to go ahead and paste this in. So let me get rid of this. I'll paste it in, and there we go. So now what this is saying is, hey, we're allowing the budget service to publish to this topic. I'll click on Save. I'll go back, we'll refresh here, and we're good to go. So now we've granted the service the ability to publish a message to our topic. Awesome! So now we have this in place. I'll click on Next, and now we can start adding our budget actions. So we have our alert, and we want to create an action based on that alert. So I'm going to click on Add action. And the first thing you'll see here is, hey, you need IAM permissions to allow Budgets to assume a role to do whatever you need. So we don't have a role in this account for this, so I'm going to create one. I'll open this up in a new tab. And let's work on creating a role for this particular service. I'm going to create a new role. We're going to select Budgets as the service and use case. I'll click on Next, and then within a managed permission policy, they actually have something we're going to use. So I'm going to search for budget. I'm going to choose this top one here, RolePolicyForResourceAdministration. So if I look at this, it's allowing us to stop, start RDS and EC2 instances as long as it's done via an automation document. So feel free to review this on your own. I'm going to select this managed policy. I'm going to go ahead and click on Next. I'll give it a name. I'll scroll down here. And under Trust policy you'll see, hey, we're allowing Budgets to assume this role, and I'm going to create it. Perfect. So now we have our role here. I'll go back to my budget action, refresh, and we're going to select our Monthly‑Budget‑Role. The next thing we configure is the action type. So what action do you want? Well, remember, there are three primary things you can do here. You can apply Service Control Policies to an OU or an org. You can apply IAM policies with denials in them. Or what we're going to do is, hey, we're going to automate instances to stop for EC2. So I'm going to choose Stop EC2. I select my region that I want to look for, so us‑east‑1. And then I choose the instance. So this is why I had that demo instance running. We can select it. And we can say, yes, I want you to automatically run this when the threshold is exceeded. And we're going to use the same alerts, but we could customize them if we want, but we won't. Now I click on Next, and then we get a review. So we have our monthly budget with a $10 amount. It's monthly recurring, and then any time it reaches 75% of our budgeted amount of actual costs, we're going to action stop EC2 instances. So now I create my budget, and there we go. We now have our budget in place. We can review this, and, of course, this is not going to trigger because I don't have enough resources running to trigger it, but that's the process to configure it. Within this you can see the different settings, any alerts, budget histories, etc. You can view in Cost Explorer. Feel free to play around with this if you have access in your own account. But for now, we're going to go ahead and wrap this demo up. That's how you create a budget, set up some alerts with some budget actions. Big thing to take away here is, remember, you have to give budget actions permissions via an IAM role and policy for it to perform your actions. Let's wrap up here, and I'll see you in the next clip.

AWS Cost and Usage Reports (CUR)
All righty, let's move on to the next service you need to be familiar with, at least at a high level called Cost and Usage Reports. First thing I want to call out here, AWS Cost and Usage Report, oftenly abbreviated as CUR, is beginning to transition to AWS Data Exports. With that in mind, it will soon be referenced as Cost and Usage report 2.0, or CUR 2.0. So if you see that on the exam, just keep that in mind. However, the general idea of using this service will be the same on the exam. With that in mind, let's have an overview of what this service does. This is considered one of the most comprehensive sets of data for cost and usage for your AWS spend. The main purpose of this service is to easily publish recurring billing reports to S3 for centralized collection. Within it, the service is going to update the reports in S3 once a day using CSV format. So it is a popular format, and it's a standardized format. With CUR, you can leverage other analytics services, so you can leverage Athena, Redshift, and QuickSight to query, analyze, and even visualize your data. These are very popular services that integrate with CUR that you need to be familiar with. And then lastly here, it's commonly used within organizations at an organization level, but you can use it for specific OUs or even down to specific member accounts. It really comes down to the use case and the scenario. With that in mind, let's look at a quick exam scenario, and then we'll break down how we can solve it using the service. Let's say you have an enterprise and that enterprise wants to monitor AWS costs, and they want to review them. The operations team is designing an architecture within Organizations within the management account. And they plan to query cost and usage reports for all member accounts once a month, and they want to get a detailed analysis of the bill. Well, let's go ahead and explore a way to solve this. In this example scenario solution, here we have AWS Organizations, and then within it we have several accounts. At the top, we have our management account, and then at the bottom we have our three different member accounts. Now the management account in the org can actually set up aggregated cost and usage reports for all member accounts that belong to it. What that does is it allows you to centralize your reporting mechanism, and then you can work on delivering the reports to an Amazon S3 bucket. Using S3 is a cost‑effective and secure way to store these reports. After you store them, you can then use Amazon Athena to analyze that data directly in S3 using standard SQL commands. One of the biggest benefits here is that all of this leverages managed services, meaning you don't have to manage any infrastructure at all. CUR, S3, and Athena are all managed services. And in addition to that, they're serverless, so Athena allows you to perform ad hoc queries without actually manipulating the data, and you don't have to manage any of the underlying infrastructure performing the queries. Now let's go ahead. We're going to wrap this clip up on CUR, and we're going to move on to another cost analysis tool called Cost Anomaly Detection.

AWS Cost Anomaly Detection
All righty, we just got done reviewing Cost and Usage Reports. Let's now take a look at a new feature or newer feature called Cost Anomaly Detection. What this is is a billing and cost management feature that leverages machine learning models and algorithms to detect and alert on unusual spend patterns within your accounts. Currently, at the time of recording this, it works with many different AWS services. You can specify linked member accounts within an organization. You can look for specific cost categories and even specific cost allocation tags. Again, remember, tagging your resources is one of the most important things you can do in AWS. Now, when you're scanning all these and trying to detect any issues, you can send alerts to several different resources. The most common ones are Amazon SNS, so you can send a message to a topic with a Pub/Sub approach. You can send messages to your Slack channels, so maybe you have live alerting via Slack. And you can use Amazon Chime. Amazon Chime is, again, just a messaging service. You don't really need to know it too in depth for this exam. Just understand that it works with this particular feature. Now moving on, let's look at some important concepts you need to be aware of. Cost Anomaly Detection ranks your anomalies based on the dollar impact that they have on your accounts. So the most important ones will have the highest dollar impact. You use this service to help you perform root cause analysis of the anomaly that was detected. So it can show you, hey, this is what we detected. Here's a general idea of why it was detected. Go ahead and dig in and do your own research. Now you might be wondering, well, that's good, but are there false positives? Yes, of course there will be false alerts. However, the general idea of this service and feature is that it learns historical patterns to help lower false alerts. So those false detections, false positives will eventually be lowered based on learning using your data. And then lastly here, there are four different ways to split the data and the detections. You can split the reports by AWS service. You can look by AWS account, region or even a specific usage type. So those are four ways to break your detections into your reports. And then lastly here, when you get alerts, you can choose to aggregate them, so you can get all of your alerts in one message, or you can send individual detections of anomalies based on each one that gets caught. With that being said, let's go and wrap up this clip on Cost Anomaly Detection. Remember, it's used to catch anomalies in your spend, and we're going to move on to another service called AWS License Manager.

AWS License Manager
All right, when you're running applications and workloads in AWS, there's going to be a time where you have to use custom third‑party licenses for specific software. So, let's talk about how you can manage those best within the cloud. AWS License Manager is a service that's meant to help simplify managing software licenses with different vendors. So some examples are Microsoft, SAP, and even Oracle. It's meant to help centrally manage your licenses across your accounts and even on‑premise environments, so it works in a hybrid setup. What happens is you set your licenses up within the service, and then it offers control and visibility into the usage of your licenses, and it also allows you to enable license usage limits. That's an important thing to keep in mind. By leveraging these features, it helps you reduce overages and penalties via inventory tracking and rule‑based controls for consumption. So again, a usage limit. It's also important to know it supports pretty much any of the most popular ways of assigning licenses. So it supports licenses for software that are based on virtual CPU count, physical core count, the number of sockets, and even just the general number of machines that are consuming your licenses. So again, there's a lot of flexibility with using this. To be honest, the biggest thing I could tell you for the exam, just keep an eye out for anything related to AWS‑hosted license management, hybrid environment license management or a way to prevent license overuse. Those are going to be the three primary indicators for you to consider AWS License Manager.

Module Summary and Exam Tips
All righty, let's wrap up this final module in this lengthy course. We're going to go over some exam tips that I think are important for you to take with you into your exam. First up, let's review Cost Explorer and AWS Budgets. Remember, Cost Explorer is a service that's meant to allow you to visualize, analyze, and even manage your cloud costs. Within the service, it offers forecasting so you can understand your expected spend based on your historical data. So it looks at old spend, what you're doing right now, and it says, hey, I'm forecasting that you're probably going to spend this amount in the upcoming months. In addition to Cost Explorer and another service that usually works hand in hand with it, we have AWS Budgets. This is exactly what it sounds like. It allows you to track spend and create budgets and alerts for your accounts and your services within an organization. You can even perform actions based on threshold breaches that you set. Speaking of those actions, these are called budget actions. Remember, you can use these to actually run automated or manually approved actions to prevent future resource creation and API actions within accounts. You accomplish this by allowing Budgets to go ahead and assume an IAM role, and then IAM role allows the actions to either apply IAM policies that deny actions, or you can apply Service Control Policies to an organization to deny actions. The big thing to remember here is you can prevent resource creation using budget actions. And then a reminder here. Please remember for the exam and the real world, you should be using cost allocation tags to make services more efficient with your tracking of your spending. For the exam itself, remember there are AWS‑generated and user‑generated tags. They both have their own prefix that you need to know for the exam. Moving on to Cost and Usage Reports and Cost Anomaly Detection. These are the final two services that I think are the most important for you to understand and remember for this exam. First up, CUR. Remember, this is transitioning to AWS Data Exports. So if you start seeing that on the exam, understand they accomplish the same thing. Overall though, what you use CUR for is to publish recurring billing reports to an S3 bucket. Once that report is there and it's centrally managed, you can interact with that data using Athena, Redshift, and even QuickSight. Please, please remember these services for integration with Cost and Usage Reports. They are popular. They do appear on the exam quite often. In addition to the services, these are very useful at an organizational level. So it's very good to aggregate cost and usage reports across an organization for centralized management and centralized spending tracking. Moving on to Cost Anomaly Detection. For the most part, I think you'll do pretty well remembering this based on the name, but this is a service or feature that belongs within the Billing and Cost Management platform. It uses machine learning to detect and alert on unusual spend patterns, in other words, anomalies. Within the feature, it actually learns your historical patterns, and it uses those to help lower false positives. So yes, you will get some false alerts, but it's going to learn from past data and understand, hey, this is actually not an alert. I'm not going to notify the team or whoever it is that you have set up to receive notifications. Now with that being said, thank you so much for your time. This is a lengthy course, but a very important one. I know your time is very valuable, so I hope you've learned everything you need to know for the exam regarding the services that we covered. Please feel free to connect with me on LinkedIn and explore the other courses within this learning path. But for now, let's wrap it up. Thank you again, and I'll see you in the near future.
